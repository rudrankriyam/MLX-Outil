

├── .circleci
    └── config.yml
├── .gitignore
├── .pre-commit-config.yaml
├── .spi.yml
├── .swift-format
├── ACKNOWLEDGMENTS.md
├── Applications
    ├── LLMEval
    │   ├── Assets.xcassets
    │   │   ├── AccentColor.colorset
    │   │   │   └── Contents.json
    │   │   ├── AppIcon.appiconset
    │   │   │   └── Contents.json
    │   │   └── Contents.json
    │   ├── ContentView.swift
    │   ├── LLMEval.entitlements
    │   ├── LLMEvalApp.swift
    │   ├── Preview Content
    │   │   └── Preview Assets.xcassets
    │   │   │   └── Contents.json
    │   ├── README.md
    │   └── ViewModels
    │   │   └── DeviceStat.swift
    ├── LoRATrainingExample
    │   ├── Assets.xcassets
    │   │   ├── AccentColor.colorset
    │   │   │   └── Contents.json
    │   │   ├── AppIcon.appiconset
    │   │   │   └── Contents.json
    │   │   └── Contents.json
    │   ├── ContentView.swift
    │   ├── LoRATrainingExample.entitlements
    │   ├── LoRATrainingExampleApp.swift
    │   ├── Preview Content
    │   │   └── Preview Assets.xcassets
    │   │   │   └── Contents.json
    │   └── README.md
    ├── MLXChatExample
    │   ├── ChatView.swift
    │   ├── MLXChatExample.entitlements
    │   ├── MLXChatExampleApp.swift
    │   ├── Models
    │   │   ├── LMModel.swift
    │   │   └── Message.swift
    │   ├── README.md
    │   ├── Services
    │   │   └── MLXService.swift
    │   ├── Support
    │   │   ├── Assets.xcassets
    │   │   │   ├── AccentColor.colorset
    │   │   │   │   └── Contents.json
    │   │   │   ├── AppIcon.appiconset
    │   │   │   │   └── Contents.json
    │   │   │   └── Contents.json
    │   │   ├── HubApi+default.swift
    │   │   ├── Preview Content
    │   │   │   └── Preview Assets.xcassets
    │   │   │   │   └── Contents.json
    │   │   └── SampleData.swift
    │   ├── ViewModels
    │   │   └── ChatViewModel.swift
    │   └── Views
    │   │   ├── ConversationView.swift
    │   │   ├── MediaPreviewView.swift
    │   │   ├── MessageView.swift
    │   │   ├── PromptField.swift
    │   │   └── Toolbar
    │   │       ├── ChatToolbarView.swift
    │   │       ├── DownloadProgressView.swift
    │   │       ├── ErrorView.swift
    │   │       └── GenerationInfoView.swift
    ├── MNISTTrainer
    │   ├── Assets.xcassets
    │   │   ├── AccentColor.colorset
    │   │   │   └── Contents.json
    │   │   ├── AppIcon.appiconset
    │   │   │   └── Contents.json
    │   │   └── Contents.json
    │   ├── ContentView.swift
    │   ├── MNISTTrainer-Info.plist
    │   ├── MNISTTrainer.entitlements
    │   ├── MNISTTrainerApp.swift
    │   ├── PredictionView.swift
    │   ├── Preview Content
    │   │   └── Preview Assets.xcassets
    │   │   │   └── Contents.json
    │   └── README.md
    ├── StableDiffusionExample
    │   ├── Assets.xcassets
    │   │   ├── AccentColor.colorset
    │   │   │   └── Contents.json
    │   │   ├── AppIcon.appiconset
    │   │   │   └── Contents.json
    │   │   └── Contents.json
    │   ├── ContentView.swift
    │   ├── Preview Content
    │   │   └── Preview Assets.xcassets
    │   │   │   └── Contents.json
    │   ├── README.md
    │   ├── StableDiffusionExample.entitlements
    │   └── StableDiffusionExampleApp.swift
    └── VLMEval
    │   ├── Assets.xcassets
    │       ├── AccentColor.colorset
    │       │   └── Contents.json
    │       ├── AppIcon.appiconset
    │       │   └── Contents.json
    │       └── Contents.json
    │   ├── ContentView.swift
    │   ├── Preview Content
    │       └── Preview Assets.xcassets
    │       │   └── Contents.json
    │   ├── README.md
    │   ├── VLMEval.entitlements
    │   └── VLMEvalApp.swift
├── CODE_OF_CONDUCT.md
├── CONTRIBUTING.md
├── Configuration
    └── Build.xcconfig
├── Data
    └── lora
    │   ├── test.jsonl
    │   ├── train.jsonl
    │   ├── valid.jsonl
    │   └── wikisql.py
├── LICENSE
├── Libraries
    ├── Embedders
    │   ├── BaseConfiguration.swift
    │   ├── Bert.swift
    │   ├── Configuration.swift
    │   ├── EmbeddingModel.swift
    │   ├── Load.swift
    │   ├── Models.swift
    │   ├── NomicBert.swift
    │   ├── Pooling.swift
    │   ├── README.md
    │   └── Tokenizer.swift
    ├── MLXLLM
    │   ├── Documentation.docc
    │   │   ├── Documentation.md
    │   │   ├── adding-model.md
    │   │   ├── evaluation.md
    │   │   └── using-model.md
    │   ├── LLMModel.swift
    │   ├── LLMModelFactory.swift
    │   ├── Lora+Data.swift
    │   ├── LoraTrain.swift
    │   ├── Models
    │   │   ├── Cohere.swift
    │   │   ├── GLM4.swift
    │   │   ├── Gemma.swift
    │   │   ├── Gemma2.swift
    │   │   ├── Gemma3Text.swift
    │   │   ├── Granite.swift
    │   │   ├── Internlm2.swift
    │   │   ├── Llama.swift
    │   │   ├── MiMo.swift
    │   │   ├── OpenELM.swift
    │   │   ├── Phi.swift
    │   │   ├── Phi3.swift
    │   │   ├── PhiMoE.swift
    │   │   ├── Qwen2.swift
    │   │   ├── Qwen3.swift
    │   │   ├── Qwen3MoE.swift
    │   │   └── Starcoder2.swift
    │   ├── README.md
    │   ├── SuScaledRotaryEmbedding.swift
    │   └── SwitchLayers.swift
    ├── MLXLMCommon
    │   ├── Adapters
    │   │   ├── LoRA
    │   │   │   ├── DoRA+Layers.swift
    │   │   │   ├── LoRA+Layers.swift
    │   │   │   ├── LoRAContainer.swift
    │   │   │   └── LoRAModel.swift
    │   │   ├── ModelAdapter.swift
    │   │   ├── ModelAdapterFactory.swift
    │   │   └── ModelAdapterTypeRegistry.swift
    │   ├── AttentionUtils.swift
    │   ├── BaseConfiguration.swift
    │   ├── Chat.swift
    │   ├── Documentation.docc
    │   │   ├── Documentation.md
    │   │   └── porting.md
    │   ├── Evaluate.swift
    │   ├── Extensions
    │   │   └── Encodable+toolResult.swift
    │   ├── KVCache.swift
    │   ├── LanguageModel.swift
    │   ├── Load.swift
    │   ├── ModelConfiguration.swift
    │   ├── ModelContainer.swift
    │   ├── ModelFactory.swift
    │   ├── Models
    │   │   └── Gemma.swift
    │   ├── Module+Extensions.swift
    │   ├── README.md
    │   ├── Registries
    │   │   ├── AbstractModelRegistry.swift
    │   │   ├── ModelTypeRegistry.swift
    │   │   └── ProcessorTypeRegistry.swift
    │   ├── Streamlined.swift
    │   ├── StringOrNumber.swift
    │   ├── Tokenizer.swift
    │   ├── Tool
    │   │   ├── Tool.swift
    │   │   ├── ToolCall.swift
    │   │   ├── ToolCallProcessor.swift
    │   │   ├── ToolParameter.swift
    │   │   └── Value.swift
    │   └── UserInput.swift
    ├── MLXMNIST
    │   ├── Files.swift
    │   ├── MNIST.swift
    │   ├── README.md
    │   └── Random.swift
    ├── MLXVLM
    │   ├── MediaProcessing.swift
    │   ├── Models
    │   │   ├── Gemma3.swift
    │   │   ├── Idefics3.swift
    │   │   ├── Paligemma.swift
    │   │   ├── Qwen25VL.swift
    │   │   ├── Qwen2VL.swift
    │   │   ├── QwenVL.swift
    │   │   └── SmolVLM2.swift
    │   ├── README.md
    │   ├── VLMModel.swift
    │   └── VLMModelFactory.swift
    └── StableDiffusion
    │   ├── Clip.swift
    │   ├── Configuration.swift
    │   ├── Image.swift
    │   ├── Load.swift
    │   ├── README.md
    │   ├── Sampler.swift
    │   ├── StableDiffusion.swift
    │   ├── Tokenizer.swift
    │   ├── UNet.swift
    │   └── VAE.swift
├── Package.resolved
├── Package.swift
├── README.md
├── Tests
    ├── MLXLMTests
    │   ├── BaseConfigurationTests.swift
    │   ├── EvalTests.swift
    │   ├── README.md
    │   ├── StreamlinedTests.swift
    │   ├── ToolTests.swift
    │   └── UserInputTests.swift
    └── mlx-libraries-Package.xctestplan
├── Tools
    ├── ExampleLLM
    │   ├── README.md
    │   └── main.swift
    ├── LinearModelTraining
    │   ├── LinearModelTraining.swift
    │   └── README.md
    ├── Tutorial
    │   └── Tutorial.swift
    ├── image-tool
    │   ├── Arguments.swift
    │   └── ImageTool.swift
    ├── llm-tool
    │   ├── Arguments.swift
    │   ├── Chat.swift
    │   ├── LLMTool.swift
    │   ├── ListCommands.swift
    │   ├── LoraCommands.swift
    │   └── README.md
    └── mnist-tool
    │   ├── MNISTTool.swift
    │   └── README.md
├── mlx-run
├── mlx-swift-examples.xcodeproj
    ├── project.pbxproj
    ├── project.xcworkspace
    │   ├── contents.xcworkspacedata
    │   └── xcshareddata
    │   │   ├── IDEWorkspaceChecks.plist
    │   │   └── swiftpm
    │   │       └── Package.resolved
    └── xcshareddata
    │   └── xcschemes
    │       ├── ExampleLLM.xcscheme
    │       ├── LLMEval.xcscheme
    │       ├── StableDiffusionExample.xcscheme
    │       ├── VLMEval.xcscheme
    │       ├── llm-tool.xcscheme
    │       └── mlx-libraries-Package.xcscheme
└── support
    ├── generate-run-all-llms.sh
    ├── run-all-llms.sh
    └── test.jpg


/.circleci/config.yml:
--------------------------------------------------------------------------------
 1 | version: 2.1
 2 | 
 3 | orbs:
 4 |   apple: ml-explore/pr-approval@0.1.0
 5 | 
 6 | parameters:
 7 |   nightly_build:
 8 |     type: boolean
 9 |     default: false
10 |   weekly_build:
11 |     type: boolean
12 |     default: false
13 | 
14 | jobs:
15 | 
16 |   mac_build_and_test:
17 |     parameters:
18 |       xcode_version:
19 |         type: string
20 |     macos:
21 |       xcode: << parameters.xcode_version >>
22 |     resource_class: macos.m1.medium.gen1
23 |     steps:
24 |       - checkout
25 |       - run: git submodule sync
26 |       - run: git submodule update --init
27 |       - run:
28 |           name: Run style checks
29 |           command: |
30 |             pip install pre-commit
31 |             brew install swift-format
32 |             pre-commit run --all
33 |             if ! git diff --quiet; then echo 'Style checks failed, please install pre-commit and run pre-commit run --all and push the change'; exit 1; fi
34 |       - run:
35 |           name: Run Tests (Xcode, macOS)
36 |           command: |
37 |             xcodebuild -version
38 |             xcrun --show-sdk-build-version
39 |             swift --version
40 |             find . -name Package.resolved -exec rm {} \;
41 |             xcodebuild test -scheme mlx-libraries-Package -destination 'platform=OS X'
42 |       - run:
43 |           name: Build Examples
44 |           command: |
45 |             xcodebuild -version
46 |             xcrun --show-sdk-build-version
47 |             swift --version
48 |             find . -name Package.resolved -exec rm {} \;
49 |             xcodebuild -scheme llm-tool
50 |             xcodebuild -scheme image-tool
51 |             xcodebuild -scheme mnist-tool
52 | 
53 | workflows:
54 |   build_and_test:
55 |     when:
56 |       and:
57 |         - matches:
58 |             pattern: "^(?!pull/)[-\\w]+

quot;
59 |             value: << pipeline.git.branch >>
60 |         - not: << pipeline.parameters.nightly_build >>
61 |         - not: << pipeline.parameters.weekly_build >>
62 |     jobs:
63 |       - mac_build_and_test:
64 |           matrix:
65 |             parameters:
66 |               xcode_version: ["16.0.0", "16.3.0"]
67 | 
68 |   prb:
69 |     when:
70 |       matches:
71 |         pattern: "^pull/\\d+(/head)?

quot;
72 |         value: << pipeline.git.branch >>
73 |     jobs:
74 |       - hold:
75 |           type: approval
76 |       - apple/authenticate:
77 |           context: pr-approval
78 |       - mac_build_and_test:
79 |           requires: [ hold ]
80 |           matrix:
81 |             parameters:
82 |               xcode_version: ["16.0.0", "16.3.0"]
83 | 


--------------------------------------------------------------------------------
/.gitignore:
--------------------------------------------------------------------------------
 1 | # Xcode
 2 | #
 3 | # gitignore contributors: remember to update Global/Xcode.gitignore, Objective-C.gitignore & Swift.gitignore
 4 | 
 5 | ## User settings
 6 | xcuserdata/
 7 | 
 8 | ## compatibility with Xcode 8 and earlier (ignoring not required starting Xcode 9)
 9 | *.xcscmblueprint
10 | *.xccheckout
11 | 
12 | ## compatibility with Xcode 3 and earlier (ignoring not required starting Xcode 4)
13 | build/
14 | DerivedData/
15 | *.moved-aside
16 | *.pbxuser
17 | !default.pbxuser
18 | *.mode1v3
19 | !default.mode1v3
20 | *.mode2v3
21 | !default.mode2v3
22 | *.perspectivev3
23 | !default.perspectivev3
24 | 
25 | ## Obj-C/Swift specific
26 | *.hmap
27 | 
28 | ## App packaging
29 | *.ipa
30 | *.dSYM.zip
31 | *.dSYM
32 | 
33 | ## Playgrounds
34 | timeline.xctimeline
35 | playground.xcworkspace
36 | 
37 | # Swift Package Manager
38 | #
39 | # Add this line if you want to avoid checking in source code from Swift Package Manager dependencies.
40 | Packages/
41 | Package.pins
42 | Package.resolved
43 | # *.xcodeproj
44 | #
45 | # Xcode automatically generates this directory with a .xcworkspacedata file and xcuserdata
46 | # hence it is not needed unless you have added a package configuration file to your project
47 | .swiftpm
48 | 
49 | .build/
50 | 
51 | # CocoaPods
52 | #
53 | # We recommend against adding the Pods directory to your .gitignore. However
54 | # you should judge for yourself, the pros and cons are mentioned at:
55 | # https://guides.cocoapods.org/using/using-cocoapods.html#should-i-check-the-pods-directory-into-source-control
56 | #
57 | # Pods/
58 | #
59 | # Add this line if you want to avoid checking in source code from the Xcode workspace
60 | # *.xcworkspace
61 | 
62 | # Carthage
63 | #
64 | # Add this line if you want to avoid checking in source code from Carthage dependencies.
65 | # Carthage/Checkouts
66 | 
67 | Carthage/Build/
68 | 
69 | # Accio dependency management
70 | Dependencies/
71 | .accio/
72 | 
73 | # fastlane
74 | #
75 | # It is recommended to not store the screenshots in the git repo.
76 | # Instead, use fastlane to re-generate the screenshots whenever they are needed.
77 | # For more information about the recommended setup visit:
78 | # https://docs.fastlane.tools/best-practices/source-control/#source-control
79 | 
80 | fastlane/report.xml
81 | fastlane/Preview.html
82 | fastlane/screenshots/**/*.png
83 | fastlane/test_output
84 | 
85 | # Code Injection
86 | #
87 | # After new code Injection tools there's a generated folder /iOSInjectionProject
88 | # https://github.com/johnno1962/injectionforxcode
89 | 
90 | iOSInjectionProject/
91 | 
92 | # OS
93 | .DS_Store
94 | 
95 | .idea


--------------------------------------------------------------------------------
/.pre-commit-config.yaml:
--------------------------------------------------------------------------------
1 | repos:
2 | - repo: local
3 |   hooks:
4 |     - id: swift-format
5 |       name: swift-format
6 |       language: system
7 |       entry: swift-format --in-place
8 |       files: '\.swift

#39;
9 | 


--------------------------------------------------------------------------------
/.spi.yml:
--------------------------------------------------------------------------------
1 | version: 1
2 | builder:
3 |   configs:
4 |     - documentation_targets: [MLXLLM, MLXVLM, MLXLMCommon, MLXMNIST, MLXEmbedders, StableDiffusion]
5 | 


--------------------------------------------------------------------------------
/.swift-format:
--------------------------------------------------------------------------------
1 | {
2 |     "version": 1,
3 |     "indentation": {
4 |         "spaces": 4
5 |     },
6 |     "spacesAroundRangeFormationOperators": true,
7 | }
8 | 


--------------------------------------------------------------------------------
/ACKNOWLEDGMENTS.md:
--------------------------------------------------------------------------------
 1 | # Individual Contributors
 2 | 
 3 | If you wish to be acknowledged for your contributions, please list your name
 4 | with a short description of your contribution(s) below. For example:
 5 | 
 6 | - Jane Smith: Added the `foo` and `bar` ops.
 7 | 
 8 | MLX Swift was developed with contributions from the following individuals:
 9 | 
10 | <a href="https://github.com/ml-explore/mlx-swift-examples/graphs/contributors">
11 |   <img class="dark-light" src="https://contrib.rocks/image?repo=ml-explore/-swift-examples&anon=0&columns=20&max=100&r=true" />
12 | </a>
13 | 
14 | 
15 | SOFTWARE.
16 | 
17 | 


--------------------------------------------------------------------------------
/Applications/LLMEval/Assets.xcassets/AccentColor.colorset/Contents.json:
--------------------------------------------------------------------------------
 1 | {
 2 |   "colors" : [
 3 |     {
 4 |       "idiom" : "universal"
 5 |     }
 6 |   ],
 7 |   "info" : {
 8 |     "author" : "xcode",
 9 |     "version" : 1
10 |   }
11 | }
12 | 


--------------------------------------------------------------------------------
/Applications/LLMEval/Assets.xcassets/AppIcon.appiconset/Contents.json:
--------------------------------------------------------------------------------
 1 | {
 2 |   "images" : [
 3 |     {
 4 |       "idiom" : "universal",
 5 |       "platform" : "ios",
 6 |       "size" : "1024x1024"
 7 |     },
 8 |     {
 9 |       "idiom" : "mac",
10 |       "scale" : "1x",
11 |       "size" : "16x16"
12 |     },
13 |     {
14 |       "idiom" : "mac",
15 |       "scale" : "2x",
16 |       "size" : "16x16"
17 |     },
18 |     {
19 |       "idiom" : "mac",
20 |       "scale" : "1x",
21 |       "size" : "32x32"
22 |     },
23 |     {
24 |       "idiom" : "mac",
25 |       "scale" : "2x",
26 |       "size" : "32x32"
27 |     },
28 |     {
29 |       "idiom" : "mac",
30 |       "scale" : "1x",
31 |       "size" : "128x128"
32 |     },
33 |     {
34 |       "idiom" : "mac",
35 |       "scale" : "2x",
36 |       "size" : "128x128"
37 |     },
38 |     {
39 |       "idiom" : "mac",
40 |       "scale" : "1x",
41 |       "size" : "256x256"
42 |     },
43 |     {
44 |       "idiom" : "mac",
45 |       "scale" : "2x",
46 |       "size" : "256x256"
47 |     },
48 |     {
49 |       "idiom" : "mac",
50 |       "scale" : "1x",
51 |       "size" : "512x512"
52 |     },
53 |     {
54 |       "idiom" : "mac",
55 |       "scale" : "2x",
56 |       "size" : "512x512"
57 |     }
58 |   ],
59 |   "info" : {
60 |     "author" : "xcode",
61 |     "version" : 1
62 |   }
63 | }
64 | 


--------------------------------------------------------------------------------
/Applications/LLMEval/Assets.xcassets/Contents.json:
--------------------------------------------------------------------------------
1 | {
2 |   "info" : {
3 |     "author" : "xcode",
4 |     "version" : 1
5 |   }
6 | }
7 | 


--------------------------------------------------------------------------------
/Applications/LLMEval/ContentView.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import AsyncAlgorithms
  4 | import MLX
  5 | import MLXLLM
  6 | import MLXLMCommon
  7 | import MarkdownUI
  8 | import Metal
  9 | import SwiftUI
 10 | import Tokenizers
 11 | 
 12 | struct ContentView: View {
 13 |     @Environment(DeviceStat.self) private var deviceStat
 14 | 
 15 |     @State var llm = LLMEvaluator()
 16 | 
 17 |     enum displayStyle: String, CaseIterable, Identifiable {
 18 |         case plain, markdown
 19 |         var id: Self { self }
 20 |     }
 21 | 
 22 |     @State private var selectedDisplayStyle = displayStyle.markdown
 23 | 
 24 |     var body: some View {
 25 |         VStack(alignment: .leading) {
 26 |             VStack {
 27 |                 HStack {
 28 |                     Text(llm.modelInfo)
 29 |                         .textFieldStyle(.roundedBorder)
 30 | 
 31 |                     Spacer()
 32 | 
 33 |                     Text(llm.stat)
 34 |                 }
 35 |                 HStack {
 36 |                     Toggle(isOn: $llm.includeWeatherTool) {
 37 |                         Text("Include tools")
 38 |                     }
 39 |                     .frame(maxWidth: 350, alignment: .leading)
 40 |                     Toggle(isOn: $llm.enableThinking) {
 41 |                         Text("Thinking")
 42 |                             .help(
 43 |                                 "Switches between thinking and non-thinking modes. Support: Qwen3")
 44 |                     }
 45 |                     Spacer()
 46 |                     if llm.running {
 47 |                         ProgressView()
 48 |                             .frame(maxHeight: 20)
 49 |                         Spacer()
 50 |                     }
 51 |                     Picker("", selection: $selectedDisplayStyle) {
 52 |                         ForEach(displayStyle.allCases, id: \.self) { option in
 53 |                             Text(option.rawValue.capitalized)
 54 |                                 .tag(option)
 55 |                         }
 56 | 
 57 |                     }
 58 |                     .pickerStyle(.segmented)
 59 |                     #if os(visionOS)
 60 |                         .frame(maxWidth: 250)
 61 |                     #else
 62 |                         .frame(maxWidth: 150)
 63 |                     #endif
 64 |                 }
 65 |             }
 66 | 
 67 |             // show the model output
 68 |             ScrollView(.vertical) {
 69 |                 ScrollViewReader { sp in
 70 |                     Group {
 71 |                         if selectedDisplayStyle == .plain {
 72 |                             Text(llm.output)
 73 |                                 .textSelection(.enabled)
 74 |                         } else {
 75 |                             Markdown(llm.output)
 76 |                                 .textSelection(.enabled)
 77 |                         }
 78 |                     }
 79 |                     .onChange(of: llm.output) { _, _ in
 80 |                         sp.scrollTo("bottom")
 81 |                     }
 82 | 
 83 |                     Spacer()
 84 |                         .frame(width: 1, height: 1)
 85 |                         .id("bottom")
 86 |                 }
 87 |             }
 88 | 
 89 |             HStack {
 90 |                 TextField("prompt", text: Bindable(llm).prompt)
 91 |                     .onSubmit(generate)
 92 |                     .disabled(llm.running)
 93 |                     #if os(visionOS)
 94 |                         .textFieldStyle(.roundedBorder)
 95 |                     #endif
 96 |                 Button(llm.running ? "stop" : "generate", action: llm.running ? cancel : generate)
 97 |             }
 98 |         }
 99 |         #if os(visionOS)
100 |             .padding(40)
101 |         #else
102 |             .padding()
103 |         #endif
104 |         .toolbar {
105 |             ToolbarItem {
106 |                 Label(
107 |                     "Memory Usage: \(deviceStat.gpuUsage.activeMemory.formatted(.byteCount(style: .memory)))",
108 |                     systemImage: "info.circle.fill"
109 |                 )
110 |                 .labelStyle(.titleAndIcon)
111 |                 .padding(.horizontal)
112 |                 .help(
113 |                     Text(
114 |                         """
115 |                         Active Memory: \(deviceStat.gpuUsage.activeMemory.formatted(.byteCount(style: .memory)))/\(GPU.memoryLimit.formatted(.byteCount(style: .memory)))
116 |                         Cache Memory: \(deviceStat.gpuUsage.cacheMemory.formatted(.byteCount(style: .memory)))/\(GPU.cacheLimit.formatted(.byteCount(style: .memory)))
117 |                         Peak Memory: \(deviceStat.gpuUsage.peakMemory.formatted(.byteCount(style: .memory)))
118 |                         """
119 |                     )
120 |                 )
121 |             }
122 |             ToolbarItem(placement: .primaryAction) {
123 |                 Button {
124 |                     Task {
125 |                         copyToClipboard(llm.output)
126 |                     }
127 |                 } label: {
128 |                     Label("Copy Output", systemImage: "doc.on.doc.fill")
129 |                 }
130 |                 .disabled(llm.output == "")
131 |                 .labelStyle(.titleAndIcon)
132 |             }
133 | 
134 |         }
135 |         .task {
136 |             // pre-load the weights on launch to speed up the first generation
137 |             _ = try? await llm.load()
138 |         }
139 |     }
140 | 
141 |     private func generate() {
142 |         llm.generate()
143 |     }
144 | 
145 |     private func cancel() {
146 |         llm.cancelGeneration()
147 |     }
148 | 
149 |     private func copyToClipboard(_ string: String) {
150 |         #if os(macOS)
151 |             NSPasteboard.general.clearContents()
152 |             NSPasteboard.general.setString(string, forType: .string)
153 |         #else
154 |             UIPasteboard.general.string = string
155 |         #endif
156 |     }
157 | }
158 | 
159 | @Observable
160 | @MainActor
161 | class LLMEvaluator {
162 | 
163 |     var running = false
164 | 
165 |     var includeWeatherTool = false
166 |     var enableThinking = false
167 | 
168 |     var prompt = ""
169 |     var output = ""
170 |     var modelInfo = ""
171 |     var stat = ""
172 | 
173 |     /// This controls which model loads. `qwen2_5_1_5b` is one of the smaller ones, so this will fit on
174 |     /// more devices.
175 |     let modelConfiguration = LLMRegistry.qwen3_1_7b_4bit
176 | 
177 |     /// parameters controlling the output
178 |     let generateParameters = GenerateParameters(maxTokens: 240, temperature: 0.6)
179 |     let updateInterval = Duration.seconds(0.25)
180 | 
181 |     /// A task responsible for handling the generation process.
182 |     var generationTask: Task<Void, Error>?
183 | 
184 |     enum LoadState {
185 |         case idle
186 |         case loaded(ModelContainer)
187 |     }
188 | 
189 |     var loadState = LoadState.idle
190 | 
191 |     let currentWeatherTool = Tool<WeatherInput, WeatherOutput>(
192 |         name: "get_current_weather",
193 |         description: "Get the current weather in a given location",
194 |         parameters: [
195 |             .required(
196 |                 "location", type: .string, description: "The city and state, e.g. San Francisco, CA"
197 |             ),
198 |             .optional(
199 |                 "unit",
200 |                 type: .string,
201 |                 description: "The unit of temperature",
202 |                 extraProperties: [
203 |                     "enum": ["celsius", "fahrenheit"],
204 |                     "default": "celsius",
205 |                 ]
206 |             ),
207 |         ]
208 |     ) { input in
209 |         let range = input.unit == "celsius" ? (min: -20.0, max: 40.0) : (min: 0, max: 100)
210 |         let temperature = Double.random(in: range.min ... range.max)
211 | 
212 |         let conditions = ["Sunny", "Cloudy", "Rainy", "Snowy", "Windy", "Stormy"].randomElement()!
213 | 
214 |         return WeatherOutput(temperature: temperature, conditions: conditions)
215 |     }
216 | 
217 |     let addTool = Tool<AddInput, AddOutput>(
218 |         name: "add_two_numbers",
219 |         description: "Add two numbers together",
220 |         parameters: [
221 |             .required("first", type: .int, description: "The first number to add"),
222 |             .required("second", type: .int, description: "The second number to add"),
223 |         ]
224 |     ) { input in
225 |         AddOutput(result: input.first + input.second)
226 |     }
227 | 
228 |     let timeTool = Tool<EmptyInput, TimeOutput>(
229 |         name: "get_time",
230 |         description: "Get the current time",
231 |         parameters: [],
232 |     ) { _ in
233 |         TimeOutput(time: Date.now.formatted())
234 |     }
235 | 
236 |     /// load and return the model -- can be called multiple times, subsequent calls will
237 |     /// just return the loaded model
238 |     func load() async throws -> ModelContainer {
239 |         switch loadState {
240 |         case .idle:
241 |             // limit the buffer cache
242 |             MLX.GPU.set(cacheLimit: 20 * 1024 * 1024)
243 | 
244 |             let modelContainer = try await LLMModelFactory.shared.loadContainer(
245 |                 configuration: modelConfiguration
246 |             ) {
247 |                 [modelConfiguration] progress in
248 |                 Task { @MainActor in
249 |                     self.modelInfo =
250 |                         "Downloading \(modelConfiguration.name): \(Int(progress.fractionCompleted * 100))%"
251 |                 }
252 |             }
253 |             let numParams = await modelContainer.perform { context in
254 |                 context.model.numParameters()
255 |             }
256 | 
257 |             self.prompt = modelConfiguration.defaultPrompt
258 |             self.modelInfo =
259 |                 "Loaded \(modelConfiguration.id). Weights: \(numParams / (1024*1024))M"
260 |             loadState = .loaded(modelContainer)
261 |             return modelContainer
262 | 
263 |         case .loaded(let modelContainer):
264 |             return modelContainer
265 |         }
266 |     }
267 | 
268 |     private func generate(prompt: String, toolResult: String? = nil) async {
269 | 
270 |         self.output = ""
271 |         var chat: [Chat.Message] = [
272 |             .system("You are a helpful assistant"),
273 |             .user(prompt),
274 |         ]
275 | 
276 |         if let toolResult {
277 |             chat.append(.tool(toolResult))
278 |         }
279 | 
280 |         let userInput = UserInput(
281 |             chat: chat,
282 |             tools: includeWeatherTool
283 |                 ? [currentWeatherTool.schema, addTool.schema, timeTool.schema] : nil,
284 |             additionalContext: ["enable_thinking": enableThinking]
285 |         )
286 | 
287 |         do {
288 |             let modelContainer = try await load()
289 | 
290 |             // each time you generate you will get something new
291 |             MLXRandom.seed(UInt64(Date.timeIntervalSinceReferenceDate * 1000))
292 | 
293 |             try await modelContainer.perform { (context: ModelContext) -> Void in
294 |                 let lmInput = try await context.processor.prepare(input: userInput)
295 |                 let stream = try MLXLMCommon.generate(
296 |                     input: lmInput, parameters: generateParameters, context: context)
297 | 
298 |                 // generate and output in batches
299 |                 for await batch in stream._throttle(
300 |                     for: updateInterval, reducing: Generation.collect)
301 |                 {
302 |                     let output = batch.compactMap { $0.chunk }.joined(separator: "")
303 |                     if !output.isEmpty {
304 |                         Task { @MainActor [output] in
305 |                             self.output += output
306 |                         }
307 |                     }
308 | 
309 |                     if let completion = batch.compactMap({ $0.info }).first {
310 |                         Task { @MainActor in
311 |                             self.stat = "\(completion.tokensPerSecond) tokens/s"
312 |                         }
313 |                     }
314 | 
315 |                     if let toolCall = batch.compactMap({ $0.toolCall }).first {
316 |                         try await handleToolCall(toolCall, prompt: prompt)
317 |                     }
318 |                 }
319 |             }
320 | 
321 |         } catch {
322 |             output = "Failed: \(error)"
323 |         }
324 |     }
325 | 
326 |     func generate() {
327 |         guard !running else { return }
328 |         let currentPrompt = prompt
329 |         prompt = ""
330 |         generationTask = Task {
331 |             running = true
332 |             await generate(prompt: currentPrompt)
333 |             running = false
334 |         }
335 |     }
336 | 
337 |     func cancelGeneration() {
338 |         generationTask?.cancel()
339 |         running = false
340 |     }
341 | 
342 |     private func handleToolCall(_ toolCall: ToolCall, prompt: String) async throws {
343 |         let result =
344 |             switch toolCall.function.name {
345 |             case currentWeatherTool.name:
346 |                 try await toolCall.execute(with: currentWeatherTool).toolResult
347 |             case addTool.name:
348 |                 try await toolCall.execute(with: addTool).toolResult
349 |             case timeTool.name:
350 |                 try await toolCall.execute(with: timeTool).toolResult
351 |             default:
352 |                 "No tool match"
353 |             }
354 | 
355 |         await generate(prompt: prompt, toolResult: result)
356 |     }
357 | }
358 | 
359 | struct WeatherInput: Codable {
360 |     let location: String
361 |     let unit: String?
362 | }
363 | 
364 | struct WeatherOutput: Codable {
365 |     let temperature: Double
366 |     let conditions: String
367 | }
368 | 
369 | struct AddInput: Codable {
370 |     let first: Int
371 |     let second: Int
372 | }
373 | 
374 | struct AddOutput: Codable {
375 |     let result: Int
376 | }
377 | 
378 | struct EmptyInput: Codable {}
379 | 
380 | struct TimeOutput: Codable {
381 |     let time: String
382 | }
383 | 


--------------------------------------------------------------------------------
/Applications/LLMEval/LLMEval.entitlements:
--------------------------------------------------------------------------------
 1 | <?xml version="1.0" encoding="UTF-8"?>
 2 | <!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
 3 | <plist version="1.0">
 4 | <dict>
 5 | 	<key>com.apple.developer.kernel.increased-memory-limit</key>
 6 | 	<true/>
 7 | 	<key>com.apple.security.app-sandbox</key>
 8 | 	<true/>
 9 | 	<key>com.apple.security.device.usb</key>
10 | 	<true/>
11 | 	<key>com.apple.security.files.user-selected.read-only</key>
12 | 	<true/>
13 | 	<key>com.apple.security.network.client</key>
14 | 	<true/>
15 | </dict>
16 | </plist>
17 | 


--------------------------------------------------------------------------------
/Applications/LLMEval/LLMEvalApp.swift:
--------------------------------------------------------------------------------
 1 | // Copyright © 2024 Apple Inc.
 2 | 
 3 | import SwiftUI
 4 | 
 5 | @main
 6 | struct LLMEvalApp: App {
 7 |     var body: some Scene {
 8 |         WindowGroup {
 9 |             ContentView()
10 |                 .environment(DeviceStat())
11 |         }
12 |     }
13 | }
14 | 


--------------------------------------------------------------------------------
/Applications/LLMEval/Preview Content/Preview Assets.xcassets/Contents.json:
--------------------------------------------------------------------------------
1 | {
2 |   "info" : {
3 |     "author" : "xcode",
4 |     "version" : 1
5 |   }
6 | }
7 | 


--------------------------------------------------------------------------------
/Applications/LLMEval/README.md:
--------------------------------------------------------------------------------
 1 | #  LLMEval
 2 | 
 3 | An example that:
 4 | 
 5 | - downloads a huggingface model (phi-2) and tokenizer
 6 | - evaluates a prompt
 7 | - displays the output as it generates text
 8 | 
 9 | You will need to set the Team on the LLMEval target in order to build and run on iOS.
10 | 
11 | Some notes about the setup:
12 | 
13 | - this downloads models from hugging face so LLMEval -> Signing & Capabilities has the "Outgoing Connections (Client)" set in the App Sandbox
14 | - LLM models are large so this uses the Increased Memory Limit entitlement on iOS to allow ... increased memory limits for devices that have more memory
15 | - `MLX.GPU.set(cacheLimit: 20 * 1024 * 1024)` is used to limit the buffer cache size
16 | - The Phi2 4 bit model is small enough to run on some iPhone models
17 |     - this can be changed by editing `let modelConfiguration = ModelConfiguration.phi4bit`
18 | 
19 | ### Trying Different Models
20 | 
21 | The example application uses Phi2 model by default, see [ContentView.swift](ContentView.swift#L58):
22 | 
23 | ```
24 |     /// this controls which model loads -- phi4bit is one of the smaller ones so this will fit on
25 |     /// more devices
26 |     let modelConfiguration = ModelConfiguration.phi4bit
27 | ```
28 | 
29 | There are some pre-configured models in [MLXLLM/LLMModelFactory.swift](../../Libraries/MLXLLM/LLMModelFactory.swift#L78)
30 | and you can load any weights from Hugging Face where there
31 | is a model architecture defined and you have enough
32 | memory.
33 | 
34 | ### Troubleshooting
35 | 
36 | If the program crashes with a very deep stack trace, you may need to build
37 | in Release configuration. This seems to depend on the size of the model.
38 | 
39 | There are a couple options:
40 | 
41 | - Build Release
42 | - Force the model evaluation to run on the main thread, e.g. using @MainActor
43 | - Build `Cmlx` with optimizations by modifying `mlx/Package.swift` and adding `.unsafeOptions(["-O3"]),` around line 87
44 | 
45 | See discussion here: https://github.com/ml-explore/mlx-swift-examples/issues/3
46 | 
47 | ### Performance
48 | 
49 | Different models have difference performance characteristics. For example Gemma 2B may outperform Phi-2 in terms of tokens / second.
50 | 
51 | You may also find that running outside the debugger boosts performance. You can do this in Xcode by pressing cmd-opt-r and unchecking "Debug Executable".
52 | 


--------------------------------------------------------------------------------
/Applications/LLMEval/ViewModels/DeviceStat.swift:
--------------------------------------------------------------------------------
 1 | import Foundation
 2 | import MLX
 3 | 
 4 | @Observable
 5 | final class DeviceStat: @unchecked Sendable {
 6 | 
 7 |     @MainActor
 8 |     var gpuUsage = GPU.snapshot()
 9 | 
10 |     private let initialGPUSnapshot = GPU.snapshot()
11 |     private var timer: Timer?
12 | 
13 |     init() {
14 |         timer = Timer.scheduledTimer(withTimeInterval: 2.0, repeats: true) { [weak self] _ in
15 |             self?.updateGPUUsages()
16 |         }
17 |     }
18 | 
19 |     deinit {
20 |         timer?.invalidate()
21 |     }
22 | 
23 |     private func updateGPUUsages() {
24 |         let gpuSnapshotDelta = initialGPUSnapshot.delta(GPU.snapshot())
25 |         DispatchQueue.main.async { [weak self] in
26 |             self?.gpuUsage = gpuSnapshotDelta
27 |         }
28 |     }
29 | 
30 | }
31 | 


--------------------------------------------------------------------------------
/Applications/LoRATrainingExample/Assets.xcassets/AccentColor.colorset/Contents.json:
--------------------------------------------------------------------------------
 1 | {
 2 |   "colors" : [
 3 |     {
 4 |       "idiom" : "universal"
 5 |     }
 6 |   ],
 7 |   "info" : {
 8 |     "author" : "xcode",
 9 |     "version" : 1
10 |   }
11 | }
12 | 


--------------------------------------------------------------------------------
/Applications/LoRATrainingExample/Assets.xcassets/AppIcon.appiconset/Contents.json:
--------------------------------------------------------------------------------
 1 | {
 2 |   "images" : [
 3 |     {
 4 |       "idiom" : "universal",
 5 |       "platform" : "ios",
 6 |       "size" : "1024x1024"
 7 |     },
 8 |     {
 9 |       "idiom" : "mac",
10 |       "scale" : "1x",
11 |       "size" : "16x16"
12 |     },
13 |     {
14 |       "idiom" : "mac",
15 |       "scale" : "2x",
16 |       "size" : "16x16"
17 |     },
18 |     {
19 |       "idiom" : "mac",
20 |       "scale" : "1x",
21 |       "size" : "32x32"
22 |     },
23 |     {
24 |       "idiom" : "mac",
25 |       "scale" : "2x",
26 |       "size" : "32x32"
27 |     },
28 |     {
29 |       "idiom" : "mac",
30 |       "scale" : "1x",
31 |       "size" : "128x128"
32 |     },
33 |     {
34 |       "idiom" : "mac",
35 |       "scale" : "2x",
36 |       "size" : "128x128"
37 |     },
38 |     {
39 |       "idiom" : "mac",
40 |       "scale" : "1x",
41 |       "size" : "256x256"
42 |     },
43 |     {
44 |       "idiom" : "mac",
45 |       "scale" : "2x",
46 |       "size" : "256x256"
47 |     },
48 |     {
49 |       "idiom" : "mac",
50 |       "scale" : "1x",
51 |       "size" : "512x512"
52 |     },
53 |     {
54 |       "idiom" : "mac",
55 |       "scale" : "2x",
56 |       "size" : "512x512"
57 |     }
58 |   ],
59 |   "info" : {
60 |     "author" : "xcode",
61 |     "version" : 1
62 |   }
63 | }
64 | 


--------------------------------------------------------------------------------
/Applications/LoRATrainingExample/Assets.xcassets/Contents.json:
--------------------------------------------------------------------------------
1 | {
2 |   "info" : {
3 |     "author" : "xcode",
4 |     "version" : 1
5 |   }
6 | }
7 | 


--------------------------------------------------------------------------------
/Applications/LoRATrainingExample/ContentView.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import MLX
  4 | import MLXLLM
  5 | import MLXLMCommon
  6 | import MLXNN
  7 | import MLXOptimizers
  8 | import SwiftUI
  9 | import Tokenizers
 10 | 
 11 | struct ContentView: View {
 12 | 
 13 |     @State var evaluator = LoRAEvaluator()
 14 | 
 15 |     @State var prompt = """
 16 |         table: 1-10015132-16
 17 |         columns: Player, No., Nationality, Position, Years in Toronto, School/Club Team
 18 |         Q: What is terrence ross' nationality
 19 |         A:
 20 |         """
 21 | 
 22 |     var body: some View {
 23 |         VStack {
 24 |             HStack {
 25 |                 if let progress = evaluator.progress {
 26 |                     if let current = progress.current, let limit = progress.limit {
 27 |                         ProgressView(progress.title, value: current, total: limit)
 28 |                     } else {
 29 |                         ProgressView(progress.title)
 30 |                     }
 31 |                 }
 32 |             }
 33 |             .frame(maxWidth: .infinity, minHeight: 25)
 34 | 
 35 |             VStack {
 36 |                 ScrollView(.vertical) {
 37 |                     ScrollViewReader { sp in
 38 |                         Group {
 39 |                             Text(evaluator.output)
 40 |                                 .textSelection(.enabled)
 41 |                                 .frame(maxWidth: .infinity)
 42 |                         }
 43 |                         .onChange(of: evaluator.output) { _, _ in
 44 |                             sp.scrollTo("bottom")
 45 |                         }
 46 |                         .padding()
 47 | 
 48 |                         Spacer()
 49 |                             .frame(width: 1, height: 1)
 50 |                             .id("bottom")
 51 |                     }
 52 |                 }
 53 | 
 54 |                 // controls for each of the different states
 55 |                 VStack {
 56 |                     switch evaluator.state {
 57 |                     case .idle:
 58 |                         Button("Start", action: start)
 59 | 
 60 |                     case .training:
 61 |                         EmptyView()
 62 | 
 63 |                     case .evaluate:
 64 |                         Group {
 65 |                             TextEditor(text: $prompt)
 66 |                                 .frame(minHeight: 60)
 67 |                             Button("Evaluate", action: evaluate)
 68 |                         }
 69 |                         .disabled(evaluator.progress != nil)
 70 | 
 71 |                     case .failed(let message):
 72 |                         Text("Failed: \(message)")
 73 |                             .bold()
 74 |                             .foregroundStyle(.red)
 75 |                     }
 76 |                 }
 77 |                 .padding()
 78 |                 .frame(maxWidth: .infinity)
 79 |             }
 80 |         }
 81 |         .padding()
 82 |     }
 83 | 
 84 |     func start() {
 85 |         Task {
 86 |             await evaluator.start()
 87 |         }
 88 |     }
 89 | 
 90 |     func evaluate() {
 91 |         Task {
 92 |             await evaluator.evaluate(prompt: prompt)
 93 |         }
 94 |     }
 95 | }
 96 | 
 97 | /// Progress reporting with a title.
 98 | struct Progress: Equatable, Sendable {
 99 |     let title: String
100 |     let current: Double?
101 |     let limit: Double?
102 | }
103 | 
104 | @Observable
105 | @MainActor
106 | class LoRAEvaluator {
107 | 
108 |     enum State: Sendable {
109 |         case idle
110 |         case training
111 |         case evaluate
112 |         case failed(String)
113 |     }
114 | 
115 |     enum ModelState: Sendable {
116 |         case idle
117 |         case loaded(ModelContainer)
118 |     }
119 | 
120 |     var state = State.idle
121 |     var progress: Progress?
122 | 
123 |     var output = ""
124 | 
125 |     private let modelConfiguration = LLMRegistry.mistral7B4bit
126 |     private var model: ModelState = .idle
127 | 
128 |     private let loraLayers = 4
129 |     private let learningRate: Float = 1e-5
130 |     private let parameters = LoRATrain.Parameters(batchSize: 1, iterations: 200)
131 | 
132 |     private let generateParameters = GenerateParameters(temperature: 0.6, topP: 0.9)
133 |     private let evaluateShowEvery = 8
134 |     private let maxTokens = 200
135 | 
136 |     private func loadModel() async throws -> ModelContainer {
137 |         switch self.model {
138 |         case .idle:
139 |             let name = modelConfiguration.name
140 |             await MainActor.run {
141 |                 progress = .init(title: "Loading \(name)", current: 0, limit: 1)
142 |             }
143 | 
144 |             let modelContainer = try await LLMModelFactory.shared.loadContainer(
145 |                 configuration: modelConfiguration
146 |             ) {
147 |                 progress in
148 |                 Task { @MainActor in
149 |                     self.progress = .init(
150 |                         title: "Download \(name)", current: progress.fractionCompleted,
151 |                         limit: 1.0)
152 |                 }
153 |             }
154 |             self.model = .loaded(modelContainer)
155 |             return modelContainer
156 | 
157 |         case .loaded(let modelContainer):
158 |             return modelContainer
159 |         }
160 |     }
161 | 
162 |     private func loadLoRAData(name: String) throws -> [String]? {
163 |         if let url = Bundle.main.url(forResource: name, withExtension: "jsonl") {
164 |             return try MLXLLM.loadLoRAData(url: url)
165 |         }
166 |         return nil
167 |     }
168 | 
169 |     func start() async {
170 |         do {
171 |             try await startInner()
172 |         } catch {
173 |             self.state = .failed("Failed: \(error)")
174 |         }
175 |     }
176 | 
177 |     nonisolated private func loraLayers(model: Module) -> LoRALinearLayers {
178 |         guard let layerProvider = model as? LoRAModel else {
179 |             // the layerProvider will indicate which Linear layers need to be replaced
180 |             fatalError(
181 |                 "Model \(type(of: model)) (\(modelConfiguration.name)) must implement the LoRALayerProvider protocol"
182 |             )
183 |         }
184 | 
185 |         return Array(layerProvider.loraLinearLayers().suffix(loraLayers))
186 |     }
187 | 
188 |     private func startInner() async throws {
189 |         // setup
190 |         GPU.set(cacheLimit: 32 * 1024 * 1024)
191 |         await MainActor.run {
192 |             output = ""
193 |             state = .training
194 |         }
195 | 
196 |         // load the model
197 |         let modelContainer = try await loadModel()
198 | 
199 |         // apply LoRA adapters and train
200 |         await modelContainer.perform { context in
201 |             LoRATrain.convert(
202 |                 model: context.model, layers: loraLayers(model: context.model))
203 |         }
204 | 
205 |         let train = try loadLoRAData(name: "train")
206 |         let valid = try loadLoRAData(name: "valid")
207 |         guard let train, let valid else {
208 |             state = .failed("Failed to load train/validation data")
209 |             return
210 |         }
211 | 
212 |         try await modelContainer.perform { context in
213 |             let optimizer = Adam(learningRate: learningRate)
214 |             try LoRATrain.train(
215 |                 model: context.model, train: train, validate: valid, optimizer: optimizer,
216 |                 tokenizer: context.tokenizer,
217 |                 parameters: parameters
218 |             ) { progress in
219 |                 Task { @MainActor in
220 |                     switch progress {
221 |                     case .train(let i, _, _, _):
222 |                         self.progress = .init(
223 |                             title: "Train", current: Double(i), limit: Double(parameters.iterations)
224 |                         )
225 |                     case .validation:
226 |                         output += "\n"
227 |                     default:
228 |                         break
229 |                     }
230 |                     output += progress.description + "\n"
231 |                 }
232 | 
233 |                 return .more
234 |             }
235 |         }
236 | 
237 |         // done training, test
238 |         self.progress = .init(title: "Testing", current: nil, limit: nil)
239 |         guard let test = try loadLoRAData(name: "test") else {
240 |             state = .failed("Failed to load test data")
241 |             return
242 |         }
243 | 
244 |         let loss = await modelContainer.perform { context in
245 |             LoRATrain.evaluate(
246 |                 model: context.model, dataset: test,
247 |                 tokenizer: context.tokenizer, batchSize: 1, batchCount: 0)
248 |         }
249 | 
250 |         self.progress = nil
251 |         self.output += "\n"
252 |         self.output += "Test loss \(loss.formatted()), ppl \(exp(loss).formatted())\n"
253 |         self.state = .evaluate
254 |     }
255 | 
256 |     func evaluate(prompt: String) async {
257 |         do {
258 |             try await evaluateInner(prompt: prompt)
259 |         } catch {
260 |             self.state = .failed("Failed: \(error)")
261 |         }
262 |     }
263 | 
264 |     func evaluateInner(prompt: String) async throws {
265 |         await MainActor.run {
266 |             self.progress = .init(title: "Evaluating", current: nil, limit: nil)
267 |             self.output = ""
268 |         }
269 | 
270 |         MLXRandom.seed(UInt64(Date.timeIntervalSinceReferenceDate * 1000))
271 | 
272 |         let modelContainer = try await loadModel()
273 | 
274 |         // evaluate
275 |         let result = try await modelContainer.perform { context in
276 |             let input = try await context.processor.prepare(input: .init(prompt: prompt))
277 |             return try MLXLMCommon.generate(
278 |                 input: input, parameters: generateParameters, context: context
279 |             ) { tokens in
280 |                 if tokens.count % evaluateShowEvery == 0 {
281 |                     let fullOutput = context.tokenizer.decode(tokens: tokens)
282 |                     Task { @MainActor in
283 |                         self.output = fullOutput
284 |                     }
285 |                 }
286 |                 return tokens.count >= maxTokens ? .stop : .more
287 |             }
288 |         }
289 | 
290 |         self.output = result.output
291 |         self.progress = nil
292 |     }
293 | }
294 | 


--------------------------------------------------------------------------------
/Applications/LoRATrainingExample/LoRATrainingExample.entitlements:
--------------------------------------------------------------------------------
 1 | <?xml version="1.0" encoding="UTF-8"?>
 2 | <!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
 3 | <plist version="1.0">
 4 | <dict>
 5 | 	<key>com.apple.developer.kernel.increased-memory-limit</key>
 6 | 	<true/>
 7 | 	<key>com.apple.security.app-sandbox</key>
 8 | 	<true/>
 9 | 	<key>com.apple.security.files.user-selected.read-only</key>
10 | 	<true/>
11 | 	<key>com.apple.security.network.client</key>
12 | 	<true/>
13 | </dict>
14 | </plist>
15 | 


--------------------------------------------------------------------------------
/Applications/LoRATrainingExample/LoRATrainingExampleApp.swift:
--------------------------------------------------------------------------------
 1 | // Copyright © 2024 Apple Inc.
 2 | 
 3 | import SwiftUI
 4 | 
 5 | @main
 6 | struct LoRATrainingExampleApp: App {
 7 |     var body: some Scene {
 8 |         WindowGroup {
 9 |             ContentView()
10 |         }
11 |     }
12 | }
13 | 


--------------------------------------------------------------------------------
/Applications/LoRATrainingExample/Preview Content/Preview Assets.xcassets/Contents.json:
--------------------------------------------------------------------------------
1 | {
2 |   "info" : {
3 |     "author" : "xcode",
4 |     "version" : 1
5 |   }
6 | }
7 | 


--------------------------------------------------------------------------------
/Applications/LoRATrainingExample/README.md:
--------------------------------------------------------------------------------
 1 | #  LoRATrainingExample
 2 | 
 3 | Example application that:
 4 | 
 5 | - downloads the `mlx-community/Mistral-7B-v0.1-hf-4bit-mlx` model from huggingface
 6 | - loads the train/valid/test data from `$SRCROOT/Data/lora` (this is copied into the build but you can imagine how it might be downloaded)
 7 | - adds LoRA adapters and trains the model
 8 | - let's you evaluate a prompt against the model
 9 | 
10 | This roughly equates to the command line example in [Tools/llm-tool](../../Tools/llm-tool) and
11 | you can read more about LoRA there.
12 | 
13 | This evaluates the LoRA adapted model rather than a fused model. This doesn't persist
14 | the LoRA weights or the fused model -- it will retrain it each time the program is launched.
15 | 
16 | ### Troubleshooting
17 | 
18 | The `mlx-community/Mistral-7B-v0.1-hf-4bit-mlx` model requires a little over 4G of
19 | memory to load an train -- this may require ~6G of physical RAM.
20 | 
21 | 
22 | 


--------------------------------------------------------------------------------
/Applications/MLXChatExample/ChatView.swift:
--------------------------------------------------------------------------------
 1 | //
 2 | //  ChatView.swift
 3 | //  MLXChatExample
 4 | //
 5 | //  Created by İbrahim Çetin on 20.04.2025.
 6 | //
 7 | 
 8 | import AVFoundation
 9 | import AVKit
10 | import SwiftUI
11 | 
12 | /// Main chat interface view that manages the conversation UI and user interactions.
13 | /// Displays messages, handles media attachments, and provides input controls.
14 | struct ChatView: View {
15 |     /// View model that manages the chat state and business logic
16 |     @Bindable private var vm: ChatViewModel
17 | 
18 |     /// Initializes the chat view with a view model
19 |     /// - Parameter viewModel: The view model to manage chat state
20 |     init(viewModel: ChatViewModel) {
21 |         self.vm = viewModel
22 |     }
23 | 
24 |     var body: some View {
25 |         NavigationStack {
26 |             VStack(spacing: 0) {
27 |                 // Display conversation history
28 |                 ConversationView(messages: vm.messages)
29 | 
30 |                 Divider()
31 | 
32 |                 // Show media previews if attachments are present
33 |                 if !vm.mediaSelection.isEmpty {
34 |                     MediaPreviewsView(mediaSelection: vm.mediaSelection)
35 |                 }
36 | 
37 |                 // Input field with send and media attachment buttons
38 |                 PromptField(
39 |                     prompt: $vm.prompt,
40 |                     sendButtonAction: vm.generate,
41 |                     // Only show media button for vision-capable models
42 |                     mediaButtonAction: vm.selectedModel.isVisionModel
43 |                         ? {
44 |                             vm.mediaSelection.isShowing = true
45 |                         } : nil
46 |                 )
47 |                 .padding()
48 |             }
49 |             .navigationTitle("MLX Chat Example")
50 |             .toolbar {
51 |                 ChatToolbarView(vm: vm)
52 |             }
53 |             // Handle media file selection
54 |             .fileImporter(
55 |                 isPresented: $vm.mediaSelection.isShowing,
56 |                 allowedContentTypes: [.image, .movie],
57 |                 onCompletion: vm.addMedia
58 |             )
59 |         }
60 |     }
61 | }
62 | 
63 | #Preview {
64 |     ChatView(viewModel: ChatViewModel(mlxService: MLXService()))
65 | }
66 | 


--------------------------------------------------------------------------------
/Applications/MLXChatExample/MLXChatExample.entitlements:
--------------------------------------------------------------------------------
 1 | <?xml version="1.0" encoding="UTF-8"?>
 2 | <!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
 3 | <plist version="1.0">
 4 | <dict>
 5 | 	<key>com.apple.developer.kernel.increased-memory-limit</key>
 6 | 	<true/>
 7 | 	<key>com.apple.security.app-sandbox</key>
 8 | 	<true/>
 9 | 	<key>com.apple.security.files.downloads.read-write</key>
10 | 	<true/>
11 | 	<key>com.apple.security.files.user-selected.read-only</key>
12 | 	<true/>
13 | 	<key>com.apple.security.network.client</key>
14 | 	<true/>
15 | </dict>
16 | </plist>
17 | 


--------------------------------------------------------------------------------
/Applications/MLXChatExample/MLXChatExampleApp.swift:
--------------------------------------------------------------------------------
 1 | //
 2 | //  MLXChatExampleApp.swift
 3 | //  MLXChatExample
 4 | //
 5 | //  Created by İbrahim Çetin on 20.04.2025.
 6 | //
 7 | 
 8 | import SwiftUI
 9 | 
10 | @main
11 | struct MLXChatExampleApp: App {
12 |     var body: some Scene {
13 |         WindowGroup {
14 |             ChatView(viewModel: ChatViewModel(mlxService: MLXService()))
15 |         }
16 |     }
17 | }
18 | 


--------------------------------------------------------------------------------
/Applications/MLXChatExample/Models/LMModel.swift:
--------------------------------------------------------------------------------
 1 | //
 2 | //  LMModel.swift
 3 | //  MLXChatExample
 4 | //
 5 | //  Created by İbrahim Çetin on 21.04.2025.
 6 | //
 7 | 
 8 | import MLXLMCommon
 9 | 
10 | /// Represents a language model configuration with its associated properties and type.
11 | /// Can represent either a large language model (LLM) or a vision-language model (VLM).
12 | struct LMModel {
13 |     /// Name of the model
14 |     let name: String
15 | 
16 |     /// Configuration settings for model initialization
17 |     let configuration: ModelConfiguration
18 | 
19 |     /// Type of the model (language or vision-language)
20 |     let type: ModelType
21 | 
22 |     /// Defines the type of language model
23 |     enum ModelType {
24 |         /// Large language model (text-only)
25 |         case llm
26 |         /// Vision-language model (supports images and text)
27 |         case vlm
28 |     }
29 | }
30 | 
31 | // MARK: - Helpers
32 | 
33 | extension LMModel {
34 |     /// Display name with additional "(Vision)" suffix for vision models
35 |     var displayName: String {
36 |         if isVisionModel {
37 |             "\(name) (Vision)"
38 |         } else {
39 |             name
40 |         }
41 |     }
42 | 
43 |     /// Whether the model is a large language model
44 |     var isLanguageModel: Bool {
45 |         type == .llm
46 |     }
47 | 
48 |     /// Whether the model is a vision-language model
49 |     var isVisionModel: Bool {
50 |         type == .vlm
51 |     }
52 | }
53 | 
54 | extension LMModel: Identifiable, Hashable {
55 |     var id: String {
56 |         name
57 |     }
58 | 
59 |     func hash(into hasher: inout Hasher) {
60 |         hasher.combine(name)
61 |     }
62 | }
63 | 


--------------------------------------------------------------------------------
/Applications/MLXChatExample/Models/Message.swift:
--------------------------------------------------------------------------------
 1 | //
 2 | //  Message.swift
 3 | //  MLXChatExample
 4 | //
 5 | //  Created by İbrahim Çetin on 20.04.2025.
 6 | //
 7 | 
 8 | import Foundation
 9 | 
10 | /// Represents a chat message in the conversation.
11 | /// Messages can contain text content and optional media attachments (images and videos).
12 | @Observable
13 | class Message: Identifiable {
14 |     /// Unique identifier for the message
15 |     let id: UUID
16 | 
17 |     /// The role of the message sender (user, assistant, or system)
18 |     let role: Role
19 | 
20 |     /// The text content of the message
21 |     var content: String
22 | 
23 |     /// Array of image URLs attached to the message
24 |     var images: [URL]
25 | 
26 |     /// Array of video URLs attached to the message
27 |     var videos: [URL]
28 | 
29 |     /// Timestamp when the message was created
30 |     let timestamp: Date
31 | 
32 |     /// Creates a new message with the specified role, content, and optional media attachments
33 |     /// - Parameters:
34 |     ///   - role: The role of the message sender
35 |     ///   - content: The text content of the message
36 |     ///   - images: Optional array of image URLs
37 |     ///   - videos: Optional array of video URLs
38 |     init(role: Role, content: String, images: [URL] = [], videos: [URL] = []) {
39 |         self.id = UUID()
40 |         self.role = role
41 |         self.content = content
42 |         self.images = images
43 |         self.videos = videos
44 |         self.timestamp = .now
45 |     }
46 | 
47 |     /// Defines the role of the message sender in the conversation
48 |     enum Role {
49 |         /// Message from the user
50 |         case user
51 |         /// Message from the AI assistant
52 |         case assistant
53 |         /// System message providing context or instructions
54 |         case system
55 |     }
56 | }
57 | 
58 | /// Convenience methods for creating different types of messages
59 | extension Message {
60 |     /// Creates a user message with optional media attachments
61 |     /// - Parameters:
62 |     ///   - content: The text content of the message
63 |     ///   - images: Optional array of image URLs
64 |     ///   - videos: Optional array of video URLs
65 |     /// - Returns: A new Message instance with user role
66 |     static func user(_ content: String, images: [URL] = [], videos: [URL] = []) -> Message {
67 |         Message(role: .user, content: content, images: images, videos: videos)
68 |     }
69 | 
70 |     /// Creates an assistant message
71 |     /// - Parameter content: The text content of the message
72 |     /// - Returns: A new Message instance with assistant role
73 |     static func assistant(_ content: String) -> Message {
74 |         Message(role: .assistant, content: content)
75 |     }
76 | 
77 |     /// Creates a system message
78 |     /// - Parameter content: The text content of the message
79 |     /// - Returns: A new Message instance with system role
80 |     static func system(_ content: String) -> Message {
81 |         Message(role: .system, content: content)
82 |     }
83 | }
84 | 


--------------------------------------------------------------------------------
/Applications/MLXChatExample/README.md:
--------------------------------------------------------------------------------
  1 | # MLX Chat Example
  2 | 
  3 | A lightweight chat application demonstrating MLX integration for iOS and macOS. Built with SwiftUI, this example project shows how to implement both Large Language Models (LLMs) and Vision Language Models (VLMs) using MLX.
  4 | 
  5 | <img alt="MLX Chat Example Screenshot" src="https://github.com/user-attachments/assets/9a20c081-61c2-4b0a-88df-f54500464d77" />
  6 | 
  7 | ## Features
  8 | 
  9 | - 🤖 LLM and VLM support with real-time text generation
 10 | - 📱 Cross-platform (iOS and macOS)
 11 | - 🖼️ Image and video input for vision models
 12 | - 💾 Efficient model caching and memory management
 13 | - ⚡️ Async/await based generation with cancellation support
 14 | - 🎨 Modern SwiftUI interface
 15 | - 📝 Comprehensive documentation and comments
 16 | 
 17 | ## Requirements
 18 | 
 19 | - iOS 17.0+ / macOS 14.0+
 20 | - Xcode 15.0+
 21 | - Swift 5.9+
 22 | 
 23 | ## Dependencies
 24 | 
 25 | - MLX: Core machine learning operations
 26 | - MLXLMCommon: Common language model utilities
 27 | - MLXLLM: Large language model support
 28 | - MLXVLM: Vision-language model support
 29 | 
 30 | ## Project Structure
 31 | 
 32 | ```
 33 | MLXChatExample/
 34 | ├── Views/                # SwiftUI views
 35 | ├── Models/               # Data models
 36 | ├── ViewModels/           # Business logic
 37 | ├── Services/             # MLX integration
 38 | └── Support/              # Utilities
 39 | ```
 40 | 
 41 | ## Technical Overview
 42 | 
 43 | The project follows MVVM architecture with clear separation between UI and business logic. The core functionality is split into two main components:
 44 | 
 45 | ### MLXService
 46 | 
 47 | Core service handling all model operations:
 48 | - Model loading and caching with memory management
 49 | - Async text generation with streaming support
 50 | - GPU memory optimization
 51 | - Model state management
 52 | - Handles both LLM and VLM model types
 53 | 
 54 | ### ChatViewModel
 55 | 
 56 | Business logic coordinator:
 57 | - Manages chat state and message history
 58 | - Handles generation lifecycle and cancellation
 59 | - Coordinates media attachments for vision models
 60 | - Performance metrics and error handling
 61 | - Provides clean interface between UI and ML service
 62 | 
 63 | ### Architecture Highlights
 64 | 
 65 | - Complete separation of UI and business logic
 66 | - SwiftUI views with async/await integration
 67 | - Modular design for easy extension
 68 | 
 69 | ### Documentation
 70 | 
 71 | The codebase is thoroughly documented with:
 72 | - Detailed class and method documentation
 73 | - Clear inline comments explaining complex logic
 74 | - DocC documentation format
 75 | 
 76 | ### Markdown Support
 77 | 
 78 | This sample app renders markdown content using SwiftUI's native `Text` view by passing the content as a `LocalizedStringKey`:
 79 | 
 80 | ```swift
 81 | Text(LocalizedStringKey(message.content))
 82 | ```
 83 | 
 84 | #### Limitations and Alternatives
 85 | 
 86 | The default SwiftUI markdown rendering only supports standard markdown syntax. It does not support advanced features like tables and task lists that are available in GitHub Flavored Markdown (GFM).
 87 | 
 88 | For more comprehensive markdown support:
 89 | 
 90 | - **GitHub Flavored Markdown**: Consider using the [swift-markdown-ui](https://github.com/gonzalezreal/swift-markdown-ui) library. However, be aware that this library currently has an [unresolved issue with text selection](https://github.com/gonzalezreal/swift-markdown-ui/issues/264), which is why it wasn't used in this example.
 91 | 
 92 | - **Enhanced Text Selection**: If you're satisfied with standard markdown but want better text selection capabilities on iOS (instead of only being able to select and copy entire content block), consider combining:
 93 |   - [SelectableText](https://github.com/kevinhermawan/SelectableText) for improved selection functionality
 94 |   - [MarkdownToAttributedString](https://github.com/madebywindmill/MarkdownToAttributedString) for markdown formatting
 95 | 
 96 | > More discussion on this can be found on [issue #297](https://github.com/ml-explore/mlx-swift-examples/issues/297)
 97 | 
 98 | ## Getting Started
 99 | 
100 | 1. Clone the repository
101 | 2. Install dependencies
102 | 3. Open in Xcode
103 | 4. Build and run
104 | 
105 | ## Contributing
106 | 
107 | This is an example project demonstrating MLX capabilities. Feel free to use it as a reference for your own projects.
108 | 
109 | ## Acknowledgments
110 | 
111 | - MLX team for the core framework
112 | 


--------------------------------------------------------------------------------
/Applications/MLXChatExample/Services/MLXService.swift:
--------------------------------------------------------------------------------
  1 | //
  2 | //  MLXService.swift
  3 | //  MLXChatExample
  4 | //
  5 | //  Created by İbrahim Çetin on 20.04.2025.
  6 | //
  7 | 
  8 | import Foundation
  9 | import MLX
 10 | import MLXLLM
 11 | import MLXLMCommon
 12 | import MLXVLM
 13 | 
 14 | /// A service class that manages machine learning models for text and vision-language tasks.
 15 | /// This class handles model loading, caching, and text generation using various LLM and VLM models.
 16 | @Observable
 17 | class MLXService {
 18 |     /// List of available models that can be used for generation.
 19 |     /// Includes both language models (LLM) and vision-language models (VLM).
 20 |     static let availableModels: [LMModel] = [
 21 |         LMModel(name: "llama3.2:1b", configuration: LLMRegistry.llama3_2_1B_4bit, type: .llm),
 22 |         LMModel(name: "qwen2.5:1.5b", configuration: LLMRegistry.qwen2_5_1_5b, type: .llm),
 23 |         LMModel(name: "smolLM:135m", configuration: LLMRegistry.smolLM_135M_4bit, type: .llm),
 24 |         LMModel(name: "qwen3:0.6b", configuration: LLMRegistry.qwen3_0_6b_4bit, type: .llm),
 25 |         LMModel(name: "qwen3:1.7b", configuration: LLMRegistry.qwen3_1_7b_4bit, type: .llm),
 26 |         LMModel(name: "qwen3:4b", configuration: LLMRegistry.qwen3_4b_4bit, type: .llm),
 27 |         LMModel(name: "qwen3:8b", configuration: LLMRegistry.qwen3_8b_4bit, type: .llm),
 28 |         LMModel(
 29 |             name: "qwen2.5VL:3b", configuration: VLMRegistry.qwen2_5VL3BInstruct4Bit, type: .vlm),
 30 |         LMModel(name: "qwen2VL:2b", configuration: VLMRegistry.qwen2VL2BInstruct4Bit, type: .vlm),
 31 |         LMModel(name: "smolVLM", configuration: VLMRegistry.smolvlminstruct4bit, type: .vlm),
 32 |         LMModel(name: "acereason:7B", configuration: LLMRegistry.acereason_7b_4bit, type: .llm),
 33 |     ]
 34 | 
 35 |     /// Cache to store loaded model containers to avoid reloading.
 36 |     private let modelCache = NSCache<NSString, ModelContainer>()
 37 | 
 38 |     /// Tracks the current model download progress.
 39 |     /// Access this property to monitor model download status.
 40 |     @MainActor
 41 |     private(set) var modelDownloadProgress: Progress?
 42 | 
 43 |     /// Loads a model from the hub or retrieves it from cache.
 44 |     /// - Parameter model: The model configuration to load
 45 |     /// - Returns: A ModelContainer instance containing the loaded model
 46 |     /// - Throws: Errors that might occur during model loading
 47 |     private func load(model: LMModel) async throws -> ModelContainer {
 48 |         // Set GPU memory limit to prevent out of memory issues
 49 |         MLX.GPU.set(cacheLimit: 20 * 1024 * 1024)
 50 | 
 51 |         // Return cached model if available to avoid reloading
 52 |         if let container = modelCache.object(forKey: model.name as NSString) {
 53 |             return container
 54 |         } else {
 55 |             // Select appropriate factory based on model type
 56 |             let factory: ModelFactory =
 57 |                 switch model.type {
 58 |                 case .llm:
 59 |                     LLMModelFactory.shared
 60 |                 case .vlm:
 61 |                     VLMModelFactory.shared
 62 |                 }
 63 | 
 64 |             // Load model and track download progress
 65 |             let container = try await factory.loadContainer(
 66 |                 hub: .default, configuration: model.configuration
 67 |             ) { progress in
 68 |                 Task { @MainActor in
 69 |                     self.modelDownloadProgress = progress
 70 |                 }
 71 |             }
 72 | 
 73 |             // Cache the loaded model for future use
 74 |             modelCache.setObject(container, forKey: model.name as NSString)
 75 | 
 76 |             return container
 77 |         }
 78 |     }
 79 | 
 80 |     /// Generates text based on the provided messages using the specified model.
 81 |     /// - Parameters:
 82 |     ///   - messages: Array of chat messages including user, assistant, and system messages
 83 |     ///   - model: The language model to use for generation
 84 |     /// - Returns: An AsyncStream of generated text tokens
 85 |     /// - Throws: Errors that might occur during generation
 86 |     func generate(messages: [Message], model: LMModel) async throws -> AsyncStream<Generation> {
 87 |         // Load or retrieve model from cache
 88 |         let modelContainer = try await load(model: model)
 89 | 
 90 |         // Map app-specific Message type to Chat.Message for model input
 91 |         let chat = messages.map { message in
 92 |             let role: Chat.Message.Role =
 93 |                 switch message.role {
 94 |                 case .assistant:
 95 |                     .assistant
 96 |                 case .user:
 97 |                     .user
 98 |                 case .system:
 99 |                     .system
100 |                 }
101 | 
102 |             // Process any attached media for VLM models
103 |             let images: [UserInput.Image] = message.images.map { imageURL in .url(imageURL) }
104 |             let videos: [UserInput.Video] = message.videos.map { videoURL in .url(videoURL) }
105 | 
106 |             return Chat.Message(
107 |                 role: role, content: message.content, images: images, videos: videos)
108 |         }
109 | 
110 |         // Prepare input for model processing
111 |         let userInput = UserInput(
112 |             chat: chat, processing: .init(resize: .init(width: 1024, height: 1024)))
113 | 
114 |         // Generate response using the model
115 |         return try await modelContainer.perform { (context: ModelContext) in
116 |             let lmInput = try await context.processor.prepare(input: userInput)
117 |             // Set temperature for response randomness (0.7 provides good balance)
118 |             let parameters = GenerateParameters(temperature: 0.7)
119 | 
120 |             return try MLXLMCommon.generate(
121 |                 input: lmInput, parameters: parameters, context: context)
122 |         }
123 |     }
124 | }
125 | 


--------------------------------------------------------------------------------
/Applications/MLXChatExample/Support/Assets.xcassets/AccentColor.colorset/Contents.json:
--------------------------------------------------------------------------------
 1 | {
 2 |   "colors" : [
 3 |     {
 4 |       "idiom" : "universal"
 5 |     }
 6 |   ],
 7 |   "info" : {
 8 |     "author" : "xcode",
 9 |     "version" : 1
10 |   }
11 | }
12 | 


--------------------------------------------------------------------------------
/Applications/MLXChatExample/Support/Assets.xcassets/AppIcon.appiconset/Contents.json:
--------------------------------------------------------------------------------
 1 | {
 2 |   "images" : [
 3 |     {
 4 |       "idiom" : "universal",
 5 |       "platform" : "ios",
 6 |       "size" : "1024x1024"
 7 |     },
 8 |     {
 9 |       "appearances" : [
10 |         {
11 |           "appearance" : "luminosity",
12 |           "value" : "dark"
13 |         }
14 |       ],
15 |       "idiom" : "universal",
16 |       "platform" : "ios",
17 |       "size" : "1024x1024"
18 |     },
19 |     {
20 |       "appearances" : [
21 |         {
22 |           "appearance" : "luminosity",
23 |           "value" : "tinted"
24 |         }
25 |       ],
26 |       "idiom" : "universal",
27 |       "platform" : "ios",
28 |       "size" : "1024x1024"
29 |     }
30 |   ],
31 |   "info" : {
32 |     "author" : "xcode",
33 |     "version" : 1
34 |   }
35 | }
36 | 


--------------------------------------------------------------------------------
/Applications/MLXChatExample/Support/Assets.xcassets/Contents.json:
--------------------------------------------------------------------------------
1 | {
2 |   "info" : {
3 |     "author" : "xcode",
4 |     "version" : 1
5 |   }
6 | }
7 | 


--------------------------------------------------------------------------------
/Applications/MLXChatExample/Support/HubApi+default.swift:
--------------------------------------------------------------------------------
 1 | //
 2 | //  HubApi+default.swift
 3 | //  MLXChatExample
 4 | //
 5 | //  Created by İbrahim Çetin on 21.04.2025.
 6 | //
 7 | 
 8 | import Foundation
 9 | @preconcurrency import Hub
10 | 
11 | /// Extension providing a default HubApi instance for downloading model files
12 | extension HubApi {
13 |     /// Default HubApi instance configured to download models to the user's Downloads directory
14 |     /// under a 'huggingface' subdirectory.
15 |     #if os(macOS)
16 |         static let `default` = HubApi(
17 |             downloadBase: URL.downloadsDirectory.appending(path: "huggingface")
18 |         )
19 |     #else
20 |         static let `default` = HubApi(
21 |             downloadBase: URL.cachesDirectory.appending(path: "huggingface")
22 |         )
23 |     #endif
24 | }
25 | 


--------------------------------------------------------------------------------
/Applications/MLXChatExample/Support/Preview Content/Preview Assets.xcassets/Contents.json:
--------------------------------------------------------------------------------
1 | {
2 |   "info" : {
3 |     "author" : "xcode",
4 |     "version" : 1
5 |   }
6 | }
7 | 


--------------------------------------------------------------------------------
/Applications/MLXChatExample/Support/SampleData.swift:
--------------------------------------------------------------------------------
 1 | //
 2 | //  SampleData.swift
 3 | //  MLXChatExample
 4 | //
 5 | //  Created by İbrahim Çetin on 20.04.2025.
 6 | //
 7 | 
 8 | @MainActor
 9 | struct SampleData {
10 |     static let conversation: [Message] = [
11 |         .system("You are a helpful assistant specializing in SwiftUI development."),
12 |         .user("I need help building a weather app in SwiftUI. Where should I start?"),
13 |         .assistant(
14 |             "I'll help you create a weather app! Let's break it down into steps. First, we'll need to design the main view to display current weather conditions. Would you like to start with that?"
15 |         ),
16 |         .user("Yes, that sounds good. What components should I use for the main view?"),
17 |         .assistant(
18 |             "For the main weather view, I recommend using a VStack as the container. You can include:\n\n1. An Image view for the weather icon\n2. Text views for temperature and conditions\n3. HStack for additional metrics like humidity and wind speed"
19 |         ),
20 |         .user("How do I make the UI look modern and polished?"),
21 |         .assistant(
22 |             "To create a modern UI, try these techniques:\n\n- Use SF Symbols for weather icons\n- Add subtle gradients with .background()\n- Include padding and spacing for better layout\n- Implement dark mode support\n\nWould you like to see some example code?"
23 |         ),
24 |         .user("Yes, please show me an example for the main weather view."),
25 |         .assistant(
26 |             "Here's a basic example:\n\nVStack(spacing: 20) {\n    Image(systemName: \"sun.max.fill\")\n        .symbolRenderingMode(.multicolor)\n        .font(.system(size: 64))\n    \n    Text(\"72°\")\n        .font(.largeTitle)\n        .bold()\n    \n    Text(\"Sunny\")\n        .font(.title2)\n}\n.padding()\n.background(.ultraThinMaterial)\n.clipShape(RoundedRectangle(cornerRadius: 20))"
27 |         ),
28 |         .user("That looks great! How would I add animations to this?"),
29 |         .assistant(
30 |             "We can add smooth animations using SwiftUI's animation modifiers. For example:\n\n1. Use withAnimation for state changes\n2. Add .animation() modifier to views\n3. Implement transitions\n\nWould you like to see how to animate the weather changes?"
31 |         ),
32 |     ]
33 | }
34 | 


--------------------------------------------------------------------------------
/Applications/MLXChatExample/ViewModels/ChatViewModel.swift:
--------------------------------------------------------------------------------
  1 | //
  2 | //  ChatViewModel.swift
  3 | //  MLXChatExample
  4 | //
  5 | //  Created by İbrahim Çetin on 20.04.2025.
  6 | //
  7 | 
  8 | import Foundation
  9 | import MLXLMCommon
 10 | import UniformTypeIdentifiers
 11 | 
 12 | /// ViewModel that manages the chat interface and coordinates with MLXService for text generation.
 13 | /// Handles user input, message history, media attachments, and generation state.
 14 | @Observable
 15 | @MainActor
 16 | class ChatViewModel {
 17 |     /// Service responsible for ML model operations
 18 |     private let mlxService: MLXService
 19 | 
 20 |     init(mlxService: MLXService) {
 21 |         self.mlxService = mlxService
 22 |     }
 23 | 
 24 |     /// Current user input text
 25 |     var prompt: String = ""
 26 | 
 27 |     /// Chat history containing system, user, and assistant messages
 28 |     var messages: [Message] = [
 29 |         .system("You are a helpful assistant!")
 30 |     ]
 31 | 
 32 |     /// Currently selected language model for generation
 33 |     var selectedModel: LMModel = MLXService.availableModels.first!
 34 | 
 35 |     /// Manages image and video attachments for the current message
 36 |     var mediaSelection = MediaSelection()
 37 | 
 38 |     /// Indicates if text generation is in progress
 39 |     var isGenerating = false
 40 | 
 41 |     /// Current generation task, used for cancellation
 42 |     private var generateTask: Task<Void, any Error>?
 43 | 
 44 |     /// Stores performance metrics from the current generation
 45 |     private var generateCompletionInfo: GenerateCompletionInfo?
 46 | 
 47 |     /// Current generation speed in tokens per second
 48 |     var tokensPerSecond: Double {
 49 |         generateCompletionInfo?.tokensPerSecond ?? 0
 50 |     }
 51 | 
 52 |     /// Progress of the current model download, if any
 53 |     var modelDownloadProgress: Progress? {
 54 |         mlxService.modelDownloadProgress
 55 |     }
 56 | 
 57 |     /// Most recent error message, if any
 58 |     var errorMessage: String?
 59 | 
 60 |     /// Generates response for the current prompt and media attachments
 61 |     func generate() async {
 62 |         // Cancel any existing generation task
 63 |         if let existingTask = generateTask {
 64 |             existingTask.cancel()
 65 |             generateTask = nil
 66 |         }
 67 | 
 68 |         isGenerating = true
 69 | 
 70 |         // Add user message with any media attachments
 71 |         messages.append(.user(prompt, images: mediaSelection.images, videos: mediaSelection.videos))
 72 |         // Add empty assistant message that will be filled during generation
 73 |         messages.append(.assistant(""))
 74 | 
 75 |         // Clear the input after sending
 76 |         clear(.prompt)
 77 | 
 78 |         generateTask = Task {
 79 |             // Process generation chunks and update UI
 80 |             for await generation in try await mlxService.generate(
 81 |                 messages: messages, model: selectedModel)
 82 |             {
 83 |                 switch generation {
 84 |                 case .chunk(let chunk):
 85 |                     // Append new text to the current assistant message
 86 |                     if let assistantMessage = messages.last {
 87 |                         assistantMessage.content += chunk
 88 |                     }
 89 |                 case .info(let info):
 90 |                     // Update performance metrics
 91 |                     generateCompletionInfo = info
 92 |                 case .toolCall(let call):
 93 |                     break
 94 |                 }
 95 |             }
 96 |         }
 97 | 
 98 |         do {
 99 |             // Handle task completion and cancellation
100 |             try await withTaskCancellationHandler {
101 |                 try await generateTask?.value
102 |             } onCancel: {
103 |                 Task { @MainActor in
104 |                     generateTask?.cancel()
105 | 
106 |                     // Mark message as cancelled
107 |                     if let assistantMessage = messages.last {
108 |                         assistantMessage.content += "\n[Cancelled]"
109 |                     }
110 |                 }
111 |             }
112 |         } catch {
113 |             errorMessage = error.localizedDescription
114 |         }
115 | 
116 |         isGenerating = false
117 |         generateTask = nil
118 |     }
119 | 
120 |     /// Processes and adds media attachments to the current message
121 |     func addMedia(_ result: Result<URL, any Error>) {
122 |         do {
123 |             let url = try result.get()
124 | 
125 |             // Determine media type and add to appropriate collection
126 |             if let mediaType = UTType(filenameExtension: url.pathExtension) {
127 |                 if mediaType.conforms(to: .image) {
128 |                     mediaSelection.images = [url]
129 |                 } else if mediaType.conforms(to: .movie) {
130 |                     mediaSelection.videos = [url]
131 |                 }
132 |             }
133 |         } catch {
134 |             errorMessage = "Failed to load media item.\n\nError: \(error)"
135 |         }
136 |     }
137 | 
138 |     /// Clears various aspects of the chat state based on provided options
139 |     func clear(_ options: ClearOption) {
140 |         if options.contains(.prompt) {
141 |             prompt = ""
142 |             mediaSelection = .init()
143 |         }
144 | 
145 |         if options.contains(.chat) {
146 |             messages = []
147 |             generateTask?.cancel()
148 |         }
149 | 
150 |         if options.contains(.meta) {
151 |             generateCompletionInfo = nil
152 |         }
153 | 
154 |         errorMessage = nil
155 |     }
156 | }
157 | 
158 | /// Manages the state of media attachments in the chat
159 | @Observable
160 | class MediaSelection {
161 |     /// Controls visibility of media selection UI
162 |     var isShowing = false
163 | 
164 |     /// Currently selected image URLs
165 |     var images: [URL] = [] {
166 |         didSet {
167 |             didSetURLs(oldValue, images)
168 |         }
169 |     }
170 | 
171 |     /// Currently selected video URLs
172 |     var videos: [URL] = [] {
173 |         didSet {
174 |             didSetURLs(oldValue, videos)
175 |         }
176 |     }
177 | 
178 |     /// Whether any media is currently selected
179 |     var isEmpty: Bool {
180 |         images.isEmpty && videos.isEmpty
181 |     }
182 | 
183 |     private func didSetURLs(_ old: [URL], _ new: [URL]) {
184 |         // the urls we get from fileImporter require SSB calls to access
185 |         new.filter { !old.contains($0) }.forEach { _ = $0.startAccessingSecurityScopedResource() }
186 |         old.filter { !new.contains($0) }.forEach { $0.stopAccessingSecurityScopedResource() }
187 |     }
188 | }
189 | 
190 | /// Options for clearing different aspects of the chat state
191 | struct ClearOption: RawRepresentable, OptionSet {
192 |     let rawValue: Int
193 | 
194 |     /// Clears current prompt and media selection
195 |     static let prompt = ClearOption(rawValue: 1 << 0)
196 |     /// Clears chat history and cancels generation
197 |     static let chat = ClearOption(rawValue: 1 << 1)
198 |     /// Clears generation metadata
199 |     static let meta = ClearOption(rawValue: 1 << 2)
200 | }
201 | 


--------------------------------------------------------------------------------
/Applications/MLXChatExample/Views/ConversationView.swift:
--------------------------------------------------------------------------------
 1 | //
 2 | //  ConversationView.swift
 3 | //  MLXChatExample
 4 | //
 5 | //  Created by İbrahim Çetin on 20.04.2025.
 6 | //
 7 | 
 8 | import SwiftUI
 9 | 
10 | /// Displays the chat conversation as a scrollable list of messages.
11 | struct ConversationView: View {
12 |     /// Array of messages to display in the conversation
13 |     let messages: [Message]
14 | 
15 |     var body: some View {
16 |         ScrollView {
17 |             LazyVStack(spacing: 12) {
18 |                 ForEach(messages) { message in
19 |                     MessageView(message)
20 |                         .padding(.horizontal, 12)
21 |                 }
22 |             }
23 |         }
24 |         .padding(.vertical, 8)
25 |         .defaultScrollAnchor(.bottom, for: .sizeChanges)
26 |     }
27 | }
28 | 
29 | #Preview {
30 |     // Display sample conversation in preview
31 |     ConversationView(messages: SampleData.conversation)
32 | }
33 | 


--------------------------------------------------------------------------------
/Applications/MLXChatExample/Views/MediaPreviewView.swift:
--------------------------------------------------------------------------------
  1 | //
  2 | //  MediaPreviewView.swift
  3 | //  MLXChatExample
  4 | //
  5 | //  Created by İbrahim Çetin on 21.04.2025.
  6 | //
  7 | 
  8 | import AVFoundation
  9 | import AVKit
 10 | import SwiftUI
 11 | 
 12 | /// A view that displays a horizontal scrollable list of media previews (images and videos).
 13 | struct MediaPreviewsView: View {
 14 |     /// The media selection containing arrays of image and video URLs
 15 |     let mediaSelection: MediaSelection
 16 | 
 17 |     var body: some View {
 18 |         ScrollView(.horizontal) {
 19 |             HStack(spacing: 8) {
 20 |                 // Display image previews
 21 |                 ForEach(mediaSelection.images, id: \.self) { imageURL in
 22 |                     MediaPreviewView(
 23 |                         mediaURL: imageURL,
 24 |                         type: .image,
 25 |                         onRemove: {
 26 |                             mediaSelection.images.removeAll(where: { $0 == imageURL })
 27 |                         }
 28 |                     )
 29 |                 }
 30 | 
 31 |                 // Display video previews
 32 |                 ForEach(mediaSelection.videos, id: \.self) { videoURL in
 33 |                     MediaPreviewView(
 34 |                         mediaURL: videoURL,
 35 |                         type: .video,
 36 |                         onRemove: {
 37 |                             mediaSelection.videos.removeAll(where: { $0 == videoURL })
 38 |                         }
 39 |                     )
 40 |                 }
 41 |             }
 42 |             .padding(.horizontal)
 43 |         }
 44 |         .padding(.top)
 45 |     }
 46 | }
 47 | 
 48 | /// A view that displays a single media item (image or video) with a remove button.
 49 | struct MediaPreviewView: View {
 50 |     /// URL of the media file to display
 51 |     let mediaURL: URL
 52 |     /// Type of media (image or video)
 53 |     let type: MediaPreviewType
 54 |     /// Callback to handle removal of the media item
 55 |     let onRemove: () -> Void
 56 | 
 57 |     var body: some View {
 58 |         ZStack(alignment: .topTrailing) {
 59 |             switch type {
 60 |             case .image:
 61 |                 // Display image with loading placeholder
 62 |                 AsyncImage(url: mediaURL) { image in
 63 |                     image
 64 |                         .resizable()
 65 |                         .scaledToFit()
 66 |                         .frame(height: 100)
 67 |                         .clipShape(RoundedRectangle(cornerRadius: 8))
 68 |                 } placeholder: {
 69 |                     ProgressView()
 70 |                         .frame(width: 150, height: 100)
 71 |                 }
 72 |             case .video:
 73 |                 // Display video player
 74 |                 VideoPlayer(player: AVPlayer(url: mediaURL))
 75 |                     .frame(width: 150, height: 100)
 76 |                     .clipShape(RoundedRectangle(cornerRadius: 8))
 77 |             }
 78 | 
 79 |             RemoveButton(action: onRemove)
 80 |         }
 81 |     }
 82 | }
 83 | 
 84 | /// A button for removing media items from the preview.
 85 | struct RemoveButton: View {
 86 |     /// Action to perform when the remove button is tapped
 87 |     let action: () -> Void
 88 | 
 89 |     var body: some View {
 90 |         Button(action: action) {
 91 |             Image(systemName: "xmark.circle.fill")
 92 |                 .foregroundStyle(.secondary)
 93 |                 .imageScale(.large)
 94 |         }
 95 |         .buttonStyle(.plain)
 96 |         .padding(4)
 97 |     }
 98 | }
 99 | 
100 | extension MediaPreviewView {
101 |     /// Defines the type of media that can be displayed in the preview
102 |     enum MediaPreviewType {
103 |         /// An image file
104 |         case image
105 |         /// A video file
106 |         case video
107 |     }
108 | }
109 | 
110 | #Preview("Remove Button") {
111 |     RemoveButton {}
112 | }
113 | 


--------------------------------------------------------------------------------
/Applications/MLXChatExample/Views/MessageView.swift:
--------------------------------------------------------------------------------
 1 | //
 2 | //  MessageView.swift
 3 | //  MLXChatExample
 4 | //
 5 | //  Created by İbrahim Çetin on 20.04.2025.
 6 | //
 7 | 
 8 | import AVKit
 9 | import SwiftUI
10 | 
11 | /// A view that displays a single message in the chat interface.
12 | /// Supports different message roles (user, assistant, system) and media attachments.
13 | struct MessageView: View {
14 |     /// The message to be displayed
15 |     let message: Message
16 | 
17 |     /// Creates a message view
18 |     /// - Parameter message: The message model to display
19 |     init(_ message: Message) {
20 |         self.message = message
21 |     }
22 | 
23 |     var body: some View {
24 |         switch message.role {
25 |         case .user:
26 |             // User messages are right-aligned with blue background
27 |             HStack {
28 |                 Spacer()
29 |                 VStack(alignment: .trailing, spacing: 8) {
30 |                     // Display first image if present
31 |                     if let firstImage = message.images.first {
32 |                         AsyncImage(url: firstImage) { image in
33 |                             image
34 |                                 .resizable()
35 |                                 .aspectRatio(contentMode: .fill)
36 |                         } placeholder: {
37 |                             ProgressView()
38 |                         }
39 |                         .frame(maxWidth: 250, maxHeight: 200)
40 |                         .clipShape(.rect(cornerRadius: 12))
41 |                     }
42 | 
43 |                     // Display first video if present
44 |                     if let firstVideo = message.videos.first {
45 |                         VideoPlayer(player: AVPlayer(url: firstVideo))
46 |                             .frame(width: 250, height: 340)
47 |                             .clipShape(.rect(cornerRadius: 12))
48 |                     }
49 | 
50 |                     // Message content with tinted background.
51 |                     // LocalizedStringKey used to trigger default handling of markdown content.
52 |                     Text(LocalizedStringKey(message.content))
53 |                         .padding(.vertical, 8)
54 |                         .padding(.horizontal, 12)
55 |                         .background(.tint, in: .rect(cornerRadius: 16))
56 |                         .textSelection(.enabled)
57 |                 }
58 |             }
59 | 
60 |         case .assistant:
61 |             // Assistant messages are left-aligned without background
62 |             // LocalizedStringKey used to trigger default handling of markdown content.
63 |             HStack {
64 |                 Text(LocalizedStringKey(message.content))
65 |                     .textSelection(.enabled)
66 | 
67 |                 Spacer()
68 |             }
69 | 
70 |         case .system:
71 |             // System messages are centered with computer icon
72 |             Label(message.content, systemImage: "desktopcomputer")
73 |                 .font(.headline)
74 |                 .foregroundColor(.secondary)
75 |                 .frame(maxWidth: .infinity, alignment: .center)
76 |         }
77 |     }
78 | }
79 | 
80 | #Preview {
81 |     VStack(spacing: 20) {
82 |         MessageView(.system("You are a helpful assistant."))
83 | 
84 |         MessageView(
85 |             .user(
86 |                 "Here's a photo",
87 |                 images: [URL(string: "https://picsum.photos/200")!]
88 |             )
89 |         )
90 | 
91 |         MessageView(.assistant("I see your photo!"))
92 |     }
93 |     .padding()
94 | }
95 | 


--------------------------------------------------------------------------------
/Applications/MLXChatExample/Views/PromptField.swift:
--------------------------------------------------------------------------------
 1 | //
 2 | //  PromptField.swift
 3 | //  MLXChatExample
 4 | //
 5 | //  Created by İbrahim Çetin on 20.04.2025.
 6 | //
 7 | 
 8 | import SwiftUI
 9 | 
10 | struct PromptField: View {
11 |     @Binding var prompt: String
12 |     @State private var task: Task<Void, Never>?
13 | 
14 |     let sendButtonAction: () async -> Void
15 |     let mediaButtonAction: (() -> Void)?
16 | 
17 |     var body: some View {
18 |         HStack {
19 |             if let mediaButtonAction {
20 |                 Button(action: mediaButtonAction) {
21 |                     Image(systemName: "photo.badge.plus")
22 |                 }
23 |             }
24 | 
25 |             TextField("Prompt", text: $prompt)
26 |                 .textFieldStyle(.roundedBorder)
27 | 
28 |             Button {
29 |                 if isRunning {
30 |                     task?.cancel()
31 |                     removeTask()
32 |                 } else {
33 |                     task = Task {
34 |                         await sendButtonAction()
35 |                         removeTask()
36 |                     }
37 |                 }
38 |             } label: {
39 |                 Image(systemName: isRunning ? "stop.circle.fill" : "paperplane.fill")
40 |             }
41 |             .keyboardShortcut(isRunning ? .cancelAction : .defaultAction)
42 |         }
43 |     }
44 | 
45 |     private var isRunning: Bool {
46 |         task != nil && !(task!.isCancelled)
47 |     }
48 | 
49 |     private func removeTask() {
50 |         task = nil
51 |     }
52 | }
53 | 
54 | #Preview {
55 |     PromptField(prompt: .constant("")) {
56 |     } mediaButtonAction: {
57 |     }
58 | }
59 | 


--------------------------------------------------------------------------------
/Applications/MLXChatExample/Views/Toolbar/ChatToolbarView.swift:
--------------------------------------------------------------------------------
 1 | //
 2 | //  ChatToolbarView.swift
 3 | //  MLXChatExample
 4 | //
 5 | //  Created by İbrahim Çetin on 21.04.2025.
 6 | //
 7 | 
 8 | import SwiftUI
 9 | 
10 | /// Toolbar view for the chat interface that displays error messages, download progress,
11 | /// generation statistics, and model selection controls.
12 | struct ChatToolbarView: View {
13 |     /// View model containing the chat state and controls
14 |     @Bindable var vm: ChatViewModel
15 | 
16 |     var body: some View {
17 |         // Display error message if present
18 |         if let errorMessage = vm.errorMessage {
19 |             ErrorView(errorMessage: errorMessage)
20 |         }
21 | 
22 |         // Show download progress for model loading
23 |         if let progress = vm.modelDownloadProgress, !progress.isFinished {
24 |             DownloadProgressView(progress: progress)
25 |         }
26 | 
27 |         // Button to clear chat history, displays generation statistics
28 |         Button {
29 |             vm.clear([.chat, .meta])
30 |         } label: {
31 |             GenerationInfoView(
32 |                 tokensPerSecond: vm.tokensPerSecond
33 |             )
34 |         }
35 | 
36 |         // Model selection picker
37 |         Picker("Model", selection: $vm.selectedModel) {
38 |             ForEach(MLXService.availableModels) { model in
39 |                 Text(model.displayName)
40 |                     .tag(model)
41 |             }
42 |         }
43 |     }
44 | }
45 | 


--------------------------------------------------------------------------------
/Applications/MLXChatExample/Views/Toolbar/DownloadProgressView.swift:
--------------------------------------------------------------------------------
 1 | //
 2 | //  DownloadProgressView.swift
 3 | //  MLXChatExample
 4 | //
 5 | //  Created by İbrahim Çetin on 21.04.2025.
 6 | //
 7 | 
 8 | import SwiftUI
 9 | 
10 | struct DownloadProgressView: View {
11 |     let progress: Progress
12 | 
13 |     @State private var isShowingDownload = false
14 | 
15 |     var body: some View {
16 |         Button {
17 |             isShowingDownload = true
18 |         } label: {
19 |             Image(systemName: "arrow.down.square")
20 |                 .foregroundStyle(.tint)
21 |         }
22 |         .popover(isPresented: $isShowingDownload, arrowEdge: .bottom) {
23 |             VStack {
24 |                 ProgressView(value: progress.fractionCompleted) {
25 |                     HStack {
26 |                         Text(progress.localizedAdditionalDescription)
27 |                             .bold()
28 |                         Spacer()
29 |                         Text(progress.localizedDescription)
30 |                     }
31 |                 }
32 | 
33 |                 Text("The model is downloading")
34 |                     .padding(.horizontal, 32)
35 |             }
36 |             .padding()
37 |         }
38 |     }
39 | }
40 | 
41 | #Preview {
42 |     DownloadProgressView(progress: Progress(totalUnitCount: 6))
43 | }
44 | 


--------------------------------------------------------------------------------
/Applications/MLXChatExample/Views/Toolbar/ErrorView.swift:
--------------------------------------------------------------------------------
 1 | //
 2 | //  ErrorView.swift
 3 | //  MLXChatExample
 4 | //
 5 | //  Created by İbrahim Çetin on 21.04.2025.
 6 | //
 7 | 
 8 | import SwiftUI
 9 | 
10 | struct ErrorView: View {
11 |     let errorMessage: String
12 | 
13 |     @State private var isShowingError = false
14 | 
15 |     var body: some View {
16 |         Button {
17 |             isShowingError = true
18 |         } label: {
19 |             Image(systemName: "exclamationmark.triangle")
20 |                 .foregroundStyle(.red)
21 |         }
22 |         .popover(isPresented: $isShowingError, arrowEdge: .bottom) {
23 |             Text(errorMessage)
24 |                 .padding()
25 |         }
26 |     }
27 | }
28 | 
29 | #Preview {
30 |     ErrorView(errorMessage: "Something went wrong!")
31 | }
32 | 


--------------------------------------------------------------------------------
/Applications/MLXChatExample/Views/Toolbar/GenerationInfoView.swift:
--------------------------------------------------------------------------------
 1 | //
 2 | //  GenerationInfoView.swift
 3 | //  MLXChatExample
 4 | //
 5 | //  Created by İbrahim Çetin on 21.04.2025.
 6 | //
 7 | 
 8 | import SwiftUI
 9 | 
10 | struct GenerationInfoView: View {
11 |     let tokensPerSecond: Double
12 | 
13 |     var body: some View {
14 |         Text("\(tokensPerSecond, format: .number.precision(.fractionLength(2))) tokens/s")
15 |     }
16 | }
17 | 
18 | #Preview {
19 |     GenerationInfoView(tokensPerSecond: 58.5834)
20 | }
21 | 


--------------------------------------------------------------------------------
/Applications/MNISTTrainer/Assets.xcassets/AccentColor.colorset/Contents.json:
--------------------------------------------------------------------------------
 1 | {
 2 |   "colors" : [
 3 |     {
 4 |       "idiom" : "universal"
 5 |     }
 6 |   ],
 7 |   "info" : {
 8 |     "author" : "xcode",
 9 |     "version" : 1
10 |   }
11 | }
12 | 


--------------------------------------------------------------------------------
/Applications/MNISTTrainer/Assets.xcassets/AppIcon.appiconset/Contents.json:
--------------------------------------------------------------------------------
 1 | {
 2 |   "images" : [
 3 |     {
 4 |       "idiom" : "universal",
 5 |       "platform" : "ios",
 6 |       "size" : "1024x1024"
 7 |     },
 8 |     {
 9 |       "idiom" : "mac",
10 |       "scale" : "1x",
11 |       "size" : "16x16"
12 |     },
13 |     {
14 |       "idiom" : "mac",
15 |       "scale" : "2x",
16 |       "size" : "16x16"
17 |     },
18 |     {
19 |       "idiom" : "mac",
20 |       "scale" : "1x",
21 |       "size" : "32x32"
22 |     },
23 |     {
24 |       "idiom" : "mac",
25 |       "scale" : "2x",
26 |       "size" : "32x32"
27 |     },
28 |     {
29 |       "idiom" : "mac",
30 |       "scale" : "1x",
31 |       "size" : "128x128"
32 |     },
33 |     {
34 |       "idiom" : "mac",
35 |       "scale" : "2x",
36 |       "size" : "128x128"
37 |     },
38 |     {
39 |       "idiom" : "mac",
40 |       "scale" : "1x",
41 |       "size" : "256x256"
42 |     },
43 |     {
44 |       "idiom" : "mac",
45 |       "scale" : "2x",
46 |       "size" : "256x256"
47 |     },
48 |     {
49 |       "idiom" : "mac",
50 |       "scale" : "1x",
51 |       "size" : "512x512"
52 |     },
53 |     {
54 |       "idiom" : "mac",
55 |       "scale" : "2x",
56 |       "size" : "512x512"
57 |     }
58 |   ],
59 |   "info" : {
60 |     "author" : "xcode",
61 |     "version" : 1
62 |   }
63 | }
64 | 


--------------------------------------------------------------------------------
/Applications/MNISTTrainer/Assets.xcassets/Contents.json:
--------------------------------------------------------------------------------
1 | {
2 |   "info" : {
3 |     "author" : "xcode",
4 |     "version" : 1
5 |   }
6 | }
7 | 


--------------------------------------------------------------------------------
/Applications/MNISTTrainer/ContentView.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import MLX
  4 | import MLXMNIST
  5 | import MLXNN
  6 | import MLXOptimizers
  7 | import SwiftUI
  8 | 
  9 | struct TrainingView: View {
 10 | 
 11 |     @Binding var trainer: ModelState
 12 | 
 13 |     var body: some View {
 14 |         VStack {
 15 |             Spacer()
 16 | 
 17 |             ScrollView(.vertical) {
 18 |                 ForEach(trainer.messages, id: \.self) {
 19 |                     Text($0)
 20 |                 }
 21 |             }
 22 | 
 23 |             HStack {
 24 |                 Spacer()
 25 |                 switch trainer.state {
 26 |                 case .untrained:
 27 |                     Button("Train") {
 28 |                         Task {
 29 |                             try! await trainer.train()
 30 |                         }
 31 |                     }
 32 |                 case .trained(let model), .predict(let model):
 33 |                     Button("Draw a digit") {
 34 |                         trainer.state = .predict(model)
 35 |                     }
 36 |                 }
 37 | 
 38 |                 Spacer()
 39 |             }
 40 |             Spacer()
 41 |         }
 42 |         .padding()
 43 |     }
 44 | }
 45 | 
 46 | struct ContentView: View {
 47 |     // the training loop
 48 |     @State var trainer = ModelState()
 49 | 
 50 |     var body: some View {
 51 |         switch trainer.state {
 52 |         case .untrained, .trained:
 53 |             TrainingView(trainer: $trainer)
 54 |         case .predict(let model):
 55 |             PredictionView(model: model)
 56 |         }
 57 |     }
 58 | }
 59 | 
 60 | @MainActor
 61 | @Observable
 62 | class ModelState {
 63 | 
 64 |     enum State {
 65 |         case untrained
 66 |         case trained(LeNetContainer)
 67 |         case predict(LeNetContainer)
 68 |     }
 69 | 
 70 |     var state: State = .untrained
 71 |     var messages = [String]()
 72 | 
 73 |     func train() async throws {
 74 |         let model = LeNetContainer()
 75 |         try await model.train(output: self)
 76 |         self.state = .trained(model)
 77 |     }
 78 | }
 79 | 
 80 | actor LeNetContainer {
 81 | 
 82 |     private let model = LeNet()
 83 | 
 84 |     let mnistImageSize: CGSize = CGSize(width: 28, height: 28)
 85 | 
 86 |     func train(output: ModelState) async throws {
 87 |         // Note: this is pretty close to the code in `mnist-tool`, just
 88 |         // wrapped in an Observable to make it easy to display in SwiftUI
 89 | 
 90 |         // download & load the training data
 91 |         let url = URL(fileURLWithPath: NSTemporaryDirectory(), isDirectory: true)
 92 |         try await download(into: url)
 93 |         let data = try load(from: url)
 94 | 
 95 |         let trainImages = data[.init(.training, .images)]!
 96 |         let trainLabels = data[.init(.training, .labels)]!
 97 |         let testImages = data[.init(.test, .images)]!
 98 |         let testLabels = data[.init(.test, .labels)]!
 99 | 
100 |         eval(model.parameters())
101 | 
102 |         // the training loop
103 |         let lg = valueAndGrad(model: model, loss)
104 |         let optimizer = SGD(learningRate: 0.1)
105 | 
106 |         // using a consistent random seed so it behaves the same way each time
107 |         MLXRandom.seed(0)
108 |         var generator: RandomNumberGenerator = SplitMix64(seed: 0)
109 | 
110 |         for e in 0 ..< 10 {
111 |             let start = Date.timeIntervalSinceReferenceDate
112 | 
113 |             for (x, y) in iterateBatches(
114 |                 batchSize: 256, x: trainImages, y: trainLabels, using: &generator)
115 |             {
116 |                 // loss and gradients
117 |                 let (_, grads) = lg(model, x, y)
118 | 
119 |                 // use SGD to update the weights
120 |                 optimizer.update(model: model, gradients: grads)
121 | 
122 |                 // eval the parameters so the next iteration is independent
123 |                 eval(model, optimizer)
124 |             }
125 | 
126 |             let accuracy = eval(model: model, x: testImages, y: testLabels)
127 | 
128 |             let end = Date.timeIntervalSinceReferenceDate
129 | 
130 |             // add to messages -- triggers display
131 |             let accuracyItem = accuracy.item(Float.self)
132 |             await MainActor.run {
133 |                 output.messages.append(
134 |                     """
135 |                     Epoch \(e): test accuracy \(accuracyItem.formatted())
136 |                     Time: \((end - start).formatted())
137 | 
138 |                     """
139 |                 )
140 |             }
141 |         }
142 |     }
143 | 
144 |     func evaluate(image: CGImage) -> Int? {
145 |         let pixelData = image.grayscaleImage(with: mnistImageSize)?.pixelData()
146 |         if let pixelData {
147 |             let x = pixelData.reshaped([1, 28, 28, 1]).asType(.float32) / 255.0
148 |             return argMax(model(x)).item()
149 |         } else {
150 |             return nil
151 |         }
152 |     }
153 | }
154 | 


--------------------------------------------------------------------------------
/Applications/MNISTTrainer/MNISTTrainer-Info.plist:
--------------------------------------------------------------------------------
1 | <?xml version="1.0" encoding="UTF-8"?>
2 | <!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
3 | <plist version="1.0">
4 | <dict/>
5 | </plist>
6 | 


--------------------------------------------------------------------------------
/Applications/MNISTTrainer/MNISTTrainer.entitlements:
--------------------------------------------------------------------------------
 1 | <?xml version="1.0" encoding="UTF-8"?>
 2 | <!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
 3 | <plist version="1.0">
 4 | <dict>
 5 | 	<key>com.apple.security.app-sandbox</key>
 6 | 	<true/>
 7 | 	<key>com.apple.security.files.user-selected.read-only</key>
 8 | 	<true/>
 9 | 	<key>com.apple.security.network.client</key>
10 | 	<true/>
11 | </dict>
12 | </plist>
13 | 


--------------------------------------------------------------------------------
/Applications/MNISTTrainer/MNISTTrainerApp.swift:
--------------------------------------------------------------------------------
 1 | // Copyright © 2024 Apple Inc.
 2 | 
 3 | import SwiftUI
 4 | 
 5 | @main
 6 | struct MNISTTrainerApp: App {
 7 |     var body: some Scene {
 8 |         WindowGroup {
 9 |             ContentView()
10 |         }
11 |     }
12 | }
13 | 


--------------------------------------------------------------------------------
/Applications/MNISTTrainer/PredictionView.swift:
--------------------------------------------------------------------------------
  1 | //
  2 | //  PredictionView.swift
  3 | //  MNISTTrainer
  4 | //
  5 | //  Created by Rounak Jain on 3/9/24.
  6 | //
  7 | 
  8 | import MLX
  9 | import MLXMNIST
 10 | import MLXNN
 11 | import SwiftUI
 12 | 
 13 | struct Canvas: View {
 14 | 
 15 |     @Binding var path: Path
 16 |     @State var lastPoint: CGPoint?
 17 | 
 18 |     var body: some View {
 19 |         path
 20 |             .stroke(.white, lineWidth: 10)
 21 |             .background(.black)
 22 |             .gesture(
 23 |                 DragGesture(minimumDistance: 0.05)
 24 |                     .onChanged { touch in
 25 |                         add(point: touch.location)
 26 |                     }
 27 |                     .onEnded { touch in
 28 |                         lastPoint = nil
 29 |                     }
 30 |             )
 31 |     }
 32 | 
 33 |     func add(point: CGPoint) {
 34 |         var newPath = path
 35 |         if let lastPoint {
 36 |             newPath.move(to: lastPoint)
 37 |             newPath.addLine(to: point)
 38 |         } else {
 39 |             newPath.move(to: point)
 40 |         }
 41 |         self.path = newPath
 42 |         lastPoint = point
 43 |     }
 44 | }
 45 | 
 46 | extension Path {
 47 |     mutating func center(to newMidPoint: CGPoint) {
 48 |         let middleX = boundingRect.midX
 49 |         let middleY = boundingRect.midY
 50 |         self = offsetBy(dx: newMidPoint.x - middleX, dy: newMidPoint.y - middleY)
 51 |     }
 52 | }
 53 | 
 54 | struct PredictionView: View {
 55 |     @State var path: Path = Path()
 56 |     @State var prediction: Int?
 57 |     let model: LeNetContainer
 58 |     let canvasSize = 150.0
 59 |     let mnistImageSize: CGSize = CGSize(width: 28, height: 28)
 60 | 
 61 |     var body: some View {
 62 |         VStack {
 63 |             if let prediction {
 64 |                 Text("You've drawn a \(prediction)")
 65 |             } else {
 66 |                 Text("Draw a digit")
 67 |             }
 68 |             Canvas(path: $path)
 69 |                 .frame(width: canvasSize, height: canvasSize)
 70 |             HStack {
 71 |                 Button("Predict") {
 72 |                     path.center(to: CGPoint(x: canvasSize / 2, y: canvasSize / 2))
 73 |                     predict()
 74 |                 }
 75 |                 Button("Clear") {
 76 |                     path = Path()
 77 |                     prediction = nil
 78 |                 }
 79 |             }
 80 |         }
 81 |     }
 82 | 
 83 |     @MainActor
 84 |     func predict() {
 85 |         let imageRenderer = ImageRenderer(
 86 |             content: Canvas(path: $path).frame(width: 150, height: 150))
 87 | 
 88 |         if let image = imageRenderer.cgImage {
 89 |             Task {
 90 |                 self.prediction = await model.evaluate(image: image)
 91 |             }
 92 |         }
 93 |     }
 94 | }
 95 | 
 96 | extension CGImage {
 97 |     func grayscaleImage(with newSize: CGSize) -> CGImage? {
 98 |         let colorSpace = CGColorSpaceCreateDeviceGray()
 99 |         let bitmapInfo = CGBitmapInfo(rawValue: CGImageAlphaInfo.none.rawValue)
100 | 
101 |         guard
102 |             let context = CGContext(
103 |                 data: nil,
104 |                 width: Int(newSize.width),
105 |                 height: Int(newSize.height),
106 |                 bitsPerComponent: 8,
107 |                 bytesPerRow: Int(newSize.width),
108 |                 space: colorSpace,
109 |                 bitmapInfo: bitmapInfo.rawValue)
110 |         else {
111 |             return nil
112 |         }
113 |         context.draw(self, in: CGRect(x: 0, y: 0, width: newSize.width, height: newSize.width))
114 |         return context.makeImage()
115 |     }
116 | 
117 |     func pixelData() -> MLXArray {
118 |         guard let data = self.dataProvider?.data else {
119 |             return []
120 |         }
121 |         let bytePtr = CFDataGetBytePtr(data)
122 |         let count = CFDataGetLength(data)
123 |         return MLXArray(UnsafeBufferPointer(start: bytePtr, count: count))
124 |     }
125 | }
126 | 


--------------------------------------------------------------------------------
/Applications/MNISTTrainer/Preview Content/Preview Assets.xcassets/Contents.json:
--------------------------------------------------------------------------------
1 | {
2 |   "info" : {
3 |     "author" : "xcode",
4 |     "version" : 1
5 |   }
6 | }
7 | 


--------------------------------------------------------------------------------
/Applications/MNISTTrainer/README.md:
--------------------------------------------------------------------------------
 1 | #  MNISTTrainer
 2 | 
 3 | This is an example of model training that works on both macOS and iOS.
 4 | The example will download the MNIST training data, create a LeNet, and train
 5 | it. It will show the epoch time and test accuracy as it trains.
 6 | 
 7 | You will need to set the Team on the MNISTTrainer target in order to build and
 8 | run on iOS.
 9 | 
10 | Some notes about the setup:
11 | 
12 | - This will download test data over the network so MNISTTrainer -> Signing & Capabilities has the "Outgoing Connections (Client)" set in the App Sandbox
13 | - The website it connects to uses http rather than https so it has a "App Transport Security Settings" in the Info.plist
14 | 


--------------------------------------------------------------------------------
/Applications/StableDiffusionExample/Assets.xcassets/AccentColor.colorset/Contents.json:
--------------------------------------------------------------------------------
 1 | {
 2 |   "colors" : [
 3 |     {
 4 |       "idiom" : "universal"
 5 |     }
 6 |   ],
 7 |   "info" : {
 8 |     "author" : "xcode",
 9 |     "version" : 1
10 |   }
11 | }
12 | 


--------------------------------------------------------------------------------
/Applications/StableDiffusionExample/Assets.xcassets/AppIcon.appiconset/Contents.json:
--------------------------------------------------------------------------------
 1 | {
 2 |   "images" : [
 3 |     {
 4 |       "idiom" : "universal",
 5 |       "platform" : "ios",
 6 |       "size" : "1024x1024"
 7 |     },
 8 |     {
 9 |       "idiom" : "mac",
10 |       "scale" : "1x",
11 |       "size" : "16x16"
12 |     },
13 |     {
14 |       "idiom" : "mac",
15 |       "scale" : "2x",
16 |       "size" : "16x16"
17 |     },
18 |     {
19 |       "idiom" : "mac",
20 |       "scale" : "1x",
21 |       "size" : "32x32"
22 |     },
23 |     {
24 |       "idiom" : "mac",
25 |       "scale" : "2x",
26 |       "size" : "32x32"
27 |     },
28 |     {
29 |       "idiom" : "mac",
30 |       "scale" : "1x",
31 |       "size" : "128x128"
32 |     },
33 |     {
34 |       "idiom" : "mac",
35 |       "scale" : "2x",
36 |       "size" : "128x128"
37 |     },
38 |     {
39 |       "idiom" : "mac",
40 |       "scale" : "1x",
41 |       "size" : "256x256"
42 |     },
43 |     {
44 |       "idiom" : "mac",
45 |       "scale" : "2x",
46 |       "size" : "256x256"
47 |     },
48 |     {
49 |       "idiom" : "mac",
50 |       "scale" : "1x",
51 |       "size" : "512x512"
52 |     },
53 |     {
54 |       "idiom" : "mac",
55 |       "scale" : "2x",
56 |       "size" : "512x512"
57 |     }
58 |   ],
59 |   "info" : {
60 |     "author" : "xcode",
61 |     "version" : 1
62 |   }
63 | }
64 | 


--------------------------------------------------------------------------------
/Applications/StableDiffusionExample/Assets.xcassets/Contents.json:
--------------------------------------------------------------------------------
1 | {
2 |   "info" : {
3 |     "author" : "xcode",
4 |     "version" : 1
5 |   }
6 | }
7 | 


--------------------------------------------------------------------------------
/Applications/StableDiffusionExample/ContentView.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import MLX
  4 | import StableDiffusion
  5 | import SwiftUI
  6 | 
  7 | struct ContentView: View {
  8 | 
  9 |     @State var prompt = "dismal swamp, dense, very dark, realistic"
 10 |     @State var negativePrompt = ""
 11 |     @State var evaluator = StableDiffusionEvaluator()
 12 |     @State var showProgress = false
 13 | 
 14 |     var body: some View {
 15 |         VStack {
 16 |             HStack {
 17 |                 if let progress = evaluator.progress {
 18 |                     ProgressView(progress.title, value: progress.current, total: progress.limit)
 19 |                 }
 20 |             }
 21 |             .frame(height: 20)
 22 | 
 23 |             Spacer()
 24 |             if let image = evaluator.image {
 25 |                 Image(image, scale: 1.0, label: Text(""))
 26 |                     .resizable()
 27 |                     .aspectRatio(contentMode: .fit)
 28 |                     .frame(minHeight: 200)
 29 |             }
 30 |             Spacer()
 31 | 
 32 |             Grid {
 33 |                 GridRow {
 34 |                     TextField("prompt", text: $prompt)
 35 |                         .onSubmit(generate)
 36 |                         .disabled(evaluator.progress != nil)
 37 |                         #if os(visionOS)
 38 |                             .textFieldStyle(.roundedBorder)
 39 |                         #endif
 40 | 
 41 |                     Button(action: { prompt = "" }) {
 42 |                         Label("clear", systemImage: "xmark.circle.fill").font(.system(size: 10))
 43 |                     }
 44 |                     .labelStyle(.iconOnly)
 45 |                     .buttonStyle(.plain)
 46 | 
 47 |                     Button("generate", action: generate)
 48 |                         .disabled(evaluator.progress != nil)
 49 |                         .keyboardShortcut("r")
 50 |                 }
 51 |                 if evaluator.modelFactory.canShowProgress
 52 |                     || evaluator.modelFactory.canUseNegativeText
 53 |                 {
 54 |                     GridRow {
 55 |                         if evaluator.modelFactory.canUseNegativeText {
 56 |                             TextField("negative prompt", text: $negativePrompt)
 57 |                                 .onSubmit(generate)
 58 |                                 .disabled(evaluator.progress != nil)
 59 |                                 #if os(visionOS)
 60 |                                     .textFieldStyle(.roundedBorder)
 61 |                                 #endif
 62 |                             Button(action: { prompt = "" }) {
 63 |                                 Label("clear", systemImage: "xmark.circle.fill").font(
 64 |                                     .system(size: 10))
 65 |                             }
 66 |                             .labelStyle(.iconOnly)
 67 |                             .buttonStyle(.plain)
 68 |                         } else {
 69 |                             EmptyView()
 70 |                             EmptyView()
 71 |                         }
 72 | 
 73 |                         if evaluator.modelFactory.canShowProgress {
 74 |                             Toggle("Show Progress", isOn: $showProgress)
 75 |                         }
 76 |                     }
 77 |                 }
 78 |             }
 79 |             .frame(minWidth: 300)
 80 |         }
 81 |         .padding()
 82 |     }
 83 | 
 84 |     private func generate() {
 85 |         Task {
 86 |             await evaluator.generate(
 87 |                 prompt: prompt, negativePrompt: negativePrompt, showProgress: showProgress)
 88 |         }
 89 |     }
 90 | }
 91 | 
 92 | /// Progress reporting with a title.
 93 | struct Progress: Equatable {
 94 |     let title: String
 95 |     let current: Double
 96 |     let limit: Double
 97 | }
 98 | 
 99 | /// Async model factory
100 | actor ModelFactory {
101 | 
102 |     enum LoadState {
103 |         case idle
104 |         case loading(Task<ModelContainer<TextToImageGenerator>, Error>)
105 |         case loaded(ModelContainer<TextToImageGenerator>)
106 |     }
107 | 
108 |     enum SDError: LocalizedError {
109 |         case unableToLoad
110 | 
111 |         var errorDescription: String? {
112 |             switch self {
113 |             case .unableToLoad:
114 |                 return String(
115 |                     localized:
116 |                         "Unable to load the Stable Diffusion model. Please check your internet connection or available storage space."
117 |                 )
118 |             }
119 |         }
120 |     }
121 | 
122 |     public nonisolated let configuration = StableDiffusionConfiguration.presetSDXLTurbo
123 | 
124 |     /// if true we show UI that lets users see the intermediate steps
125 |     public nonisolated let canShowProgress: Bool
126 | 
127 |     /// if true we show UI to give negative text
128 |     public nonisolated let canUseNegativeText: Bool
129 | 
130 |     private var loadState = LoadState.idle
131 |     private var loadConfiguration = LoadConfiguration(float16: true, quantize: false)
132 | 
133 |     public nonisolated let conserveMemory: Bool
134 | 
135 |     init() {
136 |         let defaultParameters = configuration.defaultParameters()
137 |         self.canShowProgress = defaultParameters.steps > 4
138 |         self.canUseNegativeText = defaultParameters.cfgWeight > 1
139 | 
140 |         // this will be true e.g. if the computer has 8G of memory or less
141 |         self.conserveMemory = MLX.GPU.memoryLimit < 8 * 1024 * 1024 * 1024
142 | 
143 |         if conserveMemory {
144 |             print("conserving memory")
145 |             loadConfiguration.quantize = true
146 |             MLX.GPU.set(cacheLimit: 1 * 1024 * 1024)
147 |             MLX.GPU.set(memoryLimit: 3 * 1024 * 1024 * 1024)
148 |         } else {
149 |             MLX.GPU.set(cacheLimit: 256 * 1024 * 1024)
150 |         }
151 |     }
152 | 
153 |     public func load(reportProgress: @escaping @Sendable (Progress) -> Void) async throws
154 |         -> ModelContainer<TextToImageGenerator>
155 |     {
156 |         switch loadState {
157 |         case .idle:
158 |             let task = Task {
159 |                 do {
160 |                     try await configuration.download { progress in
161 |                         if progress.fractionCompleted < 0.99 {
162 |                             reportProgress(
163 |                                 .init(
164 |                                     title: "Download", current: progress.fractionCompleted * 100,
165 |                                     limit: 100))
166 |                         }
167 |                     }
168 |                 } catch {
169 |                     let nserror = error as NSError
170 |                     if nserror.domain == NSURLErrorDomain
171 |                         && nserror.code == NSURLErrorNotConnectedToInternet
172 |                     {
173 |                         // Internet connection appears to be offline -- fall back to loading from
174 |                         // the local directory
175 |                         reportProgress(.init(title: "Offline", current: 100, limit: 100))
176 |                     } else {
177 |                         throw error
178 |                     }
179 |                 }
180 | 
181 |                 let container = try ModelContainer<TextToImageGenerator>.createTextToImageGenerator(
182 |                     configuration: configuration, loadConfiguration: loadConfiguration)
183 | 
184 |                 await container.setConserveMemory(conserveMemory)
185 | 
186 |                 try await container.perform { model in
187 |                     reportProgress(.init(title: "Loading weights", current: 0, limit: 1))
188 |                     if !conserveMemory {
189 |                         model.ensureLoaded()
190 |                     }
191 |                 }
192 | 
193 |                 return container
194 |             }
195 |             self.loadState = .loading(task)
196 | 
197 |             let container = try await task.value
198 | 
199 |             if conserveMemory {
200 |                 // if conserving memory return the model but do not keep it in memory
201 |                 self.loadState = .idle
202 |             } else {
203 |                 // cache the model in memory to make it faster to run with new prompts
204 |                 self.loadState = .loaded(container)
205 |             }
206 | 
207 |             return container
208 | 
209 |         case .loading(let task):
210 |             let generator = try await task.value
211 |             return generator
212 | 
213 |         case .loaded(let generator):
214 |             return generator
215 |         }
216 |     }
217 | 
218 | }
219 | 
220 | @Observable @MainActor
221 | class StableDiffusionEvaluator {
222 | 
223 |     var progress: Progress?
224 |     var message: String?
225 |     var image: CGImage?
226 | 
227 |     let modelFactory = ModelFactory()
228 | 
229 |     @Sendable
230 |     nonisolated private func updateProgress(progress: Progress?) {
231 |         Task { @MainActor in
232 |             self.progress = progress
233 |         }
234 |     }
235 | 
236 |     @Sendable
237 |     nonisolated private func updateImage(image: CGImage?) {
238 |         Task { @MainActor in
239 |             self.image = image
240 |         }
241 |     }
242 | 
243 |     nonisolated private func display(decoded: MLXArray) {
244 |         let raster = (decoded * 255).asType(.uint8).squeezed()
245 |         let image = Image(raster).asCGImage()
246 | 
247 |         Task { @MainActor in
248 |             updateImage(image: image)
249 |         }
250 |     }
251 | 
252 |     func generate(prompt: String, negativePrompt: String, showProgress: Bool) async {
253 |         progress = .init(title: "Preparing", current: 0, limit: 1)
254 |         message = nil
255 | 
256 |         // The parameters that control the generation of the image. See
257 |         // EvaluateParameters for more information. For example, adjusting
258 |         // the latentSize parameter will change the size of the generated
259 |         // image. `imageCount` could be used to generate a gallery of
260 |         // images at the same time.
261 |         let parameters = {
262 |             var p = modelFactory.configuration.defaultParameters()
263 |             p.prompt = prompt
264 |             p.negativePrompt = negativePrompt
265 | 
266 |             // Per measurement each step consumes memory that we want to conserve. Trade
267 |             // off steps (quality) for memory.
268 |             if modelFactory.conserveMemory {
269 |                 p.steps = 1
270 |             }
271 | 
272 |             return p
273 |         }()
274 | 
275 |         do {
276 |             // Note: The optionals are used to discard parts of the model
277 |             // as it runs. This is used to conserve memory in devices
278 |             // with less memory.
279 |             let container = try await modelFactory.load(reportProgress: updateProgress)
280 | 
281 |             try await container.performTwoStage { generator in
282 |                 // The parameters that control the generation of the image. See
283 |                 // EvaluateParameters for more information. For example adjusting
284 |                 // the `latentSize` parameter will change the size of the generated
285 |                 // image. `imageCount` could be used to generate a gallery of
286 |                 // images at the same time.
287 |                 var parameters = modelFactory.configuration.defaultParameters()
288 |                 parameters.prompt = prompt
289 |                 parameters.negativePrompt = negativePrompt
290 | 
291 |                 // Per measurement each step consumes memory that we want to conserve. Trade
292 |                 // off steps (quality) for memory.
293 |                 if modelFactory.conserveMemory {
294 |                     parameters.steps = 1
295 |                 }
296 | 
297 |                 // Generate the latent images. This is fast as it is just generating
298 |                 // the graphs that will be evaluated below.
299 |                 let latents: DenoiseIterator? = generator.generateLatents(parameters: parameters)
300 | 
301 |                 // When conserveMemory is true this will discard the first part of
302 |                 // the model and just evaluate the decode portion.
303 |                 return (generator.detachedDecoder(), latents)
304 | 
305 |             } second: { decoder, latents in
306 |                 var lastXt: MLXArray?
307 |                 for (i, xt) in latents!.enumerated() {
308 |                     lastXt = nil
309 |                     eval(xt)
310 |                     lastXt = xt
311 | 
312 |                     if showProgress, i % 10 == 0 {
313 |                         display(decoded: decoder(xt))
314 |                     }
315 | 
316 |                     updateProgress(
317 |                         progress: .init(
318 |                             title: "Generate Latents", current: Double(i),
319 |                             limit: Double(parameters.steps)))
320 |                 }
321 | 
322 |                 if let lastXt {
323 |                     display(decoded: decoder(lastXt))
324 |                 }
325 |                 updateProgress(progress: nil)
326 |             }
327 | 
328 |         } catch {
329 |             progress = nil
330 |             message = "Failed: \(error)"
331 |         }
332 |     }
333 | }
334 | 


--------------------------------------------------------------------------------
/Applications/StableDiffusionExample/Preview Content/Preview Assets.xcassets/Contents.json:
--------------------------------------------------------------------------------
1 | {
2 |   "info" : {
3 |     "author" : "xcode",
4 |     "version" : 1
5 |   }
6 | }
7 | 


--------------------------------------------------------------------------------
/Applications/StableDiffusionExample/README.md:
--------------------------------------------------------------------------------
 1 | #  StableDiffusionExample
 2 | 
 3 | An example application that runs the StableDiffusion example code.
 4 | 
 5 | See also [image-tool](../../Tools/image-tool) for a command line example.
 6 | 
 7 | This example application accepts a prompt and used the StableDiffusion example
 8 | library to render an image using:
 9 | 
10 | - [stabilityai/sdxl-turbo](https://huggingface.co/stabilityai/sdxl-turbo)
11 | 
12 | Please refer to that model for license and other information.
13 | 
14 | If you are interested in adjusting the generated images, look in 
15 | [ContentView.swift](ContentView.swift) at this method:
16 | 
17 | ```swift
18 |     func generate(prompt: String, negativePrompt: String, showProgress: Bool) async 
19 | ```
20 | 
21 | ### Troubleshooting
22 | 
23 | Stable diffusion can run in less that 4G available memory (typically a
24 | device or computer with 6G of memory or more) in a constrained mode -- it will
25 | load and unload parts of the model as it runs and it can only perform one step
26 | of diffusion. This is configured automatically, see `modelFactory.conserveMemory`
27 | in [ContentView.swift](ContentView.swift).
28 | 
29 | On a device or computer with more memory the model will be kept resident and
30 | images can be regenerated much more efficiently.
31 | 
32 | If the program exits while generating the image it may have exceeded the available
33 | memory.
34 | 


--------------------------------------------------------------------------------
/Applications/StableDiffusionExample/StableDiffusionExample.entitlements:
--------------------------------------------------------------------------------
 1 | <?xml version="1.0" encoding="UTF-8"?>
 2 | <!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
 3 | <plist version="1.0">
 4 | <dict>
 5 | 	<key>com.apple.developer.kernel.increased-memory-limit</key>
 6 | 	<true/>
 7 | 	<key>com.apple.security.app-sandbox</key>
 8 | 	<true/>
 9 | 	<key>com.apple.security.files.user-selected.read-only</key>
10 | 	<true/>
11 | 	<key>com.apple.security.network.client</key>
12 | 	<true/>
13 | </dict>
14 | </plist>
15 | 


--------------------------------------------------------------------------------
/Applications/StableDiffusionExample/StableDiffusionExampleApp.swift:
--------------------------------------------------------------------------------
 1 | // Copyright © 2024 Apple Inc.
 2 | 
 3 | import SwiftUI
 4 | 
 5 | @main
 6 | struct StableDiffusionExampleApp: App {
 7 |     var body: some Scene {
 8 |         WindowGroup {
 9 |             ContentView()
10 |         }
11 |     }
12 | }
13 | 


--------------------------------------------------------------------------------
/Applications/VLMEval/Assets.xcassets/AccentColor.colorset/Contents.json:
--------------------------------------------------------------------------------
 1 | {
 2 |   "colors" : [
 3 |     {
 4 |       "idiom" : "universal"
 5 |     }
 6 |   ],
 7 |   "info" : {
 8 |     "author" : "xcode",
 9 |     "version" : 1
10 |   }
11 | }
12 | 


--------------------------------------------------------------------------------
/Applications/VLMEval/Assets.xcassets/AppIcon.appiconset/Contents.json:
--------------------------------------------------------------------------------
 1 | {
 2 |   "images" : [
 3 |     {
 4 |       "idiom" : "universal",
 5 |       "platform" : "ios",
 6 |       "size" : "1024x1024"
 7 |     },
 8 |     {
 9 |       "appearances" : [
10 |         {
11 |           "appearance" : "luminosity",
12 |           "value" : "dark"
13 |         }
14 |       ],
15 |       "idiom" : "universal",
16 |       "platform" : "ios",
17 |       "size" : "1024x1024"
18 |     },
19 |     {
20 |       "appearances" : [
21 |         {
22 |           "appearance" : "luminosity",
23 |           "value" : "tinted"
24 |         }
25 |       ],
26 |       "idiom" : "universal",
27 |       "platform" : "ios",
28 |       "size" : "1024x1024"
29 |     }
30 |   ],
31 |   "info" : {
32 |     "author" : "xcode",
33 |     "version" : 1
34 |   }
35 | }
36 | 


--------------------------------------------------------------------------------
/Applications/VLMEval/Assets.xcassets/Contents.json:
--------------------------------------------------------------------------------
1 | {
2 |   "info" : {
3 |     "author" : "xcode",
4 |     "version" : 1
5 |   }
6 | }
7 | 


--------------------------------------------------------------------------------
/Applications/VLMEval/ContentView.swift:
--------------------------------------------------------------------------------
  1 | // Copyright 2024 Apple Inc.
  2 | 
  3 | import AVKit
  4 | import AsyncAlgorithms
  5 | import CoreImage
  6 | import MLX
  7 | import MLXLMCommon
  8 | import MLXVLM
  9 | import PhotosUI
 10 | import SwiftUI
 11 | 
 12 | #if os(iOS) || os(visionOS)
 13 |     typealias PlatformImage = UIImage
 14 | #else
 15 |     typealias PlatformImage = NSImage
 16 | #endif
 17 | 
 18 | let videoSystemPrompt =
 19 |     "Focus only on describing the key dramatic action or notable event occurring in this video segment. Skip general context or scene-setting details unless they are crucial to understanding the main action."
 20 | let imageSystemPrompt =
 21 |     "You are an image understanding model capable of describing the salient features of any image."
 22 | 
 23 | struct ContentView: View {
 24 | 
 25 |     @State var llm = VLMEvaluator()
 26 |     @Environment(DeviceStat.self) private var deviceStat
 27 | 
 28 |     @State private var selectedImage: PlatformImage? = nil {
 29 |         didSet {
 30 |             if selectedImage != nil {
 31 |                 selectedVideoURL = nil
 32 |                 player = nil
 33 |             }
 34 |         }
 35 |     }
 36 |     @State private var selectedVideoURL: URL? {
 37 |         didSet {
 38 |             if let selectedVideoURL {
 39 |                 player = AVPlayer(url: selectedVideoURL)
 40 |                 selectedImage = nil
 41 |             }
 42 |         }
 43 |     }
 44 |     @State private var showingImagePicker = false
 45 |     @State private var selectedItem: PhotosPickerItem? = nil
 46 |     @State private var player: AVPlayer? = nil
 47 | 
 48 |     private var currentImageURL: URL? {
 49 |         selectedImage == nil && selectedVideoURL == nil
 50 |             ? URL(
 51 |                 string:
 52 |                     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg"
 53 |             ) : nil
 54 |     }
 55 | 
 56 |     var body: some View {
 57 |         VStack(alignment: .leading) {
 58 |             VStack {
 59 |                 HStack {
 60 |                     Text(llm.modelInfo)
 61 |                         .textFieldStyle(.roundedBorder)
 62 | 
 63 |                     Spacer()
 64 | 
 65 |                     Text(llm.stat)
 66 |                 }
 67 | 
 68 |                 VStack {
 69 |                     if let player {
 70 |                         VideoPlayer(player: player)
 71 |                             .frame(height: 300)
 72 |                             .cornerRadius(12)
 73 |                     } else if let selectedImage {
 74 |                         Group {
 75 |                             #if os(iOS) || os(visionOS)
 76 |                                 Image(uiImage: selectedImage)
 77 |                                     .resizable()
 78 |                             #else
 79 |                                 Image(nsImage: selectedImage)
 80 |                                     .resizable()
 81 |                             #endif
 82 |                         }
 83 |                         .scaledToFit()
 84 |                         .cornerRadius(12)
 85 |                         .frame(height: 300)
 86 |                     } else if let imageURL = currentImageURL {
 87 |                         AsyncImage(url: imageURL) { phase in
 88 |                             switch phase {
 89 |                             case .empty:
 90 |                                 ProgressView()
 91 |                             case .success(let image):
 92 |                                 image
 93 |                                     .resizable()
 94 |                                     .scaledToFit()
 95 |                                     .cornerRadius(12)
 96 |                                     .frame(height: 200)
 97 |                             case .failure:
 98 |                                 Image(systemName: "photo.badge.exclamationmark")
 99 |                             @unknown default:
100 |                                 EmptyView()
101 |                             }
102 |                         }
103 |                     }
104 | 
105 |                     HStack {
106 |                         #if os(iOS) || os(visionOS)
107 |                             PhotosPicker(
108 |                                 selection: $selectedItem,
109 |                                 matching: PHPickerFilter.any(of: [
110 |                                     PHPickerFilter.images, PHPickerFilter.videos,
111 |                                 ])
112 |                             ) {
113 |                                 Label("Select Image/Video", systemImage: "photo.badge.plus")
114 |                             }
115 |                             .onChange(of: selectedItem) {
116 |                                 Task {
117 |                                     if let video = try? await selectedItem?.loadTransferable(
118 |                                         type: TransferableVideo.self)
119 |                                     {
120 |                                         selectedVideoURL = video.url
121 |                                     } else if let data = try? await selectedItem?.loadTransferable(
122 |                                         type: Data.self)
123 |                                     {
124 |                                         selectedImage = PlatformImage(data: data)
125 |                                     }
126 |                                 }
127 |                             }
128 |                         #else
129 |                             Button("Select Image/Video") {
130 |                                 showingImagePicker = true
131 |                             }
132 |                             .fileImporter(
133 |                                 isPresented: $showingImagePicker,
134 |                                 allowedContentTypes: [.image, .movie]
135 |                             ) { result in
136 |                                 switch result {
137 |                                 case .success(let file):
138 |                                     Task { @MainActor in
139 |                                         do {
140 |                                             let data = try loadData(from: file)
141 |                                             if let image = PlatformImage(data: data) {
142 |                                                 selectedImage = image
143 |                                             } else if let fileType = UTType(
144 |                                                 filenameExtension: file.pathExtension),
145 |                                                 fileType.conforms(to: .movie)
146 |                                             {
147 |                                                 if let sandboxURL = try? loadVideoToSandbox(
148 |                                                     from: file)
149 |                                                 {
150 |                                                     selectedVideoURL = sandboxURL
151 |                                                 }
152 |                                             } else {
153 |                                                 print("Failed to create image from data")
154 |                                             }
155 |                                         } catch {
156 |                                             print(
157 |                                                 "Failed to load image: \(error.localizedDescription)"
158 |                                             )
159 |                                         }
160 |                                     }
161 |                                 case .failure(let error):
162 |                                     print(error.localizedDescription)
163 |                                 }
164 |                             }
165 |                         #endif
166 | 
167 |                         if selectedImage != nil {
168 |                             Button("Clear", role: .destructive) {
169 |                                 selectedImage = nil
170 |                                 selectedItem = nil
171 |                             }
172 |                         }
173 |                     }
174 |                 }
175 |                 .padding()
176 | 
177 |                 HStack {
178 |                     Spacer()
179 |                     if llm.running {
180 |                         ProgressView()
181 |                             .frame(maxHeight: 20)
182 |                         Spacer()
183 |                     }
184 |                 }
185 |             }
186 | 
187 |             ScrollView(.vertical) {
188 |                 ScrollViewReader { sp in
189 |                     Text(llm.output)
190 |                         .textSelection(.enabled)
191 |                         .onChange(of: llm.output) { _, _ in
192 |                             sp.scrollTo("bottom")
193 |                         }
194 | 
195 |                     Spacer()
196 |                         .frame(width: 1, height: 1)
197 |                         .id("bottom")
198 |                 }
199 |             }
200 |             .frame(minHeight: 200)
201 | 
202 |             HStack {
203 |                 TextField("prompt", text: Bindable(llm).prompt)
204 |                     .onSubmit(generate)
205 |                     .disabled(llm.running)
206 |                     #if os(visionOS)
207 |                         .textFieldStyle(.roundedBorder)
208 |                     #endif
209 |                 Button(llm.running ? "stop" : "generate", action: llm.running ? cancel : generate)
210 |             }
211 |         }
212 |         .onAppear {
213 |             selectedVideoURL = URL(
214 |                 string:
215 |                     "https://videos.pexels.com/video-files/4066325/4066325-uhd_2560_1440_24fps.mp4")!
216 |         }
217 |         #if os(visionOS)
218 |             .padding(40)
219 |         #else
220 |             .padding()
221 |         #endif
222 |         .toolbar {
223 |             ToolbarItem {
224 |                 Label(
225 |                     "Memory Usage: \(deviceStat.gpuUsage.activeMemory.formatted(.byteCount(style: .memory)))",
226 |                     systemImage: "info.circle.fill"
227 |                 )
228 |                 .labelStyle(.titleAndIcon)
229 |                 .padding(.horizontal)
230 |                 .help(
231 |                     Text(
232 |                         """
233 |                         Active Memory: \(deviceStat.gpuUsage.activeMemory.formatted(.byteCount(style: .memory)))/\(GPU.memoryLimit.formatted(.byteCount(style: .memory)))
234 |                         Cache Memory: \(deviceStat.gpuUsage.cacheMemory.formatted(.byteCount(style: .memory)))/\(GPU.cacheLimit.formatted(.byteCount(style: .memory)))
235 |                         Peak Memory: \(deviceStat.gpuUsage.peakMemory.formatted(.byteCount(style: .memory)))
236 |                         """
237 |                     )
238 |                 )
239 |             }
240 |             ToolbarItem(placement: .primaryAction) {
241 |                 Button {
242 |                     Task {
243 |                         copyToClipboard(llm.output)
244 |                     }
245 |                 } label: {
246 |                     Label("Copy Output", systemImage: "doc.on.doc.fill")
247 |                 }
248 |                 .disabled(llm.output == "")
249 |                 .labelStyle(.titleAndIcon)
250 |             }
251 |         }
252 |         .task {
253 |             _ = try? await llm.load()
254 |         }
255 |     }
256 | 
257 |     private func generate() {
258 |         Task {
259 |             if let selectedImage = selectedImage {
260 |                 #if os(iOS) || os(visionOS)
261 |                     let ciImage = CIImage(image: selectedImage)
262 |                     llm.generate(image: ciImage ?? CIImage(), videoURL: nil)
263 |                 #else
264 |                     if let cgImage = selectedImage.cgImage(
265 |                         forProposedRect: nil, context: nil, hints: nil)
266 |                     {
267 |                         let ciImage = CIImage(cgImage: cgImage)
268 |                         llm.generate(image: ciImage, videoURL: nil)
269 |                     }
270 |                 #endif
271 |             } else if let imageURL = currentImageURL {
272 |                 do {
273 |                     let (data, _) = try await URLSession.shared.data(from: imageURL)
274 |                     if let ciImage = CIImage(data: data) {
275 |                         llm.generate(image: ciImage, videoURL: nil)
276 |                     }
277 |                 } catch {
278 |                     print("Failed to load image: \(error.localizedDescription)")
279 |                 }
280 |             } else {
281 |                 if let videoURL = selectedVideoURL {
282 |                     llm.generate(image: nil, videoURL: videoURL)
283 |                 }
284 |             }
285 |         }
286 |     }
287 | 
288 |     private func cancel() {
289 |         llm.cancelGeneration()
290 |     }
291 | 
292 |     #if os(macOS)
293 |         private func loadData(from url: URL) throws -> Data {
294 |             guard url.startAccessingSecurityScopedResource() else {
295 |                 throw NSError(
296 |                     domain: "FileAccess", code: -1,
297 |                     userInfo: [NSLocalizedDescriptionKey: "Failed to access the file."])
298 |             }
299 |             defer { url.stopAccessingSecurityScopedResource() }
300 |             return try Data(contentsOf: url)
301 |         }
302 | 
303 |         private func loadVideoToSandbox(from url: URL) throws -> URL {
304 |             guard url.startAccessingSecurityScopedResource() else {
305 |                 throw NSError(
306 |                     domain: "FileAccess", code: -1,
307 |                     userInfo: [NSLocalizedDescriptionKey: "Failed to access the file."])
308 |             }
309 |             defer { url.stopAccessingSecurityScopedResource() }
310 |             let sandboxURL = try SandboxFileTransfer.transferFileToTemp(from: url)
311 |             return sandboxURL
312 |         }
313 |     #endif
314 | 
315 |     private func copyToClipboard(_ string: String) {
316 |         #if os(macOS)
317 |             NSPasteboard.general.clearContents()
318 |             NSPasteboard.general.setString(string, forType: .string)
319 |         #else
320 |             UIPasteboard.general.string = string
321 |         #endif
322 |     }
323 | }
324 | 
325 | @Observable
326 | @MainActor
327 | class VLMEvaluator {
328 | 
329 |     var running = false
330 | 
331 |     var prompt = ""
332 |     var output = ""
333 |     var modelInfo = ""
334 |     var stat = ""
335 | 
336 |     /// This controls which model loads. `smolvlm` is very small even unquantized, so it will fit on
337 |     /// more devices.
338 |     let modelConfiguration = VLMRegistry.smolvlm
339 | 
340 |     /// parameters controlling the output – use values appropriate for the model selected above
341 |     let generateParameters = MLXLMCommon.GenerateParameters(
342 |         maxTokens: 800, temperature: 0.7, topP: 0.9)
343 |     let updateInterval = Duration.seconds(0.25)
344 | 
345 |     /// A task responsible for handling the generation process.
346 |     var generationTask: Task<Void, Error>?
347 | 
348 |     enum LoadState {
349 |         case idle
350 |         case loaded(ModelContainer)
351 |     }
352 | 
353 |     var loadState = LoadState.idle
354 | 
355 |     /// load and return the model -- can be called multiple times, subsequent calls will
356 |     /// just return the loaded model
357 |     func load() async throws -> ModelContainer {
358 |         switch loadState {
359 |         case .idle:
360 |             // limit the buffer cache
361 |             MLX.GPU.set(cacheLimit: 20 * 1024 * 1024)
362 | 
363 |             let modelContainer = try await VLMModelFactory.shared.loadContainer(
364 |                 configuration: modelConfiguration
365 |             ) { [modelConfiguration] progress in
366 |                 Task { @MainActor in
367 |                     self.modelInfo =
368 |                         "Downloading \(modelConfiguration.name): \(Int(progress.fractionCompleted * 100))%"
369 |                 }
370 |             }
371 | 
372 |             let numParams = await modelContainer.perform { context in
373 |                 context.model.numParameters()
374 |             }
375 | 
376 |             self.prompt = modelConfiguration.defaultPrompt
377 |             self.modelInfo = "Loaded \(modelConfiguration.id). Weights: \(numParams / (1024*1024))M"
378 |             loadState = .loaded(modelContainer)
379 |             return modelContainer
380 | 
381 |         case .loaded(let modelContainer):
382 |             return modelContainer
383 |         }
384 |     }
385 | 
386 |     private func generate(prompt: String, image: CIImage?, videoURL: URL?) async {
387 | 
388 |         self.output = ""
389 | 
390 |         do {
391 |             let modelContainer = try await load()
392 | 
393 |             // each time you generate you will get something new
394 |             MLXRandom.seed(UInt64(Date.timeIntervalSinceReferenceDate * 1000))
395 | 
396 |             try await modelContainer.perform { (context: ModelContext) -> Void in
397 |                 let images: [UserInput.Image] = if let image { [.ciImage(image)] } else { [] }
398 |                 let videos: [UserInput.Video] = if let videoURL { [.url(videoURL)] } else { [] }
399 | 
400 |                 let systemPrompt =
401 |                     if !videos.isEmpty {
402 |                         videoSystemPrompt
403 |                     } else if !images.isEmpty {
404 |                         imageSystemPrompt
405 |                     } else { "You are a helpful assistant." }
406 | 
407 |                 let chat: [Chat.Message] = [
408 |                     .system(systemPrompt),
409 |                     .user(prompt, images: images, videos: videos),
410 |                 ]
411 | 
412 |                 var userInput = UserInput(chat: chat)
413 |                 userInput.processing.resize = .init(width: 448, height: 448)
414 | 
415 |                 let lmInput = try await context.processor.prepare(input: userInput)
416 | 
417 |                 let stream = try MLXLMCommon.generate(
418 |                     input: lmInput, parameters: generateParameters, context: context)
419 | 
420 |                 // generate and output in batches
421 |                 for await batch in stream._throttle(
422 |                     for: updateInterval, reducing: Generation.collect)
423 |                 {
424 |                     let output = batch.compactMap { $0.chunk }.joined(separator: "")
425 |                     if !output.isEmpty {
426 |                         Task { @MainActor [output] in
427 |                             self.output += output
428 |                         }
429 |                     }
430 | 
431 |                     if let completion = batch.compactMap({ $0.info }).first {
432 |                         Task { @MainActor in
433 |                             self.stat = "\(completion.tokensPerSecond) tokens/s"
434 |                         }
435 |                     }
436 |                 }
437 |             }
438 |         } catch {
439 |             output = "Failed: \(error)"
440 |         }
441 |     }
442 | 
443 |     func generate(image: CIImage?, videoURL: URL?) {
444 |         guard !running else { return }
445 |         let currentPrompt = prompt
446 |         prompt = ""
447 |         generationTask = Task {
448 |             running = true
449 |             await generate(prompt: currentPrompt, image: image, videoURL: videoURL)
450 |             running = false
451 |         }
452 |     }
453 | 
454 |     func cancelGeneration() {
455 |         generationTask?.cancel()
456 |         running = false
457 |     }
458 | }
459 | 
460 | #if os(iOS) || os(visionOS)
461 |     struct TransferableVideo: Transferable {
462 |         let url: URL
463 | 
464 |         static var transferRepresentation: some TransferRepresentation {
465 |             FileRepresentation(contentType: .movie) { movie in
466 |                 SentTransferredFile(movie.url)
467 |             } importing: { received in
468 |                 let sandboxURL = try SandboxFileTransfer.transferFileToTemp(from: received.file)
469 |                 return .init(url: sandboxURL)
470 |             }
471 |         }
472 |     }
473 | #endif
474 | 
475 | struct SandboxFileTransfer {
476 |     static func transferFileToTemp(from sourceURL: URL) throws -> URL {
477 |         let tempDir = FileManager.default.temporaryDirectory
478 |         let sandboxURL = tempDir.appendingPathComponent(sourceURL.lastPathComponent)
479 | 
480 |         if FileManager.default.fileExists(atPath: sandboxURL.path()) {
481 |             try FileManager.default.removeItem(at: sandboxURL)
482 |         }
483 | 
484 |         try FileManager.default.copyItem(at: sourceURL, to: sandboxURL)
485 |         return sandboxURL
486 |     }
487 | }
488 | 


--------------------------------------------------------------------------------
/Applications/VLMEval/Preview Content/Preview Assets.xcassets/Contents.json:
--------------------------------------------------------------------------------
1 | {
2 |   "info" : {
3 |     "author" : "xcode",
4 |     "version" : 1
5 |   }
6 | }
7 | 


--------------------------------------------------------------------------------
/Applications/VLMEval/README.md:
--------------------------------------------------------------------------------
 1 | #  VLMEval
 2 | 
 3 | An example that:
 4 | 
 5 | - downloads a vision language model (Qwen-VL-2B)
 6 | - processes an image with a prompt
 7 | 
 8 | You will need to set the Team on the VLMEval target in order to build and run on macOS.
 9 | 
10 | Some notes about the setup:
11 | 
12 | - This downloads models from hugging face so VLMEval -> Signing & Capabilities has the "Outgoing Connections (Client)" set in the App Sandbox
13 | - VLM models are large so this uses significant memory
14 | - The example processes images and provides detailed analysis
15 | 
16 | ### Image Processing
17 | 
18 | The example application uses Qwen-VL-2B model by default, see [ContentView.swift](ContentView.swift):
19 | 
20 | ```swift
21 | self.modelContainer = try await VLMModelFactory.shared.loadContainer(
22 |     configuration: VLMRegistry.qwen2VL2BInstruct4Bit)
23 | ```
24 | 
25 | The application:
26 | 1. Downloads a sample image
27 | 2. Processes it through the vision language model
28 | 3. Describes the images based on the prompt, providing detailed analysis of the content, objects, colors, and composition.
29 | 
30 | ### Troubleshooting
31 | 
32 | If the program crashes with a very deep stack trace you may need to build
33 | in Release configuration. This seems to depend on the size of the model.
34 | 
35 | There are a couple options:
36 | 
37 | - Build Release
38 | - Force the model evaluation to run on the main thread, e.g. using @MainActor
39 | - Build `Cmlx` with optimizations by modifying `mlx/Package.swift` and adding `.unsafeOptions(["-O3"]),`
40 | 
41 | ### Performance
42 | 
43 | You may find that running outside the debugger boosts performance. You can do this in Xcode by pressing cmd-opt-r and unchecking "Debug Executable".
44 | 


--------------------------------------------------------------------------------
/Applications/VLMEval/VLMEval.entitlements:
--------------------------------------------------------------------------------
 1 | <?xml version="1.0" encoding="UTF-8"?>
 2 | <!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
 3 | <plist version="1.0">
 4 | <dict>
 5 | 	<key>com.apple.developer.kernel.increased-memory-limit</key>
 6 | 	<true/>
 7 | 	<key>com.apple.security.app-sandbox</key>
 8 | 	<true/>
 9 | 	<key>com.apple.security.device.usb</key>
10 | 	<true/>
11 | 	<key>com.apple.security.files.user-selected.read-only</key>
12 | 	<true/>
13 | 	<key>com.apple.security.network.client</key>
14 | 	<true/>
15 | </dict>
16 | </plist>
17 | 


--------------------------------------------------------------------------------
/Applications/VLMEval/VLMEvalApp.swift:
--------------------------------------------------------------------------------
 1 | // Copyright © 2024 Apple Inc.
 2 | 
 3 | import SwiftUI
 4 | 
 5 | @main
 6 | struct VLMEvalApp: App {
 7 |     var body: some Scene {
 8 |         WindowGroup {
 9 |             ContentView()
10 |                 .environment(DeviceStat())
11 |         }
12 |     }
13 | }
14 | 


--------------------------------------------------------------------------------
/CODE_OF_CONDUCT.md:
--------------------------------------------------------------------------------
  1 | # Contributor Covenant Code of Conduct
  2 | 
  3 | ## Our Pledge
  4 | 
  5 | We as members, contributors, and leaders pledge to make participation in our
  6 | community a harassment-free experience for everyone, regardless of age, body
  7 | size, visible or invisible disability, ethnicity, sex characteristics, gender
  8 | identity and expression, level of experience, education, socio-economic status,
  9 | nationality, personal appearance, race, caste, color, religion, or sexual
 10 | identity and orientation.
 11 | 
 12 | We pledge to act and interact in ways that contribute to an open, welcoming,
 13 | diverse, inclusive, and healthy community.
 14 | 
 15 | ## Our Standards
 16 | 
 17 | Examples of behavior that contributes to a positive environment for our
 18 | community include:
 19 | 
 20 | * Demonstrating empathy and kindness toward other people
 21 | * Being respectful of differing opinions, viewpoints, and experiences
 22 | * Giving and gracefully accepting constructive feedback
 23 | * Accepting responsibility and apologizing to those affected by our mistakes,
 24 |   and learning from the experience
 25 | * Focusing on what is best not just for us as individuals, but for the overall
 26 |   community
 27 | 
 28 | Examples of unacceptable behavior include:
 29 | 
 30 | * The use of sexualized language or imagery, and sexual attention or advances of
 31 |   any kind
 32 | * Trolling, insulting or derogatory comments, and personal or political attacks
 33 | * Public or private harassment
 34 | * Publishing others' private information, such as a physical or email address,
 35 |   without their explicit permission
 36 | * Other conduct which could reasonably be considered inappropriate in a
 37 |   professional setting
 38 | 
 39 | ## Enforcement Responsibilities
 40 | 
 41 | Community leaders are responsible for clarifying and enforcing our standards of
 42 | acceptable behavior and will take appropriate and fair corrective action in
 43 | response to any behavior that they deem inappropriate, threatening, offensive,
 44 | or harmful.
 45 | 
 46 | Community leaders have the right and responsibility to remove, edit, or reject
 47 | comments, commits, code, wiki edits, issues, and other contributions that are
 48 | not aligned to this Code of Conduct, and will communicate reasons for moderation
 49 | decisions when appropriate.
 50 | 
 51 | ## Scope
 52 | 
 53 | This Code of Conduct applies within all community spaces, and also applies when
 54 | an individual is officially representing the community in public spaces.
 55 | Examples of representing our community include using an official e-mail address,
 56 | posting via an official social media account, or acting as an appointed
 57 | representative at an online or offline event.
 58 | 
 59 | ## Enforcement
 60 | 
 61 | Instances of abusive, harassing, or otherwise unacceptable behavior may be
 62 | reported to the community leaders responsible for enforcement at
 63 | [opensource-conduct@group.apple.com](mailto:opensource-conduct@group.apple.com).
 64 | All complaints will be reviewed and investigated promptly and fairly.
 65 | 
 66 | All community leaders are obligated to respect the privacy and security of the
 67 | reporter of any incident.
 68 | 
 69 | ## Enforcement Guidelines
 70 | 
 71 | Community leaders will follow these Community Impact Guidelines in determining
 72 | the consequences for any action they deem in violation of this Code of Conduct:
 73 | 
 74 | ### 1. Correction
 75 | 
 76 | **Community Impact**: Use of inappropriate language or other behavior deemed
 77 | unprofessional or unwelcome in the community.
 78 | 
 79 | **Consequence**: A private, written warning from community leaders, providing
 80 | clarity around the nature of the violation and an explanation of why the
 81 | behavior was inappropriate. A public apology may be requested.
 82 | 
 83 | ### 2. Warning
 84 | 
 85 | **Community Impact**: A violation through a single incident or series of
 86 | actions.
 87 | 
 88 | **Consequence**: A warning with consequences for continued behavior. No
 89 | interaction with the people involved, including unsolicited interaction with
 90 | those enforcing the Code of Conduct, for a specified period of time. This
 91 | includes avoiding interactions in community spaces as well as external channels
 92 | like social media. Violating these terms may lead to a temporary or permanent
 93 | ban.
 94 | 
 95 | ### 3. Temporary Ban
 96 | 
 97 | **Community Impact**: A serious violation of community standards, including
 98 | sustained inappropriate behavior.
 99 | 
100 | **Consequence**: A temporary ban from any sort of interaction or public
101 | communication with the community for a specified period of time. No public or
102 | private interaction with the people involved, including unsolicited interaction
103 | with those enforcing the Code of Conduct, is allowed during this period.
104 | Violating these terms may lead to a permanent ban.
105 | 
106 | ### 4. Permanent Ban
107 | 
108 | **Community Impact**: Demonstrating a pattern of violation of community
109 | standards, including sustained inappropriate behavior, harassment of an
110 | individual, or aggression toward or disparagement of classes of individuals.
111 | 
112 | **Consequence**: A permanent ban from any sort of public interaction within the
113 | community.
114 | 
115 | ## Attribution
116 | 
117 | This Code of Conduct is adapted from the [Contributor Covenant][homepage],
118 | version 2.1, available at
119 | [https://www.contributor-covenant.org/version/2/1/code_of_conduct.html][v2.1].
120 | 
121 | Community Impact Guidelines were inspired by
122 | [Mozilla's code of conduct enforcement ladder][Mozilla CoC].
123 | 
124 | For answers to common questions about this code of conduct, see the FAQ at
125 | [https://www.contributor-covenant.org/faq][FAQ]. Translations are available at
126 | [https://www.contributor-covenant.org/translations][translations].
127 | 
128 | [homepage]: https://www.contributor-covenant.org
129 | [v2.1]: https://www.contributor-covenant.org/version/2/1/code_of_conduct.html
130 | [Mozilla CoC]: https://github.com/mozilla/diversity
131 | [FAQ]: https://www.contributor-covenant.org/faq
132 | [translations]: https://www.contributor-covenant.org/translations
133 | 


--------------------------------------------------------------------------------
/CONTRIBUTING.md:
--------------------------------------------------------------------------------
 1 | # Contributing to MLX Swift Examples
 2 | 
 3 | We want to make contributing to this project as easy and transparent as
 4 | possible.
 5 | 
 6 | ## Pull Requests
 7 | 
 8 | 1. Fork and submit pull requests to the repo. 
 9 | 2. If you've added code that should be tested, add tests.
10 | 3. Every PR should have passing tests (if any) and at least one review. 
11 | 4. For code formatting install `pre-commit` using something like `pip install pre-commit` and run `pre-commit install`.
12 |    If needed you may need to `brew install swift-format`.
13 |  
14 |    You can also run the formatters manually as follows:
15 |  
16 |      ```
17 |      swift-format format --in-place --recursive Libraries Tools Applications
18 |      ```
19 |  
20 |    or run `pre-commit run --all-files` to check all files in the repo.
21 |  
22 | ## Issues
23 | 
24 | We use GitHub issues to track public bugs. Please ensure your description is
25 | clear and has sufficient instructions to be able to reproduce the issue.
26 | 
27 | ## License
28 | 
29 | By contributing to MLX Swift Examples, you agree that your contributions will be licensed
30 | under the LICENSE file in the root directory of this source tree.
31 | 


--------------------------------------------------------------------------------
/Configuration/Build.xcconfig:
--------------------------------------------------------------------------------
1 | // The `DISAMBIGUATOR` configuration is to make it easier to build
2 | // and run a sample code project. Once you set your project's development team,
3 | // you'll have a unique bundle identifier. This is because the bundle identifier
4 | // is derived based on the 'DISAMBIGUATOR' value. Do not use this
5 | // approach in your own projects—it's only useful for example projects because
6 | // they are frequently downloaded and don't have a development team set.
7 | DISAMBIGUATOR=${DEVELOPMENT_TEAM}
8 | 


--------------------------------------------------------------------------------
/Data/lora/test.jsonl:
--------------------------------------------------------------------------------
  1 | {"text": "table: 1-10015132-16\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What is terrence ross' nationality\nA: SELECT Nationality FROM 1-10015132-16 WHERE Player = 'Terrence Ross'"}
  2 | {"text": "table: 1-10015132-16\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What clu was in toronto 1995-96\nA: SELECT School/Club Team FROM 1-10015132-16 WHERE Years in Toronto = '1995-96'"}
  3 | {"text": "table: 1-10015132-16\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: which club was in toronto 2003-06\nA: SELECT School/Club Team FROM 1-10015132-16 WHERE Years in Toronto = '2003-06'"}
  4 | {"text": "table: 1-10015132-16\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: how many schools or teams had jalen rose\nA: SELECT COUNT School/Club Team FROM 1-10015132-16 WHERE Player = 'Jalen Rose'"}
  5 | {"text": "table: 1-10083598-1\ncolumns: No, Date, Round, Circuit, Pole Position, Fastest Lap, Race winner, Report\nQ: Where was Assen held?\nA: SELECT Round FROM 1-10083598-1 WHERE Circuit = 'Assen'"}
  6 | {"text": "table: 1-10083598-1\ncolumns: No, Date, Round, Circuit, Pole Position, Fastest Lap, Race winner, Report\nQ: What was the number of race that Kevin Curtain won?\nA: SELECT COUNT No FROM 1-10083598-1 WHERE Pole Position = 'Kevin Curtain'"}
  7 | {"text": "table: 1-10083598-1\ncolumns: No, Date, Round, Circuit, Pole Position, Fastest Lap, Race winner, Report\nQ: What was the date of the race in Misano?\nA: SELECT Date FROM 1-10083598-1 WHERE Circuit = 'Misano'"}
  8 | {"text": "table: 1-1013129-2\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: How many different positions did Sherbrooke Faucons (qmjhl) provide in the draft?\nA: SELECT COUNT Position FROM 1-1013129-2 WHERE College/junior/club team = 'Sherbrooke Faucons (QMJHL)'"}
  9 | {"text": "table: 1-1013129-2\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: What are the nationalities of the player picked from Thunder Bay Flyers (ushl)\nA: SELECT Nationality FROM 1-1013129-2 WHERE College/junior/club team = 'Thunder Bay Flyers (USHL)'"}
 10 | {"text": "table: 1-1013129-2\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: How many different college/junior/club teams provided a player to the Washington Capitals NHL Team?\nA: SELECT COUNT College/junior/club team FROM 1-1013129-2 WHERE NHL team = 'Washington Capitals'"}
 11 | {"text": "table: 1-1013129-3\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: How many different nationalities do the players of New Jersey Devils come from?\nA: SELECT COUNT Nationality FROM 1-1013129-3 WHERE NHL team = 'New Jersey Devils'"}
 12 | {"text": "table: 1-1013129-3\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: What's Dorain Anneck's pick number?\nA: SELECT Pick FROM 1-1013129-3 WHERE Player = 'Dorain Anneck'"}
 13 | {"text": "table: 1-1013129-3\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: What is the nationality of the player from Vancouver Canucks?\nA: SELECT Nationality FROM 1-1013129-3 WHERE NHL team = 'Vancouver Canucks'"}
 14 | {"text": "table: 1-1013129-3\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: What's the pick number of the player from Springfield Olympics (Nejhl)?\nA: SELECT Pick FROM 1-1013129-3 WHERE College/junior/club team = 'Springfield Olympics (NEJHL)'"}
 15 | {"text": "table: 1-1014206-2\ncolumns: #, Shipyard, Laid down, Launched, Commissioned, Fleet, Status\nQ: When were the ships launched that were laid down on september 1, 1964?\nA: SELECT Launched FROM 1-1014206-2 WHERE Laid down = 'September 1, 1964'"}
 16 | {"text": "table: 1-1014206-2\ncolumns: #, Shipyard, Laid down, Launched, Commissioned, Fleet, Status\nQ: List the # for ships commissioned on december 18, 1965.\nA: SELECT # FROM 1-1014206-2 WHERE Commissioned = 'December 18, 1965'"}
 17 | {"text": "table: 1-1014206-2\ncolumns: #, Shipyard, Laid down, Launched, Commissioned, Fleet, Status\nQ: List the # for ships commissioned on september 30, 1967.\nA: SELECT # FROM 1-1014206-2 WHERE Commissioned = 'September 30, 1967'"}
 18 | {"text": "table: 1-1014206-2\ncolumns: #, Shipyard, Laid down, Launched, Commissioned, Fleet, Status\nQ: When were ships laid down that were commissioned on october 29, 1965?\nA: SELECT Laid down FROM 1-1014206-2 WHERE Commissioned = 'October 29, 1965'"}
 19 | {"text": "table: 1-1015521-2\ncolumns: Equivalent NATO Rank Code, Rank in Spanish, Rank in English, Commonwealth equivalent, US Air Force equivalent\nQ:  What could a spanish coronel be addressed as in the commonwealth military?\nA: SELECT Commonwealth equivalent FROM 1-1015521-2 WHERE Rank in Spanish = 'Coronel'"}
 20 | {"text": "table: 1-1015521-2\ncolumns: Equivalent NATO Rank Code, Rank in Spanish, Rank in English, Commonwealth equivalent, US Air Force equivalent\nQ: Give me a list of all spanish officer titles that could receive recognition as group captain in english\nA: SELECT Rank in English FROM 1-1015521-2 WHERE Commonwealth equivalent = 'Group Captain'"}
 21 | {"text": "table: 1-1015521-2\ncolumns: Equivalent NATO Rank Code, Rank in Spanish, Rank in English, Commonwealth equivalent, US Air Force equivalent\nQ: If you are a pilot officer in the commonwealth then what will you called as in the US air force?\nA: SELECT US Air Force equivalent FROM 1-1015521-2 WHERE Commonwealth equivalent = 'Pilot Officer'"}
 22 | {"text": "table: 1-1015521-2\ncolumns: Equivalent NATO Rank Code, Rank in Spanish, Rank in English, Commonwealth equivalent, US Air Force equivalent\nQ: If you're a major general in the US air force then what ranking will you receive in the commonwealth's air force?\nA: SELECT Commonwealth equivalent FROM 1-1015521-2 WHERE US Air Force equivalent = 'Major General'"}
 23 | {"text": "table: 1-1015521-2\ncolumns: Equivalent NATO Rank Code, Rank in Spanish, Rank in English, Commonwealth equivalent, US Air Force equivalent\nQ: If you get a ranking as major in the  english military then what would the spanish military address you as? \nA: SELECT Rank in Spanish FROM 1-1015521-2 WHERE Rank in English = 'Major'"}
 24 | {"text": "table: 1-10182508-5\ncolumns: Rank Each wrestlers total number of days as champion are ranked highest to lowest; wrestlers with the same number mean that they are tied for that certain rank., Wrestler, # of reigns, Combined defenses, Combined days\nQ: Which wrestlers have had 2 reigns?\nA: SELECT Wrestler FROM 1-10182508-5 WHERE # of reigns = 2"}
 25 | {"text": "table: 1-10182508-5\ncolumns: Rank Each wrestlers total number of days as champion are ranked highest to lowest; wrestlers with the same number mean that they are tied for that certain rank., Wrestler, # of reigns, Combined defenses, Combined days\nQ: In terms of reigns, what is the lowest number listed?\nA: SELECT MIN # of reigns FROM 1-10182508-5"}
 26 | {"text": "table: 1-10182508-5\ncolumns: Rank Each wrestlers total number of days as champion are ranked highest to lowest; wrestlers with the same number mean that they are tied for that certain rank., Wrestler, # of reigns, Combined defenses, Combined days\nQ: What rank was Bryan Danielson in this chart?\nA: SELECT Rank Each wrestlers total number of days as champion are ranked highest to lowest; wrestlers with the same number mean that they are tied for that certain rank. FROM 1-10182508-5 WHERE Wrestler = 'Bryan Danielson'"}
 27 | {"text": "table: 1-10182508-5\ncolumns: Rank Each wrestlers total number of days as champion are ranked highest to lowest; wrestlers with the same number mean that they are tied for that certain rank., Wrestler, # of reigns, Combined defenses, Combined days\nQ: How many combined days did Go Shiozaki have?\nA: SELECT Combined days FROM 1-10182508-5 WHERE Wrestler = 'Go Shiozaki'"}
 28 | {"text": "table: 1-10182508-5\ncolumns: Rank Each wrestlers total number of days as champion are ranked highest to lowest; wrestlers with the same number mean that they are tied for that certain rank., Wrestler, # of reigns, Combined defenses, Combined days\nQ: What was Go Shiozaki's rank?\nA: SELECT MIN Rank Each wrestlers total number of days as champion are ranked highest to lowest; wrestlers with the same number mean that they are tied for that certain rank. FROM 1-10182508-5 WHERE Wrestler = 'Go Shiozaki'"}
 29 | {"text": "table: 1-1024710-2\ncolumns: Member, Electorate, Province, MPs term, Election date\nQ: Which province is grey and bell electorate in\nA: SELECT Province FROM 1-1024710-2 WHERE Electorate = 'Grey and Bell'"}
 30 | {"text": "table: 1-1024710-2\ncolumns: Member, Electorate, Province, MPs term, Election date\nQ: Which province is bay of islands in\nA: SELECT Province FROM 1-1024710-2 WHERE Electorate = 'Bay of Islands'"}
 31 | {"text": "table: 1-10294071-1\ncolumns: Player, Total W\u2013L, Singles W\u2013L, Doubles W\u2013L, Ties played, Debut, Years played\nQ: what is the total number of\u00a0total w\u2013l\u00a0where\u00a0doubles w\u2013l\u00a0is 11\u201311\nA: SELECT COUNT Total W\u2013L FROM 1-10294071-1 WHERE Doubles W\u2013L = '11\u201311'"}
 32 | {"text": "table: 1-10294071-1\ncolumns: Player, Total W\u2013L, Singles W\u2013L, Doubles W\u2013L, Ties played, Debut, Years played\nQ: what is the total number of\u00a0singles w\u2013l\u00a0where\u00a0doubles w\u2013l\u00a0is 11\u201314\nA: SELECT COUNT Singles W\u2013L FROM 1-10294071-1 WHERE Doubles W\u2013L = '11\u201314'"}
 33 | {"text": "table: 1-10294071-1\ncolumns: Player, Total W\u2013L, Singles W\u2013L, Doubles W\u2013L, Ties played, Debut, Years played\nQ:  what's the\u00a0total w\u2013l\u00a0where\u00a0player\u00a0is boro jovanovi\u0107 category:articles with hcards\nA: SELECT Total W\u2013L FROM 1-10294071-1 WHERE Player = 'Boro Jovanovi\u0107 Category:Articles with hCards'"}
 34 | {"text": "table: 1-10294071-1\ncolumns: Player, Total W\u2013L, Singles W\u2013L, Doubles W\u2013L, Ties played, Debut, Years played\nQ: what is the maximum\u00a0ties played\u00a0where\u00a0player\u00a0is josip palada category:articles with hcards\nA: SELECT MAX Ties played FROM 1-10294071-1 WHERE Player = 'Josip Palada Category:Articles with hCards'"}
 35 | {"text": "table: 1-10294071-1\ncolumns: Player, Total W\u2013L, Singles W\u2013L, Doubles W\u2013L, Ties played, Debut, Years played\nQ: what is the total number of\u00a0ties played\u00a0where\u00a0total w\u2013l\u00a0is 38\u201324\nA: SELECT COUNT Ties played FROM 1-10294071-1 WHERE Total W\u2013L = '38\u201324'"}
 36 | {"text": "table: 1-10333757-1\ncolumns: Calls, Frequency, Branding, Format, Market/Rank, Timeslot, Group owner\nQ: What is the Frequency at the Market/Rank of Burlington - Plattsburgh , Vermont - New York /143?\nA: SELECT COUNT Frequency FROM 1-10333757-1 WHERE Market/Rank = 'Burlington - Plattsburgh , Vermont - New York /143'"}
 37 | {"text": "table: 1-10333757-1\ncolumns: Calls, Frequency, Branding, Format, Market/Rank, Timeslot, Group owner\nQ: What is the Branding for Group Owner Qantam of Cape Cod, LLC?\nA: SELECT Branding FROM 1-10333757-1 WHERE Group owner = 'Qantam of Cape Cod, LLC'"}
 38 | {"text": "table: 1-10333757-1\ncolumns: Calls, Frequency, Branding, Format, Market/Rank, Timeslot, Group owner\nQ: What Branding does WRKO calls use?\nA: SELECT Branding FROM 1-10333757-1 WHERE Calls = 'WRKO'"}
 39 | {"text": "table: 1-10333757-1\ncolumns: Calls, Frequency, Branding, Format, Market/Rank, Timeslot, Group owner\nQ: What is the Format for Branding of 1290 wkbk w281au 104.1?\nA: SELECT Format FROM 1-10333757-1 WHERE Branding = '1290 WKBK W281AU 104.1'"}
 40 | {"text": "table: 1-10333757-1\ncolumns: Calls, Frequency, Branding, Format, Market/Rank, Timeslot, Group owner\nQ: Which Market/Rank is associated with WCRN calls?\nA: SELECT Market/Rank FROM 1-10333757-1 WHERE Calls = 'WCRN'"}
 41 | {"text": "table: 1-10333757-1\ncolumns: Calls, Frequency, Branding, Format, Market/Rank, Timeslot, Group owner\nQ: Which Frequency is used for WEGP calls?\nA: SELECT Frequency FROM 1-10333757-1 WHERE Calls = 'WEGP'"}
 42 | {"text": "table: 1-10408617-5\ncolumns: Scheme, Tariff code, BTs retail price (regulated), Approx premium, Prefixes\nQ: What is the regulated retail price for the tariff code ff0 prs?\nA: SELECT BTs retail price (regulated) FROM 1-10408617-5 WHERE Tariff code = 'ff0 PRS'"}
 43 | {"text": "table: 1-10408617-5\ncolumns: Scheme, Tariff code, BTs retail price (regulated), Approx premium, Prefixes\nQ: What is the premium associated with tariff code g9?\nA: SELECT Approx premium FROM 1-10408617-5 WHERE Tariff code = 'g9'"}
 44 | {"text": "table: 1-10408617-5\ncolumns: Scheme, Tariff code, BTs retail price (regulated), Approx premium, Prefixes\nQ: How many tariff codes have a bts retail price of 2p/min or inclusive?\nA: SELECT COUNT Tariff code FROM 1-10408617-5 WHERE BTs retail price (regulated) = '2p/min or inclusive'"}
 45 | {"text": "table: 1-10408617-5\ncolumns: Scheme, Tariff code, BTs retail price (regulated), Approx premium, Prefixes\nQ: How many tariff codes have a bts retail price of 2.553p/min?\nA: SELECT COUNT Tariff code FROM 1-10408617-5 WHERE BTs retail price (regulated) = '2.553p/min'"}
 46 | {"text": "table: 1-10408617-5\ncolumns: Scheme, Tariff code, BTs retail price (regulated), Approx premium, Prefixes\nQ: What prefixes are priced at pence per minute, fixed at all times with a premium of 3p/min?\nA: SELECT Prefixes FROM 1-10408617-5 WHERE Scheme = 'Pence per minute, fixed at all times' AND Approx premium = '3p/min'"}
 47 | {"text": "table: 1-10408617-5\ncolumns: Scheme, Tariff code, BTs retail price (regulated), Approx premium, Prefixes\nQ: What is the bts retail price (regulated) for tariff code g10?\nA: SELECT BTs retail price (regulated) FROM 1-10408617-5 WHERE Tariff code = 'g10'"}
 48 | {"text": "table: 1-10409754-5\ncolumns: Nominative, Old orthography, New orthography, /e/ or /\u00e6/ (IPA), Tone (Latvian notation: /~/ - level, /^/ - broken), Translation\nQ: What is the tone for gen.sing. plague?\nA: SELECT Tone (Latvian notation: /~/ - level, /^/ - broken) FROM 1-10409754-5 WHERE Translation = 'Gen.Sing. plague'"}
 49 | {"text": "table: 1-10432351-1\ncolumns: Star (Pismis24-#), Spectral type, Magnitude (M bol ), Temperature (K), Radius (R \u2609 ), Mass (M \u2609 )\nQ: What is the smallest possible radius?\nA: SELECT MIN Radius (R \u2609 ) FROM 1-10432351-1"}
 50 | {"text": "table: 1-10432351-1\ncolumns: Star (Pismis24-#), Spectral type, Magnitude (M bol ), Temperature (K), Radius (R \u2609 ), Mass (M \u2609 )\nQ: What are all the spectral types for star mismis24-# is 1sw?\nA: SELECT Spectral type FROM 1-10432351-1 WHERE Star (Pismis24-#) = '1SW'"}
 51 | {"text": "table: 1-10432351-1\ncolumns: Star (Pismis24-#), Spectral type, Magnitude (M bol ), Temperature (K), Radius (R \u2609 ), Mass (M \u2609 )\nQ: If a radius is 10, what  is the lowest possible mass?\nA: SELECT MIN Mass (M \u2609 ) FROM 1-10432351-1 WHERE Radius (R \u2609 ) = 10"}
 52 | {"text": "table: 1-105344-2\ncolumns: Year, Aircraft kilometers, Departures, Flying hours, Passengers, Seat factor, Employees, Profit/loss\nQ: What percentage of seats were filled in 2006?\nA: SELECT Seat factor FROM 1-105344-2 WHERE Year = 2006"}
 53 | {"text": "table: 1-105344-2\ncolumns: Year, Aircraft kilometers, Departures, Flying hours, Passengers, Seat factor, Employees, Profit/loss\nQ: How many hours were flown in each of the years where more than 64379058.0 kilometers were flown?\nA: SELECT Flying hours FROM 1-105344-2 WHERE Aircraft kilometers > 64379058.0"}
 54 | {"text": "table: 1-105344-2\ncolumns: Year, Aircraft kilometers, Departures, Flying hours, Passengers, Seat factor, Employees, Profit/loss\nQ: Of the years that had exactly 17096 departures, what is the greatest number of aircraft kilometers flown?\nA: SELECT MAX Aircraft kilometers FROM 1-105344-2 WHERE Departures = 17096"}
 55 | {"text": "table: 1-10548224-1\ncolumns: Year, Game or event, Date contested, League or governing body, Sport, Winning team, Losing team, Final score\nQ: Which winning team beat the New York Yankees?\nA: SELECT Winning team FROM 1-10548224-1 WHERE Losing team = 'New York Yankees'"}
 56 | {"text": "table: 1-10548224-1\ncolumns: Year, Game or event, Date contested, League or governing body, Sport, Winning team, Losing team, Final score\nQ: What was the final score for the game that was contested on February 1, 2009?\nA: SELECT Final score FROM 1-10548224-1 WHERE Date contested = 'February 1, 2009'"}
 57 | {"text": "table: 1-10548224-1\ncolumns: Year, Game or event, Date contested, League or governing body, Sport, Winning team, Losing team, Final score\nQ: What sport had a final score of 3-2?\nA: SELECT Sport FROM 1-10548224-1 WHERE Final score = '3-2'"}
 58 | {"text": "table: 1-10548224-1\ncolumns: Year, Game or event, Date contested, League or governing body, Sport, Winning team, Losing team, Final score\nQ: Who was the winning team of the game that was contested on February 1, 2009?\nA: SELECT Winning team FROM 1-10548224-1 WHERE Date contested = 'February 1, 2009'"}
 59 | {"text": "table: 1-10548224-1\ncolumns: Year, Game or event, Date contested, League or governing body, Sport, Winning team, Losing team, Final score\nQ: Who was the losing team of the game that was contested on February 1, 2004?\nA: SELECT Losing team FROM 1-10548224-1 WHERE Date contested = 'February 1, 2004'"}
 60 | {"text": "table: 1-1057262-2\ncolumns: Crop (kilotonnes), New South Wales, Victoria, Queensland, Western Australia, South Australia, Tasmania, Total\nQ: what's the minimum\u00a0total\u00a0with\u00a0crop (kilotonnes)\u00a0being s lupin\nA: SELECT MIN Total FROM 1-1057262-2 WHERE Crop (kilotonnes) = 's Lupin'"}
 61 | {"text": "table: 1-1057262-2\ncolumns: Crop (kilotonnes), New South Wales, Victoria, Queensland, Western Australia, South Australia, Tasmania, Total\nQ: what's the\u00a0new south wales\u00a0with\u00a0crop (kilotonnes)\u00a0being canola\nA: SELECT New South Wales FROM 1-1057262-2 WHERE Crop (kilotonnes) = 'Canola'"}
 62 | {"text": "table: 1-1057262-2\ncolumns: Crop (kilotonnes), New South Wales, Victoria, Queensland, Western Australia, South Australia, Tasmania, Total\nQ: what's the total number of\u00a0south australia\u00a0with\u00a0victoria\u00a0value of 2173\nA: SELECT COUNT South Australia FROM 1-1057262-2 WHERE Victoria = 2173"}
 63 | {"text": "table: 1-1057262-2\ncolumns: Crop (kilotonnes), New South Wales, Victoria, Queensland, Western Australia, South Australia, Tasmania, Total\nQ: what's the minimum\u00a0tasmania value\nA: SELECT MIN Tasmania FROM 1-1057262-2"}
 64 | {"text": "table: 1-1057262-2\ncolumns: Crop (kilotonnes), New South Wales, Victoria, Queensland, Western Australia, South Australia, Tasmania, Total\nQ: what's the total number of\u00a0tasmania\u00a0with\u00a0new south wales\u00a0crop of 190 kilotonnes\nA: SELECT COUNT Tasmania FROM 1-1057262-2 WHERE New South Wales = 190"}
 65 | {"text": "table: 1-1058787-1\ncolumns: Approximate Age, Virtues, Psycho Social Crisis, Significant Relationship, Existential Question [ not in citation given ], Examples\nQ: How many significant relationships list Will as a virtue?\nA: SELECT COUNT Significant Relationship FROM 1-1058787-1 WHERE Virtues = 'Will'"}
 66 | {"text": "table: 1-1058787-1\ncolumns: Approximate Age, Virtues, Psycho Social Crisis, Significant Relationship, Existential Question [ not in citation given ], Examples\nQ: Which examples ask the existential question \"Can I Love?\"\nA: SELECT Examples FROM 1-1058787-1 WHERE Existential Question [ not in citation given ] = 'Can I Love?'"}
 67 | {"text": "table: 1-1059743-2\ncolumns: Rank, Member Association, Points, Group stage, Play-off, AFC Cup\nQ: How many countries got 796.7 points?\nA: SELECT COUNT Rank FROM 1-1059743-2 WHERE Points = '796.7'"}
 68 | {"text": "table: 1-1059743-2\ncolumns: Rank, Member Association, Points, Group stage, Play-off, AFC Cup\nQ: In what group stage were 177.2 points awarded?\nA: SELECT COUNT Group stage FROM 1-1059743-2 WHERE Points = '177.2'"}
 69 | {"text": "table: 1-1059743-2\ncolumns: Rank, Member Association, Points, Group stage, Play-off, AFC Cup\nQ: What is the lowest group to earn 886.6 points?\nA: SELECT MIN Group stage FROM 1-1059743-2 WHERE Points = '886.6'"}
 70 | {"text": "table: 1-1059743-2\ncolumns: Rank, Member Association, Points, Group stage, Play-off, AFC Cup\nQ: How many countries earned 177.2 points?\nA: SELECT COUNT Member Association FROM 1-1059743-2 WHERE Points = '177.2'"}
 71 | {"text": "table: 1-10586064-2\ncolumns: County, Precincts, Lunsford, % Lunsford, McConnell, % McConnell, Total\nQ: If % lunsford is 51.82% what is the % mcconnell in Letcher?\nA: SELECT % McConnell FROM 1-10586064-2 WHERE % Lunsford = '51.82%'"}
 72 | {"text": "table: 1-10586064-2\ncolumns: County, Precincts, Lunsford, % Lunsford, McConnell, % McConnell, Total\nQ: What country had the total 18,900 (r)?\nA: SELECT County FROM 1-10586064-2 WHERE Total = '18,900 (R)'"}
 73 | {"text": "table: 1-10586064-2\ncolumns: County, Precincts, Lunsford, % Lunsford, McConnell, % McConnell, Total\nQ: When % mcconnell is 44.54% what are the total number of counties?\nA: SELECT COUNT County FROM 1-10586064-2 WHERE % McConnell = '44.54%'"}
 74 | {"text": "table: 1-10586064-2\ncolumns: County, Precincts, Lunsford, % Lunsford, McConnell, % McConnell, Total\nQ: If % mcconnell is 47.17% what is the total number of mcconnell ?\nA: SELECT COUNT McConnell FROM 1-10586064-2 WHERE % McConnell = '47.17%'"}
 75 | {"text": "table: 1-10586064-2\ncolumns: County, Precincts, Lunsford, % Lunsford, McConnell, % McConnell, Total\nQ: What is the county of precints 515?\nA: SELECT County FROM 1-10586064-2 WHERE Precincts = 515"}
 76 | {"text": "table: 1-10601843-2\ncolumns: Stadium, Capacity, City, Country, Tenant, Opening\nQ: Which city has a capacity of 41903?\nA: SELECT City FROM 1-10601843-2 WHERE Capacity = 41903"}
 77 | {"text": "table: 1-10601843-2\ncolumns: Stadium, Capacity, City, Country, Tenant, Opening\nQ: What is the maximum capacity of the Otkrytie Arena stadium?\nA: SELECT MAX Capacity FROM 1-10601843-2 WHERE Stadium = 'Otkrytie Arena'"}
 78 | {"text": "table: 1-10601843-2\ncolumns: Stadium, Capacity, City, Country, Tenant, Opening\nQ: When did the stadium where Bursaspor is the tenant open?\nA: SELECT MIN Opening FROM 1-10601843-2 WHERE Tenant = 'Bursaspor'"}
 79 | {"text": "table: 1-10601843-2\ncolumns: Stadium, Capacity, City, Country, Tenant, Opening\nQ: How many tenants are there in the city of Samsun?\nA: SELECT COUNT Tenant FROM 1-10601843-2 WHERE City = 'Samsun'"}
 80 | {"text": "table: 1-10610087-5\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date\nQ: what's the\u00a0original air date\u00a0with\u00a0title\u00a0 \"hell\"\nA: SELECT Original air date FROM 1-10610087-5 WHERE Title = '\"Hell\"'"}
 81 | {"text": "table: 1-10638523-1\ncolumns: Particulars and Characteristics, Shivalik Zone, Mid-Hill Zone, High hill zone, Trance- n Himalaya Zone\nQ: What is the percentage of the Shivalik Zone where the percentage of the Mid-Hill Zone is 10%?\nA: SELECT Shivalik Zone FROM 1-10638523-1 WHERE Mid-Hill Zone = '10%'"}
 82 | {"text": "table: 1-10638523-1\ncolumns: Particulars and Characteristics, Shivalik Zone, Mid-Hill Zone, High hill zone, Trance- n Himalaya Zone\nQ: For mid-hill zone  what is the altitude?\nA: SELECT Mid-Hill Zone FROM 1-10638523-1 WHERE Particulars and Characteristics = 'Altitude'"}
 83 | {"text": "table: 1-10638523-1\ncolumns: Particulars and Characteristics, Shivalik Zone, Mid-Hill Zone, High hill zone, Trance- n Himalaya Zone\nQ: What are the climatic conditions for the trance- n himalaya zone?\nA: SELECT Trance- n Himalaya Zone FROM 1-10638523-1 WHERE Particulars and Characteristics = 'Climatic conditions'"}
 84 | {"text": "table: 1-10638523-1\ncolumns: Particulars and Characteristics, Shivalik Zone, Mid-Hill Zone, High hill zone, Trance- n Himalaya Zone\nQ: What is the percentage of the  trance- n himalaya zone that corresponds with the high hill zone is 25%?\nA: SELECT Trance- n Himalaya Zone FROM 1-10638523-1 WHERE High hill zone = '25%'"}
 85 | {"text": "table: 1-10644188-3\ncolumns: Total tenure rank, Uninterrupted rank, Name, State represented, Dates of service, Total tenure time, Uninterrupted time\nQ: What is the state of Ted Stevens?\nA: SELECT State represented FROM 1-10644188-3 WHERE Name = 'Ted Stevens'"}
 86 | {"text": "table: 1-10682862-68\ncolumns: Country, Players, Standard, Minor, First title, Last title\nQ: What's the standard of the country who won its first title in 1992?\nA: SELECT MAX Standard FROM 1-10682862-68 WHERE First title = 1992"}
 87 | {"text": "table: 1-10682862-68\ncolumns: Country, Players, Standard, Minor, First title, Last title\nQ: What's the smallest number of players?\nA: SELECT MIN Players FROM 1-10682862-68"}
 88 | {"text": "table: 1-10682862-68\ncolumns: Country, Players, Standard, Minor, First title, Last title\nQ: In what year was the last last title received, by any of the countries?\nA: SELECT MAX Last title FROM 1-10682862-68"}
 89 | {"text": "table: 1-10710364-1\ncolumns: Religious group, Population % 1961, Population % 1971, Population % 1981, Population % 1991, Population % 2001\nQ: What religious groups made up 0.72% of the Indian population in 2001?\nA: SELECT Religious group FROM 1-10710364-1 WHERE Population % 2001 = '0.72%'"}
 90 | {"text": "table: 1-10718868-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: What is the original air date for episode 15 of season 6?\nA: SELECT Original air date FROM 1-10718868-2 WHERE No. in season = 15"}
 91 | {"text": "table: 1-10718868-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: How many episodes in season 6 titles \"Poppin' Tags\"?\nA: SELECT COUNT No. in season FROM 1-10718868-2 WHERE Title = '\"Poppin' Tags\"'"}
 92 | {"text": "table: 1-10753917-1\ncolumns: Season, Driver, Team, Engine, Poles, Wins, Podiums, Points, Margin of defeat\nQ: Which podiums did the Williams team have with a margin of defeat of 2?\nA: SELECT Podiums FROM 1-10753917-1 WHERE Team = 'Williams' AND Margin of defeat = '2'"}
 93 | {"text": "table: 1-10753917-1\ncolumns: Season, Driver, Team, Engine, Poles, Wins, Podiums, Points, Margin of defeat\nQ: How many drivers on the williams team had a margin of defeat of 2?\nA: SELECT COUNT Driver FROM 1-10753917-1 WHERE Team = 'Williams' AND Margin of defeat = '2'"}
 94 | {"text": "table: 1-10753917-1\ncolumns: Season, Driver, Team, Engine, Poles, Wins, Podiums, Points, Margin of defeat\nQ: How many seasons was clay regazzoni the driver?\nA: SELECT COUNT Season FROM 1-10753917-1 WHERE Driver = 'Clay Regazzoni'"}
 95 | {"text": "table: 1-10753917-1\ncolumns: Season, Driver, Team, Engine, Poles, Wins, Podiums, Points, Margin of defeat\nQ: Which margin of defeats had points of 30?\nA: SELECT Margin of defeat FROM 1-10753917-1 WHERE Points = '30'"}
 96 | {"text": "table: 1-10753917-1\ncolumns: Season, Driver, Team, Engine, Poles, Wins, Podiums, Points, Margin of defeat\nQ: Which podiums did the alfa romeo team have?\nA: SELECT Podiums FROM 1-10753917-1 WHERE Team = 'Alfa Romeo'"}
 97 | {"text": "table: 1-10797636-1\ncolumns: Village (German), Village (Slovene), Number of people 1991, Percent of Slovenes 1991, Percent of Slovenes 1951\nQ: What was the percent of slovenes 1951 for bach?\nA: SELECT Percent of Slovenes 1951 FROM 1-10797636-1 WHERE Village (German) = 'Bach'"}
 98 | {"text": "table: 1-10812403-4\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What college's team is the Saskatchewan Roughriders?\nA: SELECT College FROM 1-10812403-4 WHERE CFL Team = 'Saskatchewan Roughriders'"}
 99 | {"text": "table: 1-10812403-4\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What position did Calvin Mccarty play?\nA: SELECT Position FROM 1-10812403-4 WHERE Player = 'Calvin McCarty'"}
100 | {"text": "table: 1-10812403-4\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: How many people were pick #30?\nA: SELECT COUNT Position FROM 1-10812403-4 WHERE Pick # = 30"}
101 | 


--------------------------------------------------------------------------------
/Data/lora/train.jsonl:
--------------------------------------------------------------------------------
   1 | {"text": "table: 1-1000181-1\ncolumns: State/territory, Text/background colour, Format, Current slogan, Current series, Notes\nQ: Tell me what the notes are for South Australia \nA: SELECT Notes FROM 1-1000181-1 WHERE Current slogan = 'SOUTH AUSTRALIA'"}
   2 | {"text": "table: 1-1000181-1\ncolumns: State/territory, Text/background colour, Format, Current slogan, Current series, Notes\nQ: What is the current series where the new series began in June 2011?\nA: SELECT Current series FROM 1-1000181-1 WHERE Notes = 'New series began in June 2011'"}
   3 | {"text": "table: 1-1000181-1\ncolumns: State/territory, Text/background colour, Format, Current slogan, Current series, Notes\nQ: What is the format for South Australia?\nA: SELECT Format FROM 1-1000181-1 WHERE State/territory = 'South Australia'"}
   4 | {"text": "table: 1-1000181-1\ncolumns: State/territory, Text/background colour, Format, Current slogan, Current series, Notes\nQ: Name the background colour for the Australian Capital Territory\nA: SELECT Text/background colour FROM 1-1000181-1 WHERE State/territory = 'Australian Capital Territory'"}
   5 | {"text": "table: 1-10007452-3\ncolumns: Order Year, Manufacturer, Model, Fleet Series (Quantity), Powertrain (Engine/Transmission), Fuel Propulsion\nQ: how many times is the fuel propulsion is cng?\nA: SELECT COUNT Fleet Series (Quantity) FROM 1-10007452-3 WHERE Fuel Propulsion = 'CNG'"}
   6 | {"text": "table: 1-10007452-3\ncolumns: Order Year, Manufacturer, Model, Fleet Series (Quantity), Powertrain (Engine/Transmission), Fuel Propulsion\nQ: what is the fuel propulsion where the fleet series (quantity) is 310-329 (20)?\nA: SELECT Fuel Propulsion FROM 1-10007452-3 WHERE Fleet Series (Quantity) = '310-329 (20)'"}
   7 | {"text": "table: 1-10007452-3\ncolumns: Order Year, Manufacturer, Model, Fleet Series (Quantity), Powertrain (Engine/Transmission), Fuel Propulsion\nQ: who is the manufacturer for the order year 1998?\nA: SELECT Manufacturer FROM 1-10007452-3 WHERE Order Year = '1998'"}
   8 | {"text": "table: 1-10007452-3\ncolumns: Order Year, Manufacturer, Model, Fleet Series (Quantity), Powertrain (Engine/Transmission), Fuel Propulsion\nQ: how many times is the model ge40lfr?\nA: SELECT COUNT Manufacturer FROM 1-10007452-3 WHERE Model = 'GE40LFR'"}
   9 | {"text": "table: 1-10007452-3\ncolumns: Order Year, Manufacturer, Model, Fleet Series (Quantity), Powertrain (Engine/Transmission), Fuel Propulsion\nQ: how many times is the fleet series (quantity) is 468-473 (6)?\nA: SELECT COUNT Order Year FROM 1-10007452-3 WHERE Fleet Series (Quantity) = '468-473 (6)'"}
  10 | {"text": "table: 1-10007452-3\ncolumns: Order Year, Manufacturer, Model, Fleet Series (Quantity), Powertrain (Engine/Transmission), Fuel Propulsion\nQ: what is the powertrain (engine/transmission) when the order year is 2000?\nA: SELECT Powertrain (Engine/Transmission) FROM 1-10007452-3 WHERE Order Year = '2000'"}
  11 | {"text": "table: 1-10006830-1\ncolumns: Aircraft, Description, Max Gross Weight, Total disk area, Max disk Loading\nQ: What if the description of a ch-47d chinook?\nA: SELECT Description FROM 1-10006830-1 WHERE Aircraft = 'CH-47D Chinook'"}
  12 | {"text": "table: 1-10006830-1\ncolumns: Aircraft, Description, Max Gross Weight, Total disk area, Max disk Loading\nQ: What is the max gross weight of the Robinson R-22?\nA: SELECT Max Gross Weight FROM 1-10006830-1 WHERE Aircraft = 'Robinson R-22'"}
  13 | {"text": "table: 1-10015132-1\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What school did player number 6 come from?\nA: SELECT School/Club Team FROM 1-10015132-1 WHERE No. = '6'"}
  14 | {"text": "table: 1-10015132-1\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What school did the player that has been in Toronto from 2012-present come from?\nA: SELECT School/Club Team FROM 1-10015132-1 WHERE Years in Toronto = '2012-present'"}
  15 | {"text": "table: 1-10015132-1\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What school did the player that has been in Toronto from 2010-2012 go to?\nA: SELECT School/Club Team FROM 1-10015132-1 WHERE Years in Toronto = '2010-2012'"}
  16 | {"text": "table: 1-10015132-1\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What position did the player from Baylor play?\nA: SELECT Position FROM 1-10015132-1 WHERE School/Club Team = 'Baylor'"}
  17 | {"text": "table: 1-10015132-14\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: Who played in the Toronto Raptors from 1995-96?\nA: SELECT Player FROM 1-10015132-14 WHERE Years in Toronto = '1995-96'"}
  18 | {"text": "table: 1-10015132-14\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: Which number was Patrick O'Bryant?\nA: SELECT No. FROM 1-10015132-14 WHERE Player = 'Patrick O'Bryant'"}
  19 | {"text": "table: 1-10015132-14\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What school did Patrick O'Bryant play for?\nA: SELECT School/Club Team FROM 1-10015132-14 WHERE Player = 'Patrick O'Bryant'"}
  20 | {"text": "table: 1-10015132-14\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: How many number does Fordham school have?\nA: SELECT COUNT No. FROM 1-10015132-14 WHERE School/Club Team = 'Fordham'"}
  21 | {"text": "table: 1-10015132-14\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: Which school was in Toronto in 2001-02?\nA: SELECT School/Club Team FROM 1-10015132-14 WHERE Years in Toronto = '2001-02'"}
  22 | {"text": "table: 1-10015132-21\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: Which school did the player that played 2004-05 attend?\nA: SELECT School/Club Team FROM 1-10015132-21 WHERE Years in Toronto = '2004-05'"}
  23 | {"text": "table: 1-10015132-21\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: Which position does Loren Woods play?\nA: SELECT Position FROM 1-10015132-21 WHERE Player = 'Loren Woods'"}
  24 | {"text": "table: 1-10015132-21\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What number is the player that played 1998-2001\nA: SELECT MIN No. FROM 1-10015132-21 WHERE Years in Toronto = '1998-2001'"}
  25 | {"text": "table: 1-10015132-21\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: Which country is the player that went to Georgetown from?\nA: SELECT Nationality FROM 1-10015132-21 WHERE School/Club Team = 'Georgetown'"}
  26 | {"text": "table: 1-10015132-21\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: Which school did Herb Williams go to?\nA: SELECT School/Club Team FROM 1-10015132-21 WHERE Player = 'Herb Williams'"}
  27 | {"text": "table: 1-10015132-3\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: When did the player from Hawaii play for Toronto?\nA: SELECT Years in Toronto FROM 1-10015132-3 WHERE School/Club Team = 'Hawaii'"}
  28 | {"text": "table: 1-10015132-3\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: During what period did Dell Curry play for Toronto?\nA: SELECT Years in Toronto FROM 1-10015132-3 WHERE Player = 'Dell Curry'"}
  29 | {"text": "table: 1-10015132-3\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What's the number of the player from Boise State?\nA: SELECT No. FROM 1-10015132-3 WHERE School/Club Team = 'Boise State'"}
  30 | {"text": "table: 1-10015132-3\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What's Dell Curry nationality?\nA: SELECT Nationality FROM 1-10015132-3 WHERE Player = 'Dell Curry'"}
  31 | {"text": "table: 1-10015132-7\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: which player is from georgia\nA: SELECT Player FROM 1-10015132-7 WHERE School/Club Team = 'Georgia'"}
  32 | {"text": "table: 1-10015132-7\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: what school is rudy gay from\nA: SELECT School/Club Team FROM 1-10015132-7 WHERE Player = 'Rudy Gay'"}
  33 | {"text": "table: 1-10015132-7\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: what nationality is the player who played from 1997-98\nA: SELECT Nationality FROM 1-10015132-7 WHERE Years in Toronto = '1997-98'"}
  34 | {"text": "table: 1-10015132-7\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: what position did the player from connecticut play\nA: SELECT Position FROM 1-10015132-7 WHERE School/Club Team = 'Connecticut'"}
  35 | {"text": "table: 1-10015132-2\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: During which years was Marcus Banks in Toronto?\nA: SELECT Years in Toronto FROM 1-10015132-2 WHERE Player = 'Marcus Banks'"}
  36 | {"text": "table: 1-10015132-2\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: Which positions were in Toronto in 2004?\nA: SELECT Position FROM 1-10015132-2 WHERE Years in Toronto = '2004'"}
  37 | {"text": "table: 1-10015132-2\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What nationality is the player Muggsy Bogues?\nA: SELECT Nationality FROM 1-10015132-2 WHERE Player = 'Muggsy Bogues'"}
  38 | {"text": "table: 1-10015132-2\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What years was the player Lonny Baxter in Toronto?\nA: SELECT Years in Toronto FROM 1-10015132-2 WHERE Player = 'Lonny Baxter'"}
  39 | {"text": "table: 1-10015132-2\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: How many players were with the school or club team La Salle?\nA: SELECT COUNT Player FROM 1-10015132-2 WHERE School/Club Team = 'La Salle'"}
  40 | {"text": "table: 1-10021158-3\ncolumns: Year, Tournaments played, Cuts made*, Wins, 2nd, Top 10s, Best finish, Earnings ($), Money list rank, Scoring average, Scoring rank\nQ: When the scoring rank was 117, what was the best finish?\nA: SELECT Best finish FROM 1-10021158-3 WHERE Scoring rank = '117'"}
  41 | {"text": "table: 1-10021158-3\ncolumns: Year, Tournaments played, Cuts made*, Wins, 2nd, Top 10s, Best finish, Earnings ($), Money list rank, Scoring average, Scoring rank\nQ: When the best finish was T69, how many people came in 2nd?\nA: SELECT 2nd FROM 1-10021158-3 WHERE Best finish = 'T69'"}
  42 | {"text": "table: 1-10021158-3\ncolumns: Year, Tournaments played, Cuts made*, Wins, 2nd, Top 10s, Best finish, Earnings ($), Money list rank, Scoring average, Scoring rank\nQ: How many wins were there when the money list rank was 183?\nA: SELECT COUNT Wins FROM 1-10021158-3 WHERE Money list rank = '183'"}
  43 | {"text": "table: 1-10021158-3\ncolumns: Year, Tournaments played, Cuts made*, Wins, 2nd, Top 10s, Best finish, Earnings ($), Money list rank, Scoring average, Scoring rank\nQ: When the money list rank was n/a, what was the scoring average?\nA: SELECT Scoring average FROM 1-10021158-3 WHERE Money list rank = 'n/a'"}
  44 | {"text": "table: 1-10021158-3\ncolumns: Year, Tournaments played, Cuts made*, Wins, 2nd, Top 10s, Best finish, Earnings ($), Money list rank, Scoring average, Scoring rank\nQ: What time was the highest for 2nd finishers?\nA: SELECT MAX 2nd FROM 1-10021158-3"}
  45 | {"text": "table: 1-1004033-1\ncolumns: Season, Player, Position, Nationality, Team, Draft Pick #, Draft Class, College\nQ: When did the Metrostars have their first Rookie of the Year winner?\nA: SELECT MIN Season FROM 1-1004033-1 WHERE Team = 'MetroStars'"}
  46 | {"text": "table: 1-1004033-1\ncolumns: Season, Player, Position, Nationality, Team, Draft Pick #, Draft Class, College\nQ: What college did the Rookie of the Year from the Columbus Crew attend?\nA: SELECT College FROM 1-1004033-1 WHERE Team = 'Columbus Crew'"}
  47 | {"text": "table: 1-1004033-1\ncolumns: Season, Player, Position, Nationality, Team, Draft Pick #, Draft Class, College\nQ: How many teams had a #1 draft pick that won the Rookie of the Year Award?\nA: SELECT COUNT Team FROM 1-1004033-1 WHERE Draft Pick # = '1'"}
  48 | {"text": "table: 1-1004033-1\ncolumns: Season, Player, Position, Nationality, Team, Draft Pick #, Draft Class, College\nQ: What position did the #10 draft pick play?\nA: SELECT Position FROM 1-1004033-1 WHERE Draft Pick # = '10'"}
  49 | {"text": "table: 1-10023387-1\ncolumns: Player, Years Played, Total W-L, Singles W-L, Doubles W-L\nQ: what's the\u00a0years played\u00a0with\u00a0singles w-l\u00a0of 3\u20132\nA: SELECT Years Played FROM 1-10023387-1 WHERE Singles W-L = '3\u20132'"}
  50 | {"text": "table: 1-10023387-1\ncolumns: Player, Years Played, Total W-L, Singles W-L, Doubles W-L\nQ: what's the\u00a0doubles w-l\u00a0for player\u00a0seol jae-min (none)\nA: SELECT Doubles W-L FROM 1-10023387-1 WHERE Player = 'Seol Jae-Min (none)'"}
  51 | {"text": "table: 1-10023387-1\ncolumns: Player, Years Played, Total W-L, Singles W-L, Doubles W-L\nQ: what's the\u00a0singles w-l\u00a0for kim doo-hwan\nA: SELECT Singles W-L FROM 1-10023387-1 WHERE Player = 'Kim Doo-Hwan'"}
  52 | {"text": "table: 1-10023387-1\ncolumns: Player, Years Played, Total W-L, Singles W-L, Doubles W-L\nQ: what's the total number of\u00a0singles w-l\u00a0with\u00a0doubles w-l\u00a0of 0\u20130 and\u00a0total w-l\u00a0of 3\u20131\nA: SELECT COUNT Singles W-L FROM 1-10023387-1 WHERE Doubles W-L = '0\u20130' AND Total W-L = '3\u20131'"}
  53 | {"text": "table: 1-10023387-1\ncolumns: Player, Years Played, Total W-L, Singles W-L, Doubles W-L\nQ: what's the\u00a0doubles w-l\u00a0with\u00a0years played\u00a0value of 1 (1968)\nA: SELECT Doubles W-L FROM 1-10023387-1 WHERE Years Played = '1 (1968)'"}
  54 | {"text": "table: 1-10023387-1\ncolumns: Player, Years Played, Total W-L, Singles W-L, Doubles W-L\nQ: what\u00a0years are played\u00a0for player\u00a0 im chung-yang\nA: SELECT Years Played FROM 1-10023387-1 WHERE Player = 'Im Chung-Yang'"}
  55 | {"text": "table: 1-10020178-1\ncolumns: Name, Canton, Height (meters), Crest length (meters), Type, Year of construction, Name of the Lake\nQ: What is the name of the 375 crest length?\nA: SELECT Name FROM 1-10020178-1 WHERE Crest length (meters) = 375"}
  56 | {"text": "table: 1-10020178-1\ncolumns: Name, Canton, Height (meters), Crest length (meters), Type, Year of construction, Name of the Lake\nQ: What is year of construction of spitallamm?\nA: SELECT MIN Year of construction FROM 1-10020178-1 WHERE Name = 'Spitallamm'"}
  57 | {"text": "table: 1-10020178-1\ncolumns: Name, Canton, Height (meters), Crest length (meters), Type, Year of construction, Name of the Lake\nQ: What is the canton of grande dixence?\nA: SELECT Canton FROM 1-10020178-1 WHERE Name = 'Grande Dixence'"}
  58 | {"text": "table: 1-10020178-1\ncolumns: Name, Canton, Height (meters), Crest length (meters), Type, Year of construction, Name of the Lake\nQ: What is the name where lago di luzzone is?\nA: SELECT Name FROM 1-10020178-1 WHERE Name of the Lake = 'Lago di Luzzone'"}
  59 | {"text": "table: 1-100518-1\ncolumns: Name, Direction, Mantra, Weapon, Consort, Graha (Planet), Guardian M\u0101t\u1e5bk\u0101\nQ: What is the  guardian m\u0101t\u1e5bk\u0101 for the guardian whose consort is Sv\u0101h\u0101?\nA: SELECT Guardian M\u0101t\u1e5bk\u0101 FROM 1-100518-1 WHERE Consort = 'Sv\u0101h\u0101'"}
  60 | {"text": "table: 1-100518-1\ncolumns: Name, Direction, Mantra, Weapon, Consort, Graha (Planet), Guardian M\u0101t\u1e5bk\u0101\nQ: Where the mantra is \"o\u1e43 ya\u1e43 v\u0101yuve nama\u1e25\", what is the direction of the guardian?\nA: SELECT Direction FROM 1-100518-1 WHERE Mantra = 'O\u1e43 Ya\u1e43 V\u0101yuve Nama\u1e25'"}
  61 | {"text": "table: 1-100518-1\ncolumns: Name, Direction, Mantra, Weapon, Consort, Graha (Planet), Guardian M\u0101t\u1e5bk\u0101\nQ: What weapon is used by the guardian whose consort is \u015bac\u012b?\nA: SELECT Weapon FROM 1-100518-1 WHERE Consort = '\u015aac\u012b'"}
  62 | {"text": "table: 1-100518-1\ncolumns: Name, Direction, Mantra, Weapon, Consort, Graha (Planet), Guardian M\u0101t\u1e5bk\u0101\nQ: What are the directions for the guardian whose weapon is kha\u1e0dga (sword)?\nA: SELECT Direction FROM 1-100518-1 WHERE Weapon = 'Kha\u1e0dga (sword)'"}
  63 | {"text": "table: 1-100518-1\ncolumns: Name, Direction, Mantra, Weapon, Consort, Graha (Planet), Guardian M\u0101t\u1e5bk\u0101\nQ: What are the weapons used by guardians for the direction East?\nA: SELECT Weapon FROM 1-100518-1 WHERE Direction = 'East'"}
  64 | {"text": "table: 1-100518-1\ncolumns: Name, Direction, Mantra, Weapon, Consort, Graha (Planet), Guardian M\u0101t\u1e5bk\u0101\nQ: What are the directions for the guardian whose graha (planet) is b\u1e5bhaspati (Jupiter)?\nA: SELECT Direction FROM 1-100518-1 WHERE Graha (Planet) = 'B\u1e5bhaspati (Jupiter)'"}
  65 | {"text": "table: 1-10054296-1\ncolumns: Member, Headquarters, Classification, Chapters, Founded, UCCFS\nQ: What is the number of chapters listed for the fraternity with a headquarters in Austin, Texas?\nA: SELECT MAX Chapters FROM 1-10054296-1 WHERE Classification = 'Fraternity' AND Headquarters = 'Austin, Texas'"}
  66 | {"text": "table: 1-10054296-1\ncolumns: Member, Headquarters, Classification, Chapters, Founded, UCCFS\nQ: What are the members listed with the sorority classification\nA: SELECT Member FROM 1-10054296-1 WHERE Classification = 'Sorority'"}
  67 | {"text": "table: 1-10054296-1\ncolumns: Member, Headquarters, Classification, Chapters, Founded, UCCFS\nQ: Name the member that has 12 chapters\nA: SELECT Member FROM 1-10054296-1 WHERE Chapters = 12"}
  68 | {"text": "table: 1-10054296-1\ncolumns: Member, Headquarters, Classification, Chapters, Founded, UCCFS\nQ: Where is the headquarters of Alpha Nu Omega\nA: SELECT Headquarters FROM 1-10054296-1 WHERE Member = 'Alpha Nu Omega'"}
  69 | {"text": "table: 1-1007688-1\ncolumns: Year, Typhus, Typhoid fever, Relapsing fever, Smallpox, Malaria\nQ: what is the number of relapsing fever when malaria is 3000\nA: SELECT MIN Relapsing fever FROM 1-1007688-1 WHERE Malaria = '3000'"}
  70 | {"text": "table: 1-1007688-1\ncolumns: Year, Typhus, Typhoid fever, Relapsing fever, Smallpox, Malaria\nQ: what is the typhoid fever number for the year 1934\nA: SELECT Typhoid fever FROM 1-1007688-1 WHERE Year = '1934'"}
  71 | {"text": "table: 1-1007688-1\ncolumns: Year, Typhus, Typhoid fever, Relapsing fever, Smallpox, Malaria\nQ: What are all the typhus number when smallpox is 4\nA: SELECT Typhus FROM 1-1007688-1 WHERE Smallpox = 4"}
  72 | {"text": "table: 1-1007688-1\ncolumns: Year, Typhus, Typhoid fever, Relapsing fever, Smallpox, Malaria\nQ: what is the number of smallpox when typhoid fever is 293\nA: SELECT MAX Smallpox FROM 1-1007688-1 WHERE Typhoid fever = 293"}
  73 | {"text": "table: 1-1007688-1\ncolumns: Year, Typhus, Typhoid fever, Relapsing fever, Smallpox, Malaria\nQ: what is the typhoid fever number for the year 1929\nA: SELECT Typhoid fever FROM 1-1007688-1 WHERE Year = '1929'"}
  74 | {"text": "table: 1-10082596-1\ncolumns: School, Location, Founded, Affiliation, Enrollment, Team Nickname, Primary conference\nQ: How many schools are in Bloomington, IN?\nA: SELECT COUNT Founded FROM 1-10082596-1 WHERE Location = 'Bloomington, IN'"}
  75 | {"text": "table: 1-10082596-1\ncolumns: School, Location, Founded, Affiliation, Enrollment, Team Nickname, Primary conference\nQ: How many of the schools are designated private/Presbyterian?\nA: SELECT COUNT Location FROM 1-10082596-1 WHERE Affiliation = 'Private/Presbyterian'"}
  76 | {"text": "table: 1-10082596-1\ncolumns: School, Location, Founded, Affiliation, Enrollment, Team Nickname, Primary conference\nQ: In what year was Lindenwood University founded?\nA: SELECT MIN Founded FROM 1-10082596-1 WHERE School = 'Lindenwood University'"}
  77 | {"text": "table: 1-10082596-1\ncolumns: School, Location, Founded, Affiliation, Enrollment, Team Nickname, Primary conference\nQ: How many of the schools listed are in Ames, IA?\nA: SELECT COUNT Primary conference FROM 1-10082596-1 WHERE Location = 'Ames, IA'"}
  78 | {"text": "table: 1-1008653-9\ncolumns: Country ( exonym ), Capital ( exonym ), Country ( endonym ), Capital ( endonym ), Official or native language(s) (alphabet/script)\nQ: What is the capital (endonym) where Douglas is the Capital (exonym)?\nA: SELECT Capital ( endonym ) FROM 1-1008653-9 WHERE Capital ( exonym ) = 'Douglas'"}
  79 | {"text": "table: 1-1008653-9\ncolumns: Country ( exonym ), Capital ( exonym ), Country ( endonym ), Capital ( endonym ), Official or native language(s) (alphabet/script)\nQ: How many countries (endonym) has the capital (endonym) of Jakarta?\nA: SELECT COUNT Country ( endonym ) FROM 1-1008653-9 WHERE Capital ( endonym ) = 'Jakarta'"}
  80 | {"text": "table: 1-1008653-9\ncolumns: Country ( exonym ), Capital ( exonym ), Country ( endonym ), Capital ( endonym ), Official or native language(s) (alphabet/script)\nQ: What is the country (exonym) where the official or native language(s) (alphabet/script) is Icelandic?\nA: SELECT Country ( exonym ) FROM 1-1008653-9 WHERE Official or native language(s) (alphabet/script) = 'Icelandic'"}
  81 | {"text": "table: 1-1008653-9\ncolumns: Country ( exonym ), Capital ( exonym ), Country ( endonym ), Capital ( endonym ), Official or native language(s) (alphabet/script)\nQ: In which country (endonym) is Irish English the official or native language(s) (alphabet/script)?\nA: SELECT Country ( endonym ) FROM 1-1008653-9 WHERE Official or native language(s) (alphabet/script) = 'Irish English'"}
  82 | {"text": "table: 1-1008653-9\ncolumns: Country ( exonym ), Capital ( exonym ), Country ( endonym ), Capital ( endonym ), Official or native language(s) (alphabet/script)\nQ: Which country (exonym) is the country (endonym) isle of man ellan vannin?\nA: SELECT Country ( exonym ) FROM 1-1008653-9 WHERE Country ( endonym ) = 'Isle of Man Ellan Vannin'"}
  83 | {"text": "table: 1-1009087-1\ncolumns: Season, Network, Season premiere, Season finale, TV season, Ranking, Viewers (in millions)\nQ: The season premiere aired on September 11, 2000 aired on how many networks? \nA: SELECT COUNT Network FROM 1-1009087-1 WHERE Season premiere = 'September 11, 2000'"}
  84 | {"text": "table: 1-1009087-1\ncolumns: Season, Network, Season premiere, Season finale, TV season, Ranking, Viewers (in millions)\nQ: What was the ranking of the season finale aired on May 8, 2006? \nA: SELECT Ranking FROM 1-1009087-1 WHERE Season finale = 'May 8, 2006'"}
  85 | {"text": "table: 1-1011906-1\ncolumns: Regional County Municipality (RCM), Population Canada 2011 Census, Land Area, Density (pop. per km2), Seat of RCM\nQ: what is the minimum\u00a0population canada 2011 census\u00a0with\u00a0seat of rcm\u00a0being cowansville\nA: SELECT MIN Population Canada 2011 Census FROM 1-1011906-1 WHERE Seat of RCM = 'Cowansville'"}
  86 | {"text": "table: 1-1011906-1\ncolumns: Regional County Municipality (RCM), Population Canada 2011 Census, Land Area, Density (pop. per km2), Seat of RCM\nQ: what's the\u00a0land area\u00a0with\u00a0seat of rcm\u00a0being granby\nA: SELECT Land Area FROM 1-1011906-1 WHERE Seat of RCM = 'Granby'"}
  87 | {"text": "table: 1-101196-1\ncolumns: County, English name, Irish name, Population, Irish speakers\nQ: What is the population for County Mayo with the English Name Carrowteige?\nA: SELECT Population FROM 1-101196-1 WHERE County = 'County Mayo' AND English name = 'Carrowteige'"}
  88 | {"text": "table: 1-101196-1\ncolumns: County, English name, Irish name, Population, Irish speakers\nQ: What is the Irish name listed with 62% Irish speakers?\nA: SELECT Irish name FROM 1-101196-1 WHERE Irish speakers = '62%'"}
  89 | {"text": "table: 1-101196-1\ncolumns: County, English name, Irish name, Population, Irish speakers\nQ: What is the population for the Irish Name Leitir meall\u00e1in?\nA: SELECT Population FROM 1-101196-1 WHERE Irish name = 'Leitir Meall\u00e1in'"}
  90 | {"text": "table: 1-101196-1\ncolumns: County, English name, Irish name, Population, Irish speakers\nQ: What is the county for the Irish name Carna?\nA: SELECT County FROM 1-101196-1 WHERE Irish name = 'Carna'"}
  91 | {"text": "table: 1-101196-1\ncolumns: County, English name, Irish name, Population, Irish speakers\nQ: How many County Kerry have 53% Irish speakers?\nA: SELECT COUNT English name FROM 1-101196-1 WHERE Irish speakers = '53%' AND County = 'County Kerry'"}
  92 | {"text": "table: 1-101196-1\ncolumns: County, English name, Irish name, Population, Irish speakers\nQ: What is the population for the English name Spiddal?\nA: SELECT Population FROM 1-101196-1 WHERE English name = 'Spiddal'"}
  93 | {"text": "table: 1-10118412-6\ncolumns: State/Territory, Asian American Population (2010 Census), Chinese, Filipino, Indian, Japanese, Korean, Vietnamese, Other Asian\nQ: What is the the Chinese population for the state that has a Filipino population of 1474707?\nA: SELECT MIN Chinese FROM 1-10118412-6 WHERE Filipino = 1474707"}
  94 | {"text": "table: 1-10118412-6\ncolumns: State/Territory, Asian American Population (2010 Census), Chinese, Filipino, Indian, Japanese, Korean, Vietnamese, Other Asian\nQ: How many States have an Indian population of 30947?\nA: SELECT COUNT Filipino FROM 1-10118412-6 WHERE Indian = 30947"}
  95 | {"text": "table: 1-10118412-6\ncolumns: State/Territory, Asian American Population (2010 Census), Chinese, Filipino, Indian, Japanese, Korean, Vietnamese, Other Asian\nQ: What is the highest Indian population?\nA: SELECT MAX Indian FROM 1-10118412-6"}
  96 | {"text": "table: 1-10121127-1\ncolumns: UN Operation name, UN Operation title, Location, Dates of Australian involvement, Number of Australians involved, Australian role\nQ: What is Australia's role in the UN operation Unama?\nA: SELECT Australian role FROM 1-10121127-1 WHERE UN Operation name = 'UNAMA'"}
  97 | {"text": "table: 1-10121127-1\ncolumns: UN Operation name, UN Operation title, Location, Dates of Australian involvement, Number of Australians involved, Australian role\nQ: What is the UN operation title with the UN operation name, Uncok?\nA: SELECT UN Operation title FROM 1-10121127-1 WHERE UN Operation name = 'UNCOK'"}
  98 | {"text": "table: 1-10121127-1\ncolumns: UN Operation name, UN Operation title, Location, Dates of Australian involvement, Number of Australians involved, Australian role\nQ: How many Australians were in the UN commission on Korea?\nA: SELECT COUNT Number of Australians involved FROM 1-10121127-1 WHERE UN Operation title = 'UN Commission on Korea'"}
  99 | {"text": "table: 1-10121127-1\ncolumns: UN Operation name, UN Operation title, Location, Dates of Australian involvement, Number of Australians involved, Australian role\nQ: When was it where 65 Australians were involved in the UN?\nA: SELECT Dates of Australian involvement FROM 1-10121127-1 WHERE Number of Australians involved = '65'"}
 100 | {"text": "table: 1-10120207-8\ncolumns: Season, Timeslot ( ET ), Season premiere, Season finale, TV season, Rank, Viewers (millions)\nQ: What year is the season with the 10.73 million views?\nA: SELECT TV season FROM 1-10120207-8 WHERE Viewers (millions) = '10.73'"}
 101 | {"text": "table: 1-10120207-8\ncolumns: Season, Timeslot ( ET ), Season premiere, Season finale, TV season, Rank, Viewers (millions)\nQ: What is the season year where the rank is 39?\nA: SELECT TV season FROM 1-10120207-8 WHERE Rank = '39'"}
 102 | {"text": "table: 1-10120207-8\ncolumns: Season, Timeslot ( ET ), Season premiere, Season finale, TV season, Rank, Viewers (millions)\nQ: What is the number of season premieres were 10.17 people watched?\nA: SELECT COUNT Season premiere FROM 1-10120207-8 WHERE Viewers (millions) = '10.17'"}
 103 | {"text": "table: 1-10120207-8\ncolumns: Season, Timeslot ( ET ), Season premiere, Season finale, TV season, Rank, Viewers (millions)\nQ: What is the year of the season that was 12?\nA: SELECT TV season FROM 1-10120207-8 WHERE Season = 12"}
 104 | {"text": "table: 1-1012730-1\ncolumns: Year, Starts, Wins, Top 5, Top 10, Poles, Avg. Start, Avg. Finish, Winnings, Position, Team(s)\nQ: In 2012 what was the average finish?\nA: SELECT Avg. Finish FROM 1-1012730-1 WHERE Year = 2012"}
 105 | {"text": "table: 1-1012730-1\ncolumns: Year, Starts, Wins, Top 5, Top 10, Poles, Avg. Start, Avg. Finish, Winnings, Position, Team(s)\nQ: How many wins happened in 1983?\nA: SELECT MIN Wins FROM 1-1012730-1 WHERE Year = 1983"}
 106 | {"text": "table: 1-1012730-1\ncolumns: Year, Starts, Wins, Top 5, Top 10, Poles, Avg. Start, Avg. Finish, Winnings, Position, Team(s)\nQ: How many top tens had an average start of 29.4?\nA: SELECT COUNT Top 10 FROM 1-1012730-1 WHERE Avg. Start = '29.4'"}
 107 | {"text": "table: 1-1012730-1\ncolumns: Year, Starts, Wins, Top 5, Top 10, Poles, Avg. Start, Avg. Finish, Winnings, Position, Team(s)\nQ: How many poles had an average finish of 19.1?\nA: SELECT MAX Poles FROM 1-1012730-1 WHERE Avg. Finish = '19.1'"}
 108 | {"text": "table: 1-1012730-1\ncolumns: Year, Starts, Wins, Top 5, Top 10, Poles, Avg. Start, Avg. Finish, Winnings, Position, Team(s)\nQ: How many starts did Hendrick motorsports have?\nA: SELECT MIN Starts FROM 1-1012730-1 WHERE Team(s) = 'Hendrick Motorsports'"}
 109 | {"text": "table: 1-1013129-10\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: NHL players are all centre in Florida panthers.\nA: SELECT Player FROM 1-1013129-10 WHERE Position = 'Centre' AND NHL team = 'Florida Panthers'"}
 110 | {"text": "table: 1-1013129-10\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: NHL team player San Jose Sharks is United States nationally.\nA: SELECT Player FROM 1-1013129-10 WHERE NHL team = 'San Jose Sharks' AND Nationality = 'United States'"}
 111 | {"text": "table: 1-1013129-10\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: All players are position mark polak.\nA: SELECT Position FROM 1-1013129-10 WHERE Player = 'Mark Polak'"}
 112 | {"text": "table: 1-1013129-10\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: Position in nhl team centre are all smaller pick than 243.0\nA: SELECT NHL team FROM 1-1013129-10 WHERE Position = 'Centre' AND Pick < 243.0"}
 113 | {"text": "table: 1-1013129-11\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: What college/junior/club teams do the players from the St. Louis Blues come from?\nA: SELECT College/junior/club team FROM 1-1013129-11 WHERE NHL team = 'St. Louis Blues'"}
 114 | {"text": "table: 1-1013129-11\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: What teams do the players from TPS (Finland) play for?\nA: SELECT NHL team FROM 1-1013129-11 WHERE College/junior/club team = 'TPS (Finland)'"}
 115 | {"text": "table: 1-1013129-11\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: What high school team did Doug Nolan play for?\nA: SELECT College/junior/club team FROM 1-1013129-11 WHERE Player = 'Doug Nolan'"}
 116 | {"text": "table: 1-1013129-11\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: What club team is Per Gustafsson play for?\nA: SELECT College/junior/club team FROM 1-1013129-11 WHERE Player = 'Per Gustafsson'"}
 117 | {"text": "table: 1-1013129-11\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: What is the nationality of Shayne Wright?\nA: SELECT Nationality FROM 1-1013129-11 WHERE Player = 'Shayne Wright'"}
 118 | {"text": "table: 1-10128185-2\ncolumns: Song, Mobiles, Northern Ireland, Northern England, Scotland, Southern England, Wales, Total\nQ: How many votes did Southern England cast whilst Northern Ireland cast 3?\nA: SELECT Southern England FROM 1-10128185-2 WHERE Northern Ireland = 3"}
 119 | {"text": "table: 1-10128185-2\ncolumns: Song, Mobiles, Northern Ireland, Northern England, Scotland, Southern England, Wales, Total\nQ: What was the lowest number of votes Scotland cast?\nA: SELECT MIN Scotland FROM 1-10128185-2"}
 120 | {"text": "table: 1-10128185-2\ncolumns: Song, Mobiles, Northern Ireland, Northern England, Scotland, Southern England, Wales, Total\nQ: What is the total number of votes if Scotland cast 35?\nA: SELECT COUNT Scotland FROM 1-10128185-2 WHERE Total = 35"}
 121 | {"text": "table: 1-10128185-2\ncolumns: Song, Mobiles, Northern Ireland, Northern England, Scotland, Southern England, Wales, Total\nQ: How many votes did Northern Ireland cast if the total was 35?\nA: SELECT Northern Ireland FROM 1-10128185-2 WHERE Total = 35"}
 122 | {"text": "table: 1-10128185-2\ncolumns: Song, Mobiles, Northern Ireland, Northern England, Scotland, Southern England, Wales, Total\nQ: How many votes did Wales cast when Northern England cast 6?\nA: SELECT MIN Wales FROM 1-10128185-2 WHERE Northern England = 6"}
 123 | {"text": "table: 1-1012730-2\ncolumns: Year, Starts, Wins, Top 5, Top 10, Poles, Avg. Start, Avg. Finish, Winnings, Position, Team(s)\nQ: What teams had 9 in the top 5 and 1 wins?\nA: SELECT Team(s) FROM 1-1012730-2 WHERE Top 5 = 9 AND Wins = 1"}
 124 | {"text": "table: 1-1013129-1\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: What teams did the player vadim sharifijanov play for?\nA: SELECT College/junior/club team FROM 1-1013129-1 WHERE Player = 'Vadim Sharifijanov'"}
 125 | {"text": "table: 1-1013129-1\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: What positions do the hartford whalers nhl team have?\nA: SELECT Position FROM 1-1013129-1 WHERE NHL team = 'Hartford Whalers'"}
 126 | {"text": "table: 1-1013129-1\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: What is the smallest pick for the player, brett lindros?\nA: SELECT MIN Pick FROM 1-1013129-1 WHERE Player = 'Brett Lindros'"}
 127 | {"text": "table: 1-1013129-1\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: What positions does the college/junior/club team, molot perm (russia) have?\nA: SELECT Position FROM 1-1013129-1 WHERE College/junior/club team = 'Molot Perm (Russia)'"}
 128 | {"text": "table: 1-1013129-1\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: The nhl team new york islanders is what nationality?\nA: SELECT Nationality FROM 1-1013129-1 WHERE NHL team = 'New York Islanders'"}
 129 | {"text": "table: 1-1013168-3\ncolumns: District, Vacator, Reason for change, Successor, Date successor seated\nQ: What is the name of the vacator for district Louisiana 1st?\nA: SELECT Vacator FROM 1-1013168-3 WHERE District = 'Louisiana 1st'"}
 130 | {"text": "table: 1-101336-1\ncolumns: Formula, Notation, T c (K), No. of Cu-O planes in unit cell, Crystal structure\nQ: What is the notion when the crystal structure is tetragonal and the formula is bi 2 sr 2 cacu 2 o 8\nA: SELECT Notation FROM 1-101336-1 WHERE Crystal structure = 'Tetragonal' AND Formula = 'Bi 2 Sr 2 CaCu 2 O 8'"}
 131 | {"text": "table: 1-101336-1\ncolumns: Formula, Notation, T c (K), No. of Cu-O planes in unit cell, Crystal structure\nQ: How many times is the formula tl 2 ba 2 cuo 6?\nA: SELECT No. of Cu-O planes in unit cell FROM 1-101336-1 WHERE Formula = 'Tl 2 Ba 2 CuO 6'"}
 132 | {"text": "table: 1-101336-1\ncolumns: Formula, Notation, T c (K), No. of Cu-O planes in unit cell, Crystal structure\nQ: What is the crystal structure for the formula yba 2 cu 3 o 7?\nA: SELECT Crystal structure FROM 1-101336-1 WHERE Formula = 'YBa 2 Cu 3 O 7'"}
 133 | {"text": "table: 1-101336-1\ncolumns: Formula, Notation, T c (K), No. of Cu-O planes in unit cell, Crystal structure\nQ: What is the number for t c (k) when the notation is tl-2212?\nA: SELECT COUNT T c (K) FROM 1-101336-1 WHERE Notation = 'Tl-2212'"}
 134 | {"text": "table: 1-10138926-1\ncolumns: #, City, 1981 Census, 1991 Census, 2001 Census, 2010 Est., Region\nQ: How many 2010 estimations have been done in the city of Cremona?\nA: SELECT COUNT 2010 Est. FROM 1-10138926-1 WHERE City = 'Cremona'"}
 135 | {"text": "table: 1-10138926-1\ncolumns: #, City, 1981 Census, 1991 Census, 2001 Census, 2010 Est., Region\nQ: What's the 2001 census of the region of Abruzzo where the 1871 census is bigger than 51092.0?\nA: SELECT MIN 2001 Census FROM 1-10138926-1 WHERE Region = 'Abruzzo' AND 1981 Census > 51092.0"}
 136 | {"text": "table: 1-10138926-1\ncolumns: #, City, 1981 Census, 1991 Census, 2001 Census, 2010 Est., Region\nQ: What's the 1991 census of the city of Carpi?\nA: SELECT MAX 1991 Census FROM 1-10138926-1 WHERE City = 'Carpi'"}
 137 | {"text": "table: 1-10138926-1\ncolumns: #, City, 1981 Census, 1991 Census, 2001 Census, 2010 Est., Region\nQ: How many 2001 censuses are there on number 13?\nA: SELECT COUNT 2001 Census FROM 1-10138926-1 WHERE # = 13"}
 138 | {"text": "table: 1-10138926-1\ncolumns: #, City, 1981 Census, 1991 Census, 2001 Census, 2010 Est., Region\nQ: What's the 1981 census of Livorno?\nA: SELECT 1981 Census FROM 1-10138926-1 WHERE City = 'Livorno'"}
 139 | {"text": "table: 1-1013129-8\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: Which NHL team has player Mike Loach?\nA: SELECT NHL team FROM 1-1013129-8 WHERE Player = 'Mike Loach'"}
 140 | {"text": "table: 1-1013129-8\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: What is the NHL team that has Peter Strom?\nA: SELECT NHL team FROM 1-1013129-8 WHERE Player = 'Peter Strom'"}
 141 | {"text": "table: 1-1013129-8\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: What team is Keith Mccambridge on?\nA: SELECT College/junior/club team FROM 1-1013129-8 WHERE Player = 'Keith McCambridge'"}
 142 | {"text": "table: 1-1013129-8\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: How many nationalities are the pick 193?\nA: SELECT COUNT Nationality FROM 1-1013129-8 WHERE Pick = 193"}
 143 | {"text": "table: 1-1013168-2\ncolumns: State (class), Vacator, Reason for change, Successor, Date of successors formal installation\nQ: Who was the succesor that was formally installed on November 8, 1978?\nA: SELECT Successor FROM 1-1013168-2 WHERE Date of successors formal installation = 'November 8, 1978'"}
 144 | {"text": "table: 1-1014319-1\ncolumns: Week, Dance/song, Horwood, Goodman, Dixon, Tonioli, Total, Result\nQ: How many songs received a 10 from Goodman and were rated by Tonioli?\nA: SELECT COUNT Tonioli FROM 1-1014319-1 WHERE Goodman = '10'"}
 145 | {"text": "table: 1-1014319-1\ncolumns: Week, Dance/song, Horwood, Goodman, Dixon, Tonioli, Total, Result\nQ: What score did Goodman give to all songs with safe results, which received a 7 from Horwood and have a total score of 31?\nA: SELECT Goodman FROM 1-1014319-1 WHERE Total = '31' AND Horwood = '7' AND Result = 'Safe'"}
 146 | {"text": "table: 1-1014319-1\ncolumns: Week, Dance/song, Horwood, Goodman, Dixon, Tonioli, Total, Result\nQ: What score did Dixon give to the song \"samba / young hearts run free\", which was in second place?\nA: SELECT Dixon FROM 1-1014319-1 WHERE Dance/song = 'Samba / Young Hearts Run Free' AND Result = 'Second place'"}
 147 | {"text": "table: 1-1014319-1\ncolumns: Week, Dance/song, Horwood, Goodman, Dixon, Tonioli, Total, Result\nQ: How many scores did Goodman give to \"samba / young hearts run free\", which was in second place?\nA: SELECT COUNT Goodman FROM 1-1014319-1 WHERE Result = 'Second place' AND Dance/song = 'Samba / Young Hearts Run Free'"}
 148 | {"text": "table: 1-1015421-1\ncolumns: Class, Operator, No. Built, Year Built, Cars per Set, Unit nos.\nQ: What year was number 7 built?\nA: SELECT Year Built FROM 1-1015421-1 WHERE No. Built = 7"}
 149 | {"text": "table: 1-1015914-24\ncolumns: Case/Suffix, we two, you and I, you two, them two (the two), who-two\nQ: What is we two when the case/suffix is loc.?\nA: SELECT we two FROM 1-1015914-24 WHERE Case/Suffix = 'loc.'"}
 150 | {"text": "table: 1-1015914-24\ncolumns: Case/Suffix, we two, you and I, you two, them two (the two), who-two\nQ: What is them two (the two) when we two is ngalbelpa?\nA: SELECT them two (the two) FROM 1-1015914-24 WHERE we two = 'ngalbelpa'"}
 151 | {"text": "table: 1-1015914-24\ncolumns: Case/Suffix, we two, you and I, you two, them two (the two), who-two\nQ: What is them two (the two) when you and i is ng\u0153balngu?\nA: SELECT them two (the two) FROM 1-1015914-24 WHERE you and I = 'ng\u0153balngu'"}
 152 | {"text": "table: 1-1015914-24\ncolumns: Case/Suffix, we two, you and I, you two, them two (the two), who-two\nQ: What is who-two where you and i is ng\u0153ban?\nA: SELECT who-two FROM 1-1015914-24 WHERE you and I = 'ng\u0153ban'"}
 153 | {"text": "table: 1-1015914-24\ncolumns: Case/Suffix, we two, you and I, you two, them two (the two), who-two\nQ: What is we two where you two is ngipen?\nA: SELECT we two FROM 1-1015914-24 WHERE you two = 'ngipen'"}
 154 | {"text": "table: 1-1015914-24\ncolumns: Case/Suffix, we two, you and I, you two, them two (the two), who-two\nQ: What is who-two when you two is ngipelngu?\nA: SELECT who-two FROM 1-1015914-24 WHERE you two = 'ngipelngu'"}
 155 | {"text": "table: 1-10160447-1\ncolumns: Position, Driver, Points, Winnings, Series\nQ: what's the\u00a0points\u00a0with\u00a0driver\u00a0 mark martin\nA: SELECT Points FROM 1-10160447-1 WHERE Driver = 'Mark Martin'"}
 156 | {"text": "table: 1-10160447-1\ncolumns: Position, Driver, Points, Winnings, Series\nQ: what's the\u00a0points\u00a0with\u00a0driver\u00a0 rusty wallace\nA: SELECT Points FROM 1-10160447-1 WHERE Driver = 'Rusty Wallace'"}
 157 | {"text": "table: 1-10160447-1\ncolumns: Position, Driver, Points, Winnings, Series\nQ: what's the total number of\u00a0position\u00a0with\u00a0driver\u00a0 robby gordon\nA: SELECT COUNT Position FROM 1-10160447-1 WHERE Driver = 'Robby Gordon'"}
 158 | {"text": "table: 1-10160447-1\ncolumns: Position, Driver, Points, Winnings, Series\nQ: what's the maximum\u00a0position\u00a0with\u00a0winnings\u00a0 $50,000\nA: SELECT MAX Position FROM 1-10160447-1 WHERE Winnings = '$50,000'"}
 159 | {"text": "table: 1-10236830-6\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: What actor was nominted for an award in the film Anastasiya Slutskaya?\nA: SELECT Actors Name FROM 1-10236830-6 WHERE Film Name = 'Anastasiya Slutskaya'"}
 160 | {"text": "table: 1-10236830-6\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: What was the film Falling up nominated for?\nA: SELECT Nomination FROM 1-10236830-6 WHERE Film Name = 'Falling Up'"}
 161 | {"text": "table: 1-10236830-6\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: What is the name of the actress that was nominated for best actress in a leading role in the film Chopin: Desire for love?\nA: SELECT Actors Name FROM 1-10236830-6 WHERE Film Name = 'Chopin: Desire for Love' AND Nomination = 'Best Actress in a Leading Role'"}
 162 | {"text": "table: 1-10236830-6\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: What is the name of the actress that was nominated for best actress in a leading role in the film Chopin: Desire for love?\nA: SELECT Actors Name FROM 1-10236830-6 WHERE Film Name = 'Chopin: Desire for Love' AND Nomination = 'Best Actress in a Leading Role'"}
 163 | {"text": "table: 1-10236830-6\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: Which films does the actor Alla Sergiyko star in?\nA: SELECT Film Name FROM 1-10236830-6 WHERE Actors Name = 'Alla Sergiyko'"}
 164 | {"text": "table: 1-10236830-6\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: Which nominations was the film 27 Stolen Kisses nominated for?\nA: SELECT Nomination FROM 1-10236830-6 WHERE Film Name = '27 Stolen Kisses'"}
 165 | {"text": "table: 1-10236830-4\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: Which actor from Serbia was nominated for best actor in a supporting role?\nA: SELECT Actors Name FROM 1-10236830-4 WHERE Nomination = 'Best Actor in a Supporting Role' AND Country = 'Serbia'"}
 166 | {"text": "table: 1-10236830-4\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: Vsevolod Shilovskiy is from what country?\nA: SELECT Country FROM 1-10236830-4 WHERE Actors Name = 'Vsevolod Shilovskiy'"}
 167 | {"text": "table: 1-10236830-4\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: Which nominations are connected to the film Totalitarian Romance?\nA: SELECT Nomination FROM 1-10236830-4 WHERE Film Name = 'Totalitarian Romance'"}
 168 | {"text": "table: 1-10236830-4\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: Srdjan Dragojevic worked on a film which earned what nomination?\nA: SELECT Nomination FROM 1-10236830-4 WHERE Director = 'Srdjan Dragojevic'"}
 169 | {"text": "table: 1-10236830-4\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: Which actors are from Ukraine?\nA: SELECT Actors Name FROM 1-10236830-4 WHERE Country = 'Ukraine'"}
 170 | {"text": "table: 1-10236830-1\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: What was the film that vadim ilyenko directed?\nA: SELECT Film Name FROM 1-10236830-1 WHERE Director = 'Vadim Ilyenko'"}
 171 | {"text": "table: 1-10236830-1\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: What was the actors name that vadim ilyenko directed?\nA: SELECT Actors Name FROM 1-10236830-1 WHERE Director = 'Vadim Ilyenko'"}
 172 | {"text": "table: 1-10236830-1\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: What was the actors name for fuchzhou and nomination was best non-professional actor?\nA: SELECT Actors Name FROM 1-10236830-1 WHERE Film Name = 'Fuchzhou' AND Nomination = 'Best Non-Professional Actor'"}
 173 | {"text": "table: 1-10236830-1\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: What film did michaylo ilyenko make with best actor in a supporting role?\nA: SELECT Film Name FROM 1-10236830-1 WHERE Director = 'Michaylo Ilyenko' AND Nomination = 'Best Actor in a Supporting Role'"}
 174 | {"text": "table: 1-10236830-1\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: What was the actor's name for best debut?\nA: SELECT Actors Name FROM 1-10236830-1 WHERE Nomination = 'Best Debut'"}
 175 | {"text": "table: 1-10236830-1\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: What was the number of nominations for natalia raskokoha?\nA: SELECT COUNT Nomination FROM 1-10236830-1 WHERE Actors Name = 'Natalia Raskokoha'"}
 176 | {"text": "table: 1-10240125-1\ncolumns: Season, Division, League Apps, League Goals, FA Cup Apps, FA Cup Goals, Total Apps, Total Goals\nQ: What is the highest value of Total Goals?\nA: SELECT MAX Total Goals FROM 1-10240125-1"}
 177 | {"text": "table: 1-10240125-1\ncolumns: Season, Division, League Apps, League Goals, FA Cup Apps, FA Cup Goals, Total Apps, Total Goals\nQ: When FA Cup Apps is 9 what is the smallest number of FA Cup Goals?\nA: SELECT MIN FA Cup Goals FROM 1-10240125-1 WHERE FA Cup Apps = 9"}
 178 | {"text": "table: 1-10240125-1\ncolumns: Season, Division, League Apps, League Goals, FA Cup Apps, FA Cup Goals, Total Apps, Total Goals\nQ: What is the smallest number of Total Goals?\nA: SELECT MIN Total Goals FROM 1-10240125-1"}
 179 | {"text": "table: 1-10264179-2\ncolumns: Round, Circuit, Date, Pole Position, Fastest Lap, Winning Driver, Winning Team\nQ: What circuit was the race where Hideki Mutoh had the fastest lap?\nA: SELECT Circuit FROM 1-10264179-2 WHERE Fastest Lap = 'Hideki Mutoh'"}
 180 | {"text": "table: 1-10269427-3\ncolumns: Episode #, Production code, Title, Directed by, Written by, Airdate\nQ: what is the minimum production code with title \"foreign exchange problem / turn about\"\nA: SELECT MIN Production code FROM 1-10269427-3 WHERE Title = '\"Foreign Exchange Problem / Turn About\"'"}
 181 | {"text": "table: 1-10269427-3\ncolumns: Episode #, Production code, Title, Directed by, Written by, Airdate\nQ: what is the episode # for title \"the yindianapolis 500 / personality problem\"\nA: SELECT Episode # FROM 1-10269427-3 WHERE Title = '\"The Yindianapolis 500 / Personality Problem\"'"}
 182 | {"text": "table: 1-10269427-3\ncolumns: Episode #, Production code, Title, Directed by, Written by, Airdate\nQ: what is the episode # for production code 227\nA: SELECT Episode # FROM 1-10269427-3 WHERE Production code = 227"}
 183 | {"text": "table: 1-10269427-3\ncolumns: Episode #, Production code, Title, Directed by, Written by, Airdate\nQ: who directed the movie written by is sib ventress / aydrea ten bosch\nA: SELECT Directed by FROM 1-10269427-3 WHERE Written by = 'Sib Ventress / Aydrea ten Bosch'"}
 184 | {"text": "table: 1-10269427-3\ncolumns: Episode #, Production code, Title, Directed by, Written by, Airdate\nQ: what is the production code with title \"skirting the issue / moon over my yinnie\"\nA: SELECT Production code FROM 1-10269427-3 WHERE Title = '\"Skirting the Issue / Moon Over my Yinnie\"'"}
 185 | {"text": "table: 1-10240125-2\ncolumns: Season, Division, League Apps, League Goals, FA Cup Apps, FA Cup Goals, Total Apps, Total Goals\nQ: Whatis the number of total goals maximum?\nA: SELECT MAX Total Goals FROM 1-10240125-2"}
 186 | {"text": "table: 1-10262329-1\ncolumns: Assembly Type, Adhesive Type, Time(Sec), Temp (\u00b0C), Pressure\nQ: HOW MANY TEMPERATURE INTERVALS ARE POSSIBLE TO USE WITH ACRYL? \nA: SELECT COUNT Temp (\u00b0C) FROM 1-10262329-1 WHERE Adhesive Type = 'Acryl'"}
 187 | {"text": "table: 1-1028356-3\ncolumns: Outcome, Year, Championship, Surface, Partner, Opponents, Score\nQ: How many matches where played with Jim Pugh?\nA: SELECT COUNT Opponents FROM 1-1028356-3 WHERE Partner = 'Jim Pugh'"}
 188 | {"text": "table: 1-1028356-3\ncolumns: Outcome, Year, Championship, Surface, Partner, Opponents, Score\nQ: What is the score with partner Jim Pugh?\nA: SELECT Score FROM 1-1028356-3 WHERE Partner = 'Jim Pugh'"}
 189 | {"text": "table: 1-1028356-3\ncolumns: Outcome, Year, Championship, Surface, Partner, Opponents, Score\nQ: How many matched scored 3\u20136, 7\u20136(5), 6\u20133?\nA: SELECT COUNT Surface FROM 1-1028356-3 WHERE Score = '3\u20136, 7\u20136(5), 6\u20133'"}
 190 | {"text": "table: 1-1028356-3\ncolumns: Outcome, Year, Championship, Surface, Partner, Opponents, Score\nQ: What is the score of the match with partner Jim Pugh?\nA: SELECT Score FROM 1-1028356-3 WHERE Partner = 'Jim Pugh'"}
 191 | {"text": "table: 1-1028356-3\ncolumns: Outcome, Year, Championship, Surface, Partner, Opponents, Score\nQ: What year was the championship in Wimbledon (2)?\nA: SELECT Year FROM 1-1028356-3 WHERE Championship = 'Wimbledon (2)'"}
 192 | {"text": "table: 1-1028356-3\ncolumns: Outcome, Year, Championship, Surface, Partner, Opponents, Score\nQ: What is the score of the match with opponents Gretchen Magers Kelly Jones?\nA: SELECT Score FROM 1-1028356-3 WHERE Opponents = 'Gretchen Magers Kelly Jones'"}
 193 | {"text": "table: 1-10284385-1\ncolumns: Begin Date, End Date, Representative, Date of birth, House term, State served, Party, Age (years, days)\nQ: How many birthdays does Earl Hanley Beshlin have?\nA: SELECT COUNT Date of birth FROM 1-10284385-1 WHERE Representative = 'Earl Hanley Beshlin'"}
 194 | {"text": "table: 1-10284385-1\ncolumns: Begin Date, End Date, Representative, Date of birth, House term, State served, Party, Age (years, days)\nQ: Which politican party has a birthday of November 10, 1880\nA: SELECT Party FROM 1-10284385-1 WHERE Date of birth = 'November 10, 1880'"}
 195 | {"text": "table: 1-10284385-1\ncolumns: Begin Date, End Date, Representative, Date of birth, House term, State served, Party, Age (years, days)\nQ: Which representative has a birthday of January 31, 1866?\nA: SELECT Representative FROM 1-10284385-1 WHERE Date of birth = 'January 31, 1866'"}
 196 | {"text": "table: 1-10295819-1\ncolumns: Player, Current singles ranking, Current doubles ranking, First year played, Ties played, Total W\u2013L, Singles W\u2013L, Doubles W\u2013L\nQ: What is the Singles W-L for the players named  Laurynas Grigelis?\nA: SELECT Singles W\u2013L FROM 1-10295819-1 WHERE Player = 'Laurynas Grigelis'"}
 197 | {"text": "table: 1-10295819-1\ncolumns: Player, Current singles ranking, Current doubles ranking, First year played, Ties played, Total W\u2013L, Singles W\u2013L, Doubles W\u2013L\nQ: What is the Current singles ranking for the player named Mantas Bugaili\u0161kis?\nA: SELECT Current singles ranking FROM 1-10295819-1 WHERE Player = 'Mantas Bugaili\u0161kis'"}
 198 | {"text": "table: 1-10312547-1\ncolumns: Character, 1954 Broadway, 1955 broadcast, 1960 broadcast, 1979 Broadway, 1990 Broadway, 1991 Broadway, 1998 Broadway, 1999 Broadway\nQ: How many playerd Mrs. Darling in the 1999 Broadway?\nA: SELECT COUNT 1999 Broadway FROM 1-10312547-1 WHERE Character = 'Mrs. Darling'"}
 199 | {"text": "table: 1-10312547-1\ncolumns: Character, 1954 Broadway, 1955 broadcast, 1960 broadcast, 1979 Broadway, 1990 Broadway, 1991 Broadway, 1998 Broadway, 1999 Broadway\nQ: Who played Peter Pan in the 1990 Broadway?\nA: SELECT 1990 Broadway FROM 1-10312547-1 WHERE Character = 'Peter Pan'"}
 200 | {"text": "table: 1-10312547-1\ncolumns: Character, 1954 Broadway, 1955 broadcast, 1960 broadcast, 1979 Broadway, 1990 Broadway, 1991 Broadway, 1998 Broadway, 1999 Broadway\nQ: Who played in the 1991 Broadway before Barbara McCulloh played the part in the 1999 Broadway?\nA: SELECT 1991 Broadway FROM 1-10312547-1 WHERE 1999 Broadway = 'Barbara McCulloh'"}
 201 | {"text": "table: 1-10312547-1\ncolumns: Character, 1954 Broadway, 1955 broadcast, 1960 broadcast, 1979 Broadway, 1990 Broadway, 1991 Broadway, 1998 Broadway, 1999 Broadway\nQ: Who played in the 1990 Broadway after Tom Halloran played the character in the 1955 Broadcast?\nA: SELECT 1990 Broadway FROM 1-10312547-1 WHERE 1955 broadcast = 'Tom Halloran'"}
 202 | {"text": "table: 1-10312547-1\ncolumns: Character, 1954 Broadway, 1955 broadcast, 1960 broadcast, 1979 Broadway, 1990 Broadway, 1991 Broadway, 1998 Broadway, 1999 Broadway\nQ: What character did Drake English play in the 1998 Broadway?\nA: SELECT Character FROM 1-10312547-1 WHERE 1998 Broadway = 'Drake English'"}
 203 | {"text": "table: 1-103084-4\ncolumns: Year, Broadcast date, BBC One total viewing, BBC One Rank, BBC Two total viewing, BBC Two Rank\nQ: What date was BBC One total viewing greater then 11616996.338225884?\nA: SELECT Broadcast date FROM 1-103084-4 WHERE BBC One total viewing > 11616996.338225884"}
 204 | {"text": "table: 1-103084-4\ncolumns: Year, Broadcast date, BBC One total viewing, BBC One Rank, BBC Two total viewing, BBC Two Rank\nQ: How many years did BBC One rank 20th?\nA: SELECT COUNT Year FROM 1-103084-4 WHERE BBC One Rank = '20th'"}
 205 | {"text": "table: 1-103084-4\ncolumns: Year, Broadcast date, BBC One total viewing, BBC One Rank, BBC Two total viewing, BBC Two Rank\nQ: What year was the BBC two total viewing 7,530,000?\nA: SELECT MIN Year FROM 1-103084-4 WHERE BBC Two total viewing = '7,530,000'"}
 206 | {"text": "table: 1-10321124-1\ncolumns: \u2193 Function / Genus \u2192, Shigella, Salmonella, Yersinia, Escherichia\nQ:  how many\u00a0\u2193 function / genus \u2192\u00a0with\u00a0escherichia\u00a0being espd\nA: SELECT COUNT \u2193 Function / Genus \u2192 FROM 1-10321124-1 WHERE Escherichia = 'EspD'"}
 207 | {"text": "table: 1-10321124-1\ncolumns: \u2193 Function / Genus \u2192, Shigella, Salmonella, Yersinia, Escherichia\nQ: what's the\u00a0salmonella\u00a0with\u00a0escherichia\u00a0being espd\nA: SELECT Salmonella FROM 1-10321124-1 WHERE Escherichia = 'EspD'"}
 208 | {"text": "table: 1-10321124-1\ncolumns: \u2193 Function / Genus \u2192, Shigella, Salmonella, Yersinia, Escherichia\nQ: what's the\u00a0\u2193 function / genus \u2192\u00a0with\u00a0shigella\u00a0being spa32\nA: SELECT \u2193 Function / Genus \u2192 FROM 1-10321124-1 WHERE Shigella = 'Spa32'"}
 209 | {"text": "table: 1-10321124-1\ncolumns: \u2193 Function / Genus \u2192, Shigella, Salmonella, Yersinia, Escherichia\nQ: what's the\u00a0salmonella\u00a0with\u00a0shigella\u00a0being ipgc\nA: SELECT Salmonella FROM 1-10321124-1 WHERE Shigella = 'IpgC'"}
 210 | {"text": "table: 1-10321124-1\ncolumns: \u2193 Function / Genus \u2192, Shigella, Salmonella, Yersinia, Escherichia\nQ: what's the\u00a0salmonella\u00a0with\u00a0escherichia\u00a0being sepb (escn)\nA: SELECT Salmonella FROM 1-10321124-1 WHERE Escherichia = 'SepB (EscN)'"}
 211 | {"text": "table: 1-10321124-1\ncolumns: \u2193 Function / Genus \u2192, Shigella, Salmonella, Yersinia, Escherichia\nQ: what's the\u00a0shigella\u00a0with\u00a0yersinia\u00a0being yscp\nA: SELECT Shigella FROM 1-10321124-1 WHERE Yersinia = 'YscP'"}
 212 | {"text": "table: 1-10321805-1\ncolumns: Year (Ceremony), Film title used in nomination, Original title, Director, Result\nQ: How many original titles did Marriage Italian-Style have? \nA: SELECT COUNT Original title FROM 1-10321805-1 WHERE Film title used in nomination = 'Marriage Italian-Style'"}
 213 | {"text": "table: 1-10321805-1\ncolumns: Year (Ceremony), Film title used in nomination, Original title, Director, Result\nQ: What year was a movie with the original title La Leggenda del Santo Bevitore submitted?\nA: SELECT Year (Ceremony) FROM 1-10321805-1 WHERE Original title = 'La leggenda del santo bevitore'"}
 214 | {"text": "table: 1-10335-1\ncolumns: Camp, Estimated deaths, Operational, Occupied territory, Current country of location, Primary means for mass killings\nQ: what's the\u00a0camp\u00a0with\u00a0estimated deaths\u00a0of 600,000\nA: SELECT Camp FROM 1-10335-1 WHERE Estimated deaths = '600,000'"}
 215 | {"text": "table: 1-10335-1\ncolumns: Camp, Estimated deaths, Operational, Occupied territory, Current country of location, Primary means for mass killings\nQ: what's the\u00a0operational period\u00a0with\u00a0camp\u00a0 sajmi\u0161te\nA: SELECT Operational FROM 1-10335-1 WHERE Camp = 'Sajmi\u0161te'"}
 216 | {"text": "table: 1-10335-1\ncolumns: Camp, Estimated deaths, Operational, Occupied territory, Current country of location, Primary means for mass killings\nQ: what's the\u00a0estimated deaths\u00a0with\u00a0operational period\u00a0of 17 march 1942 \u2013 end of june 1943\nA: SELECT Estimated deaths FROM 1-10335-1 WHERE Operational = '17 March 1942 \u2013 end of June 1943'"}
 217 | {"text": "table: 1-10335-1\ncolumns: Camp, Estimated deaths, Operational, Occupied territory, Current country of location, Primary means for mass killings\nQ: what's the\u00a0current country of location\u00a0with\u00a0operational period\u00a0\u00a0of summer of 1941 to 28 june 1944\nA: SELECT Current country of location FROM 1-10335-1 WHERE Operational = 'Summer of 1941 to 28 June 1944'"}
 218 | {"text": "table: 1-10335-1\ncolumns: Camp, Estimated deaths, Operational, Occupied territory, Current country of location, Primary means for mass killings\nQ: what's the\u00a0occupied territory\u00a0with\u00a0estimated deaths\u00a0of 600,000\nA: SELECT Occupied territory FROM 1-10335-1 WHERE Estimated deaths = '600,000'"}
 219 | {"text": "table: 1-10335-1\ncolumns: Camp, Estimated deaths, Operational, Occupied territory, Current country of location, Primary means for mass killings\nQ: what's the\u00a0occupied territory\u00a0with\u00a0operational\u00a0period of may 1940 \u2013 january 1945\nA: SELECT Occupied territory FROM 1-10335-1 WHERE Operational = 'May 1940 \u2013 January 1945'"}
 220 | {"text": "table: 1-10360823-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: Which overall pick was traded to the Cleveland Browns?\nA: SELECT Overall FROM 1-10360823-1 WHERE College = 'Traded to the Cleveland Browns'"}
 221 | {"text": "table: 1-10360823-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: Overall pick 240 was a pick in which round?\nA: SELECT Round FROM 1-10360823-1 WHERE Overall = 240"}
 222 | {"text": "table: 1-10360823-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: Which overall pick number went to college at Youngstown State?\nA: SELECT MIN Overall FROM 1-10360823-1 WHERE College = 'Youngstown State'"}
 223 | {"text": "table: 1-10360823-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: What position is played by pick 255 overall?\nA: SELECT Position FROM 1-10360823-1 WHERE Overall = 255"}
 224 | {"text": "table: 1-10360823-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: Which player was chosen in round 17?\nA: SELECT Player name FROM 1-10360823-1 WHERE Round = 17"}
 225 | {"text": "table: 1-10361453-2\ncolumns: Game, Date, Opponent, Result, Vikings points, Opponents, Record, Attendance\nQ: The record of 7-3 had the largest attendance of what?\nA: SELECT MAX Attendance FROM 1-10361453-2 WHERE Record = '7-3'"}
 226 | {"text": "table: 1-10361453-2\ncolumns: Game, Date, Opponent, Result, Vikings points, Opponents, Record, Attendance\nQ: The record of 9-4 was against which opponent?\nA: SELECT Opponent FROM 1-10361453-2 WHERE Record = '9-4'"}
 227 | {"text": "table: 1-10361453-2\ncolumns: Game, Date, Opponent, Result, Vikings points, Opponents, Record, Attendance\nQ: The game number of 8 had a record of what?\nA: SELECT Record FROM 1-10361453-2 WHERE Game = 8"}
 228 | {"text": "table: 1-10360656-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: What round was Steve Stonebreaker drafted?\nA: SELECT MAX Round FROM 1-10360656-1 WHERE Player name = 'Steve Stonebreaker'"}
 229 | {"text": "table: 1-10360656-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: Who was the top picki n the draft?\nA: SELECT MIN Choice FROM 1-10360656-1"}
 230 | {"text": "table: 1-10360656-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: What round was Bill Hill drafted?\nA: SELECT Choice FROM 1-10360656-1 WHERE Player name = 'Bill Hill'"}
 231 | {"text": "table: 1-10360656-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: What was the name of the quarterback drafted?\nA: SELECT Player name FROM 1-10360656-1 WHERE Position = 'Quarterback'"}
 232 | {"text": "table: 1-10361625-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: Where is the college where Keith Hartwig plays?\nA: SELECT College FROM 1-10361625-1 WHERE Player name = 'Keith Hartwig'"}
 233 | {"text": "table: 1-10361625-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: What is the name of the linebacker at Illinois college?\nA: SELECT Player name FROM 1-10361625-1 WHERE Position = 'Linebacker' AND College = 'Illinois'"}
 234 | {"text": "table: 1-10361625-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: What is the greatest round of overall 83?\nA: SELECT MAX Round FROM 1-10361625-1 WHERE Overall = 83"}
 235 | {"text": "table: 1-10361625-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: Which round did Tommy Kramer play in>\nA: SELECT Round FROM 1-10361625-1 WHERE Player name = 'Tommy Kramer'"}
 236 | {"text": "table: 1-10361625-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: What is Rice's collage score?\nA: SELECT Overall FROM 1-10361625-1 WHERE College = 'Rice'"}
 237 | {"text": "table: 1-10361230-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: Where does the defensive back position appear first?\nA: SELECT MIN Round FROM 1-10361230-1 WHERE Position = 'Defensive Back'"}
 238 | {"text": "table: 1-10361230-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: What is Bruce Cerone overall?\nA: SELECT MIN Overall FROM 1-10361230-1 WHERE Player name = 'Bruce Cerone'"}
 239 | {"text": "table: 1-10361230-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: Which player went to Emporia State?\nA: SELECT Player name FROM 1-10361230-1 WHERE College = 'Emporia State'"}
 240 | {"text": "table: 1-10361230-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: What is the highest choice?\nA: SELECT MAX Choice FROM 1-10361230-1"}
 241 | {"text": "table: 1-10361230-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: What college did Bill Cappleman go to?\nA: SELECT College FROM 1-10361230-1 WHERE Player name = 'Bill Cappleman'"}
 242 | {"text": "table: 1-1036189-1\ncolumns: Headstamp ID, Primer Annulus Color, Bullet Tip Color, Other Features, Functional Type\nQ: For the headstamp id of h2, what was the color of the bullet tip?\nA: SELECT Bullet Tip Color FROM 1-1036189-1 WHERE Headstamp ID = 'H2'"}
 243 | {"text": "table: 1-1036189-1\ncolumns: Headstamp ID, Primer Annulus Color, Bullet Tip Color, Other Features, Functional Type\nQ: For the functional type of light ball, what were the other features?\nA: SELECT Other Features FROM 1-1036189-1 WHERE Functional Type = 'Light Ball'"}
 244 | {"text": "table: 1-1036189-1\ncolumns: Headstamp ID, Primer Annulus Color, Bullet Tip Color, Other Features, Functional Type\nQ: How many primers annulus colors were there when the color of the bullet tip was white?\nA: SELECT COUNT Primer Annulus Color FROM 1-1036189-1 WHERE Bullet Tip Color = 'White'"}
 245 | {"text": "table: 1-1036189-1\ncolumns: Headstamp ID, Primer Annulus Color, Bullet Tip Color, Other Features, Functional Type\nQ: How many bullet tips colors had other features of a blue band on case base?\nA: SELECT COUNT Bullet Tip Color FROM 1-1036189-1 WHERE Other Features = 'Blue band on case base'"}
 246 | {"text": "table: 1-1037590-1\ncolumns: Year, Games, Games started, Completions, Attempts, Completion %, Yards, Yards/Attempt, Touchdowns, Interceptions, Rating\nQ: How many touchdowns were scored in the year with a completion percentage of 56.0?\nA: SELECT MIN Touchdowns FROM 1-1037590-1 WHERE Completion % = '56.0'"}
 247 | {"text": "table: 1-1037590-1\ncolumns: Year, Games, Games started, Completions, Attempts, Completion %, Yards, Yards/Attempt, Touchdowns, Interceptions, Rating\nQ: What number of completions are recorded for the year with 12 games started?\nA: SELECT Completions FROM 1-1037590-1 WHERE Games started = 12"}
 248 | {"text": "table: 1-1037590-1\ncolumns: Year, Games, Games started, Completions, Attempts, Completion %, Yards, Yards/Attempt, Touchdowns, Interceptions, Rating\nQ: How many years were there with 348 attempts?\nA: SELECT COUNT Yards FROM 1-1037590-1 WHERE Attempts = 348"}
 249 | {"text": "table: 1-10402018-1\ncolumns: Character, Australia & New Zealand (Sydney - first run, Melbourne, Auckland), London, Toronto / Broadway, Brazil, UK Tour, US Tour, Italy (Milan, Rome, Trieste)\nQ: How many characters is by Babs Rubenstein?\nA: SELECT COUNT London FROM 1-10402018-1 WHERE US Tour = 'Babs Rubenstein'"}
 250 | {"text": "table: 1-10402018-1\ncolumns: Character, Australia & New Zealand (Sydney - first run, Melbourne, Auckland), London, Toronto / Broadway, Brazil, UK Tour, US Tour, Italy (Milan, Rome, Trieste)\nQ: Which person is in the tronto/broadway and has a uk tour of n/a\nA: SELECT Toronto / Broadway FROM 1-10402018-1 WHERE UK Tour = 'n/a'"}
 251 | {"text": "table: 1-10402018-1\ncolumns: Character, Australia & New Zealand (Sydney - first run, Melbourne, Auckland), London, Toronto / Broadway, Brazil, UK Tour, US Tour, Italy (Milan, Rome, Trieste)\nQ: How many people play Frank in London?\nA: SELECT COUNT London FROM 1-10402018-1 WHERE Character = 'Frank'"}
 252 | {"text": "table: 1-10399701-2\ncolumns: School Year, Class A, Class AA, Class AAA, Class AAAA, Class AAAAA\nQ: Who was Class AAA during the school year of 2000-01?\nA: SELECT Class AAA FROM 1-10399701-2 WHERE School Year = '2000-01'"}
 253 | {"text": "table: 1-10399701-2\ncolumns: School Year, Class A, Class AA, Class AAA, Class AAAA, Class AAAAA\nQ: Who was Class AAA during the same year that Class A was (tie) Apple Springs/Texline?\nA: SELECT Class AAA FROM 1-10399701-2 WHERE Class A = '(tie) Apple Springs/Texline'"}
 254 | {"text": "table: 1-10399701-2\ncolumns: School Year, Class A, Class AA, Class AAA, Class AAAA, Class AAAAA\nQ: Who was Class AAAAA during the school year of 1995-96?\nA: SELECT Class AAAAA FROM 1-10399701-2 WHERE School Year = '1995-96'"}
 255 | {"text": "table: 1-10399701-2\ncolumns: School Year, Class A, Class AA, Class AAA, Class AAAA, Class AAAAA\nQ: Who was Class AAA during the same year that Class AAAAA was Brownsville Pace?\nA: SELECT Class AAA FROM 1-10399701-2 WHERE Class AAAAA = 'Brownsville Pace'"}
 256 | {"text": "table: 1-10399701-2\ncolumns: School Year, Class A, Class AA, Class AAA, Class AAAA, Class AAAAA\nQ: What was the total number of Class AAA during the same year that Class AAA was White Oak?\nA: SELECT COUNT Class AA FROM 1-10399701-2 WHERE Class AAA = 'White Oak'"}
 257 | {"text": "table: 1-10392906-2\ncolumns: Week, Date, Kickoff, Opponent, Final score, Team record, Game site, Attendance\nQ: How many records are listed on Friday, May 25?\nA: SELECT COUNT Team record FROM 1-10392906-2 WHERE Date = 'Friday, May 25'"}
 258 | {"text": "table: 1-10392906-2\ncolumns: Week, Date, Kickoff, Opponent, Final score, Team record, Game site, Attendance\nQ: How many opponents were played on Saturday, June 9?\nA: SELECT COUNT Opponent FROM 1-10392906-2 WHERE Date = 'Saturday, June 9'"}
 259 | {"text": "table: 1-10392906-2\ncolumns: Week, Date, Kickoff, Opponent, Final score, Team record, Game site, Attendance\nQ: In what week was the first game played at the Commerzbank-Arena?\nA: SELECT MIN Week FROM 1-10392906-2 WHERE Game site = 'Commerzbank-Arena'"}
 260 | {"text": "table: 1-10413597-5\ncolumns: No. in series, No. in season, Title, Setting, Directed by, Written by, U.S. viewers (million), Original air date\nQ: What was the original air date of an episode set in 1544?\nA: SELECT Original air date FROM 1-10413597-5 WHERE Setting = '1544'"}
 261 | {"text": "table: 1-10413597-5\ncolumns: No. in series, No. in season, Title, Setting, Directed by, Written by, U.S. viewers (million), Original air date\nQ: How many settings where there for episode 29 of the season?\nA: SELECT COUNT Setting FROM 1-10413597-5 WHERE No. in series = 29"}
 262 | {"text": "table: 1-10413597-5\ncolumns: No. in series, No. in season, Title, Setting, Directed by, Written by, U.S. viewers (million), Original air date\nQ: Who wrote the episode that was set in winter 1541/february 13, 1542?\nA: SELECT Written by FROM 1-10413597-5 WHERE Setting = 'Winter 1541/February 13, 1542'"}
 263 | {"text": "table: 1-10413597-4\ncolumns: No. in series, No. in season, Title, Setting, Directed by, Written by, Original air date\nQ: What episode number of the season was \"The Northern Uprising\"?\nA: SELECT No. in season FROM 1-10413597-4 WHERE Title = '\"The Northern Uprising\"'"}
 264 | {"text": "table: 1-10416547-1\ncolumns: Date, Album name, Track, Track title, Lyricist, Music genre/style, Major instrument(s), Lyrics theme/style, Duration\nQ: What is the name of the track that lasts 5:30?\nA: SELECT Track title FROM 1-10416547-1 WHERE Duration = '5:30'"}
 265 | {"text": "table: 1-10416547-1\ncolumns: Date, Album name, Track, Track title, Lyricist, Music genre/style, Major instrument(s), Lyrics theme/style, Duration\nQ: What is the album namethat has the track title Sweetness \u751c\u751c\u7684 (ti\u00e1n ti\u00e1n de)?\nA: SELECT Album name FROM 1-10416547-1 WHERE Track title = 'Sweetness \u751c\u751c\u7684 (Ti\u00e1n ti\u00e1n de)'"}
 266 | {"text": "table: 1-10416547-1\ncolumns: Date, Album name, Track, Track title, Lyricist, Music genre/style, Major instrument(s), Lyrics theme/style, Duration\nQ: What is the duration of the song where the major instrument is the piano and the date is 2004-02-03?\nA: SELECT Duration FROM 1-10416547-1 WHERE Major instrument(s) = 'Piano' AND Date = '2004-02-03'"}
 267 | {"text": "table: 1-10416547-1\ncolumns: Date, Album name, Track, Track title, Lyricist, Music genre/style, Major instrument(s), Lyrics theme/style, Duration\nQ: What is the total number of lyricist where the lyrics theme is romance and the song lasts 3:50?\nA: SELECT COUNT Lyricist FROM 1-10416547-1 WHERE Lyrics theme/style = 'Romance' AND Duration = '3:50'"}
 268 | {"text": "table: 1-10416547-1\ncolumns: Date, Album name, Track, Track title, Lyricist, Music genre/style, Major instrument(s), Lyrics theme/style, Duration\nQ: What is the major instrument of the song that lasts 4:32? \nA: SELECT Major instrument(s) FROM 1-10416547-1 WHERE Duration = '4:32'"}
 269 | {"text": "table: 1-10416547-1\ncolumns: Date, Album name, Track, Track title, Lyricist, Music genre/style, Major instrument(s), Lyrics theme/style, Duration\nQ: What is the total number of music genre/style in which the lyrics are a detective story?\nA: SELECT COUNT Music genre/style FROM 1-10416547-1 WHERE Lyrics theme/style = 'Detective story'"}
 270 | {"text": "table: 1-1046071-1\ncolumns: Year, Division, League, Regular Season, Playoffs, Open Cup\nQ: What is the playoffs for the usl pro select league?\nA: SELECT Playoffs FROM 1-1046071-1 WHERE League = 'USL Pro Select League'"}
 271 | {"text": "table: 1-1046071-1\ncolumns: Year, Division, League, Regular Season, Playoffs, Open Cup\nQ: What is the number of the division for the 1st round?\nA: SELECT COUNT Division FROM 1-1046071-1 WHERE Open Cup = '1st Round'"}
 272 | {"text": "table: 1-10420426-1\ncolumns: Season, Series, Team, Races, Wins, Poles, F/Laps, Podiums, Points, Position\nQ: What was the team where series is formula renault 2.0 nec?\nA: SELECT Team FROM 1-10420426-1 WHERE Series = 'Formula Renault 2.0 NEC'"}
 273 | {"text": "table: 1-10420426-1\ncolumns: Season, Series, Team, Races, Wins, Poles, F/Laps, Podiums, Points, Position\nQ: What is the total number of poles for arden international?\nA: SELECT COUNT Poles FROM 1-10420426-1 WHERE Team = 'Arden International'"}
 274 | {"text": "table: 1-10420426-1\ncolumns: Season, Series, Team, Races, Wins, Poles, F/Laps, Podiums, Points, Position\nQ: What is the number of wins for gp2 series for racing engineering?\nA: SELECT COUNT Wins FROM 1-10420426-1 WHERE Series = 'GP2 Series' AND Team = 'Racing Engineering'"}
 275 | {"text": "table: 1-10420426-1\ncolumns: Season, Series, Team, Races, Wins, Poles, F/Laps, Podiums, Points, Position\nQ: What is the number of podiums for season 2010 for campionato italiano superstars.\nA: SELECT COUNT Podiums FROM 1-10420426-1 WHERE Season = '2010' AND Series = 'Campionato Italiano Superstars'"}
 276 | {"text": "table: 1-10420426-1\ncolumns: Season, Series, Team, Races, Wins, Poles, F/Laps, Podiums, Points, Position\nQ: What is the podium for 144 points?\nA: SELECT Podiums FROM 1-10420426-1 WHERE Points = 144"}
 277 | {"text": "table: 1-10470082-3\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: How many writers had an US air date of september 25, 1993?\nA: SELECT COUNT Writer FROM 1-10470082-3 WHERE US air date = 'September 25, 1993'"}
 278 | {"text": "table: 1-10470082-3\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: How many villians were in No. 25?\nA: SELECT COUNT Villains FROM 1-10470082-3 WHERE No. = 25"}
 279 | {"text": "table: 1-1046454-1\ncolumns: Year, Division, League, Regular Season, Playoffs, Open Cup\nQ: what being the maximum\u00a0year\u00a0where\u00a0regular season\u00a0is 4th, northwest\nA: SELECT MAX Year FROM 1-1046454-1 WHERE Regular Season = '4th, Northwest'"}
 280 | {"text": "table: 1-1046454-1\ncolumns: Year, Division, League, Regular Season, Playoffs, Open Cup\nQ: what is the total number of\u00a0playoffs\u00a0where\u00a0regular season\u00a0is 6th, southwest\nA: SELECT COUNT Playoffs FROM 1-1046454-1 WHERE Regular Season = '6th, Southwest'"}
 281 | {"text": "table: 1-1046454-1\ncolumns: Year, Division, League, Regular Season, Playoffs, Open Cup\nQ: what is the maximum\u00a0division\nA: SELECT MAX Division FROM 1-1046454-1"}
 282 | {"text": "table: 1-1046454-1\ncolumns: Year, Division, League, Regular Season, Playoffs, Open Cup\nQ:  what's the\u00a0league\u00a0where\u00a0regular season\u00a0is 2nd, northwest\nA: SELECT League FROM 1-1046454-1 WHERE Regular Season = '2nd, Northwest'"}
 283 | {"text": "table: 1-1046454-1\ncolumns: Year, Division, League, Regular Season, Playoffs, Open Cup\nQ: what are all the regular season where year is 2011\nA: SELECT Regular Season FROM 1-1046454-1 WHERE Year = 2011"}
 284 | {"text": "table: 1-10470082-5\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: How many titles have the number 11\nA: SELECT COUNT Title FROM 1-10470082-5 WHERE # = 11"}
 285 | {"text": "table: 1-10470082-5\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: How many have Mrs. briar as a villain\nA: SELECT COUNT No. FROM 1-10470082-5 WHERE Villains = 'Mrs. Briar'"}
 286 | {"text": "table: 1-10470082-5\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: how many have the number 8\nA: SELECT COUNT No. FROM 1-10470082-5 WHERE # = 8"}
 287 | {"text": "table: 1-10470082-5\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: How many have the title \"the tale of the room for rent\"\nA: SELECT COUNT # FROM 1-10470082-5 WHERE Title = '\"The Tale of the Room for Rent\"'"}
 288 | {"text": "table: 1-10470082-6\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: What is the name of the episode told by Kiki and directed by Will Dixon?\nA: SELECT Title FROM 1-10470082-6 WHERE Storyteller = 'Kiki' AND Director = 'Will Dixon'"}
 289 | {"text": "table: 1-10470082-6\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: Who is the storyteller in the episode called \"The Tale of the Jagged Sign\"?\nA: SELECT Storyteller FROM 1-10470082-6 WHERE Title = '\"The Tale of the Jagged Sign\"'"}
 290 | {"text": "table: 1-10470082-6\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: Who wrote Episode #3?\nA: SELECT Writer FROM 1-10470082-6 WHERE # = 3"}
 291 | {"text": "table: 1-10470082-7\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: Who are the villains in the episode titled \"The Tale of the Forever Game\"?\nA: SELECT Villains FROM 1-10470082-7 WHERE Title = '\"The Tale of The Forever Game\"'"}
 292 | {"text": "table: 1-10470082-7\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: How many villains appeared in the episode titled \"The Tale of the Virtual Pets\"?\nA: SELECT COUNT Villains FROM 1-10470082-7 WHERE Title = '\"The Tale of The Virtual Pets\"'"}
 293 | {"text": "table: 1-10470082-7\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: Who are the villains in the episodes where Megan is the storyteller and Lorette LeBlanc is the director?\nA: SELECT Villains FROM 1-10470082-7 WHERE Storyteller = 'Megan' AND Director = 'Lorette LeBlanc'"}
 294 | {"text": "table: 1-10470082-7\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: What is the largest # for an episode that was written by Allison Lea Bingeman?\nA: SELECT MAX # FROM 1-10470082-7 WHERE Writer = 'Allison Lea Bingeman'"}
 295 | {"text": "table: 1-10477224-1\ncolumns: Sepal length, Sepal width, Petal length, Petal width, Species\nQ: Name the species when petal width is 2.0 and petal length is 4.9\nA: SELECT Species FROM 1-10477224-1 WHERE Petal width = '2.0' AND Petal length = '4.9'"}
 296 | {"text": "table: 1-10477224-1\ncolumns: Sepal length, Sepal width, Petal length, Petal width, Species\nQ: Name the sepal width for i.virginica with petal length of 5.1\nA: SELECT Sepal width FROM 1-10477224-1 WHERE Species = 'I.virginica' AND Petal length = '5.1'"}
 297 | {"text": "table: 1-10477224-1\ncolumns: Sepal length, Sepal width, Petal length, Petal width, Species\nQ: Name the number of species with sepal width of 3.4 and sepal length of 5.4\nA: SELECT COUNT Species FROM 1-10477224-1 WHERE Sepal width = '3.4' AND Sepal length = '5.4'"}
 298 | {"text": "table: 1-10477224-1\ncolumns: Sepal length, Sepal width, Petal length, Petal width, Species\nQ: Name the sepal length for sepal width of 2.8 and petal length of 5.1\nA: SELECT Sepal length FROM 1-10477224-1 WHERE Sepal width = '2.8' AND Petal length = '5.1'"}
 299 | {"text": "table: 1-10477224-1\ncolumns: Sepal length, Sepal width, Petal length, Petal width, Species\nQ: Name the sepal width when sepal length is 6.5 and petal width is 2.2\nA: SELECT Sepal width FROM 1-10477224-1 WHERE Sepal length = '6.5' AND Petal width = '2.2'"}
 300 | {"text": "table: 1-10477224-1\ncolumns: Sepal length, Sepal width, Petal length, Petal width, Species\nQ: Name the sepal lengh when sepal width is 2.9 and petal width 1.3\nA: SELECT Sepal length FROM 1-10477224-1 WHERE Sepal width = '2.9' AND Petal width = '1.3'"}
 301 | {"text": "table: 1-10470082-4\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: Who is the director and what number is the episode for episode #1 of Are You Afraid of the Dark season 3?\nA: SELECT COUNT Director FROM 1-10470082-4 WHERE # = 1"}
 302 | {"text": "table: 1-10470082-4\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: Who is the director of the episode whom Scott Peters is the writer?\nA: SELECT Director FROM 1-10470082-4 WHERE Writer = 'Scott Peters'"}
 303 | {"text": "table: 1-10470082-4\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: Who is the villain in episode #7?\nA: SELECT Villains FROM 1-10470082-4 WHERE # = 7"}
 304 | {"text": "table: 1-10470082-8\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: Who wrote episode #1 in season 7?\nA: SELECT COUNT Writer FROM 1-10470082-8 WHERE # = 1"}
 305 | {"text": "table: 1-10470082-8\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: When did the episode written by Jim Morris air?\nA: SELECT US air date FROM 1-10470082-8 WHERE Writer = 'Jim Morris'"}
 306 | {"text": "table: 1-10527215-3\ncolumns: Rd, Name, Pole Position, Fastest Lap, Winning driver, Winning team, Report\nQ: What was Datsun Twin 200's fastest lap?\nA: SELECT Fastest Lap FROM 1-10527215-3 WHERE Name = 'Datsun Twin 200'"}
 307 | {"text": "table: 1-10527215-3\ncolumns: Rd, Name, Pole Position, Fastest Lap, Winning driver, Winning team, Report\nQ: In the Datsun Twin 200 race, what was the fastest lap?\nA: SELECT Fastest Lap FROM 1-10527215-3 WHERE Name = 'Datsun Twin 200'"}
 308 | {"text": "table: 1-10527215-3\ncolumns: Rd, Name, Pole Position, Fastest Lap, Winning driver, Winning team, Report\nQ: What's the report for the True Value 500?\nA: SELECT Report FROM 1-10527215-3 WHERE Name = 'True Value 500'"}
 309 | {"text": "table: 1-10527215-3\ncolumns: Rd, Name, Pole Position, Fastest Lap, Winning driver, Winning team, Report\nQ: What was Johnny Rutherford's fastest lap while Al Unser was the pole position?\nA: SELECT Fastest Lap FROM 1-10527215-3 WHERE Winning driver = 'Johnny Rutherford' AND Pole Position = 'Al Unser'"}
 310 | {"text": "table: 1-10527215-3\ncolumns: Rd, Name, Pole Position, Fastest Lap, Winning driver, Winning team, Report\nQ: What's the report on Penske Racing winning while the pole position was Al Unser?\nA: SELECT COUNT Report FROM 1-10527215-3 WHERE Pole Position = 'Al Unser' AND Winning team = 'Penske Racing'"}
 311 | {"text": "table: 1-104858-1\ncolumns: Country, Membership (from 2010), Name of member organization, Year current Scouting organization joined WOSM, Year member organization was founded, Admits boys/girls\nQ: Which countries have a scouting organization that was founded in 1926, and joined WOSM in 1930?\nA: SELECT Country FROM 1-104858-1 WHERE Year current Scouting organization joined WOSM = '1930' AND Year member organization was founded = '1926'"}
 312 | {"text": "table: 1-104858-1\ncolumns: Country, Membership (from 2010), Name of member organization, Year current Scouting organization joined WOSM, Year member organization was founded, Admits boys/girls\nQ: Does Venezuela admit only boys, only girls, or both?\nA: SELECT Admits boys/girls FROM 1-104858-1 WHERE Country = 'Venezuela'"}
 313 | {"text": "table: 1-104858-1\ncolumns: Country, Membership (from 2010), Name of member organization, Year current Scouting organization joined WOSM, Year member organization was founded, Admits boys/girls\nQ: Which organizations were founded in 1972, but became WOSM members until 1977?\nA: SELECT Name of member organization FROM 1-104858-1 WHERE Year member organization was founded = '1972' AND Year current Scouting organization joined WOSM = '1977'"}
 314 | {"text": "table: 1-104858-1\ncolumns: Country, Membership (from 2010), Name of member organization, Year current Scouting organization joined WOSM, Year member organization was founded, Admits boys/girls\nQ: Does the Scout Association of Hong Kong admit boys, girls, or both?\nA: SELECT Admits boys/girls FROM 1-104858-1 WHERE Name of member organization = 'The Scout Association of Hong Kong'"}
 315 | {"text": "table: 1-104858-1\ncolumns: Country, Membership (from 2010), Name of member organization, Year current Scouting organization joined WOSM, Year member organization was founded, Admits boys/girls\nQ: Does the Ghana Scout Association (founded in 1912) admit boys, girls, or both?\nA: SELECT Admits boys/girls FROM 1-104858-1 WHERE Year member organization was founded = '1912' AND Name of member organization = 'The Ghana Scout Association'"}
 316 | {"text": "table: 1-10528691-4\ncolumns: Model, Introduction, Discontinued, CPU Speed, Print resolution (DPI) Resolution is given in dots per inch (DPI), Print speed (PPM), Standard memory, Maximum memory\nQ: What is the model number introduced May 1999?\nA: SELECT MAX Model FROM 1-10528691-4 WHERE Introduction = 'May 1999'"}
 317 | {"text": "table: 1-10528691-4\ncolumns: Model, Introduction, Discontinued, CPU Speed, Print resolution (DPI) Resolution is given in dots per inch (DPI), Print speed (PPM), Standard memory, Maximum memory\nQ: What is the print resolution (FPI) for December 2002?\nA: SELECT Print resolution (DPI) Resolution is given in dots per inch (DPI) FROM 1-10528691-4 WHERE Introduction = 'December 2002'"}
 318 | {"text": "table: 1-10528691-4\ncolumns: Model, Introduction, Discontinued, CPU Speed, Print resolution (DPI) Resolution is given in dots per inch (DPI), Print speed (PPM), Standard memory, Maximum memory\nQ: What is the maximum memory for the model discontinued in November 2001?\nA: SELECT Maximum memory FROM 1-10528691-4 WHERE Discontinued = 'November 2001'"}
 319 | {"text": "table: 1-1053802-1\ncolumns: Region/country, Local title, Network, Winners, Main presenters\nQ: What is main presenters of La Granja?\nA: SELECT Main presenters FROM 1-1053802-1 WHERE Local title = 'La Granja'"}
 320 | {"text": "table: 1-1053802-1\ncolumns: Region/country, Local title, Network, Winners, Main presenters\nQ: What is the main presenter of bulgaria?\nA: SELECT Main presenters FROM 1-1053802-1 WHERE Region/country = 'Bulgaria'"}
 321 | {"text": "table: 1-1053802-1\ncolumns: Region/country, Local title, Network, Winners, Main presenters\nQ: How many winners are there of farma?\nA: SELECT COUNT Winners FROM 1-1053802-1 WHERE Local title = 'Farma'"}
 322 | {"text": "table: 1-10556257-1\ncolumns: Season, Team, League Apps, League Goals, Cup Apps, Cup Goals\nQ: What is the most cup goals for seasson 1911-12?\nA: SELECT MAX Cup Goals FROM 1-10556257-1 WHERE Season = '1911-12'"}
 323 | {"text": "table: 1-10556257-1\ncolumns: Season, Team, League Apps, League Goals, Cup Apps, Cup Goals\nQ: What is the league apps for season 1923-24?\nA: SELECT League Apps FROM 1-10556257-1 WHERE Season = '1923-24'"}
 324 | {"text": "table: 1-10556257-1\ncolumns: Season, Team, League Apps, League Goals, Cup Apps, Cup Goals\nQ: What is the team for season 1911-12?\nA: SELECT Team FROM 1-10556257-1 WHERE Season = '1911-12'"}
 325 | {"text": "table: 1-10566855-1\ncolumns: Season, Premier, Runner-up, Score, Margin, Venue, Attendance\nQ: what's the minimum\u00a0attendance\u00a0with\u00a0score\u00a0 10.16 (76) \u2013 9.22 (76)\nA: SELECT MIN Attendance FROM 1-10566855-1 WHERE Score = '10.16 (76) \u2013 9.22 (76)'"}
 326 | {"text": "table: 1-10566855-1\ncolumns: Season, Premier, Runner-up, Score, Margin, Venue, Attendance\nQ: who's the\u00a0premier\u00a0with\u00a0in 1970\nA: SELECT Premier FROM 1-10566855-1 WHERE Season = 1970"}
 327 | {"text": "table: 1-10566855-1\ncolumns: Season, Premier, Runner-up, Score, Margin, Venue, Attendance\nQ: who are all the runner-up for premier in richmond\nA: SELECT Runner-up FROM 1-10566855-1 WHERE Premier = 'Richmond'"}
 328 | {"text": "table: 1-10566855-1\ncolumns: Season, Premier, Runner-up, Score, Margin, Venue, Attendance\nQ: what is the minimum attendance with score 8.16 (64) \u2013 8.12 (60)\nA: SELECT MIN Attendance FROM 1-10566855-1 WHERE Score = '8.16 (64) \u2013 8.12 (60)'"}
 329 | {"text": "table: 1-10568553-1\ncolumns: County, Location, Street Names, Milepost, Roads Intersected, Notes\nQ: How many mileposts are there on Anne Street?\nA: SELECT COUNT Milepost FROM 1-10568553-1 WHERE Street Names = 'Anne Street'"}
 330 | {"text": "table: 1-10568553-1\ncolumns: County, Location, Street Names, Milepost, Roads Intersected, Notes\nQ: Which street is 12.2 miles long?\nA: SELECT Street Names FROM 1-10568553-1 WHERE Milepost = '12.2'"}
 331 | {"text": "table: 1-10568553-1\ncolumns: County, Location, Street Names, Milepost, Roads Intersected, Notes\nQ: Where does Route 24 intersect?\nA: SELECT Location FROM 1-10568553-1 WHERE Roads Intersected = 'Route 24'"}
 332 | {"text": "table: 1-10568553-1\ncolumns: County, Location, Street Names, Milepost, Roads Intersected, Notes\nQ: Where is milepost 12.8?\nA: SELECT Location FROM 1-10568553-1 WHERE Milepost = '12.8'"}
 333 | {"text": "table: 1-1057262-1\ncolumns: Commodity, 2001-02, 2002-03, 2003-04, 2004-05, 2005-06, 2006-07\nQ: What is the minimum amount for wool for 2001-02?\nA: SELECT MIN 2001-02 FROM 1-1057262-1 WHERE Commodity = 'Wool'"}
 334 | {"text": "table: 1-1057316-1\ncolumns: Serial number, Wheel arrangement ( Whyte notation ), Build date, Operational owner(s), Disposition\nQ: Who were the operational owners during the construction date of April 1892?\nA: SELECT Operational owner(s) FROM 1-1057316-1 WHERE Build date = 'April 1892'"}
 335 | {"text": "table: 1-1057316-1\ncolumns: Serial number, Wheel arrangement ( Whyte notation ), Build date, Operational owner(s), Disposition\nQ: Where can you find Colorado and Southern Railway #9?\nA: SELECT Disposition FROM 1-1057316-1 WHERE Operational owner(s) = 'Colorado and Southern Railway #9'"}
 336 | {"text": "table: 1-1057316-1\ncolumns: Serial number, Wheel arrangement ( Whyte notation ), Build date, Operational owner(s), Disposition\nQ: What is the wheel arrangement for the train in Riverdale, Georgia?\nA: SELECT Wheel arrangement ( Whyte notation ) FROM 1-1057316-1 WHERE Disposition = 'Riverdale, Georgia'"}
 337 | {"text": "table: 1-1057316-1\ncolumns: Serial number, Wheel arrangement ( Whyte notation ), Build date, Operational owner(s), Disposition\nQ: When was the train 2053 built?\nA: SELECT Build date FROM 1-1057316-1 WHERE Serial number = '2053'"}
 338 | {"text": "table: 1-1057316-1\ncolumns: Serial number, Wheel arrangement ( Whyte notation ), Build date, Operational owner(s), Disposition\nQ: How many wheels does the train owned by Texas and New Orleans Railroad #319 have?\nA: SELECT COUNT Wheel arrangement ( Whyte notation ) FROM 1-1057316-1 WHERE Operational owner(s) = 'Texas and New Orleans Railroad #319'"}
 339 | {"text": "table: 1-10577579-3\ncolumns: Institution, Location, Men\u2019s Nickname, Women\u2019s Nickname, Founded, Type, Enrollment, Joined, Left, Current Conference, Classification\nQ: Which college has the men's nickname of the blazers?\nA: SELECT Institution FROM 1-10577579-3 WHERE Men\u2019s Nickname = 'Blazers'"}
 340 | {"text": "table: 1-10577579-3\ncolumns: Institution, Location, Men\u2019s Nickname, Women\u2019s Nickname, Founded, Type, Enrollment, Joined, Left, Current Conference, Classification\nQ: Name the joined for the wolfpack women's nickname\nA: SELECT Joined FROM 1-10577579-3 WHERE Women\u2019s Nickname = 'Wolfpack'"}
 341 | {"text": "table: 1-10577579-3\ncolumns: Institution, Location, Men\u2019s Nickname, Women\u2019s Nickname, Founded, Type, Enrollment, Joined, Left, Current Conference, Classification\nQ: Name the left of the Lady Pilots.\nA: SELECT Left FROM 1-10577579-3 WHERE Women\u2019s Nickname = 'Lady Pilots'"}
 342 | {"text": "table: 1-10577579-3\ncolumns: Institution, Location, Men\u2019s Nickname, Women\u2019s Nickname, Founded, Type, Enrollment, Joined, Left, Current Conference, Classification\nQ: Name the women's nickname when the enrollment is 1500 in mobile, Alabama.\nA: SELECT Women\u2019s Nickname FROM 1-10577579-3 WHERE Enrollment = 1500 AND Location = 'Mobile, Alabama'"}
 343 | {"text": "table: 1-10577579-3\ncolumns: Institution, Location, Men\u2019s Nickname, Women\u2019s Nickname, Founded, Type, Enrollment, Joined, Left, Current Conference, Classification\nQ: Which conference is in Jackson, Mississippi?\nA: SELECT Current Conference FROM 1-10577579-3 WHERE Location = 'Jackson, Mississippi'"}
 344 | {"text": "table: 1-10577579-3\ncolumns: Institution, Location, Men\u2019s Nickname, Women\u2019s Nickname, Founded, Type, Enrollment, Joined, Left, Current Conference, Classification\nQ: What is the men's nickname at the school that has the lady wildcats women's nickname?\nA: SELECT Men\u2019s Nickname FROM 1-10577579-3 WHERE Women\u2019s Nickname = 'Lady Wildcats'"}
 345 | {"text": "table: 1-10577579-2\ncolumns: Institution, Location, Mens Nickname, Womens Nickname, Founded, Type, Enrollment, Joined\nQ: What is the Mens Nickname for the member location of Jacksonville, florida?\nA: SELECT Mens Nickname FROM 1-10577579-2 WHERE Location = 'Jacksonville, Florida'"}
 346 | {"text": "table: 1-10577579-2\ncolumns: Institution, Location, Mens Nickname, Womens Nickname, Founded, Type, Enrollment, Joined\nQ: What is the enrollment for the institution that was founded in 1866 and is a private/(african methodist) type?\nA: SELECT MAX Enrollment FROM 1-10577579-2 WHERE Founded = 1866 AND Type = 'Private/(African Methodist)'"}
 347 | {"text": "table: 1-10577579-2\ncolumns: Institution, Location, Mens Nickname, Womens Nickname, Founded, Type, Enrollment, Joined\nQ: That is the year founded for the institution location of Nashville, Tennessee?\nA: SELECT MIN Founded FROM 1-10577579-2 WHERE Location = 'Nashville, Tennessee'"}
 348 | {"text": "table: 1-10577579-2\ncolumns: Institution, Location, Mens Nickname, Womens Nickname, Founded, Type, Enrollment, Joined\nQ: What is the year the institution Tougaloo College joined?\nA: SELECT Joined FROM 1-10577579-2 WHERE Institution = 'Tougaloo College'"}
 349 | {"text": "table: 1-10592536-8\ncolumns: Team, Outgoing manager, Manner of departure, Date of vacancy, Replaced by, Date of appointment, Position in table\nQ: What is the date of vacancy when the date of appointment is 28 november 2007 and replaced by is alex mcleish?\nA: SELECT Date of vacancy FROM 1-10592536-8 WHERE Date of appointment = '28 November 2007' AND Replaced by = 'Alex McLeish'"}
 350 | {"text": "table: 1-10592536-8\ncolumns: Team, Outgoing manager, Manner of departure, Date of vacancy, Replaced by, Date of appointment, Position in table\nQ: What is the date of appointment when the date of vacancy is 21 december 2007?\nA: SELECT Date of appointment FROM 1-10592536-8 WHERE Date of vacancy = '21 December 2007'"}
 351 | {"text": "table: 1-10592536-8\ncolumns: Team, Outgoing manager, Manner of departure, Date of vacancy, Replaced by, Date of appointment, Position in table\nQ: Who replaced when team is wigan athletic?\nA: SELECT Replaced by FROM 1-10592536-8 WHERE Team = 'Wigan Athletic'"}
 352 | {"text": "table: 1-10592536-8\ncolumns: Team, Outgoing manager, Manner of departure, Date of vacancy, Replaced by, Date of appointment, Position in table\nQ: What is the date of vacancy when the team is manchester city and replaced by is mark hughes?\nA: SELECT Date of vacancy FROM 1-10592536-8 WHERE Team = 'Manchester City' AND Replaced by = 'Mark Hughes'"}
 353 | {"text": "table: 1-10592536-8\ncolumns: Team, Outgoing manager, Manner of departure, Date of vacancy, Replaced by, Date of appointment, Position in table\nQ: What is the date of appointment when replaced by is roy hodgson?\nA: SELECT Date of appointment FROM 1-10592536-8 WHERE Replaced by = 'Roy Hodgson'"}
 354 | {"text": "table: 1-10592536-8\ncolumns: Team, Outgoing manager, Manner of departure, Date of vacancy, Replaced by, Date of appointment, Position in table\nQ: Who replaced when position in table is pre-season?\nA: SELECT Replaced by FROM 1-10592536-8 WHERE Position in table = 'Pre-season'"}
 355 | {"text": "table: 1-1059743-1\ncolumns: Rank, Member Association, Points, Group stage, Play-off, AFC Cup\nQ: How many games had a score value of 813.5 in post-season play?\nA: SELECT COUNT Play-off FROM 1-1059743-1 WHERE Points = '813.5'"}
 356 | {"text": "table: 1-1059743-1\ncolumns: Rank, Member Association, Points, Group stage, Play-off, AFC Cup\nQ: Did any team score games that totaled up to 860.5?\nA: SELECT Play-off FROM 1-1059743-1 WHERE Points = '860.5'"}
 357 | {"text": "table: 1-10595672-1\ncolumns: Date, Opponent, Home / Away, Score, High points, High rebounds, High assists, Location/Attendance, Record\nQ: What was the score of the game when the team reached a record of 6-9?\nA: SELECT Score FROM 1-10595672-1 WHERE Record = '6-9'"}
 358 | {"text": "table: 1-10581768-2\ncolumns: Institution, Nickname, Location, Founded, Type, Enrollment\nQ: What type institution is point park university\nA: SELECT Type FROM 1-10581768-2 WHERE Institution = 'Point Park University'"}
 359 | {"text": "table: 1-10581768-2\ncolumns: Institution, Nickname, Location, Founded, Type, Enrollment\nQ: How many institutions are located in wilmore, kentucky and private\nA: SELECT MAX Founded FROM 1-10581768-2 WHERE Type = 'Private' AND Location = 'Wilmore, Kentucky'"}
 360 | {"text": "table: 1-10581768-2\ncolumns: Institution, Nickname, Location, Founded, Type, Enrollment\nQ: point park university is what type of institution\nA: SELECT Type FROM 1-10581768-2 WHERE Institution = 'Point Park University'"}
 361 | {"text": "table: 1-10581768-2\ncolumns: Institution, Nickname, Location, Founded, Type, Enrollment\nQ: how many founded dates are listed for carlow university 1\nA: SELECT COUNT Founded FROM 1-10581768-2 WHERE Institution = 'Carlow University 1'"}
 362 | {"text": "table: 1-10610087-6\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date\nQ: Who wrote the episode titled \"black\"?\nA: SELECT Written by FROM 1-10610087-6 WHERE Title = '\"Black\"'"}
 363 | {"text": "table: 1-10610087-6\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date\nQ: Who are the writers for the episode \"solo\"?\nA: SELECT Written by FROM 1-10610087-6 WHERE Title = '\"Solo\"'"}
 364 | {"text": "table: 1-10610087-3\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date\nQ: what is the original air date of the episode no in season 9?\nA: SELECT Original air date FROM 1-10610087-3 WHERE No. in season = 9"}
 365 | {"text": "table: 1-10610087-3\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date\nQ: What is the title of the episode written by denis leary, peter tolan and evan reilly?\nA: SELECT Title FROM 1-10610087-3 WHERE Written by = 'Denis Leary, Peter Tolan and Evan Reilly'"}
 366 | {"text": "table: 1-10610087-3\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date\nQ: How many episodes were titles \"voicemail\"?\nA: SELECT COUNT Directed by FROM 1-10610087-3 WHERE Title = '\"Voicemail\"'"}
 367 | {"text": "table: 1-10602294-1\ncolumns: Name, Dates active, Peak classification, Windspeeds, Pressure, Areas affected, Damage (USD), Deaths, Refs\nQ: When was Kamba active?\nA: SELECT Dates active FROM 1-10602294-1 WHERE Name = 'Kamba'"}
 368 | {"text": "table: 1-10602294-1\ncolumns: Name, Dates active, Peak classification, Windspeeds, Pressure, Areas affected, Damage (USD), Deaths, Refs\nQ: What was the cyclone's pressure in the storm that death was equal to 95km/h (60mph)?\nA: SELECT Pressure FROM 1-10602294-1 WHERE Deaths = '95km/h (60mph)'"}
 369 | {"text": "table: 1-10602294-1\ncolumns: Name, Dates active, Peak classification, Windspeeds, Pressure, Areas affected, Damage (USD), Deaths, Refs\nQ: What were the active dates for the storm that had 185km/h (115mph) deaths?\nA: SELECT Dates active FROM 1-10602294-1 WHERE Deaths = '185km/h (115mph)'"}
 370 | {"text": "table: 1-10602294-1\ncolumns: Name, Dates active, Peak classification, Windspeeds, Pressure, Areas affected, Damage (USD), Deaths, Refs\nQ: What was the damage (usd) from the cyclones that measured 1003hPa (29.62inHg) pressure?\nA: SELECT Damage (USD) FROM 1-10602294-1 WHERE Pressure = '1003hPa (29.62inHg)'"}
 371 | {"text": "table: 1-10621256-1\ncolumns: Player, Matches, Inns, N/O, Runs, High Score, Average, 100, 50, Catches, Stump\nQ:  what's the\u00a0average\u00a0where\u00a0high score\u00a0is 120\nA: SELECT Average FROM 1-10621256-1 WHERE High Score = 120"}
 372 | {"text": "table: 1-10621256-1\ncolumns: Player, Matches, Inns, N/O, Runs, High Score, Average, 100, 50, Catches, Stump\nQ:  what's the\u00a0player\u00a0where\u00a050\u00a0is 2 and\u00a0n/o\u00a0is 0\nA: SELECT Player FROM 1-10621256-1 WHERE 50 = 2 AND N/O = 0"}
 373 | {"text": "table: 1-10621256-1\ncolumns: Player, Matches, Inns, N/O, Runs, High Score, Average, 100, 50, Catches, Stump\nQ:  what's the\u00a0player\u00a0where\u00a0inns\u00a0is 21\nA: SELECT Player FROM 1-10621256-1 WHERE Inns = 21"}
 374 | {"text": "table: 1-106367-2\ncolumns: General election, # of candidates, # of seats won, % of popular vote, Result\nQ: Which general election had a pq majority and a 44.75% of the popular vote?\nA: SELECT General election FROM 1-106367-2 WHERE Result = 'PQ majority' AND % of popular vote = '44.75%'"}
 375 | {"text": "table: 1-106367-2\ncolumns: General election, # of candidates, # of seats won, % of popular vote, Result\nQ: What is the least number of candidates running were there when 80 seats were won?\nA: SELECT MIN # of candidates FROM 1-106367-2 WHERE # of seats won = 80"}
 376 | {"text": "table: 1-106367-2\ncolumns: General election, # of candidates, # of seats won, % of popular vote, Result\nQ: How many seats were won in the election with 125 candidates?\nA: SELECT COUNT # of seats won FROM 1-106367-2 WHERE # of candidates = 125"}
 377 | {"text": "table: 1-10647639-1\ncolumns: Week, Date, Opponent, Result, Game site, Record, Attendance\nQ: How many weeks are there?\nA: SELECT MAX Week FROM 1-10647639-1"}
 378 | {"text": "table: 1-10647639-1\ncolumns: Week, Date, Opponent, Result, Game site, Record, Attendance\nQ: How many people attended the game against the indianapolis colts?\nA: SELECT COUNT Attendance FROM 1-10647639-1 WHERE Opponent = 'Indianapolis Colts'"}
 379 | {"text": "table: 1-10647639-1\ncolumns: Week, Date, Opponent, Result, Game site, Record, Attendance\nQ: On december 16, 1985, all the records were what?\nA: SELECT Record FROM 1-10647639-1 WHERE Date = 'December 16, 1985'"}
 380 | {"text": "table: 1-10646790-2\ncolumns: Week, Date, Opponent, Result, Stadium, Record, Attendance\nQ: How many results are there for the 0-4 record?\nA: SELECT COUNT Result FROM 1-10646790-2 WHERE Record = '0-4'"}
 381 | {"text": "table: 1-10646790-2\ncolumns: Week, Date, Opponent, Result, Stadium, Record, Attendance\nQ: How many weeks are there that include the date October 11, 1969.\nA: SELECT COUNT Week FROM 1-10646790-2 WHERE Date = 'October 11, 1969'"}
 382 | {"text": "table: 1-10646790-2\ncolumns: Week, Date, Opponent, Result, Stadium, Record, Attendance\nQ: How many weeks are there that include the date November 9, 1969.\nA: SELECT COUNT Week FROM 1-10646790-2 WHERE Date = 'November 9, 1969'"}
 383 | {"text": "table: 1-10646790-2\ncolumns: Week, Date, Opponent, Result, Stadium, Record, Attendance\nQ: How many records are there at the War Memorial Stadium?\nA: SELECT COUNT Record FROM 1-10646790-2 WHERE Stadium = 'War Memorial Stadium'"}
 384 | {"text": "table: 1-10646790-2\ncolumns: Week, Date, Opponent, Result, Stadium, Record, Attendance\nQ: What was the minimum attendance on December 7, 1969?\nA: SELECT MIN Attendance FROM 1-10646790-2 WHERE Date = 'December 7, 1969'"}
 385 | {"text": "table: 1-10647401-1\ncolumns: Week, Opponent, Result, Stadium, Record, Attendance\nQ: What week corresponds to the last one to be played at the memorial stadium?\nA: SELECT MAX Week FROM 1-10647401-1 WHERE Stadium = 'Memorial Stadium'"}
 386 | {"text": "table: 1-10647401-1\ncolumns: Week, Opponent, Result, Stadium, Record, Attendance\nQ: In which stadium is the week 5 game played?\nA: SELECT Stadium FROM 1-10647401-1 WHERE Week = 5"}
 387 | {"text": "table: 1-10664957-2\ncolumns: 1st players choice, 2nd players choice, Probability 1st player wins, Probability 2nd player wins, Probability of a draw\nQ: In Penney's game what is the probability where the 1st player wins if the probability of a draw is 8.28% and the 2nd player chooses B BR?\nA: SELECT Probability 1st player wins FROM 1-10664957-2 WHERE Probability of a draw = '8.28%' AND 2nd players choice = 'B BR'"}
 388 | {"text": "table: 1-10664957-2\ncolumns: 1st players choice, 2nd players choice, Probability 1st player wins, Probability 2nd player wins, Probability of a draw\nQ: If the first player chooses RB B, what is the second player's choices?\nA: SELECT 2nd players choice FROM 1-10664957-2 WHERE 1st players choice = 'RB B'"}
 389 | {"text": "table: 1-10664957-2\ncolumns: 1st players choice, 2nd players choice, Probability 1st player wins, Probability 2nd player wins, Probability of a draw\nQ: What is the probability where the second player wins where their choice is R RB and the first player has a 5.18% chance of winning?\nA: SELECT Probability 2nd player wins FROM 1-10664957-2 WHERE 2nd players choice = 'R RB' AND Probability 1st player wins = '5.18%'"}
 390 | {"text": "table: 1-10664957-2\ncolumns: 1st players choice, 2nd players choice, Probability 1st player wins, Probability 2nd player wins, Probability of a draw\nQ: What are the chances the first player will win if the 2nd player has an 80.11% chance of winning with the choice of R RB?\nA: SELECT Probability 1st player wins FROM 1-10664957-2 WHERE Probability 2nd player wins = '80.11%' AND 2nd players choice = 'R RB'"}
 391 | {"text": "table: 1-10664957-2\ncolumns: 1st players choice, 2nd players choice, Probability 1st player wins, Probability 2nd player wins, Probability of a draw\nQ: What are the chances that player 2 wins if player 1's choice is BB R?\nA: SELECT Probability 2nd player wins FROM 1-10664957-2 WHERE 1st players choice = 'BB R'"}
 392 | {"text": "table: 1-10664957-2\ncolumns: 1st players choice, 2nd players choice, Probability 1st player wins, Probability 2nd player wins, Probability of a draw\nQ: How high is the chance that player 1 wins if player 2 has an 88.29% chance of winning with the choice of R RB?\nA: SELECT Probability 1st player wins FROM 1-10664957-2 WHERE Probability 2nd player wins = '88.29%' AND 2nd players choice = 'R RB'"}
 393 | {"text": "table: 1-10650711-1\ncolumns: Pick #, NFL Team, Player, Position, College\nQ:  what is the\u00a0nfl team\u00a0where\u00a0player\u00a0is thane gash\nA: SELECT NFL Team FROM 1-10650711-1 WHERE Player = 'Thane Gash'"}
 394 | {"text": "table: 1-10650711-1\ncolumns: Pick #, NFL Team, Player, Position, College\nQ: what is the maximum\u00a0pick #\u00a0where\u00a0player\u00a0is anthony blaylock\nA: SELECT MAX Pick # FROM 1-10650711-1 WHERE Player = 'Anthony Blaylock'"}
 395 | {"text": "table: 1-10650711-1\ncolumns: Pick #, NFL Team, Player, Position, College\nQ:  what's the\u00a0nfl team\u00a0where\u00a0player\u00a0is clifford charlton\nA: SELECT NFL Team FROM 1-10650711-1 WHERE Player = 'Clifford Charlton'"}
 396 | {"text": "table: 1-10650711-1\ncolumns: Pick #, NFL Team, Player, Position, College\nQ:  what's the\u00a0position\u00a0where\u00a0player\u00a0is anthony blaylock\nA: SELECT Position FROM 1-10650711-1 WHERE Player = 'Anthony Blaylock'"}
 397 | {"text": "table: 1-10650711-1\ncolumns: Pick #, NFL Team, Player, Position, College\nQ: what is the minimum\u00a0pick #\u00a0where\u00a0position\u00a0is defensive tackle\nA: SELECT MIN Pick # FROM 1-10650711-1 WHERE Position = 'Defensive Tackle'"}
 398 | {"text": "table: 1-1067441-1\ncolumns: Province, Population (2004 estimate), Area (km\u00b2), Density, GDP (2003, PPS in mil. \u20ac ), GDP per cap. (2003, in \u20ac)\nQ: Which province has a density of 971.4?\nA: SELECT Province FROM 1-1067441-1 WHERE Density = '971.4'"}
 399 | {"text": "table: 1-1067441-1\ncolumns: Province, Population (2004 estimate), Area (km\u00b2), Density, GDP (2003, PPS in mil. \u20ac ), GDP per cap. (2003, in \u20ac)\nQ: What is Friesland's gdp per capita?\nA: SELECT MIN GDP per cap. (2003, in \u20ac) FROM 1-1067441-1 WHERE Province = 'Friesland'"}
 400 | {"text": "table: 1-1067441-1\ncolumns: Province, Population (2004 estimate), Area (km\u00b2), Density, GDP (2003, PPS in mil. \u20ac ), GDP per cap. (2003, in \u20ac)\nQ: What is the area of the place that has a population density of 331.4?\nA: SELECT MAX Area (km\u00b2) FROM 1-1067441-1 WHERE Density = '331.4'"}
 401 | {"text": "table: 1-1067441-1\ncolumns: Province, Population (2004 estimate), Area (km\u00b2), Density, GDP (2003, PPS in mil. \u20ac ), GDP per cap. (2003, in \u20ac)\nQ: Which province has a gdp of 38355\u20ac  million euros?\nA: SELECT Province FROM 1-1067441-1 WHERE GDP (2003, PPS in mil. \u20ac ) = 38355"}
 402 | {"text": "table: 1-1067441-1\ncolumns: Province, Population (2004 estimate), Area (km\u00b2), Density, GDP (2003, PPS in mil. \u20ac ), GDP per cap. (2003, in \u20ac)\nQ: What is the population estimate for the place that gad a 18496\u20ac  million euro gdp?\nA: SELECT Population (2004 estimate) FROM 1-1067441-1 WHERE GDP (2003, PPS in mil. \u20ac ) = 18496"}
 403 | {"text": "table: 1-10701133-1\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Featured character(s), Original air date, U.S. viewers (million)\nQ: What is the title when original air date is may15,2008?\nA: SELECT Title FROM 1-10701133-1 WHERE Original air date = 'May15,2008'"}
 404 | {"text": "table: 1-10701133-1\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Featured character(s), Original air date, U.S. viewers (million)\nQ: What is the highest no. in season?\nA: SELECT MAX No. in season FROM 1-10701133-1"}
 405 | {"text": "table: 1-10701133-1\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Featured character(s), Original air date, U.S. viewers (million)\nQ: Who directed the episode where u.s. viewers (million) is 12.90?\nA: SELECT Directed by FROM 1-10701133-1 WHERE U.S. viewers (million) = '12.90'"}
 406 | {"text": "table: 1-1067134-1\ncolumns: DVD Name, # of Ep, Region 1, Region 2, Region 4\nQ: How many episodes aired in Region 2 beginning May 26, 2008?\nA: SELECT MIN # of Ep FROM 1-1067134-1 WHERE Region 2 = 'May 26, 2008'"}
 407 | {"text": "table: 1-1067134-1\ncolumns: DVD Name, # of Ep, Region 1, Region 2, Region 4\nQ: What date did the DVD for season six come out in region 2?\nA: SELECT Region 2 FROM 1-1067134-1 WHERE DVD Name = 'Season Six'"}
 408 | {"text": "table: 1-1067134-1\ncolumns: DVD Name, # of Ep, Region 1, Region 2, Region 4\nQ: What is the least amount of season epidsodes?\nA: SELECT MIN # of Ep FROM 1-1067134-1"}
 409 | {"text": "table: 1-1067134-1\ncolumns: DVD Name, # of Ep, Region 1, Region 2, Region 4\nQ: What DVD season/name for region 2 was released August 22, 2010?\nA: SELECT DVD Name FROM 1-1067134-1 WHERE Region 2 = 'August 22, 2010'"}
 410 | {"text": "table: 1-10705060-1\ncolumns: Season, Series, Team Name, Races, Poles, Wins, Points, Position\nQ: How many points for 2005?\nA: SELECT COUNT Points FROM 1-10705060-1 WHERE Season = '2005'"}
 411 | {"text": "table: 1-10705060-1\ncolumns: Season, Series, Team Name, Races, Poles, Wins, Points, Position\nQ: what is the score for the dams?\nA: SELECT Points FROM 1-10705060-1 WHERE Team Name = 'DAMS'"}
 412 | {"text": "table: 1-10705060-1\ncolumns: Season, Series, Team Name, Races, Poles, Wins, Points, Position\nQ: how many positions in 2009?\nA: SELECT COUNT Position FROM 1-10705060-1 WHERE Season = '2009'"}
 413 | {"text": "table: 1-10705060-1\ncolumns: Season, Series, Team Name, Races, Poles, Wins, Points, Position\nQ: what is the least number of poles?\nA: SELECT MIN Poles FROM 1-10705060-1"}
 414 | {"text": "table: 1-10705060-1\ncolumns: Season, Series, Team Name, Races, Poles, Wins, Points, Position\nQ: Which series with 62 points?\nA: SELECT Series FROM 1-10705060-1 WHERE Points = 62"}
 415 | {"text": "table: 1-10705060-1\ncolumns: Season, Series, Team Name, Races, Poles, Wins, Points, Position\nQ: What is the total for 10th position?\nA: SELECT COUNT Points FROM 1-10705060-1 WHERE Position = '10th'"}
 416 | {"text": "table: 1-10707142-2\ncolumns: Rnd, Race Name, Circuit, City/Location, Date, Pole position, Winning driver, Winning team, Report\nQ: how many reports of races took place on october 16?\nA: SELECT COUNT Report FROM 1-10707142-2 WHERE Date = 'October 16'"}
 417 | {"text": "table: 1-10707142-2\ncolumns: Rnd, Race Name, Circuit, City/Location, Date, Pole position, Winning driver, Winning team, Report\nQ: what is the name of the report that lists the race name as long beach grand prix?\nA: SELECT Report FROM 1-10707142-2 WHERE Race Name = 'Long Beach Grand Prix'"}
 418 | {"text": "table: 1-10707142-2\ncolumns: Rnd, Race Name, Circuit, City/Location, Date, Pole position, Winning driver, Winning team, Report\nQ: what is the report called where the circuit took place at the nazareth speedway?\nA: SELECT Report FROM 1-10707142-2 WHERE Circuit = 'Nazareth Speedway'"}
 419 | {"text": "table: 1-10707142-2\ncolumns: Rnd, Race Name, Circuit, City/Location, Date, Pole position, Winning driver, Winning team, Report\nQ: what is the name of the race where newman/haas racing is the winning team and rick mears is at the pole position?\nA: SELECT Race Name FROM 1-10707142-2 WHERE Winning team = 'Newman/Haas Racing' AND Pole position = 'Rick Mears'"}
 420 | {"text": "table: 1-10707142-2\ncolumns: Rnd, Race Name, Circuit, City/Location, Date, Pole position, Winning driver, Winning team, Report\nQ: meadowlands sports complex is the circuit at which city/location?\nA: SELECT City/Location FROM 1-10707142-2 WHERE Circuit = 'Meadowlands Sports Complex'"}
 421 | {"text": "table: 1-10710364-2\ncolumns: Religious group, Population %, Growth (1991\u20132001), Sex ratio (total), Literacy (%), Work participation (%), Sex ratio (rural), Sex ratio (urban), Sex ratio (child)\nQ: What is the literacy rate for groups that grew 103.1% between 1991 and 2001?\nA: SELECT Literacy (%) FROM 1-10710364-2 WHERE Growth (1991\u20132001) = '103.1%'"}
 422 | {"text": "table: 1-10710364-2\ncolumns: Religious group, Population %, Growth (1991\u20132001), Sex ratio (total), Literacy (%), Work participation (%), Sex ratio (rural), Sex ratio (urban), Sex ratio (child)\nQ: What is the lowest sex ratio in rural areas?\nA: SELECT MIN Sex ratio (rural) FROM 1-10710364-2"}
 423 | {"text": "table: 1-10710364-2\ncolumns: Religious group, Population %, Growth (1991\u20132001), Sex ratio (total), Literacy (%), Work participation (%), Sex ratio (rural), Sex ratio (urban), Sex ratio (child)\nQ: What is the lowest child sex ratio in groups where employment is 31.3%?\nA: SELECT MIN Sex ratio (child) FROM 1-10710364-2 WHERE Work participation (%) = '31.3%'"}
 424 | {"text": "table: 1-10710364-2\ncolumns: Religious group, Population %, Growth (1991\u20132001), Sex ratio (total), Literacy (%), Work participation (%), Sex ratio (rural), Sex ratio (urban), Sex ratio (child)\nQ: What is the population percentage of the group where the rural sex ratio is 953?\nA: SELECT Population % FROM 1-10710364-2 WHERE Sex ratio (rural) = 953"}
 425 | {"text": "table: 1-10715317-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: What is the original air date for the title \"felonious monk\"?\nA: SELECT Original air date FROM 1-10715317-2 WHERE Title = '\"Felonious Monk\"'"}
 426 | {"text": "table: 1-10715317-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: What is the title of the episode directed by Peter Markle and written by Jerry Stahl?\nA: SELECT Title FROM 1-10715317-2 WHERE Directed by = 'Peter Markle' AND Written by = 'Jerry Stahl'"}
 427 | {"text": "table: 1-10715317-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: How many episodes were titled \"identity crisis\"?\nA: SELECT COUNT Directed by FROM 1-10715317-2 WHERE Title = '\"Identity Crisis\"'"}
 428 | {"text": "table: 1-10716893-3\ncolumns: Year, Network, Host, Pre-race analyst, Lap-by-lap, Color commentator(s), Pit reporters\nQ: Who does the lap-by-lap in 2011?\nA: SELECT Lap-by-lap FROM 1-10716893-3 WHERE Year = 2011"}
 429 | {"text": "table: 1-10716893-3\ncolumns: Year, Network, Host, Pre-race analyst, Lap-by-lap, Color commentator(s), Pit reporters\nQ: Which network has Marty Reid as host and lap-by-lap broadcaster?\nA: SELECT Network FROM 1-10716893-3 WHERE Lap-by-lap = 'Marty Reid' AND Host = 'Marty Reid'"}
 430 | {"text": "table: 1-10716893-3\ncolumns: Year, Network, Host, Pre-race analyst, Lap-by-lap, Color commentator(s), Pit reporters\nQ: How many pre-race analysis occur when Allen Bestwick does the lap-by-lap?\nA: SELECT COUNT Pre-race analyst FROM 1-10716893-3 WHERE Lap-by-lap = 'Allen Bestwick'"}
 431 | {"text": "table: 1-10718192-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: What's the highest season number of an episode in the series?\nA: SELECT MAX No. in season FROM 1-10718192-2"}
 432 | {"text": "table: 1-10718192-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: When did the episode titled \"Lucky Strike\" air for the first time?\nA: SELECT Original air date FROM 1-10718192-2 WHERE Title = '\"Lucky Strike\"'"}
 433 | {"text": "table: 1-10718192-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: Who was the writer of the episode titled \"One Hit Wonder\"?\nA: SELECT Written by FROM 1-10718192-2 WHERE Title = '\"One Hit Wonder\"'"}
 434 | {"text": "table: 1-10718192-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: What's the date of the first airing of the episode with series number 63?\nA: SELECT Original air date FROM 1-10718192-2 WHERE No. in series = 63"}
 435 | {"text": "table: 1-10718525-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: How many titles got a viewership of 26.53 million?\nA: SELECT COUNT Title FROM 1-10718525-2 WHERE U.S. viewers (millions) = '26.53'"}
 436 | {"text": "table: 1-10718525-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: How many viewers tuned into the show directed by Matt Earl Beesley?\nA: SELECT U.S. viewers (millions) FROM 1-10718525-2 WHERE Directed by = 'Matt Earl Beesley'"}
 437 | {"text": "table: 1-10718631-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: Who wrote episode 94?\nA: SELECT Written by FROM 1-10718631-2 WHERE No. in series = 94"}
 438 | {"text": "table: 1-10718631-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: Which episode was the number in the season where the number in the season is 10?\nA: SELECT No. in series FROM 1-10718631-2 WHERE No. in season = 10"}
 439 | {"text": "table: 1-10718631-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: How many episodes were in the season that had the epiosde of \"crow's feet\"?\nA: SELECT No. in season FROM 1-10718631-2 WHERE Title = '\"Crow's Feet\"'"}
 440 | {"text": "table: 1-10718631-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: When did the 113 episode air?\nA: SELECT Original air date FROM 1-10718631-2 WHERE No. in series = 113"}
 441 | {"text": "table: 1-10718631-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: How many titles were there for the 113 episode?\nA: SELECT COUNT Title FROM 1-10718631-2 WHERE No. in series = 113"}
 442 | {"text": "table: 1-10718984-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: What is the number in the season that Marlene Meyer wrote and 20.49 million people watched?\nA: SELECT MAX No. in season FROM 1-10718984-2 WHERE Written by = 'Marlene Meyer' AND U.S. viewers (millions) = '20.49'"}
 443 | {"text": "table: 1-10718984-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: When did the no. 23 show originally air?\nA: SELECT Original air date FROM 1-10718984-2 WHERE No. in season = 23"}
 444 | {"text": "table: 1-10725629-2\ncolumns: Rnd, Race Name, Circuit, City/Location, Date, Pole position, Fastest lap, Winning driver, Winning team, Report\nQ: Which circuits had a race on October 4?\nA: SELECT Circuit FROM 1-10725629-2 WHERE Date = 'October 4'"}
 445 | {"text": "table: 1-10725629-2\ncolumns: Rnd, Race Name, Circuit, City/Location, Date, Pole position, Fastest lap, Winning driver, Winning team, Report\nQ: In which reports does Michael Andretti have the pole position and Galles-Kraco Racing is the winning team?\nA: SELECT Report FROM 1-10725629-2 WHERE Pole position = 'Michael Andretti' AND Winning team = 'Galles-Kraco Racing'"}
 446 | {"text": "table: 1-10725629-2\ncolumns: Rnd, Race Name, Circuit, City/Location, Date, Pole position, Fastest lap, Winning driver, Winning team, Report\nQ: How many rounds were there of the Bosch Spark Plug Grand Prix?\nA: SELECT COUNT Rnd FROM 1-10725629-2 WHERE Race Name = 'Bosch Spark Plug Grand Prix'"}
 447 | {"text": "table: 1-10725629-2\ncolumns: Rnd, Race Name, Circuit, City/Location, Date, Pole position, Fastest lap, Winning driver, Winning team, Report\nQ: Which rounds were held on August 9?\nA: SELECT Rnd FROM 1-10725629-2 WHERE Date = 'August 9'"}
 448 | {"text": "table: 1-10725629-2\ncolumns: Rnd, Race Name, Circuit, City/Location, Date, Pole position, Fastest lap, Winning driver, Winning team, Report\nQ: On how many dates did the Michigan International Speedway hold a round?\nA: SELECT COUNT Date FROM 1-10725629-2 WHERE Circuit = 'Michigan International Speedway'"}
 449 | {"text": "table: 1-10722506-6\ncolumns: Conference, # of Bids, Record, Win %, Round of 32, Sweet Sixteen, Elite Eight, Final Four, Championship Game\nQ: Name the total number of bids of the sun belt conference\nA: SELECT COUNT # of Bids FROM 1-10722506-6 WHERE Conference = 'Sun Belt'"}
 450 | {"text": "table: 1-10722506-6\ncolumns: Conference, # of Bids, Record, Win %, Round of 32, Sweet Sixteen, Elite Eight, Final Four, Championship Game\nQ: Name the round of 32 in conference usa\nA: SELECT Round of 32 FROM 1-10722506-6 WHERE Conference = 'Conference USA'"}
 451 | {"text": "table: 1-10722506-6\ncolumns: Conference, # of Bids, Record, Win %, Round of 32, Sweet Sixteen, Elite Eight, Final Four, Championship Game\nQ: What is the record when round of 32 is 0 and metro atlantic conference?\nA: SELECT Record FROM 1-10722506-6 WHERE Round of 32 = 0 AND Conference = 'Metro Atlantic'"}
 452 | {"text": "table: 1-10722506-6\ncolumns: Conference, # of Bids, Record, Win %, Round of 32, Sweet Sixteen, Elite Eight, Final Four, Championship Game\nQ: What is the number of bids with elite eight larger than 1.0\nA: SELECT COUNT # of Bids FROM 1-10722506-6 WHERE Elite Eight > 1.0"}
 453 | {"text": "table: 1-10749143-2\ncolumns: Series #, Season #, Title, Directed by, Written by, Original air date, Production code, U.S. viewers (millions)\nQ: Who directed the episode with production code 7aff03?\nA: SELECT Directed by FROM 1-10749143-2 WHERE Production code = '7AFF03'"}
 454 | {"text": "table: 1-10749143-2\ncolumns: Series #, Season #, Title, Directed by, Written by, Original air date, Production code, U.S. viewers (millions)\nQ: What is the title of the episode wtih 10.34 million U.S viewers?\nA: SELECT Title FROM 1-10749143-2 WHERE U.S. viewers (millions) = '10.34'"}
 455 | {"text": "table: 1-10748727-1\ncolumns: Season, Series, Team Name, Races, Wins, Poles, F/Laps, Podiums, Points, Position\nQ: What place is the team that completed 6 races?\nA: SELECT Position FROM 1-10748727-1 WHERE Races = 6"}
 456 | {"text": "table: 1-10748727-1\ncolumns: Season, Series, Team Name, Races, Wins, Poles, F/Laps, Podiums, Points, Position\nQ: how much did the british formula three called \"fortec motorsport\" score?\nA: SELECT Points FROM 1-10748727-1 WHERE Series = 'British Formula Three' AND Team Name = 'Fortec Motorsport'"}
 457 | {"text": "table: 1-10748727-1\ncolumns: Season, Series, Team Name, Races, Wins, Poles, F/Laps, Podiums, Points, Position\nQ: how many races were in 2009 with 0 wins?\nA: SELECT Races FROM 1-10748727-1 WHERE Season = '2009' AND Wins = 0"}
 458 | {"text": "table: 1-10748727-1\ncolumns: Season, Series, Team Name, Races, Wins, Poles, F/Laps, Podiums, Points, Position\nQ: What years did art grand prix compete?\nA: SELECT Season FROM 1-10748727-1 WHERE Team Name = 'ART Grand Prix'"}
 459 | {"text": "table: 1-10748727-1\ncolumns: Season, Series, Team Name, Races, Wins, Poles, F/Laps, Podiums, Points, Position\nQ: What year had a score of 9?\nA: SELECT Season FROM 1-10748727-1 WHERE Points = '9'"}
 460 | {"text": "table: 1-10748727-1\ncolumns: Season, Series, Team Name, Races, Wins, Poles, F/Laps, Podiums, Points, Position\nQ: what is the greatest number of wins by japanese formula three?\nA: SELECT MAX Wins FROM 1-10748727-1 WHERE Series = 'Japanese Formula Three'"}
 461 | {"text": "table: 1-10749367-3\ncolumns: #, Air Date, Challenge, Winner, Test-taker, Passed?\nQ: Who took test #4?\nA: SELECT Test-taker FROM 1-10749367-3 WHERE # = 4"}
 462 | {"text": "table: 1-10749367-3\ncolumns: #, Air Date, Challenge, Winner, Test-taker, Passed?\nQ: What episode aired on 18 April 2007?\nA: SELECT MIN # FROM 1-10749367-3 WHERE Air Date = '18 April 2007'"}
 463 | {"text": "table: 1-10749367-3\ncolumns: #, Air Date, Challenge, Winner, Test-taker, Passed?\nQ: Who had the challenge of night driving?\nA: SELECT Test-taker FROM 1-10749367-3 WHERE Challenge = 'Night Driving'"}
 464 | {"text": "table: 1-10798928-1\ncolumns: Year (Ceremony), Film title used in nomination, Original title, Director, Result\nQ: How many directors were there for the film Course Completed?\nA: SELECT COUNT Director FROM 1-10798928-1 WHERE Film title used in nomination = 'Course Completed'"}
 465 | {"text": "table: 1-10798928-1\ncolumns: Year (Ceremony), Film title used in nomination, Original title, Director, Result\nQ: Who directed El Nido?\nA: SELECT Director FROM 1-10798928-1 WHERE Original title = 'El nido'"}
 466 | {"text": "table: 1-10798928-1\ncolumns: Year (Ceremony), Film title used in nomination, Original title, Director, Result\nQ: Who directed Dulcinea?\nA: SELECT Director FROM 1-10798928-1 WHERE Original title = 'Dulcinea'"}
 467 | {"text": "table: 1-10797463-1\ncolumns: Village (German), Village (Slovenian), Number of people 1991, Percent of Slovenes 1991, Percent of Slovenes 1951\nQ: What are the slovenian names of the villages that had 65.9% of slovenes in 1951?\nA: SELECT Village (Slovenian) FROM 1-10797463-1 WHERE Percent of Slovenes 1951 = '65.9%'"}
 468 | {"text": "table: 1-10797463-1\ncolumns: Village (German), Village (Slovenian), Number of people 1991, Percent of Slovenes 1991, Percent of Slovenes 1951\nQ: What are the slovenian names of the villages that had 16.7% of slovenes in 1991?\nA: SELECT Village (Slovenian) FROM 1-10797463-1 WHERE Percent of Slovenes 1991 = '16.7%'"}
 469 | {"text": "table: 1-10797463-1\ncolumns: Village (German), Village (Slovenian), Number of people 1991, Percent of Slovenes 1991, Percent of Slovenes 1951\nQ: How many villages had 21.7% of slovenes in 1991?\nA: SELECT COUNT Village (German) FROM 1-10797463-1 WHERE Percent of Slovenes 1991 = '21.7%'"}
 470 | {"text": "table: 1-10797463-1\ncolumns: Village (German), Village (Slovenian), Number of people 1991, Percent of Slovenes 1991, Percent of Slovenes 1951\nQ: what percent of slovenes did the village called \u010dahor\u010de in slovenian have in 1991?\nA: SELECT Percent of Slovenes 1991 FROM 1-10797463-1 WHERE Village (Slovenian) = '\u010cahor\u010de'"}
 471 | {"text": "table: 1-10797463-1\ncolumns: Village (German), Village (Slovenian), Number of people 1991, Percent of Slovenes 1991, Percent of Slovenes 1951\nQ: What is the slovenian name for the village that in german is known as st.margarethen?\nA: SELECT Village (Slovenian) FROM 1-10797463-1 WHERE Village (German) = 'St.Margarethen'"}
 472 | {"text": "table: 1-10812293-4\ncolumns: Game, Date, Team, Score, High points, High rebounds, High assists, Location Attendance, Record\nQ: For games on December 20, how many points did the scoring leaders get?\nA: SELECT High points FROM 1-10812293-4 WHERE Date = 'December 20'"}
 473 | {"text": "table: 1-10812293-4\ncolumns: Game, Date, Team, Score, High points, High rebounds, High assists, Location Attendance, Record\nQ: Who was the scoring leader and how many points did he get in games on December 23?\nA: SELECT High points FROM 1-10812293-4 WHERE Date = 'December 23'"}
 474 | {"text": "table: 1-10812938-3\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What is the pick # for the position de?\nA: SELECT Pick # FROM 1-10812938-3 WHERE Position = 'DE'"}
 475 | {"text": "table: 1-10812938-3\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: Which player went to college at Saint Mary's?\nA: SELECT Player FROM 1-10812938-3 WHERE College = 'Saint Mary's'"}
 476 | {"text": "table: 1-10812938-3\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What is the position for the player with cfl team Winnipeg blue bombers?\nA: SELECT Position FROM 1-10812938-3 WHERE CFL Team = 'Winnipeg Blue Bombers'"}
 477 | {"text": "table: 1-10812938-3\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: Which player went to college at Laval?\nA: SELECT Player FROM 1-10812938-3 WHERE College = 'Laval'"}
 478 | {"text": "table: 1-10812938-3\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What was the college for the player with the cfl team of Edmonton Eskimos (via calgary)?\nA: SELECT College FROM 1-10812938-3 WHERE CFL Team = 'Edmonton Eskimos (via Calgary)'"}
 479 | {"text": "table: 1-10812938-5\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What's the team of the player from St. Francis Xavier College?\nA: SELECT CFL Team FROM 1-10812938-5 WHERE College = 'St. Francis Xavier'"}
 480 | {"text": "table: 1-10812938-5\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What player is on the Montreal Alouettes CFl team?\nA: SELECT Player FROM 1-10812938-5 WHERE CFL Team = 'Montreal Alouettes'"}
 481 | {"text": "table: 1-10812938-5\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What's the pick number of the player from Toronto Argonauts?\nA: SELECT MIN Pick # FROM 1-10812938-5 WHERE CFL Team = 'Toronto Argonauts'"}
 482 | {"text": "table: 1-10812938-5\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What's the pick number of the player whose position is CB?\nA: SELECT Pick # FROM 1-10812938-5 WHERE Position = 'CB'"}
 483 | {"text": "table: 1-10812938-5\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What's the pick number of the player from New Mexico?\nA: SELECT MAX Pick # FROM 1-10812938-5 WHERE College = 'New Mexico'"}
 484 | {"text": "table: 1-10812938-5\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What player went to Ohio State College?\nA: SELECT Player FROM 1-10812938-5 WHERE College = 'Ohio State'"}
 485 | {"text": "table: 1-1081459-1\ncolumns: Number Range, Builder, Introduced, No. Built, Region, Withdrawn\nQ: What is the minimum introduced value for the Departmental region?\nA: SELECT MIN Introduced FROM 1-1081459-1 WHERE Region = 'Departmental'"}
 486 | {"text": "table: 1-1081459-1\ncolumns: Number Range, Builder, Introduced, No. Built, Region, Withdrawn\nQ: What is the smallest introduced value?\nA: SELECT MIN Introduced FROM 1-1081459-1"}
 487 | {"text": "table: 1-10812938-4\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: Which CFL Teams drafted an OL in 2006?\nA: SELECT CFL Team FROM 1-10812938-4 WHERE Position = 'OL'"}
 488 | {"text": "table: 1-10812938-4\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: Which college is aligned to the Saskatchewan Roughriders?\nA: SELECT College FROM 1-10812938-4 WHERE CFL Team = 'Saskatchewan Roughriders'"}
 489 | {"text": "table: 1-10812938-4\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What Position did the Hamilton Tiger-Cats (via Ottawa) pick in the 2006 Draft.\nA: SELECT Position FROM 1-10812938-4 WHERE CFL Team = 'Hamilton Tiger-Cats (via Ottawa)'"}
 490 | {"text": "table: 1-10812938-4\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What is the earliest pick listed in the table.\nA: SELECT MIN Pick # FROM 1-10812938-4"}
 491 | {"text": "table: 1-10842344-1\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, Production code, U.S. viewers (millions)\nQ: What episode number had production code e4423?\nA: SELECT MAX No. in season FROM 1-10842344-1 WHERE Production code = 'E4423'"}
 492 | {"text": "table: 1-10842344-1\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, Production code, U.S. viewers (millions)\nQ: What's the latest episode in a season where the U.S. viewers totaled 14.37 million?\nA: SELECT MAX No. in season FROM 1-10842344-1 WHERE U.S. viewers (millions) = '14.37'"}
 493 | {"text": "table: 1-10842344-1\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, Production code, U.S. viewers (millions)\nQ: Who directed the episode \"Escape\"?\nA: SELECT Directed by FROM 1-10842344-1 WHERE Title = '\"Escape\"'"}
 494 | {"text": "table: 1-10819266-8\ncolumns: Season, Episodes, Time slot (EST), Season premiere, Season finale, TV season, Rank, Viewers (in millions)\nQ: How many seasons is the season finale on May 26, 2010?\nA: SELECT COUNT Season FROM 1-10819266-8 WHERE Season finale = 'May 26, 2010'"}
 495 | {"text": "table: 1-10819266-8\ncolumns: Season, Episodes, Time slot (EST), Season premiere, Season finale, TV season, Rank, Viewers (in millions)\nQ: What episodes are there where the season premier is September 23, 2009?\nA: SELECT Episodes FROM 1-10819266-8 WHERE Season premiere = 'September 23, 2009'"}
 496 | {"text": "table: 1-10819266-8\ncolumns: Season, Episodes, Time slot (EST), Season premiere, Season finale, TV season, Rank, Viewers (in millions)\nQ: What is the season finale for season 4?\nA: SELECT Season finale FROM 1-10819266-8 WHERE Season = 4"}
 497 | {"text": "table: 1-10819266-8\ncolumns: Season, Episodes, Time slot (EST), Season premiere, Season finale, TV season, Rank, Viewers (in millions)\nQ: How many season premiers have a rank of #21?\nA: SELECT Season premiere FROM 1-10819266-8 WHERE Rank = '#21'"}
 498 | {"text": "table: 1-10819266-8\ncolumns: Season, Episodes, Time slot (EST), Season premiere, Season finale, TV season, Rank, Viewers (in millions)\nQ: What are the seasons where September 26, 2007 is the season premier?\nA: SELECT TV season FROM 1-10819266-8 WHERE Season premiere = 'September 26, 2007'"}
 499 | {"text": "table: 1-10818465-1\ncolumns: Model, RU, Max processors, Processor frequency, Max memory, Max disk capacity, GA Date\nQ: What max processor has a maximum memory of 256 gb?\nA: SELECT Max processors FROM 1-10818465-1 WHERE Max memory = '256 GB'"}
 500 | {"text": "table: 1-10818465-1\ncolumns: Model, RU, Max processors, Processor frequency, Max memory, Max disk capacity, GA Date\nQ: What is the max memory of the t5120 model?\nA: SELECT Max memory FROM 1-10818465-1 WHERE Model = 'T5120'"}
 501 | {"text": "table: 1-10818465-1\ncolumns: Model, RU, Max processors, Processor frequency, Max memory, Max disk capacity, GA Date\nQ: What is the lowest ru?\nA: SELECT MIN RU FROM 1-10818465-1"}
 502 | {"text": "table: 1-10818465-1\ncolumns: Model, RU, Max processors, Processor frequency, Max memory, Max disk capacity, GA Date\nQ: What ga date do the models with 1.0, 1.2, 1.4ghz processor frequencies have?\nA: SELECT GA Date FROM 1-10818465-1 WHERE Processor frequency = '1.0, 1.2, 1.4GHz'"}
 503 | {"text": "table: 1-10818465-1\ncolumns: Model, RU, Max processors, Processor frequency, Max memory, Max disk capacity, GA Date\nQ: What is the ga date of the t5120 model?\nA: SELECT GA Date FROM 1-10818465-1 WHERE Model = 'T5120'"}
 504 | {"text": "table: 1-10815352-1\ncolumns: League, Sport, Country, Season, Games, Average attendance, Total attendance\nQ: What is the sport of the La Liga league?\nA: SELECT Sport FROM 1-10815352-1 WHERE League = 'La Liga'"}
 505 | {"text": "table: 1-10815352-1\ncolumns: League, Sport, Country, Season, Games, Average attendance, Total attendance\nQ: What's the minimum total attendance of the Premier League association football?\nA: SELECT MIN Total attendance FROM 1-10815352-1 WHERE Sport = 'Association football' AND League = 'Premier League'"}
 506 | {"text": "table: 1-10815352-1\ncolumns: League, Sport, Country, Season, Games, Average attendance, Total attendance\nQ: What's the average attendance of the leagues in the season of 2013?\nA: SELECT MIN Average attendance FROM 1-10815352-1 WHERE Season = '2013'"}
 507 | {"text": "table: 1-10815352-1\ncolumns: League, Sport, Country, Season, Games, Average attendance, Total attendance\nQ: What's the total attendance of the leagues in season of 2010?\nA: SELECT COUNT Total attendance FROM 1-10815352-1 WHERE Season = '2010'"}
 508 | {"text": "table: 1-10874596-1\ncolumns: Year [e ] (Ceremony), Film title used in nomination, Original title, Director, Result\nQ: Who were the directors of the film submitted with the title Young T\u00f6rless?\nA: SELECT Director FROM 1-10874596-1 WHERE Film title used in nomination = 'Young T\u00f6rless'"}
 509 | {"text": "table: 1-10874596-1\ncolumns: Year [e ] (Ceremony), Film title used in nomination, Original title, Director, Result\nQ: What was the original title of the film submitted with the title A Woman in Flames?\nA: SELECT Original title FROM 1-10874596-1 WHERE Film title used in nomination = 'A Woman in Flames'"}
 510 | {"text": "table: 1-10874596-1\ncolumns: Year [e ] (Ceremony), Film title used in nomination, Original title, Director, Result\nQ: In what years was a film submitted with the title The Enigma of Kaspar Hauser?\nA: SELECT Year [e ] (Ceremony) FROM 1-10874596-1 WHERE Film title used in nomination = 'The Enigma of Kaspar Hauser'"}
 511 | {"text": "table: 1-10874596-1\ncolumns: Year [e ] (Ceremony), Film title used in nomination, Original title, Director, Result\nQ: Who were the directors of the film with the original title o.k.?\nA: SELECT Director FROM 1-10874596-1 WHERE Original title = 'o.k.'"}
 512 | {"text": "table: 1-1087659-2\ncolumns: Year, Division, League, Reg. Season, Playoffs, Avg. Attendance\nQ: What is the division for the division semifinals playoffs?\nA: SELECT Division FROM 1-1087659-2 WHERE Playoffs = 'Division Semifinals'"}
 513 | {"text": "table: 1-10908676-7\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date\nQ: What is the number in series of \"say uncle\"?\nA: SELECT No. in series FROM 1-10908676-7 WHERE Title = '\"Say Uncle\"'"}
 514 | {"text": "table: 1-10908676-7\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date\nQ: What is the title written by David Mamet?\nA: SELECT Title FROM 1-10908676-7 WHERE Written by = 'David Mamet'"}
 515 | {"text": "table: 1-10942714-1\ncolumns: Rank, English title, Chinese title, Average, Peak, Premiere, Finale, HK viewers\nQ: What was the finale for \u6f6e\u7206\u5927\u72c0\nA: SELECT Finale FROM 1-10942714-1 WHERE Chinese title = '\u6f6e\u7206\u5927\u72c0'"}
 516 | {"text": "table: 1-10942714-1\ncolumns: Rank, English title, Chinese title, Average, Peak, Premiere, Finale, HK viewers\nQ: What was the finale for  \u6f6e\u7206\u5927\u72c0\nA: SELECT Finale FROM 1-10942714-1 WHERE Chinese title = '\u6f6e\u7206\u5927\u72c0'"}
 517 | {"text": "table: 1-10942714-1\ncolumns: Rank, English title, Chinese title, Average, Peak, Premiere, Finale, HK viewers\nQ: How many viewers were there for the premier with 34\nA: SELECT HK viewers FROM 1-10942714-1 WHERE Premiere = 34"}
 518 | {"text": "table: 1-10942714-1\ncolumns: Rank, English title, Chinese title, Average, Peak, Premiere, Finale, HK viewers\nQ: How many are listed under \u6f6e\u7206\u5927\u72c0\nA: SELECT COUNT Peak FROM 1-10942714-1 WHERE Chinese title = '\u6f6e\u7206\u5927\u72c0'"}
 519 | {"text": "table: 1-10953197-2\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: Who was the director of the episode with a production code of 2393059?\nA: SELECT Director FROM 1-10953197-2 WHERE Production code = '2393059'"}
 520 | {"text": "table: 1-10953197-2\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: How many people wrote \"Michael's Game\"?\nA: SELECT COUNT Writer(s) FROM 1-10953197-2 WHERE Title = '\"Michael's Game\"'"}
 521 | {"text": "table: 1-10953197-2\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: When did the episode title \"Duet For One\" air?\nA: SELECT Original air date FROM 1-10953197-2 WHERE Title = '\"Duet for One\"'"}
 522 | {"text": "table: 1-10935548-1\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, Production code, U.S. viewers (millions)\nQ: Which episode had 16.38 million U.S. viewers?\nA: SELECT Title FROM 1-10935548-1 WHERE U.S. viewers (millions) = '16.38'"}
 523 | {"text": "table: 1-10935548-1\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, Production code, U.S. viewers (millions)\nQ: What is the production code of the episode written by Jos\u00e9 Molina that aired on October 12, 2004?\nA: SELECT Production code FROM 1-10935548-1 WHERE Written by = 'Jos\u00e9 Molina' AND Original air date = 'October 12, 2004'"}
 524 | {"text": "table: 1-10935548-1\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, Production code, U.S. viewers (millions)\nQ: What was the original air date of the episode \"Quarry\"?\nA: SELECT Original air date FROM 1-10935548-1 WHERE Title = '\"Quarry\"'"}
 525 | {"text": "table: 1-10935548-1\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, Production code, U.S. viewers (millions)\nQ: Which episode was directed by Jean de Segonzac?\nA: SELECT Title FROM 1-10935548-1 WHERE Directed by = 'Jean de Segonzac'"}
 526 | {"text": "table: 1-10953197-3\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: what are the original air dates with a production code of 2394087\nA: SELECT Original air date FROM 1-10953197-3 WHERE Production code = '2394087'"}
 527 | {"text": "table: 1-10953197-3\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: Who are the writers for the title \"boxing sydney\"\nA: SELECT Writer(s) FROM 1-10953197-3 WHERE Title = '\"Boxing Sydney\"'"}
 528 | {"text": "table: 1-10953197-3\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: What are the production codes for the title \"all about brooke\"\nA: SELECT Production code FROM 1-10953197-3 WHERE Title = '\"All About Brooke\"'"}
 529 | {"text": "table: 1-10953197-3\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: Who are the writer(s) for the production code 2394084\nA: SELECT Writer(s) FROM 1-10953197-3 WHERE Production code = '2394084'"}
 530 | {"text": "table: 1-10953197-4\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: What's the total number of episodes with the production code 2395113A?\nA: SELECT COUNT Title FROM 1-10953197-4 WHERE Production code = '2395113A'"}
 531 | {"text": "table: 1-10953197-4\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: What's the number of the episode called \"Melrose Unglued\"?\nA: SELECT MAX No. in series FROM 1-10953197-4 WHERE Title = '\"Melrose Unglued\"'"}
 532 | {"text": "table: 1-10953197-4\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: Who's the writer for the episode with a production code 2395114?\nA: SELECT Writer(s) FROM 1-10953197-4 WHERE Production code = '2395114'"}
 533 | {"text": "table: 1-10953197-4\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: Who directed the episode titled \"Full Metal Betsy\"?\nA: SELECT Director FROM 1-10953197-4 WHERE Title = '\"Full Metal Betsy\"'"}
 534 | {"text": "table: 1-10953197-4\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: What's the number of the episode with production code 2395118?\nA: SELECT No. in season FROM 1-10953197-4 WHERE Production code = '2395118'"}
 535 | {"text": "table: 1-10953197-4\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: Who was the writer for the episode with production code 2395096?\nA: SELECT Writer(s) FROM 1-10953197-4 WHERE Production code = '2395096'"}
 536 | {"text": "table: 1-10932739-2\ncolumns: Planet, Planet Type, Semimajor Axis ( AU ), Orbital Period, Radial velocity (m/s), Detectable by:\nQ: What generation of spectrograph is most likely to detect a planet with a radial velocity of 0.089 m/s?\nA: SELECT Detectable by: FROM 1-10932739-2 WHERE Radial velocity (m/s) = '0.089'"}
 537 | {"text": "table: 1-10932739-2\ncolumns: Planet, Planet Type, Semimajor Axis ( AU ), Orbital Period, Radial velocity (m/s), Detectable by:\nQ: How long is the orbital period for the planet that has a semimajor axis of 5.20 au?\nA: SELECT Orbital Period FROM 1-10932739-2 WHERE Semimajor Axis ( AU ) = '5.20'"}
 538 | {"text": "table: 1-10932739-2\ncolumns: Planet, Planet Type, Semimajor Axis ( AU ), Orbital Period, Radial velocity (m/s), Detectable by:\nQ: What generation of spectrograph is Jupiter detected by?\nA: SELECT Detectable by: FROM 1-10932739-2 WHERE Planet = 'Jupiter'"}
 539 | {"text": "table: 1-10932739-2\ncolumns: Planet, Planet Type, Semimajor Axis ( AU ), Orbital Period, Radial velocity (m/s), Detectable by:\nQ: Which planet has an orbital period of 11.86 years?\nA: SELECT Planet FROM 1-10932739-2 WHERE Orbital Period = '11.86 years'"}
 540 | {"text": "table: 1-10953197-7\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: who directed the production code 2398204\nA: SELECT Director FROM 1-10953197-7 WHERE Production code = '2398204'"}
 541 | {"text": "table: 1-10953197-7\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: when did \"unpleasantville\" air?\nA: SELECT Original air date FROM 1-10953197-7 WHERE Title = '\"Unpleasantville\"'"}
 542 | {"text": "table: 1-10960039-1\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What is player Alexis Bwenge's pick number?\nA: SELECT Pick # FROM 1-10960039-1 WHERE Player = 'Alexis Bwenge'"}
 543 | {"text": "table: 1-10960039-1\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What player is pick #2?\nA: SELECT Player FROM 1-10960039-1 WHERE Pick # = 2"}
 544 | {"text": "table: 1-10960039-1\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: Which player's college is Saskatchewan?\nA: SELECT Player FROM 1-10960039-1 WHERE College = 'Saskatchewan'"}
 545 | {"text": "table: 1-10960039-1\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What is McMaster College's pick number?\nA: SELECT MIN Pick # FROM 1-10960039-1 WHERE College = 'McMaster'"}
 546 | {"text": "table: 1-10953197-6\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: give the least number of times an episode was shown from 1997-1998\nA: SELECT MIN No. in season FROM 1-10953197-6"}
 547 | {"text": "table: 1-10953197-6\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: when was the episode named \"the doctor is in... deep\" first broadcast \nA: SELECT Original air date FROM 1-10953197-6 WHERE Title = '\"The Doctor Is In... Deep\"'"}
 548 | {"text": "table: 1-10953197-6\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: how many times does the episode called \"coop de grace\" appear \nA: SELECT COUNT No. in series FROM 1-10953197-6 WHERE Title = '\"Coop de Grace\"'"}
 549 | {"text": "table: 1-10953197-6\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: what is season 6 sum of both the number of times processing ID 2397162 was assigned and the number of times chip chalmers managed an episode \nA: SELECT MAX No. in season FROM 1-10953197-6 WHERE Director = 'Chip Chalmers' AND Production code = '2397162'"}
 550 | {"text": "table: 1-10966926-2\ncolumns: Round, Choice, Player name, Position, Height, Weight, College\nQ: Which player went to Michigan State?\nA: SELECT Player name FROM 1-10966926-2 WHERE College = 'Michigan State'"}
 551 | {"text": "table: 1-10966926-2\ncolumns: Round, Choice, Player name, Position, Height, Weight, College\nQ: Which player went to college in Oklahoma?\nA: SELECT Player name FROM 1-10966926-2 WHERE College = 'Oklahoma'"}
 552 | {"text": "table: 1-10966926-2\ncolumns: Round, Choice, Player name, Position, Height, Weight, College\nQ: Which position does Colt Brennan play?\nA: SELECT Position FROM 1-10966926-2 WHERE Player name = 'Colt Brennan'"}
 553 | {"text": "table: 1-10966926-2\ncolumns: Round, Choice, Player name, Position, Height, Weight, College\nQ: What is the height of the person that weighs 320 pounds?\nA: SELECT Height FROM 1-10966926-2 WHERE Weight = 320"}
 554 | {"text": "table: 1-10975034-4\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: How many colleges have a DB position?\nA: SELECT COUNT College FROM 1-10975034-4 WHERE Position = 'DB'"}
 555 | {"text": "table: 1-10975034-4\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What is the maximum number of picks for the CFL team Calgary Stampeders?\nA: SELECT MAX Pick # FROM 1-10975034-4 WHERE CFL Team = 'Calgary Stampeders'"}
 556 | {"text": "table: 1-10975034-4\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: How many CFL teams are from York college?\nA: SELECT COUNT CFL Team FROM 1-10975034-4 WHERE College = 'York'"}
 557 | {"text": "table: 1-10975034-4\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What CFL teams are part of Simon Fraser college?\nA: SELECT CFL Team FROM 1-10975034-4 WHERE College = 'Simon Fraser'"}
 558 | {"text": "table: 1-10975034-4\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: Which players have a pick number of 27?\nA: SELECT Player FROM 1-10975034-4 WHERE Pick # = 27"}
 559 | {"text": "table: 1-10960039-6\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: How many times were players named brett ralph were selected?\nA: SELECT COUNT Pick # FROM 1-10960039-6 WHERE Player = 'Brett Ralph'"}
 560 | {"text": "table: 1-10960039-6\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What schools did lenard semajuste play for?\nA: SELECT College FROM 1-10960039-6 WHERE Player = 'Lenard Semajuste'"}
 561 | {"text": "table: 1-10960039-6\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What is the highest selection number for the saskatchewan roughriders team?\nA: SELECT MAX Pick # FROM 1-10960039-6 WHERE CFL Team = 'Saskatchewan Roughriders'"}
 562 | {"text": "table: 1-10960039-6\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: How many fb players were drafted?\nA: SELECT COUNT Pick # FROM 1-10960039-6 WHERE Position = 'FB'"}
 563 | {"text": "table: 1-10960039-6\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: How many players played for adams state school?\nA: SELECT COUNT Player FROM 1-10960039-6 WHERE College = 'Adams State'"}
 564 | {"text": "table: 1-10960039-6\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What teams drafted players that played for northwood school?\nA: SELECT CFL Team FROM 1-10960039-6 WHERE College = 'Northwood'"}
 565 | {"text": "table: 1-10975034-5\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What college did Craig Zimmer go to?\nA: SELECT College FROM 1-10975034-5 WHERE Player = 'Craig Zimmer'"}
 566 | {"text": "table: 1-10975034-5\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What is the pick number of regina?\nA: SELECT Pick # FROM 1-10975034-5 WHERE College = 'Regina'"}
 567 | {"text": "table: 1-10975034-5\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What is the player who is lb and cfl team is saskatchewan roughriders?\nA: SELECT Player FROM 1-10975034-5 WHERE Position = 'LB' AND CFL Team = 'Saskatchewan Roughriders'"}
 568 | {"text": "table: 1-10975034-5\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What is the cfl team that has a position of ol?\nA: SELECT CFL Team FROM 1-10975034-5 WHERE Position = 'OL'"}
 569 | {"text": "table: 1-10975034-5\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What is the number of position where the pick number is 43?\nA: SELECT COUNT Position FROM 1-10975034-5 WHERE Pick # = 43"}
 570 | {"text": "table: 1-10975034-5\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What is the cfl team with ryan folk?\nA: SELECT CFL Team FROM 1-10975034-5 WHERE Player = 'Ryan Folk'"}
 571 | {"text": "table: 1-10979230-5\ncolumns: Romaji title, Japanese title, Release date, Reference, Oricon\nQ: What release date is when kids-270 is a reference? \nA: SELECT Release date FROM 1-10979230-5 WHERE Reference = 'KIDS-270'"}
 572 | {"text": "table: 1-10979230-5\ncolumns: Romaji title, Japanese title, Release date, Reference, Oricon\nQ: what is the title where romaji is titles da.i.su.ki\nA: SELECT Japanese title FROM 1-10979230-5 WHERE Romaji title = 'Da.i.su.ki'"}
 573 | {"text": "table: 1-10979230-5\ncolumns: Romaji title, Japanese title, Release date, Reference, Oricon\nQ: what are the title in japanese where the reference is kids-430?\nA: SELECT Japanese title FROM 1-10979230-5 WHERE Reference = 'KIDS-430'"}
 574 | {"text": "table: 1-10979230-5\ncolumns: Romaji title, Japanese title, Release date, Reference, Oricon\nQ: who is the reference when romaji title is heartbreak sniper?\nA: SELECT Reference FROM 1-10979230-5 WHERE Romaji title = 'Heartbreak Sniper'"}
 575 | {"text": "table: 1-10979230-4\ncolumns: Romaji title, Japanese title, Release date, Reference, Oricon\nQ: What rank is \u611b\u306e\u30d0\u30ab on the Japanese singles chart?\nA: SELECT COUNT Oricon FROM 1-10979230-4 WHERE Japanese title = '\u611b\u306e\u30d0\u30ab'"}
 576 | {"text": "table: 1-10979230-4\ncolumns: Romaji title, Japanese title, Release date, Reference, Oricon\nQ: How many songs have mi-chemin as their Japanese name and romanji name?\nA: SELECT COUNT Romaji title FROM 1-10979230-4 WHERE Japanese title = 'Mi-Chemin'"}
 577 | {"text": "table: 1-1099080-1\ncolumns: Condition, Prothrombin time, Partial thromboplastin time, Bleeding time, Platelet count\nQ: What was the  partial thromboplastin time for factor x deficiency as seen in amyloid purpura\nA: SELECT Partial thromboplastin time FROM 1-1099080-1 WHERE Condition = 'Factor X deficiency as seen in amyloid purpura'"}
 578 | {"text": "table: 1-1099080-1\ncolumns: Condition, Prothrombin time, Partial thromboplastin time, Bleeding time, Platelet count\nQ: How many conditions have an unaffected prothrombin time and a prolonged bleeding time\nA: SELECT COUNT Condition FROM 1-1099080-1 WHERE Prothrombin time = 'Unaffected' AND Bleeding time = 'Prolonged'"}
 579 | {"text": "table: 1-1099080-1\ncolumns: Condition, Prothrombin time, Partial thromboplastin time, Bleeding time, Platelet count\nQ: What was the bleeding time for the factor x deficiency as seen in amyloid purpura\nA: SELECT Bleeding time FROM 1-1099080-1 WHERE Condition = 'Factor X deficiency as seen in amyloid purpura'"}
 580 | {"text": "table: 1-1099080-1\ncolumns: Condition, Prothrombin time, Partial thromboplastin time, Bleeding time, Platelet count\nQ: What conditions had both prolonged bleeding times and prolonged partial thromboplastin times\nA: SELECT Condition FROM 1-1099080-1 WHERE Partial thromboplastin time = 'Prolonged' AND Bleeding time = 'Prolonged'"}
 581 | {"text": "table: 1-1099080-1\ncolumns: Condition, Prothrombin time, Partial thromboplastin time, Bleeding time, Platelet count\nQ: What was the bleeding time for  factor xii deficiency\nA: SELECT Bleeding time FROM 1-1099080-1 WHERE Condition = 'Factor XII deficiency'"}
 582 | {"text": "table: 1-1099080-1\ncolumns: Condition, Prothrombin time, Partial thromboplastin time, Bleeding time, Platelet count\nQ: What were the bleeding times when both the platelet count was unaffected and the partial thromboplastin time was unaffected\nA: SELECT Bleeding time FROM 1-1099080-1 WHERE Partial thromboplastin time = 'Unaffected' AND Platelet count = 'Unaffected'"}
 583 | {"text": "table: 1-11019212-1\ncolumns: Location, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday\nQ: what's the\u00a0tuesday\u00a0time with\u00a0location\u00a0being millhopper\nA: SELECT Tuesday FROM 1-11019212-1 WHERE Location = 'Millhopper'"}
 584 | {"text": "table: 1-11019212-1\ncolumns: Location, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday\nQ: what's the\u00a0wednesday time\u00a0with\u00a0monday\u00a0being 10:00-8:00\nA: SELECT Wednesday FROM 1-11019212-1 WHERE Monday = '10:00-8:00'"}
 585 | {"text": "table: 1-11019212-1\ncolumns: Location, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday\nQ: what's the\u00a0thursday\u00a0time with\u00a0location\u00a0being hawthorne\nA: SELECT Thursday FROM 1-11019212-1 WHERE Location = 'Hawthorne'"}
 586 | {"text": "table: 1-11019212-1\ncolumns: Location, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday\nQ: what's the\u00a0saturday\u00a0time with\u00a0wednesday\u00a0being 10:00-5:00\nA: SELECT Saturday FROM 1-11019212-1 WHERE Wednesday = '10:00-5:00'"}
 587 | {"text": "table: 1-11019212-1\ncolumns: Location, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday\nQ: what's the\u00a0thursday\u00a0time with\u00a0sunday\u00a0being 1:00-5:00 and\u00a0tuesday\u00a0being 1:00-7:00\nA: SELECT Thursday FROM 1-11019212-1 WHERE Sunday = '1:00-5:00' AND Tuesday = '1:00-7:00'"}
 588 | {"text": "table: 1-11019212-1\ncolumns: Location, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday\nQ: what's the\u00a0monday\u00a0time with\u00a0tuesday\u00a0being 9:00-6:00\nA: SELECT Monday FROM 1-11019212-1 WHERE Tuesday = '9:00-6:00'"}
 589 | {"text": "table: 1-11056278-3\ncolumns: Rnd, Race Name, Pole position, Fastest lap, Winning driver, Winning team, Report\nQ: What are all the reports where Paul Tracy had the fastest lap?\nA: SELECT Report FROM 1-11056278-3 WHERE Fastest lap = 'Paul Tracy'"}
 590 | {"text": "table: 1-11056278-3\ncolumns: Rnd, Race Name, Pole position, Fastest lap, Winning driver, Winning team, Report\nQ: Who drove the fastest lap at the Tenneco Automotive Grand Prix of Detroit?\nA: SELECT Fastest lap FROM 1-11056278-3 WHERE Race Name = 'Tenneco Automotive Grand Prix of Detroit'"}
 591 | {"text": "table: 1-11056278-3\ncolumns: Rnd, Race Name, Pole position, Fastest lap, Winning driver, Winning team, Report\nQ: Who had the fastest lap in the races won by Max Papis?\nA: SELECT Fastest lap FROM 1-11056278-3 WHERE Winning driver = 'Max Papis'"}
 592 | {"text": "table: 1-11056278-3\ncolumns: Rnd, Race Name, Pole position, Fastest lap, Winning driver, Winning team, Report\nQ: In Round 6, how many winning drivers were there?\nA: SELECT COUNT Winning driver FROM 1-11056278-3 WHERE Rnd = 6"}
 593 | {"text": "table: 1-1104312-5\ncolumns: English name, Original name, Area in km\u00b2, Population at 2010 Census, Number of settlements and villages\nQ: What are the original names of the districts where the population in the 2010 census was 210450?\nA: SELECT Original name FROM 1-1104312-5 WHERE Population at 2010 Census = 210450"}
 594 | {"text": "table: 1-1104312-5\ncolumns: English name, Original name, Area in km\u00b2, Population at 2010 Census, Number of settlements and villages\nQ: What is the original name of the district with the current English name of South Bogor?\nA: SELECT Original name FROM 1-1104312-5 WHERE English name = 'South Bogor'"}
 595 | {"text": "table: 1-1104312-5\ncolumns: English name, Original name, Area in km\u00b2, Population at 2010 Census, Number of settlements and villages\nQ: What is the listed population from the 2010 census of West Bogor?\nA: SELECT MIN Population at 2010 Census FROM 1-1104312-5 WHERE English name = 'West Bogor'"}
 596 | {"text": "table: 1-1104312-5\ncolumns: English name, Original name, Area in km\u00b2, Population at 2010 Census, Number of settlements and villages\nQ: How many districts have an area of 17.72 KM2?\nA: SELECT COUNT English name FROM 1-1104312-5 WHERE Area in km\u00b2 = '17.72'"}
 597 | {"text": "table: 1-1104312-5\ncolumns: English name, Original name, Area in km\u00b2, Population at 2010 Census, Number of settlements and villages\nQ: What is the area in km2 for the district whose original name was Kecamatan Bogor Timur?\nA: SELECT Area in km\u00b2 FROM 1-1104312-5 WHERE Original name = 'Kecamatan Bogor Timur'"}
 598 | {"text": "table: 1-11066073-1\ncolumns: Pilot car No., Colour, Serial No., Engine No., Registration No.\nQ: What is the number of colour with the regisration number of mg-509?\nA: SELECT COUNT Colour FROM 1-11066073-1 WHERE Registration No. = 'MG-509'"}
 599 | {"text": "table: 1-11058032-1\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: What is the title of the episode directed by Mark Tinker?\nA: SELECT Title FROM 1-11058032-1 WHERE Directed by = 'Mark Tinker'"}
 600 | {"text": "table: 1-11058032-1\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: What episode in the season was directed by Jeff Melman?\nA: SELECT MIN No. in season FROM 1-11058032-1 WHERE Directed by = 'Jeff Melman'"}
 601 | {"text": "table: 1-11058032-1\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: How many episodes had 16.03 million viewers?\nA: SELECT COUNT No. in series FROM 1-11058032-1 WHERE U.S. viewers (millions) = '16.03'"}
 602 | {"text": "table: 1-11071897-1\ncolumns: Interregnum began, Interregnum ended, Duration, Count Palatine of Saxony, Count Palatine of the Rhine\nQ: What is the number of interregnum for duration 3 months, 6 days?\nA: SELECT COUNT Interregnum ended FROM 1-11071897-1 WHERE Duration = '3 months, 6 days'"}
 603 | {"text": "table: 1-11075747-4\ncolumns: Series #, Episode #, Title, Directed by, Written by, Original air date\nQ: Who directed Episode 8?\nA: SELECT Directed by FROM 1-11075747-4 WHERE Episode # = 8"}
 604 | {"text": "table: 1-11075747-4\ncolumns: Series #, Episode #, Title, Directed by, Written by, Original air date\nQ: Who directed the episode called \"Tell-tale Heart\"?\nA: SELECT Directed by FROM 1-11075747-4 WHERE Title = '\"Tell-Tale Heart\"'"}
 605 | {"text": "table: 1-11075747-4\ncolumns: Series #, Episode #, Title, Directed by, Written by, Original air date\nQ: What was the original air date for Series 36?\nA: SELECT Original air date FROM 1-11075747-4 WHERE Series # = 36"}
 606 | {"text": "table: 1-11075747-4\ncolumns: Series #, Episode #, Title, Directed by, Written by, Original air date\nQ: Who wrote Series 38?\nA: SELECT Written by FROM 1-11075747-4 WHERE Series # = 38"}
 607 | {"text": "table: 1-1108394-24\ncolumns: 1973 Democratic initial primary, Manhattan, The Bronx, Brooklyn, Queens, Richmond [Staten Is.], Total, %\nQ: What is the percentage for manhattan 45,901?\nA: SELECT COUNT % FROM 1-1108394-24 WHERE Manhattan = '45,901'"}
 608 | {"text": "table: 1-1108394-24\ncolumns: 1973 Democratic initial primary, Manhattan, The Bronx, Brooklyn, Queens, Richmond [Staten Is.], Total, %\nQ: Who won the 1973 democratic initial primary for queens of 19%?\nA: SELECT 1973 Democratic initial primary FROM 1-1108394-24 WHERE Queens = '19%'"}
 609 | {"text": "table: 1-1108394-24\ncolumns: 1973 Democratic initial primary, Manhattan, The Bronx, Brooklyn, Queens, Richmond [Staten Is.], Total, %\nQ: What is the manhattan for richmond 35%?\nA: SELECT Manhattan FROM 1-1108394-24 WHERE Richmond [Staten Is.] = '35%'"}
 610 | {"text": "table: 1-1108394-24\ncolumns: 1973 Democratic initial primary, Manhattan, The Bronx, Brooklyn, Queens, Richmond [Staten Is.], Total, %\nQ: What is the queens where richmond staten is 42%?\nA: SELECT Queens FROM 1-1108394-24 WHERE Richmond [Staten Is.] = '42%'"}
 611 | {"text": "table: 1-1108394-43\ncolumns: 1932 (before recount), party, Manhattan, The Bronx, Brooklyn, Queens, Richmond [Staten Is.], Total, %\nQ: what's the\u00a0party\u00a0with\u00a0brooklyn\u00a0value of 51.0%\nA: SELECT party FROM 1-1108394-43 WHERE Brooklyn = '51.0%'"}
 612 | {"text": "table: 1-1108394-43\ncolumns: 1932 (before recount), party, Manhattan, The Bronx, Brooklyn, Queens, Richmond [Staten Is.], Total, %\nQ: what's the\u00a0brooklyn\u00a0with\u00a0queens\u00a0value of 16.8%\nA: SELECT Brooklyn FROM 1-1108394-43 WHERE Queens = '16.8%'"}
 613 | {"text": "table: 1-1108394-43\ncolumns: 1932 (before recount), party, Manhattan, The Bronx, Brooklyn, Queens, Richmond [Staten Is.], Total, %\nQ: what is the minimum total\nA: SELECT MIN Total FROM 1-1108394-43"}
 614 | {"text": "table: 1-1108394-43\ncolumns: 1932 (before recount), party, Manhattan, The Bronx, Brooklyn, Queens, Richmond [Staten Is.], Total, %\nQ: what's the\u00a0%\u00a0with\u00a0total\u00a0value of 249887 and\u00a0queens\u00a0value of 6.8%\nA: SELECT % FROM 1-1108394-43 WHERE Total = 249887 AND Queens = '6.8%'"}
 615 | {"text": "table: 1-11094950-1\ncolumns: Team, Location, Joined, Conference, Division, Previous Conference\nQ: Which teams were in the central division and located in livonia?\nA: SELECT Team FROM 1-11094950-1 WHERE Division = 'Central' AND Location = 'Livonia'"}
 616 | {"text": "table: 1-11094950-1\ncolumns: Team, Location, Joined, Conference, Division, Previous Conference\nQ: Which teams are located in highland township?\nA: SELECT Team FROM 1-11094950-1 WHERE Location = 'Highland Township'"}
 617 | {"text": "table: 1-11094950-1\ncolumns: Team, Location, Joined, Conference, Division, Previous Conference\nQ: What conference was the churchill chargers team in?\nA: SELECT Conference FROM 1-11094950-1 WHERE Team = 'Churchill Chargers'"}
 618 | {"text": "table: 1-11111116-7\ncolumns: No. in season, No. in series, Title, Directed by, Written by, Original air date, U.S. viewers (million)\nQ: What was the titles of the episodes written by ken lazebnik?\nA: SELECT Title FROM 1-11111116-7 WHERE Written by = 'Ken LaZebnik'"}
 619 | {"text": "table: 1-11111116-7\ncolumns: No. in season, No. in series, Title, Directed by, Written by, Original air date, U.S. viewers (million)\nQ: Who directed an episode that had 2.81 million U.S. viewers?\nA: SELECT Directed by FROM 1-11111116-7 WHERE U.S. viewers (million) = '2.81'"}
 620 | {"text": "table: 1-11111116-7\ncolumns: No. in season, No. in series, Title, Directed by, Written by, Original air date, U.S. viewers (million)\nQ: What were the names of the episodes that had 3.02 million U.S. viewers?\nA: SELECT Title FROM 1-11111116-7 WHERE U.S. viewers (million) = '3.02'"}
 621 | {"text": "table: 1-11111116-7\ncolumns: No. in season, No. in series, Title, Directed by, Written by, Original air date, U.S. viewers (million)\nQ: What were the original air dates of the episode named \"winds of war\"?\nA: SELECT Original air date FROM 1-11111116-7 WHERE Title = '\"Winds of War\"'"}
 622 | {"text": "table: 1-11111116-7\ncolumns: No. in season, No. in series, Title, Directed by, Written by, Original air date, U.S. viewers (million)\nQ: Who directed episodes that had 2.61 million U.S. viewers?\nA: SELECT Directed by FROM 1-11111116-7 WHERE U.S. viewers (million) = '2.61'"}
 623 | {"text": "table: 1-11111116-8\ncolumns: No. in season, No. in series, Title, Directed by, Written by, Original air date, U.S. viewers (million)\nQ: How many millions of U.S. viewers watched \"Brace for Impact\"?\nA: SELECT U.S. viewers (million) FROM 1-11111116-8 WHERE Title = '\"Brace for Impact\"'"}
 624 | {"text": "table: 1-11111116-8\ncolumns: No. in season, No. in series, Title, Directed by, Written by, Original air date, U.S. viewers (million)\nQ: How many millions of U.S. viewers watched the episode that first aired on March 31, 2013?\nA: SELECT U.S. viewers (million) FROM 1-11111116-8 WHERE Original air date = 'March 31, 2013'"}
 625 | {"text": "table: 1-11111116-8\ncolumns: No. in season, No. in series, Title, Directed by, Written by, Original air date, U.S. viewers (million)\nQ: Who wrote the episodes that were viewed by 2.12 million viewers?\nA: SELECT Written by FROM 1-11111116-8 WHERE U.S. viewers (million) = '2.12'"}
 626 | {"text": "table: 1-11111116-6\ncolumns: No. in season, No. in series, Title, Directed by, Written by, Original air date, U.S. viewers (million)\nQ: The episode written by Rebecca Dameron aired on what date? \nA: SELECT Original air date FROM 1-11111116-6 WHERE Written by = 'Rebecca Dameron'"}
 627 | {"text": "table: 1-11111116-6\ncolumns: No. in season, No. in series, Title, Directed by, Written by, Original air date, U.S. viewers (million)\nQ: Which episode in the series drew 3.6 million U.S. viewers? \nA: SELECT MIN No. in series FROM 1-11111116-6 WHERE U.S. viewers (million) = '3.6'"}
 628 | {"text": "table: 1-11111116-6\ncolumns: No. in season, No. in series, Title, Directed by, Written by, Original air date, U.S. viewers (million)\nQ: Who wrote the episode that aired on April 17, 2011? \nA: SELECT Written by FROM 1-11111116-6 WHERE Original air date = 'April 17, 2011'"}
 629 | {"text": "table: 1-11111116-6\ncolumns: No. in season, No. in series, Title, Directed by, Written by, Original air date, U.S. viewers (million)\nQ: How many times did episode 79 originally air? \nA: SELECT COUNT Original air date FROM 1-11111116-6 WHERE No. in series = 79"}
 630 | {"text": "table: 1-11111116-6\ncolumns: No. in season, No. in series, Title, Directed by, Written by, Original air date, U.S. viewers (million)\nQ: How many millions of views in the country watched \"Line of Departure\"?\nA: SELECT U.S. viewers (million) FROM 1-11111116-6 WHERE Title = '\"Line of Departure\"'"}
 631 | {"text": "table: 1-11148572-1\ncolumns: Season, MLS Cup Winner, MLS Cup Runner-Up, MLS Supporters Shield Winner, MLS Supporters Shield Runner-Up\nQ: What is the name of the shield winner in which the mls cup winner and mls cup runner up is colorado rapids?\nA: SELECT MLS Cup Winner FROM 1-11148572-1 WHERE MLS Cup Runner-Up = 'Colorado Rapids'"}
 632 | {"text": "table: 1-11148572-1\ncolumns: Season, MLS Cup Winner, MLS Cup Runner-Up, MLS Supporters Shield Winner, MLS Supporters Shield Runner-Up\nQ: What is the name of the shield winner in which the mls cup winner and mls supporters shield runner up is Chivas usa?\nA: SELECT MLS Cup Winner FROM 1-11148572-1 WHERE MLS Supporters Shield Runner-Up = 'Chivas USA'"}
 633 | {"text": "table: 1-11148572-1\ncolumns: Season, MLS Cup Winner, MLS Cup Runner-Up, MLS Supporters Shield Winner, MLS Supporters Shield Runner-Up\nQ: who is the of the shield winnerin which the mls cup runner-up and mls cup winner is real salt lake?\nA: SELECT MLS Cup Runner-Up FROM 1-11148572-1 WHERE MLS Cup Winner = 'Real Salt Lake'"}
 634 | {"text": "table: 1-11148572-1\ncolumns: Season, MLS Cup Winner, MLS Cup Runner-Up, MLS Supporters Shield Winner, MLS Supporters Shield Runner-Up\nQ: Which shield winner has the mls cup runner up and the season is 2000?\nA: SELECT MLS Cup Runner-Up FROM 1-11148572-1 WHERE Season = 2000"}
 635 | {"text": "table: 1-1112176-1\ncolumns: Season, Division, League Apps (Sub), League Goals, FA Cup Apps (Sub), FA Cup Goals, FL Cup Apps (Sub), FL Cup Goals, Other Apps, Other Goals, Total Apps (Sub), Total Goals\nQ: League apps (sub) maximum?\nA: SELECT MAX League Apps (Sub) FROM 1-1112176-1"}
 636 | {"text": "table: 1-1112176-1\ncolumns: Season, Division, League Apps (Sub), League Goals, FA Cup Apps (Sub), FA Cup Goals, FL Cup Apps (Sub), FL Cup Goals, Other Apps, Other Goals, Total Apps (Sub), Total Goals\nQ: When total goals is 11 what was the league apps (sub)?\nA: SELECT MAX League Apps (Sub) FROM 1-1112176-1 WHERE Total Goals = 11"}
 637 | {"text": "table: 1-11129123-1\ncolumns: Episode Air Date, Audition City, Date, First Audition Venue, Callback Date, Callback Venue, Golden Tickets\nQ: Which city had the charleston area convention center as its callback location\nA: SELECT Audition City FROM 1-11129123-1 WHERE Callback Venue = 'Charleston Area Convention Center'"}
 638 | {"text": "table: 1-11129123-1\ncolumns: Episode Air Date, Audition City, Date, First Audition Venue, Callback Date, Callback Venue, Golden Tickets\nQ: When did the callbacks from  rancho bernardo inn air\nA: SELECT Episode Air Date FROM 1-11129123-1 WHERE Callback Venue = 'Rancho Bernardo Inn'"}
 639 | {"text": "table: 1-11147852-1\ncolumns: City of license/Market, Station, Channel TV ( DT ), Year of affiliation, Owned since\nQ: The station located in Albuquerque has been owned since what year?\nA: SELECT Owned since FROM 1-11147852-1 WHERE City of license/Market = 'Albuquerque'"}
 640 | {"text": "table: 1-11147852-1\ncolumns: City of license/Market, Station, Channel TV ( DT ), Year of affiliation, Owned since\nQ: What channels have stations that were affiliated in 2002?\nA: SELECT Channel TV ( DT ) FROM 1-11147852-1 WHERE Year of affiliation = '2002'"}
 641 | {"text": "table: 1-11147852-1\ncolumns: City of license/Market, Station, Channel TV ( DT ), Year of affiliation, Owned since\nQ: What market is KTFK-DT in?\nA: SELECT City of license/Market FROM 1-11147852-1 WHERE Station = 'KTFK-DT'"}
 642 | {"text": "table: 1-11167610-1\ncolumns: Trim, Engine, Turbo, Fuel Delivery, Power, Torque, Transmission, Performance\nQ:  what's the\u00a0engine\u00a0where\u00a0performance\u00a0is 0\u2013100km/h: 10.5s, vmax km/h (mph)\nA: SELECT Engine FROM 1-11167610-1 WHERE Performance = '0\u2013100km/h: 10.5s, VMax km/h (mph)'"}
 643 | {"text": "table: 1-11167610-1\ncolumns: Trim, Engine, Turbo, Fuel Delivery, Power, Torque, Transmission, Performance\nQ:  what's the\u00a0turbo\u00a0where\u00a0trim\u00a0is 2.0 20v\nA: SELECT Turbo FROM 1-11167610-1 WHERE Trim = '2.0 20v'"}
 644 | {"text": "table: 1-11167610-1\ncolumns: Trim, Engine, Turbo, Fuel Delivery, Power, Torque, Transmission, Performance\nQ:  what's the\u00a0torque\u00a0where\u00a0performance\u00a0is 0\u2013100km/h: 7.5s auto, vmax: km/h (mph)\nA: SELECT Torque FROM 1-11167610-1 WHERE Performance = '0\u2013100km/h: 7.5s auto, VMax: km/h (mph)'"}
 645 | {"text": "table: 1-11167610-1\ncolumns: Trim, Engine, Turbo, Fuel Delivery, Power, Torque, Transmission, Performance\nQ:  what's the\u00a0transmission\u00a0where\u00a0turbo\u00a0is yes (mitsubishi td04-16t )\nA: SELECT Transmission FROM 1-11167610-1 WHERE Turbo = 'Yes (Mitsubishi TD04-16t )'"}
 646 | {"text": "table: 1-11167610-1\ncolumns: Trim, Engine, Turbo, Fuel Delivery, Power, Torque, Transmission, Performance\nQ:  what's the\u00a0fuel delivery\u00a0where\u00a0power\u00a0is hp (kw) @6500 rpm\nA: SELECT Fuel Delivery FROM 1-11167610-1 WHERE Power = 'hp (kW) @6500 rpm'"}
 647 | {"text": "table: 1-11167610-1\ncolumns: Trim, Engine, Turbo, Fuel Delivery, Power, Torque, Transmission, Performance\nQ: \" what's the engine with turbo being yes (mitsubishi td04-15g ) \"\nA: SELECT Engine FROM 1-11167610-1 WHERE Turbo = 'Yes (Mitsubishi TD04-15g )'"}
 648 | {"text": "table: 1-11173827-1\ncolumns: Rank, English title, Chinese title, Average, Peak, Premiere, Finale, HK viewers\nQ: What is the english title that has finale as 33 and peak as 42?\nA: SELECT English title FROM 1-11173827-1 WHERE Finale = 33 AND Peak = 42"}
 649 | {"text": "table: 1-11173827-1\ncolumns: Rank, English title, Chinese title, Average, Peak, Premiere, Finale, HK viewers\nQ: What is the english title where the premiere is less than 30.0 and the finale is bigger than 36.0?\nA: SELECT English title FROM 1-11173827-1 WHERE Premiere < 30.0 AND Finale > 36.0"}
 650 | {"text": "table: 1-11173827-1\ncolumns: Rank, English title, Chinese title, Average, Peak, Premiere, Finale, HK viewers\nQ: What is the rank of the chinese title \u7de3\u4f86\u81ea\u6709\u6a5f?\nA: SELECT Rank FROM 1-11173827-1 WHERE Chinese title = '\u7de3\u4f86\u81ea\u6709\u6a5f'"}
 651 | {"text": "table: 1-11173827-1\ncolumns: Rank, English title, Chinese title, Average, Peak, Premiere, Finale, HK viewers\nQ: What amount is the number of hk viewers where chinese title is \u5341\u5144\u5f1f?\nA: SELECT HK viewers FROM 1-11173827-1 WHERE Chinese title = '\u5341\u5144\u5f1f'"}
 652 | {"text": "table: 1-11178271-1\ncolumns: #, Episode, Air Date, Rating, Share, 18\u201349 (Rating/Share), Viewers (m), Weekly Rank\nQ: What is the weekly rank with an air date is november 12, 2007?\nA: SELECT Weekly Rank FROM 1-11178271-1 WHERE Air Date = 'November 12, 2007'"}
 653 | {"text": "table: 1-11178271-1\ncolumns: #, Episode, Air Date, Rating, Share, 18\u201349 (Rating/Share), Viewers (m), Weekly Rank\nQ: What is the air date of the episode \"blowback\"?\nA: SELECT Air Date FROM 1-11178271-1 WHERE Episode = '\"Blowback\"'"}
 654 | {"text": "table: 1-11178271-1\ncolumns: #, Episode, Air Date, Rating, Share, 18\u201349 (Rating/Share), Viewers (m), Weekly Rank\nQ: What is the lowest weekly rank with an air date of november 26, 2007?\nA: SELECT MIN Weekly Rank FROM 1-11178271-1 WHERE Air Date = 'November 26, 2007'"}
 655 | {"text": "table: 1-11178271-1\ncolumns: #, Episode, Air Date, Rating, Share, 18\u201349 (Rating/Share), Viewers (m), Weekly Rank\nQ: What is the episode where 18-49 has a rating/share of 3.5/9\nA: SELECT Episode FROM 1-11178271-1 WHERE 18\u201349 (Rating/Share) = '3.5/9'"}
 656 | {"text": "table: 1-11178271-1\ncolumns: #, Episode, Air Date, Rating, Share, 18\u201349 (Rating/Share), Viewers (m), Weekly Rank\nQ: What is the viewers where the rating is 5.3?\nA: SELECT Viewers (m) FROM 1-11178271-1 WHERE Rating = '5.3'"}
 657 | {"text": "table: 1-11178271-1\ncolumns: #, Episode, Air Date, Rating, Share, 18\u201349 (Rating/Share), Viewers (m), Weekly Rank\nQ: What is the 18-49 rating/share where the viewers is 5.61?\nA: SELECT 18\u201349 (Rating/Share) FROM 1-11178271-1 WHERE Viewers (m) = '5.61'"}
 658 | {"text": "table: 1-11206787-5\ncolumns: Team, Stadium, Capacity, Highest, Lowest, Average\nQ: What is the highest of balmoor/\nA: SELECT Highest FROM 1-11206787-5 WHERE Stadium = 'Balmoor'"}
 659 | {"text": "table: 1-11206787-5\ncolumns: Team, Stadium, Capacity, Highest, Lowest, Average\nQ: What is the number of capacity at somerset park?\nA: SELECT COUNT Capacity FROM 1-11206787-5 WHERE Stadium = 'Somerset Park'"}
 660 | {"text": "table: 1-11206787-5\ncolumns: Team, Stadium, Capacity, Highest, Lowest, Average\nQ: What is the minimum capacity where airdrie united is?\nA: SELECT MIN Capacity FROM 1-11206787-5 WHERE Team = 'Airdrie United'"}
 661 | {"text": "table: 1-11206787-5\ncolumns: Team, Stadium, Capacity, Highest, Lowest, Average\nQ: What is the stadium for alloa athletic?\nA: SELECT Stadium FROM 1-11206787-5 WHERE Team = 'Alloa Athletic'"}
 662 | {"text": "table: 1-11206787-5\ncolumns: Team, Stadium, Capacity, Highest, Lowest, Average\nQ: What is the highest of ayr united?\nA: SELECT MIN Highest FROM 1-11206787-5 WHERE Team = 'Ayr United'"}
 663 | {"text": "table: 1-11206787-5\ncolumns: Team, Stadium, Capacity, Highest, Lowest, Average\nQ: What is the average?\nA: SELECT MIN Average FROM 1-11206787-5"}
 664 | {"text": "table: 1-11190568-7\ncolumns: Team, Outgoing manager, Manner of departure, Date of vacancy, Replaced by, Date of appointment, Position\nQ: When are team Galway's dates of appointment?\nA: SELECT Date of appointment FROM 1-11190568-7 WHERE Team = 'Galway'"}
 665 | {"text": "table: 1-11190568-7\ncolumns: Team, Outgoing manager, Manner of departure, Date of vacancy, Replaced by, Date of appointment, Position\nQ: When are the vacancy dates for outgoing manager Damien Fox?\nA: SELECT Date of vacancy FROM 1-11190568-7 WHERE Outgoing manager = 'Damien Fox'"}
 666 | {"text": "table: 1-11190568-7\ncolumns: Team, Outgoing manager, Manner of departure, Date of vacancy, Replaced by, Date of appointment, Position\nQ: When is the date of vacancy of Davy Fitzgerald being a replacement?\nA: SELECT Date of vacancy FROM 1-11190568-7 WHERE Replaced by = 'Davy FitzGerald'"}
 667 | {"text": "table: 1-11190568-7\ncolumns: Team, Outgoing manager, Manner of departure, Date of vacancy, Replaced by, Date of appointment, Position\nQ: Which team has the outgoing manager John Meyler?\nA: SELECT Team FROM 1-11190568-7 WHERE Outgoing manager = 'John Meyler'"}
 668 | {"text": "table: 1-11200856-1\ncolumns: Hand, 1 credit, 2 credits, 3 credits, 4 credits, 5 credits\nQ: How many times is 3 credits 180?\nA: SELECT COUNT 1 credit FROM 1-11200856-1 WHERE 3 credits = 180"}
 669 | {"text": "table: 1-11200856-1\ncolumns: Hand, 1 credit, 2 credits, 3 credits, 4 credits, 5 credits\nQ: What is the hand for 4 credits is 1600?\nA: SELECT Hand FROM 1-11200856-1 WHERE 4 credits = 1600"}
 670 | {"text": "table: 1-11200856-1\ncolumns: Hand, 1 credit, 2 credits, 3 credits, 4 credits, 5 credits\nQ: How many 3 credits are there with 5 credits of 5?\nA: SELECT COUNT 3 credits FROM 1-11200856-1 WHERE 5 credits = '5'"}
 671 | {"text": "table: 1-11200856-1\ncolumns: Hand, 1 credit, 2 credits, 3 credits, 4 credits, 5 credits\nQ: How many 4 credits is the hand two pair?\nA: SELECT COUNT 4 credits FROM 1-11200856-1 WHERE Hand = 'Two pair'"}
 672 | {"text": "table: 1-11210576-3\ncolumns: Character, Position, Actor, First Episode, Final Episode, Duration, Final Episode Count\nQ: What duration is listed for Christian de la Fuente?\nA: SELECT Duration FROM 1-11210576-3 WHERE Actor = 'Christian de la Fuente'"}
 673 | {"text": "table: 1-11210576-3\ncolumns: Character, Position, Actor, First Episode, Final Episode, Duration, Final Episode Count\nQ: What was the final episode for Dea Agent?\nA: SELECT Final Episode FROM 1-11210576-3 WHERE Position = 'DEA Agent'"}
 674 | {"text": "table: 1-11207040-6\ncolumns: Team, Outgoing manager, Manner of departure, Date of vacancy, Replaced by, Date of appointment\nQ: What days is greenock morton vacant?\nA: SELECT Date of vacancy FROM 1-11207040-6 WHERE Team = 'Greenock Morton'"}
 675 | {"text": "table: 1-11207040-6\ncolumns: Team, Outgoing manager, Manner of departure, Date of vacancy, Replaced by, Date of appointment\nQ: What are the dates of the outgoing manager colin hendry does appointments? \nA: SELECT Date of appointment FROM 1-11207040-6 WHERE Outgoing manager = 'Colin Hendry'"}
 676 | {"text": "table: 1-11207040-6\ncolumns: Team, Outgoing manager, Manner of departure, Date of vacancy, Replaced by, Date of appointment\nQ: What teams does jim mcinally manage?\nA: SELECT Team FROM 1-11207040-6 WHERE Outgoing manager = 'Jim McInally'"}
 677 | {"text": "table: 1-11207040-6\ncolumns: Team, Outgoing manager, Manner of departure, Date of vacancy, Replaced by, Date of appointment\nQ: What days are vacant that were replaced by john brown?\nA: SELECT Date of vacancy FROM 1-11207040-6 WHERE Replaced by = 'John Brown'"}
 678 | {"text": "table: 1-11206916-2\ncolumns: Team, Outgoing manager, Manner of departure, Date of vacancy, Replaced by, Date of appointment\nQ: What manner of departure is listed with an appointment date of 13 march 2008\nA: SELECT Manner of departure FROM 1-11206916-2 WHERE Date of appointment = '13 March 2008'"}
 679 | {"text": "table: 1-11206916-2\ncolumns: Team, Outgoing manager, Manner of departure, Date of vacancy, Replaced by, Date of appointment\nQ: What is the date of appointment for outgoing manager Campbell Money\nA: SELECT Date of appointment FROM 1-11206916-2 WHERE Outgoing manager = 'Campbell Money'"}
 680 | {"text": "table: 1-11207040-5\ncolumns: Team, Stadium, Capacity, Highest, Lowest, Average\nQ: What is the lowest attendance that East End Park has ever had?\nA: SELECT MIN Lowest FROM 1-11207040-5 WHERE Stadium = 'East End Park'"}
 681 | {"text": "table: 1-11207040-5\ncolumns: Team, Stadium, Capacity, Highest, Lowest, Average\nQ: What team plays at Palmerston Park?\nA: SELECT Team FROM 1-11207040-5 WHERE Stadium = 'Palmerston Park'"}
 682 | {"text": "table: 1-11207040-5\ncolumns: Team, Stadium, Capacity, Highest, Lowest, Average\nQ: What is the lowest attandance recorded at Cappielow?\nA: SELECT MIN Lowest FROM 1-11207040-5 WHERE Stadium = 'Cappielow'"}
 683 | {"text": "table: 1-11207040-5\ncolumns: Team, Stadium, Capacity, Highest, Lowest, Average\nQ: What is the highest attendance at a game played by St. Johnstone?\nA: SELECT MAX Highest FROM 1-11207040-5 WHERE Team = 'St. Johnstone'"}
 684 | {"text": "table: 1-11207040-5\ncolumns: Team, Stadium, Capacity, Highest, Lowest, Average\nQ: What is the highest attandence at a Hamilton Academical game?\nA: SELECT MIN Highest FROM 1-11207040-5 WHERE Team = 'Hamilton Academical'"}
 685 | {"text": "table: 1-11214772-1\ncolumns: Year, Champion, Score, Runner-Up, Location, Semi-Finalist #1, Semi-Finalist #2\nQ:  who is the\u00a0champion\u00a0where\u00a0semi-finalist #2\u00a0is na and\u00a0location\u00a0is morrisville, nc\nA: SELECT Champion FROM 1-11214772-1 WHERE Semi-Finalist #2 = 'NA' AND Location = 'Morrisville, NC'"}
 686 | {"text": "table: 1-11214772-1\ncolumns: Year, Champion, Score, Runner-Up, Location, Semi-Finalist #1, Semi-Finalist #2\nQ:  what's the\u00a0score\u00a0where\u00a0year\u00a0is 2007\nA: SELECT Score FROM 1-11214772-1 WHERE Year = '2007'"}
 687 | {"text": "table: 1-11214772-1\ncolumns: Year, Champion, Score, Runner-Up, Location, Semi-Finalist #1, Semi-Finalist #2\nQ: what is the total number of\u00a0semi-finalist #2\u00a0where\u00a0runner-up\u00a0is east carolina\nA: SELECT COUNT Semi-Finalist #2 FROM 1-11214772-1 WHERE Runner-Up = 'East Carolina'"}
 688 | {"text": "table: 1-11214772-1\ncolumns: Year, Champion, Score, Runner-Up, Location, Semi-Finalist #1, Semi-Finalist #2\nQ:  who is the\u00a0semi-finalist #1\u00a0where\u00a0runner-up\u00a0is elon university\nA: SELECT Semi-Finalist #1 FROM 1-11214772-1 WHERE Runner-Up = 'Elon University'"}
 689 | {"text": "table: 1-11214772-1\ncolumns: Year, Champion, Score, Runner-Up, Location, Semi-Finalist #1, Semi-Finalist #2\nQ:  who is the\u00a0runner-up\u00a0where\u00a0year\u00a0is 2004 and\u00a0champion\u00a0is north carolina state\nA: SELECT Runner-Up FROM 1-11214772-1 WHERE Year = '2004' AND Champion = 'North Carolina State'"}
 690 | {"text": "table: 1-11214772-1\ncolumns: Year, Champion, Score, Runner-Up, Location, Semi-Finalist #1, Semi-Finalist #2\nQ:  who is the\u00a0runner-up\u00a0where\u00a0location\u00a0is ellenton, fl and\u00a0year\u00a0is 2004\nA: SELECT Runner-Up FROM 1-11214772-1 WHERE Location = 'Ellenton, FL' AND Year = '2004'"}
 691 | {"text": "table: 1-11214212-1\ncolumns: Year, Numer of Jamaicans granted British citizenship, Naturalisation by residence, Naturalisation by marriage, Registration of a minor child, Registration by other means\nQ: what's the\u00a0naturalisation  by marriage\u00a0with\u00a0numer of jamaicans granted british citizenship\u00a0being 3165\nA: SELECT Naturalisation by marriage FROM 1-11214212-1 WHERE Numer of Jamaicans granted British citizenship = 3165"}
 692 | {"text": "table: 1-11214212-1\ncolumns: Year, Numer of Jamaicans granted British citizenship, Naturalisation by residence, Naturalisation by marriage, Registration of a minor child, Registration by other means\nQ:  how many\u00a0numer of jamaicans granted british citizenship\u00a0with\u00a0naturalisation  by marriage\u00a0being 1060\nA: SELECT COUNT Numer of Jamaicans granted British citizenship FROM 1-11214212-1 WHERE Naturalisation by marriage = 1060"}
 693 | {"text": "table: 1-11214212-1\ncolumns: Year, Numer of Jamaicans granted British citizenship, Naturalisation by residence, Naturalisation by marriage, Registration of a minor child, Registration by other means\nQ: what's the\u00a0naturalisation by marriage\u00a0with\u00a0regbeingtration of a minor child\u00a0being 114\nA: SELECT Naturalisation by marriage FROM 1-11214212-1 WHERE Registration of a minor child = 114"}
 694 | {"text": "table: 1-11214212-1\ncolumns: Year, Numer of Jamaicans granted British citizenship, Naturalisation by residence, Naturalisation by marriage, Registration of a minor child, Registration by other means\nQ: what's the\u00a0numer of jamaicans granted british  citizenship\u00a0with\u00a0naturalisation by residence\u00a0being 927\nA: SELECT Numer of Jamaicans granted British citizenship FROM 1-11214212-1 WHERE Naturalisation by residence = 927"}
 695 | {"text": "table: 1-11214212-1\ncolumns: Year, Numer of Jamaicans granted British citizenship, Naturalisation by residence, Naturalisation by marriage, Registration of a minor child, Registration by other means\nQ: what is the maximum\u00a0year\u00a0with\u00a0registration of a minor child\u00a0being 281\nA: SELECT MAX Year FROM 1-11214212-1 WHERE Registration of a minor child = 281"}
 696 | {"text": "table: 1-11220799-2\ncolumns: Episode Titles, First air date, Reward, Immunity, Exiled, Eliminated, Vote, Finish\nQ: How many episodes had their first air date on March 6, 2008?\nA: SELECT COUNT Episode Titles FROM 1-11220799-2 WHERE First air date = 'March 6, 2008'"}
 697 | {"text": "table: 1-11220799-2\ncolumns: Episode Titles, First air date, Reward, Immunity, Exiled, Eliminated, Vote, Finish\nQ: What were the results of episodes with the first air date of March 6, 2008?\nA: SELECT Finish FROM 1-11220799-2 WHERE First air date = 'March 6, 2008'"}
 698 | {"text": "table: 1-11230937-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: How many millions of viewers watched episode 15?\nA: SELECT U.S. viewers (millions) FROM 1-11230937-2 WHERE No. in season = 15"}
 699 | {"text": "table: 1-11230937-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: How many millions of viewers watched the \"Throwing Heat\" episode?\nA: SELECT U.S. viewers (millions) FROM 1-11230937-2 WHERE Title = '\"Throwing Heat\"'"}
 700 | {"text": "table: 1-11230937-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: How many millions of viewers watched the episode directed by Anthony Hemingway?\nA: SELECT U.S. viewers (millions) FROM 1-11230937-2 WHERE Directed by = 'Anthony Hemingway'"}
 701 | {"text": "table: 1-11222744-2\ncolumns: Year, Title, Format, Studio, Release Date, Copyright Information, Catalog Number\nQ: The Catalog number is 80809 what is the title?\nA: SELECT Title FROM 1-11222744-2 WHERE Catalog Number = '80809'"}
 702 | {"text": "table: 1-11222744-2\ncolumns: Year, Title, Format, Studio, Release Date, Copyright Information, Catalog Number\nQ: where title is beginning callanetics , what is the total of format ?\nA: SELECT COUNT Format FROM 1-11222744-2 WHERE Title = 'Beginning Callanetics'"}
 703 | {"text": "table: 1-11222744-2\ncolumns: Year, Title, Format, Studio, Release Date, Copyright Information, Catalog Number\nQ: where catalog number is 81258 , what are all the studio ?\nA: SELECT Studio FROM 1-11222744-2 WHERE Catalog Number = '81258'"}
 704 | {"text": "table: 1-11222744-2\ncolumns: Year, Title, Format, Studio, Release Date, Copyright Information, Catalog Number\nQ: where title is am/pm callanetics , what are all the copyright information?\nA: SELECT Copyright Information FROM 1-11222744-2 WHERE Title = 'AM/PM Callanetics'"}
 705 | {"text": "table: 1-11236195-2\ncolumns: Season, Grand FinalDate, WinningTeam, Score, LosingTeam, Location, GF Attendance, Clive Churchill Medal\nQ: What was the GF attendance at the location of Sydney Football Stadium, Sydney (6)?\nA: SELECT COUNT GF Attendance FROM 1-11236195-2 WHERE Location = 'Sydney Football Stadium, Sydney (6)'"}
 706 | {"text": "table: 1-11236195-2\ncolumns: Season, Grand FinalDate, WinningTeam, Score, LosingTeam, Location, GF Attendance, Clive Churchill Medal\nQ: Which losing team had a score of 24-12?\nA: SELECT LosingTeam FROM 1-11236195-2 WHERE Score = '24-12'"}
 707 | {"text": "table: 1-11236195-2\ncolumns: Season, Grand FinalDate, WinningTeam, Score, LosingTeam, Location, GF Attendance, Clive Churchill Medal\nQ: What was the losing team in the 1993 season?\nA: SELECT LosingTeam FROM 1-11236195-2 WHERE Season = 1993"}
 708 | {"text": "table: 1-1123802-1\ncolumns: Engine, Power, continuous, Critical altitude This is the highest altitude at which the engine can achieve its full continuous power rating. Above this altitude, power falls off with height as with a naturally aspirated engine . See Supercharger#Altitude effects for details., Power, takeoff, Compression ratio, Supercharger gear ratio, Octane rating, Dry weight\nQ: What was the compression ration when the engine was Wasp Jr. T1B2?\nA: SELECT Compression ratio FROM 1-1123802-1 WHERE Engine = 'Wasp Jr. T1B2'"}
 709 | {"text": "table: 1-1123802-1\ncolumns: Engine, Power, continuous, Critical altitude This is the highest altitude at which the engine can achieve its full continuous power rating. Above this altitude, power falls off with height as with a naturally aspirated engine . See Supercharger#Altitude effects for details., Power, takeoff, Compression ratio, Supercharger gear ratio, Octane rating, Dry weight\nQ: What is the compression ration when the continuous power is hp (kw) at 2,200 RPM and the octane rating is 80/87?\nA: SELECT Compression ratio FROM 1-1123802-1 WHERE Power, continuous = 'hp (kW) at 2,200 RPM' AND Octane rating = '80/87'"}
 710 | {"text": "table: 1-1123802-1\ncolumns: Engine, Power, continuous, Critical altitude This is the highest altitude at which the engine can achieve its full continuous power rating. Above this altitude, power falls off with height as with a naturally aspirated engine . See Supercharger#Altitude effects for details., Power, takeoff, Compression ratio, Supercharger gear ratio, Octane rating, Dry weight\nQ: What is the compression ratio when the continuous power is  hp (KW) at 2,200 RPM and the critical altitude is at sea level?\nA: SELECT COUNT Compression ratio FROM 1-1123802-1 WHERE Power, continuous = 'hp (kW) at 2,200 RPM' AND Critical altitude This is the highest altitude at which the engine can achieve its full continuous power rating. Above this altitude, power falls off with height as with a naturally aspirated engine . See Supercharger#Altitude effects for details. = 'sea level'"}
 711 | {"text": "table: 1-1123802-1\ncolumns: Engine, Power, continuous, Critical altitude This is the highest altitude at which the engine can achieve its full continuous power rating. Above this altitude, power falls off with height as with a naturally aspirated engine . See Supercharger#Altitude effects for details., Power, takeoff, Compression ratio, Supercharger gear ratio, Octane rating, Dry weight\nQ: When the engine is Wasp Jr. T1B2, what is the number needed for takeoff power?\nA: SELECT COUNT Power, takeoff FROM 1-1123802-1 WHERE Engine = 'Wasp Jr. T1B2'"}
 712 | {"text": "table: 1-1123802-1\ncolumns: Engine, Power, continuous, Critical altitude This is the highest altitude at which the engine can achieve its full continuous power rating. Above this altitude, power falls off with height as with a naturally aspirated engine . See Supercharger#Altitude effects for details., Power, takeoff, Compression ratio, Supercharger gear ratio, Octane rating, Dry weight\nQ: When critical altitude is sea level, what is the compression ration for a supercharger gear ratio of 7:1?\nA: SELECT Compression ratio FROM 1-1123802-1 WHERE Critical altitude This is the highest altitude at which the engine can achieve its full continuous power rating. Above this altitude, power falls off with height as with a naturally aspirated engine . See Supercharger#Altitude effects for details. = 'sea level' AND Supercharger gear ratio = '7:1'"}
 713 | {"text": "table: 1-11235334-2\ncolumns: #, Episode, Air Date, Timeslot, Viewers, Weekly Rank for Living\nQ: How many episodes aired on october 27, 2008\nA: SELECT COUNT Episode FROM 1-11235334-2 WHERE Air Date = 'October 27, 2008'"}
 714 | {"text": "table: 1-11235334-2\ncolumns: #, Episode, Air Date, Timeslot, Viewers, Weekly Rank for Living\nQ: The episode \"chapter five: dressed to kill\" had a weekly ranking of what?\nA: SELECT Weekly Rank for Living FROM 1-11235334-2 WHERE Episode = '\"Chapter Five: Dressed to Kill\"'"}
 715 | {"text": "table: 1-11235334-2\ncolumns: #, Episode, Air Date, Timeslot, Viewers, Weekly Rank for Living\nQ: what is the most # that aired on september 29, 2008?\nA: SELECT MAX # FROM 1-11235334-2 WHERE Air Date = 'September 29, 2008'"}
 716 | {"text": "table: 1-11236195-5\ncolumns: Season, Grand FinalDate, WinningTeam, Score, LosingTeam, Location, GF Attendance, Clive Churchill Medal\nQ: How many seasons did the canterbury bulldogs (8) win?\nA: SELECT COUNT Season FROM 1-11236195-5 WHERE WinningTeam = 'Canterbury Bulldogs (8)'"}
 717 | {"text": "table: 1-11236195-5\ncolumns: Season, Grand FinalDate, WinningTeam, Score, LosingTeam, Location, GF Attendance, Clive Churchill Medal\nQ: How many teams lost at the sydney football stadium, sydney (11)?\nA: SELECT COUNT LosingTeam FROM 1-11236195-5 WHERE Location = 'Sydney Football Stadium, Sydney (11)'"}
 718 | {"text": "table: 1-11236195-5\ncolumns: Season, Grand FinalDate, WinningTeam, Score, LosingTeam, Location, GF Attendance, Clive Churchill Medal\nQ: What was the date that the st. george-illawarra dragons lost?\nA: SELECT Grand FinalDate FROM 1-11236195-5 WHERE LosingTeam = 'St. George-Illawarra Dragons'"}
 719 | {"text": "table: 1-11236195-5\ncolumns: Season, Grand FinalDate, WinningTeam, Score, LosingTeam, Location, GF Attendance, Clive Churchill Medal\nQ: Brett kimmorley, who was chosen for the clive churchill medal belonged to what team?\nA: SELECT WinningTeam FROM 1-11236195-5 WHERE Clive Churchill Medal = 'Brett Kimmorley'"}
 720 | {"text": "table: 1-11244302-1\ncolumns: #, Episode, Air Date, Time slot (EST), Rating, Share, 18-49 (Rating/Share), Viewers (m), Rank (Overall)\nQ: What time slots have a 6.3 rating\nA: SELECT Time slot (EST) FROM 1-11244302-1 WHERE Rating = '6.3'"}
 721 | {"text": "table: 1-11244302-1\ncolumns: #, Episode, Air Date, Time slot (EST), Rating, Share, 18-49 (Rating/Share), Viewers (m), Rank (Overall)\nQ: What time slot is the episode \"the way we weren't\" in\nA: SELECT Time slot (EST) FROM 1-11244302-1 WHERE Episode = '\"The Way We Weren't\"'"}
 722 | {"text": "table: 1-11244302-1\ncolumns: #, Episode, Air Date, Time slot (EST), Rating, Share, 18-49 (Rating/Share), Viewers (m), Rank (Overall)\nQ: What time slot is the episode \"who's your daddy\" in\nA: SELECT Time slot (EST) FROM 1-11244302-1 WHERE Episode = '\"Who's Your Daddy\"'"}
 723 | {"text": "table: 1-11244302-1\ncolumns: #, Episode, Air Date, Time slot (EST), Rating, Share, 18-49 (Rating/Share), Viewers (m), Rank (Overall)\nQ: Which air date had an 11 share\nA: SELECT Air Date FROM 1-11244302-1 WHERE Share = 11"}
 724 | {"text": "table: 1-11244302-1\ncolumns: #, Episode, Air Date, Time slot (EST), Rating, Share, 18-49 (Rating/Share), Viewers (m), Rank (Overall)\nQ: Which air date had the 18-49 rating/share of 3.3/9\nA: SELECT Air Date FROM 1-11244302-1 WHERE 18-49 (Rating/Share) = '3.3/9'"}
 725 | {"text": "table: 1-11240028-3\ncolumns: Character, Portrayed by, Relationship, First appearance, Last appearance\nQ: Which characters had their first experience in the episode \"consequences\"?\nA: SELECT Character FROM 1-11240028-3 WHERE First appearance = '\"Consequences\"'"}
 726 | {"text": "table: 1-11240028-3\ncolumns: Character, Portrayed by, Relationship, First appearance, Last appearance\nQ: What episode had the last appearances of the late wife of mac taylor?\nA: SELECT Last appearance FROM 1-11240028-3 WHERE Relationship = 'Late wife of Mac Taylor'"}
 727 | {"text": "table: 1-11240028-3\ncolumns: Character, Portrayed by, Relationship, First appearance, Last appearance\nQ: Which characters were portrayed by reed garrett?\nA: SELECT Portrayed by FROM 1-11240028-3 WHERE Character = 'Reed Garrett'"}
 728 | {"text": "table: 1-11240028-3\ncolumns: Character, Portrayed by, Relationship, First appearance, Last appearance\nQ: How many characters were portrayed by the informant of don flack?\nA: SELECT COUNT Portrayed by FROM 1-11240028-3 WHERE Relationship = 'Informant of Don Flack'"}
 729 | {"text": "table: 1-11240028-3\ncolumns: Character, Portrayed by, Relationship, First appearance, Last appearance\nQ: What episode was the last appearance of the character, rikki sandoval?\nA: SELECT Last appearance FROM 1-11240028-3 WHERE Character = 'Rikki Sandoval'"}
 730 | {"text": "table: 1-11240028-1\ncolumns: Character, Portrayed by, First appearance, Last appearance, Duration, Episodes\nQ: On which episode did actress Sela Ward make her last appearance?\nA: SELECT Last appearance FROM 1-11240028-1 WHERE Portrayed by = 'Sela Ward'"}
 731 | {"text": "table: 1-11240028-1\ncolumns: Character, Portrayed by, First appearance, Last appearance, Duration, Episodes\nQ: Which actors first appeared in \"Zoo York\"?\nA: SELECT Portrayed by FROM 1-11240028-1 WHERE First appearance = '\"Zoo York\"'"}
 732 | {"text": "table: 1-11240028-1\ncolumns: Character, Portrayed by, First appearance, Last appearance, Duration, Episodes\nQ: How many episodes did actress Vanessa Ferlito appear in?\nA: SELECT Episodes FROM 1-11240028-1 WHERE Portrayed by = 'Vanessa Ferlito'"}
 733 | {"text": "table: 1-11240028-1\ncolumns: Character, Portrayed by, First appearance, Last appearance, Duration, Episodes\nQ: Which actors first appeared in episode \"Blink\" 1, 2, 3?\nA: SELECT Portrayed by FROM 1-11240028-1 WHERE First appearance = '\"Blink\" 1, 2, 3'"}
 734 | {"text": "table: 1-11240028-1\ncolumns: Character, Portrayed by, First appearance, Last appearance, Duration, Episodes\nQ: What was the duration of Robert Joy's portrayal?\nA: SELECT COUNT Duration FROM 1-11240028-1 WHERE Portrayed by = 'Robert Joy'"}
 735 | {"text": "table: 1-11240028-1\ncolumns: Character, Portrayed by, First appearance, Last appearance, Duration, Episodes\nQ: Which episode did actor A. J. Buckley last appear in?\nA: SELECT Last appearance FROM 1-11240028-1 WHERE Portrayed by = 'A. J. Buckley'"}
 736 | {"text": "table: 1-11250-4\ncolumns: Club, Position in 2012\u201313, First season in top division, Number of seasons in top division, Number of seasons in the Premier League, First season of current spell in top division, Top division titles, Last top division title\nQ: What is the least top division titles?\nA: SELECT MIN Top division titles FROM 1-11250-4"}
 737 | {"text": "table: 1-11250-4\ncolumns: Club, Position in 2012\u201313, First season in top division, Number of seasons in top division, Number of seasons in the Premier League, First season of current spell in top division, Top division titles, Last top division title\nQ: What is the least number of seasons in top division?\nA: SELECT MIN Number of seasons in top division FROM 1-11250-4"}
 738 | {"text": "table: 1-11253290-2\ncolumns: #, Episode, Rating, Share, Rating/Share (18-49), Viewers (millions), Rank (timeslot), Rank (night), Rank (week)\nQ: How many viewers (millions) were there for rank (week) 20?\nA: SELECT COUNT Viewers (millions) FROM 1-11253290-2 WHERE Rank (week) = '20'"}
 739 | {"text": "table: 1-11253290-2\ncolumns: #, Episode, Rating, Share, Rating/Share (18-49), Viewers (millions), Rank (timeslot), Rank (night), Rank (week)\nQ: What is the rank (timeslot) with the episode name \"dangerous liaisons\"?\nA: SELECT Rank (timeslot) FROM 1-11253290-2 WHERE Episode = '\"Dangerous Liaisons\"'"}
 740 | {"text": "table: 1-11253290-2\ncolumns: #, Episode, Rating, Share, Rating/Share (18-49), Viewers (millions), Rank (timeslot), Rank (night), Rank (week)\nQ: What is the lowest rank (night) for having viewers (millions) 5.25?\nA: SELECT MIN Rank (night) FROM 1-11253290-2 WHERE Viewers (millions) = '5.25'"}
 741 | {"text": "table: 1-11253290-2\ncolumns: #, Episode, Rating, Share, Rating/Share (18-49), Viewers (millions), Rank (timeslot), Rank (night), Rank (week)\nQ: How many times was the episode named \"conference call\"?\nA: SELECT COUNT # FROM 1-11253290-2 WHERE Episode = '\"Conference Call\"'"}
 742 | {"text": "table: 1-11253290-2\ncolumns: #, Episode, Rating, Share, Rating/Share (18-49), Viewers (millions), Rank (timeslot), Rank (night), Rank (week)\nQ: How many times was the rank (night) 11?\nA: SELECT COUNT Viewers (millions) FROM 1-11253290-2 WHERE Rank (night) = 11"}
 743 | {"text": "table: 1-11251601-2\ncolumns: Country, Carbon dioxide emissions per year (10 6 Tons) (2006), Percentage of global total, Avg. emission per km 2 of its land (tons), Carbon dioxide emissions per year (Tons per person) (2007)\nQ: WHAT WAS THE AMOUNT OF CARBON DIOXIDE EMISSIONS  IN 2006 IN THE COUNTRY WHOSE  CO2 EMISSIONS (TONS PER PERSON)  REACHED 1.4 IN 2OO7?\nA: SELECT Carbon dioxide emissions per year (10 6 Tons) (2006) FROM 1-11251601-2 WHERE Carbon dioxide emissions per year (Tons per person) (2007) = '1.4'"}
 744 | {"text": "table: 1-11251601-2\ncolumns: Country, Carbon dioxide emissions per year (10 6 Tons) (2006), Percentage of global total, Avg. emission per km 2 of its land (tons), Carbon dioxide emissions per year (Tons per person) (2007)\nQ: HOW MANY TONS OF CO2 EMISSIONS DID RUSSIA PRODUCE IN 2006?\nA: SELECT MAX Carbon dioxide emissions per year (10 6 Tons) (2006) FROM 1-11251601-2 WHERE Country = 'Russia'"}
 745 | {"text": "table: 1-11251601-2\ncolumns: Country, Carbon dioxide emissions per year (10 6 Tons) (2006), Percentage of global total, Avg. emission per km 2 of its land (tons), Carbon dioxide emissions per year (Tons per person) (2007)\nQ: WHAT PERCENTAGE OF GLOBAL TOTAL EMISSIONS DID INDIA PRODUCE?\nA: SELECT Percentage of global total FROM 1-11251601-2 WHERE Country = 'India'"}
 746 | {"text": "table: 1-11251601-2\ncolumns: Country, Carbon dioxide emissions per year (10 6 Tons) (2006), Percentage of global total, Avg. emission per km 2 of its land (tons), Carbon dioxide emissions per year (Tons per person) (2007)\nQ: HOW MUCH IS THE PERCENTAGE OF GLOBAL TOTAL EMISSIONS IN THE COUNTRY THAT PRODUCED 4.9 TONS PER PERSON IN 2007?\nA: SELECT Percentage of global total FROM 1-11251601-2 WHERE Carbon dioxide emissions per year (Tons per person) (2007) = '4.9'"}
 747 | {"text": "table: 1-11251601-2\ncolumns: Country, Carbon dioxide emissions per year (10 6 Tons) (2006), Percentage of global total, Avg. emission per km 2 of its land (tons), Carbon dioxide emissions per year (Tons per person) (2007)\nQ: WHAT WAS THE AVERAGE EMISSION PER KM 2 IN INDIA?\nA: SELECT MAX Avg. emission per km 2 of its land (tons) FROM 1-11251601-2 WHERE Country = 'India'"}
 748 | {"text": "table: 1-11251109-3\ncolumns: #, Episode, Air Date, Timeslot (EST), Season, Rating, Share, 18\u201349, Viewers (m), Rank (#)\nQ: What is the rank number that aired october 26, 2007?\nA: SELECT Rank (#) FROM 1-11251109-3 WHERE Air Date = 'October 26, 2007'"}
 749 | {"text": "table: 1-11251109-3\ncolumns: #, Episode, Air Date, Timeslot (EST), Season, Rating, Share, 18\u201349, Viewers (m), Rank (#)\nQ: What is the number of rank with the viewership of 5.96 million?\nA: SELECT COUNT Rank (#) FROM 1-11251109-3 WHERE Viewers (m) = '5.96'"}
 750 | {"text": "table: 1-11251109-3\ncolumns: #, Episode, Air Date, Timeslot (EST), Season, Rating, Share, 18\u201349, Viewers (m), Rank (#)\nQ: What is the viewership on november 9, 2007?\nA: SELECT Viewers (m) FROM 1-11251109-3 WHERE Air Date = 'November 9, 2007'"}
 751 | {"text": "table: 1-11254821-2\ncolumns: Finishing position, Points awarded (Platinum), Points awarded (Gold), Points awarded (Silver), Points awarded (Satellite)\nQ: How many platinum points were awarded when 6 gold points were awarded?\nA: SELECT MAX Points awarded (Platinum) FROM 1-11254821-2 WHERE Points awarded (Gold) = 6"}
 752 | {"text": "table: 1-11254821-2\ncolumns: Finishing position, Points awarded (Platinum), Points awarded (Gold), Points awarded (Silver), Points awarded (Satellite)\nQ: What was the range of finishing position for 15 awarded platinum points?\nA: SELECT Finishing position FROM 1-11254821-2 WHERE Points awarded (Platinum) = 15"}
 753 | {"text": "table: 1-11254821-2\ncolumns: Finishing position, Points awarded (Platinum), Points awarded (Gold), Points awarded (Silver), Points awarded (Satellite)\nQ: How many platinum points were awarded for 5th place?\nA: SELECT MAX Points awarded (Platinum) FROM 1-11254821-2 WHERE Finishing position = '5th'"}
 754 | {"text": "table: 1-11254821-2\ncolumns: Finishing position, Points awarded (Platinum), Points awarded (Gold), Points awarded (Silver), Points awarded (Satellite)\nQ: How many platinum points were awarded when 70 silver points were awarded?\nA: SELECT Points awarded (Platinum) FROM 1-11254821-2 WHERE Points awarded (Silver) = 70"}
 755 | {"text": "table: 1-11254821-2\ncolumns: Finishing position, Points awarded (Platinum), Points awarded (Gold), Points awarded (Silver), Points awarded (Satellite)\nQ: How many platinum points were awarded when 9 gold points were awarded?\nA: SELECT Points awarded (Platinum) FROM 1-11254821-2 WHERE Points awarded (Gold) = 9"}
 756 | {"text": "table: 1-11274401-2\ncolumns: No., Episode, Air Date, Timeslot, Rating, Share, 18\u201349 (Rating/Share), Viewers (m), Rank (#)\nQ: How did the episode rank that had 2.65 million viewers?\nA: SELECT Rank (#) FROM 1-11274401-2 WHERE Viewers (m) = '2.65'"}
 757 | {"text": "table: 1-11274401-2\ncolumns: No., Episode, Air Date, Timeslot, Rating, Share, 18\u201349 (Rating/Share), Viewers (m), Rank (#)\nQ: What was the share for the first episode that ranked 85?\nA: SELECT MIN Share FROM 1-11274401-2 WHERE Rank (#) = '85'"}
 758 | {"text": "table: 1-11274401-2\ncolumns: No., Episode, Air Date, Timeslot, Rating, Share, 18\u201349 (Rating/Share), Viewers (m), Rank (#)\nQ: Which timeslot did episode no. 15 hold?\nA: SELECT Timeslot FROM 1-11274401-2 WHERE No. = 15"}
 759 | {"text": "table: 1-11274401-3\ncolumns: No., Episode, Air Date, Timeslot, Rating, Share, 18\u201349 (Rating/Share), Viewers (m), Rank (#)\nQ: What was the timeslot for the episode that aired on May 12, 2009?\nA: SELECT Timeslot FROM 1-11274401-3 WHERE Air Date = 'May 12, 2009'"}
 760 | {"text": "table: 1-11274401-3\ncolumns: No., Episode, Air Date, Timeslot, Rating, Share, 18\u201349 (Rating/Share), Viewers (m), Rank (#)\nQ: What's the 18-49 (rating/share) of the episode that originally aired on May 5, 2009?\nA: SELECT 18\u201349 (Rating/Share) FROM 1-11274401-3 WHERE Air Date = 'May 5, 2009'"}
 761 | {"text": "table: 1-11274401-3\ncolumns: No., Episode, Air Date, Timeslot, Rating, Share, 18\u201349 (Rating/Share), Viewers (m), Rank (#)\nQ: What's the total number of episodes whose original airings were viewed by 1.82 million viewers?\nA: SELECT COUNT Air Date FROM 1-11274401-3 WHERE Viewers (m) = '1.82'"}
 762 | {"text": "table: 1-11274401-3\ncolumns: No., Episode, Air Date, Timeslot, Rating, Share, 18\u201349 (Rating/Share), Viewers (m), Rank (#)\nQ: What's the rating of the episode originally aired on May 5, 2009?\nA: SELECT Rating FROM 1-11274401-3 WHERE Air Date = 'May 5, 2009'"}
 763 | {"text": "table: 1-11274401-3\ncolumns: No., Episode, Air Date, Timeslot, Rating, Share, 18\u201349 (Rating/Share), Viewers (m), Rank (#)\nQ: What episode was seen by 2.05 million viewers?\nA: SELECT Episode FROM 1-11274401-3 WHERE Viewers (m) = '2.05'"}
 764 | {"text": "table: 1-11256021-1\ncolumns: Date, Founder, Extroversion Scales, People-task orientation scale, Introverted, Task-Oriented, Extroverted, Task-Oriented, Extroverted, Relationship-Oriented, Introverted, Relationship Oriented, Moderate\nQ:  what's the\u00a0extroverted, relationship-oriented\u00a0where\u00a0extroverted, task-oriented\u00a0is director\nA: SELECT Extroverted, Relationship-Oriented FROM 1-11256021-1 WHERE Extroverted, Task-Oriented = 'Director'"}
 765 | {"text": "table: 1-11256021-1\ncolumns: Date, Founder, Extroversion Scales, People-task orientation scale, Introverted, Task-Oriented, Extroverted, Task-Oriented, Extroverted, Relationship-Oriented, Introverted, Relationship Oriented, Moderate\nQ:  what's the\u00a0extroverted, relationship-oriented\u00a0where\u00a0moderate\u00a0is introverted sanguine\nA: SELECT Extroverted, Relationship-Oriented FROM 1-11256021-1 WHERE Moderate = 'Introverted Sanguine'"}
 766 | {"text": "table: 1-11256021-1\ncolumns: Date, Founder, Extroversion Scales, People-task orientation scale, Introverted, Task-Oriented, Extroverted, Task-Oriented, Extroverted, Relationship-Oriented, Introverted, Relationship Oriented, Moderate\nQ:  what's the\u00a0founder\u00a0where\u00a0moderate\u00a0is ether\nA: SELECT Founder FROM 1-11256021-1 WHERE Moderate = 'ether'"}
 767 | {"text": "table: 1-11256021-1\ncolumns: Date, Founder, Extroversion Scales, People-task orientation scale, Introverted, Task-Oriented, Extroverted, Task-Oriented, Extroverted, Relationship-Oriented, Introverted, Relationship Oriented, Moderate\nQ:  what's the\u00a0extroverted, relationship-oriented\u00a0where\u00a0date\u00a0is c. 1928\nA: SELECT Extroverted, Relationship-Oriented FROM 1-11256021-1 WHERE Date = 'c. 1928'"}
 768 | {"text": "table: 1-11256021-1\ncolumns: Date, Founder, Extroversion Scales, People-task orientation scale, Introverted, Task-Oriented, Extroverted, Task-Oriented, Extroverted, Relationship-Oriented, Introverted, Relationship Oriented, Moderate\nQ:  who is the\u00a0founder\u00a0where\u00a0date\u00a0is c. 1900\nA: SELECT Founder FROM 1-11256021-1 WHERE Date = 'c. 1900'"}
 769 | {"text": "table: 1-11256021-1\ncolumns: Date, Founder, Extroversion Scales, People-task orientation scale, Introverted, Task-Oriented, Extroverted, Task-Oriented, Extroverted, Relationship-Oriented, Introverted, Relationship Oriented, Moderate\nQ:  what's the\u00a0people-task orientation scale\u00a0where\u00a0extroverted, relationship-oriented\u00a0is team type\nA: SELECT People-task orientation scale FROM 1-11256021-1 WHERE Extroverted, Relationship-Oriented = 'Team Type'"}
 770 | {"text": "table: 1-11303072-5\ncolumns: Wicket, Runs, Batting partners, Batting team, Fielding team, Venue, Season\nQ: What is the batting team where the runs are 276?\nA: SELECT Batting team FROM 1-11303072-5 WHERE Runs = '276'"}
 771 | {"text": "table: 1-11303072-5\ncolumns: Wicket, Runs, Batting partners, Batting team, Fielding team, Venue, Season\nQ: Name the batting team at Durham\nA: SELECT Batting team FROM 1-11303072-5 WHERE Fielding team = 'Durham'"}
 772 | {"text": "table: 1-11303072-5\ncolumns: Wicket, Runs, Batting partners, Batting team, Fielding team, Venue, Season\nQ: What is the batting team with the batting partnets of thilina kandamby and rangana herath?\nA: SELECT Batting team FROM 1-11303072-5 WHERE Batting partners = 'Thilina Kandamby and Rangana Herath'"}
 773 | {"text": "table: 1-11303072-5\ncolumns: Wicket, Runs, Batting partners, Batting team, Fielding team, Venue, Season\nQ: What is the fielding team with 155 runs?\nA: SELECT Fielding team FROM 1-11303072-5 WHERE Runs = '155'"}
 774 | {"text": "table: 1-11303072-5\ncolumns: Wicket, Runs, Batting partners, Batting team, Fielding team, Venue, Season\nQ: What is the batting partners with runs of 226?\nA: SELECT Batting partners FROM 1-11303072-5 WHERE Runs = '226'"}
 775 | {"text": "table: 1-11303072-9\ncolumns: Rank, Dismissals, Player, Nationality, Catches, Stumpings, Career Span\nQ: What is the nationality of David Bairstow?\nA: SELECT Nationality FROM 1-11303072-9 WHERE Player = 'David Bairstow'"}
 776 | {"text": "table: 1-11303072-9\ncolumns: Rank, Dismissals, Player, Nationality, Catches, Stumpings, Career Span\nQ: What are the players whose rank is 2?\nA: SELECT Player FROM 1-11303072-9 WHERE Rank = 2"}
 777 | {"text": "table: 1-11303072-9\ncolumns: Rank, Dismissals, Player, Nationality, Catches, Stumpings, Career Span\nQ: How many stumpings has Paul Nixon in his career?\nA: SELECT Stumpings FROM 1-11303072-9 WHERE Player = 'Paul Nixon'"}
 778 | {"text": "table: 1-11303072-9\ncolumns: Rank, Dismissals, Player, Nationality, Catches, Stumpings, Career Span\nQ: Where is Adam Gilchrist from?\nA: SELECT Nationality FROM 1-11303072-9 WHERE Player = 'Adam Gilchrist'"}
 779 | {"text": "table: 1-1130632-1\ncolumns: No. in series, Title, Directed by, Written by, Featured character(s), Original air date, U.S. viewers (million)\nQ: What are the title that have 19.48 million u.s. viewers?\nA: SELECT Title FROM 1-1130632-1 WHERE U.S. viewers (million) = '19.48'"}
 780 | {"text": "table: 1-1130632-1\ncolumns: No. in series, Title, Directed by, Written by, Featured character(s), Original air date, U.S. viewers (million)\nQ: Which titles have 18.73 u.s. viewers.\nA: SELECT Title FROM 1-1130632-1 WHERE U.S. viewers (million) = '18.73'"}
 781 | {"text": "table: 1-1130632-1\ncolumns: No. in series, Title, Directed by, Written by, Featured character(s), Original air date, U.S. viewers (million)\nQ: Who wrote all the shows with 18.73 u.s. viewers?\nA: SELECT Written by FROM 1-1130632-1 WHERE U.S. viewers (million) = '18.73'"}
 782 | {"text": "table: 1-1131183-2\ncolumns: Rank ( WJC ), Rank (ARDA), Metro area, Number of Jews (WJC), Number of Jews (ASARB)\nQ: What is the rank where the area is Los Angeles?\nA: SELECT Rank ( WJC ) FROM 1-1131183-2 WHERE Metro area = 'Los Angeles'"}
 783 | {"text": "table: 1-1131183-2\ncolumns: Rank ( WJC ), Rank (ARDA), Metro area, Number of Jews (WJC), Number of Jews (ASARB)\nQ: What is the number of jews where the rank is 1?\nA: SELECT COUNT Number of Jews (WJC) FROM 1-1131183-2 WHERE Rank (ARDA) = 1"}
 784 | {"text": "table: 1-1131183-2\ncolumns: Rank ( WJC ), Rank (ARDA), Metro area, Number of Jews (WJC), Number of Jews (ASARB)\nQ: What is the number of jews asarb where the metro area is philadelphia?\nA: SELECT Number of Jews (ASARB) FROM 1-1131183-2 WHERE Metro area = 'Philadelphia'"}
 785 | {"text": "table: 1-11318462-5\ncolumns: Crew, Open 1st VIII, Open 2nd VIII, Open 3rd VIII, U16 1st VIII, U16 2nd VIII, U16 3rd VIII, U15 1st IV, U15 2nd IV, U15 3rd IV, U15 4th IV, U15 5th IV, U15 6th IV\nQ: what are all the open 1st viii with u15 6th iv being bgs\nA: SELECT Open 1st VIII FROM 1-11318462-5 WHERE U15 6th IV = 'BGS'"}
 786 | {"text": "table: 1-11318462-5\ncolumns: Crew, Open 1st VIII, Open 2nd VIII, Open 3rd VIII, U16 1st VIII, U16 2nd VIII, U16 3rd VIII, U15 1st IV, U15 2nd IV, U15 3rd IV, U15 4th IV, U15 5th IV, U15 6th IV\nQ: what are all the u16 2nd viii with u15 3rd iv being bbc\nA: SELECT U16 2nd VIII FROM 1-11318462-5 WHERE U15 3rd IV = 'BBC'"}
 787 | {"text": "table: 1-11318462-5\ncolumns: Crew, Open 1st VIII, Open 2nd VIII, Open 3rd VIII, U16 1st VIII, U16 2nd VIII, U16 3rd VIII, U15 1st IV, U15 2nd IV, U15 3rd IV, U15 4th IV, U15 5th IV, U15 6th IV\nQ: what are all the open 1st viii with u15 4th iv being gt\nA: SELECT Open 1st VIII FROM 1-11318462-5 WHERE U15 4th IV = 'GT'"}
 788 | {"text": "table: 1-11318462-5\ncolumns: Crew, Open 1st VIII, Open 2nd VIII, Open 3rd VIII, U16 1st VIII, U16 2nd VIII, U16 3rd VIII, U15 1st IV, U15 2nd IV, U15 3rd IV, U15 4th IV, U15 5th IV, U15 6th IV\nQ: how many crew had u15 3rd iv being bgs and u15 1st iv being acgs and open 1st viii being acgs\nA: SELECT COUNT Crew FROM 1-11318462-5 WHERE U15 3rd IV = 'BGS' AND U15 1st IV = 'ACGS' AND Open 1st VIII = 'ACGS'"}
 789 | {"text": "table: 1-11318462-5\ncolumns: Crew, Open 1st VIII, Open 2nd VIII, Open 3rd VIII, U16 1st VIII, U16 2nd VIII, U16 3rd VIII, U15 1st IV, U15 2nd IV, U15 3rd IV, U15 4th IV, U15 5th IV, U15 6th IV\nQ: what are all the u15 3rd iv with u15 4th iv being bbc\nA: SELECT U15 3rd IV FROM 1-11318462-5 WHERE U15 4th IV = 'BBC'"}
 790 | {"text": "table: 1-11318462-5\ncolumns: Crew, Open 1st VIII, Open 2nd VIII, Open 3rd VIII, U16 1st VIII, U16 2nd VIII, U16 3rd VIII, U15 1st IV, U15 2nd IV, U15 3rd IV, U15 4th IV, U15 5th IV, U15 6th IV\nQ: how many open 2nd viii had u15 3rd iv being gt\nA: SELECT COUNT Open 2nd VIII FROM 1-11318462-5 WHERE U15 3rd IV = 'GT'"}
 791 | {"text": "table: 1-11318462-29\ncolumns: School, Location, Enrolment, Founded, Denomination, Day/Boarding, School Colours, Abbreviation, In competition since\nQ: How many schools have an enrollment of 850?\nA: SELECT COUNT Founded FROM 1-11318462-29 WHERE Enrolment = 850"}
 792 | {"text": "table: 1-11318462-29\ncolumns: School, Location, Enrolment, Founded, Denomination, Day/Boarding, School Colours, Abbreviation, In competition since\nQ: What is the location of the school named Brisbane Girls' Grammar School?\nA: SELECT Location FROM 1-11318462-29 WHERE School = 'Brisbane Girls' Grammar School'"}
 793 | {"text": "table: 1-11318462-29\ncolumns: School, Location, Enrolment, Founded, Denomination, Day/Boarding, School Colours, Abbreviation, In competition since\nQ: How many schools are located in South Brisbane?\nA: SELECT COUNT School FROM 1-11318462-29 WHERE Location = 'South Brisbane'"}
 794 | {"text": "table: 1-11318462-29\ncolumns: School, Location, Enrolment, Founded, Denomination, Day/Boarding, School Colours, Abbreviation, In competition since\nQ: When was SPLC founded?\nA: SELECT MIN Founded FROM 1-11318462-29 WHERE Abbreviation = 'SPLC'"}
 795 | {"text": "table: 1-11318462-29\ncolumns: School, Location, Enrolment, Founded, Denomination, Day/Boarding, School Colours, Abbreviation, In competition since\nQ: What is the enrollment of STM which has been in competition since 1990?\nA: SELECT COUNT Enrolment FROM 1-11318462-29 WHERE In competition since = 1990 AND Abbreviation = 'STM'"}
 796 | {"text": "table: 1-1132568-3\ncolumns: Rd., Grand Prix, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: What number is the Monaco Grand Prix?\nA: SELECT Rd. FROM 1-1132568-3 WHERE Grand Prix = 'Monaco Grand Prix'"}
 797 | {"text": "table: 1-1132568-3\ncolumns: Rd., Grand Prix, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: Who is in the pole position for the French Grand Prix?\nA: SELECT Pole Position FROM 1-1132568-3 WHERE Grand Prix = 'French Grand Prix'"}
 798 | {"text": "table: 1-1132568-3\ncolumns: Rd., Grand Prix, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: What are the numbers for the raceways that are constructed by Ferrari, with Michael Schumacher holding the fastest lap and pole position?\nA: SELECT Rd. FROM 1-1132568-3 WHERE Fastest Lap = 'Michael Schumacher' AND Constructor = 'Ferrari' AND Pole Position = 'Michael Schumacher'"}
 799 | {"text": "table: 1-1132568-3\ncolumns: Rd., Grand Prix, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: How many on the list are called the Austrian Grand Prix?\nA: SELECT COUNT Rd. FROM 1-1132568-3 WHERE Grand Prix = 'Austrian Grand Prix'"}
 800 | {"text": "table: 1-1132568-3\ncolumns: Rd., Grand Prix, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: What number is the Canadian Grand Prix on the list?\nA: SELECT Rd. FROM 1-1132568-3 WHERE Grand Prix = 'Canadian Grand Prix'"}
 801 | {"text": "table: 1-1132588-3\ncolumns: Rd., Grand Prix, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: What is the rd for the canadian grand prix?\nA: SELECT Rd. FROM 1-1132588-3 WHERE Grand Prix = 'Canadian Grand Prix'"}
 802 | {"text": "table: 1-1132588-3\ncolumns: Rd., Grand Prix, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: What is the fastest lap for the european grand prix?\nA: SELECT Fastest Lap FROM 1-1132588-3 WHERE Grand Prix = 'European Grand Prix'"}
 803 | {"text": "table: 1-1132588-3\ncolumns: Rd., Grand Prix, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: What is the pole position for the ferrari at the austrian grand prix?\nA: SELECT Pole Position FROM 1-1132588-3 WHERE Constructor = 'Ferrari' AND Grand Prix = 'Austrian Grand Prix'"}
 804 | {"text": "table: 1-11326124-3\ncolumns: Edition, Zone, Round, Date, Against, Surface, Opponent, Outcome, Result\nQ: What was the result of round 2r?\nA: SELECT Outcome FROM 1-11326124-3 WHERE Round = '2R'"}
 805 | {"text": "table: 1-11326124-3\ncolumns: Edition, Zone, Round, Date, Against, Surface, Opponent, Outcome, Result\nQ: Who did Tina Pisnik verse?\nA: SELECT Against FROM 1-11326124-3 WHERE Opponent = 'Tina Pisnik'"}
 806 | {"text": "table: 1-11326124-3\ncolumns: Edition, Zone, Round, Date, Against, Surface, Opponent, Outcome, Result\nQ: How many rounds were 2r?\nA: SELECT COUNT Result FROM 1-11326124-3 WHERE Round = '2R'"}
 807 | {"text": "table: 1-11326124-3\ncolumns: Edition, Zone, Round, Date, Against, Surface, Opponent, Outcome, Result\nQ: Name the outcome for round 2r\nA: SELECT Outcome FROM 1-11326124-3 WHERE Round = '2R'"}
 808 | {"text": "table: 1-11354111-3\ncolumns: #, Episode, Air Date, Rating, Share, Rating/Share 18\u201349, Viewers (m), Timeslot Rank, Night Rank, Overall Rank\nQ: what's the night rank with viewers (m) of 6.63\nA: SELECT Night Rank FROM 1-11354111-3 WHERE Viewers (m) = '6.63'"}
 809 | {"text": "table: 1-11354111-3\ncolumns: #, Episode, Air Date, Rating, Share, Rating/Share 18\u201349, Viewers (m), Timeslot Rank, Night Rank, Overall Rank\nQ: what's the overall rank with viewers (m) of 7.44\nA: SELECT Overall Rank FROM 1-11354111-3 WHERE Viewers (m) = '7.44'"}
 810 | {"text": "table: 1-11354111-3\ncolumns: #, Episode, Air Date, Rating, Share, Rating/Share 18\u201349, Viewers (m), Timeslot Rank, Night Rank, Overall Rank\nQ: what's the overall rank with rating/share 18\u201349 of 2.1/5\nA: SELECT COUNT Overall Rank FROM 1-11354111-3 WHERE Rating/Share 18\u201349 = '2.1/5'"}
 811 | {"text": "table: 1-11354111-3\ncolumns: #, Episode, Air Date, Rating, Share, Rating/Share 18\u201349, Viewers (m), Timeslot Rank, Night Rank, Overall Rank\nQ: what's the night rank with rating of 6.2\nA: SELECT Night Rank FROM 1-11354111-3 WHERE Rating = '6.2'"}
 812 | {"text": "table: 1-11354111-3\ncolumns: #, Episode, Air Date, Rating, Share, Rating/Share 18\u201349, Viewers (m), Timeslot Rank, Night Rank, Overall Rank\nQ: what's the viewers (m) with episode of \"legacy\"\nA: SELECT Viewers (m) FROM 1-11354111-3 WHERE Episode = '\"Legacy\"'"}
 813 | {"text": "table: 1-1137142-1\ncolumns: Season, Group A Winner, Group B Winner, Group C Winner, Group D Winner\nQ: What is the number of group b winner for francavilla?\nA: SELECT COUNT Group B Winner FROM 1-1137142-1 WHERE Group C Winner = 'Francavilla'"}
 814 | {"text": "table: 1-1137142-1\ncolumns: Season, Group A Winner, Group B Winner, Group C Winner, Group D Winner\nQ: What is the group a winner for modena?\nA: SELECT Group A Winner FROM 1-1137142-1 WHERE Group B Winner = 'Modena'"}
 815 | {"text": "table: 1-1137142-1\ncolumns: Season, Group A Winner, Group B Winner, Group C Winner, Group D Winner\nQ: What is the group a winner for vis pesaro?\nA: SELECT Group A Winner FROM 1-1137142-1 WHERE Group C Winner = 'Vis Pesaro'"}
 816 | {"text": "table: 1-1137142-1\ncolumns: Season, Group A Winner, Group B Winner, Group C Winner, Group D Winner\nQ: What group a winner was for nocerina?\nA: SELECT Group A Winner FROM 1-1137142-1 WHERE Group D Winner = 'Nocerina'"}
 817 | {"text": "table: 1-1137142-1\ncolumns: Season, Group A Winner, Group B Winner, Group C Winner, Group D Winner\nQ: What was the group d winner for modena?\nA: SELECT Group D Winner FROM 1-1137142-1 WHERE Group B Winner = 'Modena'"}
 818 | {"text": "table: 1-1137695-3\ncolumns: Round, Grand Prix, Pole Position, Fastest Lap, Winning Driver, Winning Constructor, Report\nQ: Who had the fastest lap at the brazilian grand prix?\nA: SELECT Fastest Lap FROM 1-1137695-3 WHERE Grand Prix = 'Brazilian Grand Prix'"}
 819 | {"text": "table: 1-1137695-3\ncolumns: Round, Grand Prix, Pole Position, Fastest Lap, Winning Driver, Winning Constructor, Report\nQ: Who was on the pole position at the monaco grand prix?\nA: SELECT Pole Position FROM 1-1137695-3 WHERE Grand Prix = 'Monaco Grand Prix'"}
 820 | {"text": "table: 1-1137695-3\ncolumns: Round, Grand Prix, Pole Position, Fastest Lap, Winning Driver, Winning Constructor, Report\nQ: Who was the winning driver when Michael Schumacher had the pole and the fastest lap?\nA: SELECT Winning Driver FROM 1-1137695-3 WHERE Fastest Lap = 'Michael Schumacher' AND Pole Position = 'Michael Schumacher'"}
 821 | {"text": "table: 1-1137704-2\ncolumns: Round, Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Winning Constructor, Report\nQ: what are all the\u00a0location\u00a0where\u00a0date\u00a0is 5 april\nA: SELECT Location FROM 1-1137704-2 WHERE Date = '5 April'"}
 822 | {"text": "table: 1-1137704-2\ncolumns: Round, Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Winning Constructor, Report\nQ: what are all the\u00a0pole position\u00a0where\u00a0date\u00a0is 26 july\nA: SELECT Pole Position FROM 1-1137704-2 WHERE Date = '26 July'"}
 823 | {"text": "table: 1-1137704-2\ncolumns: Round, Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Winning Constructor, Report\nQ: who are all the\u00a0winning constructors\u00a0where\u00a0fastest lap\u00a0is riccardo patrese and\u00a0location\u00a0is interlagos\nA: SELECT Winning Constructor FROM 1-1137704-2 WHERE Fastest Lap = 'Riccardo Patrese' AND Location = 'Interlagos'"}
 824 | {"text": "table: 1-1137704-2\ncolumns: Round, Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Winning Constructor, Report\nQ: what are all the\u00a0report\u00a0where\u00a0winning constructor\u00a0is williams - renault and\u00a0grand prix\u00a0is south african grand prix\nA: SELECT Report FROM 1-1137704-2 WHERE Winning Constructor = 'Williams - Renault' AND Grand Prix = 'South African Grand Prix'"}
 825 | {"text": "table: 1-1137704-2\ncolumns: Round, Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Winning Constructor, Report\nQ: whatthe minimum\u00a0round\u00a0where\u00a0grand prix\u00a0is german grand prix\nA: SELECT MIN Round FROM 1-1137704-2 WHERE Grand Prix = 'German Grand Prix'"}
 826 | {"text": "table: 1-1137704-2\ncolumns: Round, Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Winning Constructor, Report\nQ: what of the total number of\u00a0date\u00a0where\u00a0grand prix\u00a0is portuguese grand prix\nA: SELECT COUNT Date FROM 1-1137704-2 WHERE Grand Prix = 'Portuguese Grand Prix'"}
 827 | {"text": "table: 1-1137707-2\ncolumns: Round, Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Winning Constructor, Report\nQ: What is the number of pole position with a round of 15?\nA: SELECT COUNT Pole Position FROM 1-1137707-2 WHERE Round = 15"}
 828 | {"text": "table: 1-1137707-2\ncolumns: Round, Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Winning Constructor, Report\nQ: What is the date of the circuit gilles villeneuve?\nA: SELECT Date FROM 1-1137707-2 WHERE Location = 'Circuit Gilles Villeneuve'"}
 829 | {"text": "table: 1-1137707-2\ncolumns: Round, Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Winning Constructor, Report\nQ: What is the location of thierry boutsen?\nA: SELECT Location FROM 1-1137707-2 WHERE Fastest Lap = 'Thierry Boutsen'"}
 830 | {"text": "table: 1-1137718-2\ncolumns: Rd., Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: Who had the pole position at the German Grand Prix?\nA: SELECT Pole Position FROM 1-1137718-2 WHERE Grand Prix = 'German Grand Prix'"}
 831 | {"text": "table: 1-1137718-2\ncolumns: Rd., Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: Which rd. occurred on 22 October?\nA: SELECT MIN Rd. FROM 1-1137718-2 WHERE Date = '22 October'"}
 832 | {"text": "table: 1-1137718-2\ncolumns: Rd., Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: Who was the winning driver on 13 August?\nA: SELECT Winning Driver FROM 1-1137718-2 WHERE Date = '13 August'"}
 833 | {"text": "table: 1-1137718-2\ncolumns: Rd., Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: What was the fastest lap at the Mexican Grand Prix?\nA: SELECT Fastest Lap FROM 1-1137718-2 WHERE Grand Prix = 'Mexican Grand Prix'"}
 834 | {"text": "table: 1-1137718-2\ncolumns: Rd., Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: Which rd. took place at Hockenheimring?\nA: SELECT MIN Rd. FROM 1-1137718-2 WHERE Location = 'Hockenheimring'"}
 835 | {"text": "table: 1-1137718-2\ncolumns: Rd., Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: How many drivers had the fastest lap at Silverstone?\nA: SELECT COUNT Fastest Lap FROM 1-1137718-2 WHERE Location = 'Silverstone'"}
 836 | {"text": "table: 1-11381701-3\ncolumns: Source, Date, Method, iOS, Android, BlackBerry, Symbian / Series 40, Bada, Windows, Other\nQ: What is the percentage of Android use when Windows is 1.15%?\nA: SELECT Android FROM 1-11381701-3 WHERE Windows = '1.15%'"}
 837 | {"text": "table: 1-11381701-3\ncolumns: Source, Date, Method, iOS, Android, BlackBerry, Symbian / Series 40, Bada, Windows, Other\nQ: On which dates was the value of Bada 0.05%?\nA: SELECT Date FROM 1-11381701-3 WHERE Bada = '0.05%'"}
 838 | {"text": "table: 1-11381701-3\ncolumns: Source, Date, Method, iOS, Android, BlackBerry, Symbian / Series 40, Bada, Windows, Other\nQ: When the value of \"other\" is 0.7%, what is the percentage for Windows?\nA: SELECT Windows FROM 1-11381701-3 WHERE Other = '0.7%'"}
 839 | {"text": "table: 1-11381701-3\ncolumns: Source, Date, Method, iOS, Android, BlackBerry, Symbian / Series 40, Bada, Windows, Other\nQ: When Symbian/Series 40 is 0.40%, what is the percentage of \"other\"?\nA: SELECT Other FROM 1-11381701-3 WHERE Symbian / Series 40 = '0.40%'"}
 840 | {"text": "table: 1-11381701-3\ncolumns: Source, Date, Method, iOS, Android, BlackBerry, Symbian / Series 40, Bada, Windows, Other\nQ: Which source shows Blackberry at 2.9%?\nA: SELECT Source FROM 1-11381701-3 WHERE BlackBerry = '2.9%'"}
 841 | {"text": "table: 1-11390711-4\ncolumns: English Name, Japanese orthography, Pronouciation, abbreviation, Provider(IAI), Foundation\nQ: Which colleges have the english abbreviation MTC?\nA: SELECT English Name FROM 1-11390711-4 WHERE abbreviation = 'MTC'"}
 842 | {"text": "table: 1-11390711-4\ncolumns: English Name, Japanese orthography, Pronouciation, abbreviation, Provider(IAI), Foundation\nQ: What is the Japanese orthography for the English name National Farmers Academy?\nA: SELECT Japanese orthography FROM 1-11390711-4 WHERE English Name = 'National Farmers Academy'"}
 843 | {"text": "table: 1-11390711-4\ncolumns: English Name, Japanese orthography, Pronouciation, abbreviation, Provider(IAI), Foundation\nQ: What is the abbreviation for the college pronounced \"k\u014dk\u016b daigakk\u014d\"?\nA: SELECT abbreviation FROM 1-11390711-4 WHERE Pronouciation = 'K\u014dk\u016b Daigakk\u014d'"}
 844 | {"text": "table: 1-11390711-4\ncolumns: English Name, Japanese orthography, Pronouciation, abbreviation, Provider(IAI), Foundation\nQ: How many providers were founded in 1964?\nA: SELECT COUNT Provider(IAI) FROM 1-11390711-4 WHERE Foundation = 1964"}
 845 | {"text": "table: 1-11390711-4\ncolumns: English Name, Japanese orthography, Pronouciation, abbreviation, Provider(IAI), Foundation\nQ: What is the Japanese orthography for National Fisheries University?\nA: SELECT Japanese orthography FROM 1-11390711-4 WHERE English Name = 'National Fisheries University'"}
 846 | {"text": "table: 1-11391954-3\ncolumns: Country, Total, Marathon (mens), Marathon (womens), Half Marathon (mens), Half Marathon (womens)\nQ: What is the minimum number for the half marathon (womens)?\nA: SELECT MIN Half Marathon (womens) FROM 1-11391954-3"}
 847 | {"text": "table: 1-11391954-3\ncolumns: Country, Total, Marathon (mens), Marathon (womens), Half Marathon (mens), Half Marathon (womens)\nQ: Whatis the total number of half marathon (mens) that represented kazakhstan?\nA: SELECT COUNT Half Marathon (mens) FROM 1-11391954-3 WHERE Country = 'Kazakhstan'"}
 848 | {"text": "table: 1-11391954-3\ncolumns: Country, Total, Marathon (mens), Marathon (womens), Half Marathon (mens), Half Marathon (womens)\nQ: What is amount of countries where half marathon (women) is larger than 1.0?\nA: SELECT COUNT Country FROM 1-11391954-3 WHERE Half Marathon (womens) > 1.0"}
 849 | {"text": "table: 1-11391954-3\ncolumns: Country, Total, Marathon (mens), Marathon (womens), Half Marathon (mens), Half Marathon (womens)\nQ: How many times is Moldova the winner of half marathon (womens)?\nA: SELECT COUNT Half Marathon (womens) FROM 1-11391954-3 WHERE Country = 'Moldova'"}
 850 | {"text": "table: 1-11391954-3\ncolumns: Country, Total, Marathon (mens), Marathon (womens), Half Marathon (mens), Half Marathon (womens)\nQ: Which country has half marathon (womens) that is larger than 1.0?\nA: SELECT Country FROM 1-11391954-3 WHERE Half Marathon (womens) > 1.0"}
 851 | {"text": "table: 1-1139087-2\ncolumns: Rd., Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: What is the make of the car that won the brazilian grand prix?\nA: SELECT Constructor FROM 1-1139087-2 WHERE Grand Prix = 'Brazilian Grand Prix'"}
 852 | {"text": "table: 1-1139087-2\ncolumns: Rd., Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: Who drove the fastest lap for round 8?\nA: SELECT Fastest Lap FROM 1-1139087-2 WHERE Rd. = 8"}
 853 | {"text": "table: 1-1139087-2\ncolumns: Rd., Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: What day was the grand prix in jerez?\nA: SELECT Date FROM 1-1139087-2 WHERE Location = 'Jerez'"}
 854 | {"text": "table: 1-1139087-2\ncolumns: Rd., Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: What event was in detroit?\nA: SELECT Grand Prix FROM 1-1139087-2 WHERE Location = 'Detroit'"}
 855 | {"text": "table: 1-1139087-2\ncolumns: Rd., Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: How many events did nigel mansell drive the fastest and a mclaren - honda win?\nA: SELECT COUNT Grand Prix FROM 1-1139087-2 WHERE Constructor = 'McLaren - Honda' AND Fastest Lap = 'Nigel Mansell'"}
 856 | {"text": "table: 1-1139087-2\ncolumns: Rd., Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: What day is the french grand prix\nA: SELECT Date FROM 1-1139087-2 WHERE Grand Prix = 'French Grand Prix'"}
 857 | {"text": "table: 1-1139835-3\ncolumns: Year, Winners, Grand Finalist, Scores, Venue, Crowd, Margin, Season Result\nQ:  who is the\u00a0winners\u00a0where\u00a0season result\u00a0is 7th\nA: SELECT Winners FROM 1-1139835-3 WHERE Season Result = '7th'"}
 858 | {"text": "table: 1-1139835-3\ncolumns: Year, Winners, Grand Finalist, Scores, Venue, Crowd, Margin, Season Result\nQ:  who is the\u00a0winners\u00a0where\u00a0season result\u00a0is 9th\nA: SELECT Winners FROM 1-1139835-3 WHERE Season Result = '9th'"}
 859 | {"text": "table: 1-1139835-3\ncolumns: Year, Winners, Grand Finalist, Scores, Venue, Crowd, Margin, Season Result\nQ:  what's the\u00a0grand finalist\u00a0where\u00a0winners\u00a0is collingwood\nA: SELECT Grand Finalist FROM 1-1139835-3 WHERE Winners = 'Collingwood'"}
 860 | {"text": "table: 1-1139835-3\ncolumns: Year, Winners, Grand Finalist, Scores, Venue, Crowd, Margin, Season Result\nQ:  who is the\u00a0season result\u00a0where\u00a0margin\u00a0is 51\nA: SELECT Season Result FROM 1-1139835-3 WHERE Margin = 51"}
 861 | {"text": "table: 1-1139835-3\ncolumns: Year, Winners, Grand Finalist, Scores, Venue, Crowd, Margin, Season Result\nQ:  who is the\u00a0grand finalist\u00a0where\u00a0scores\u00a0is 11.11 (77) \u2013 10.8 (68)\nA: SELECT Grand Finalist FROM 1-1139835-3 WHERE Scores = '11.11 (77) \u2013 10.8 (68)'"}
 862 | {"text": "table: 1-1139835-3\ncolumns: Year, Winners, Grand Finalist, Scores, Venue, Crowd, Margin, Season Result\nQ:  who is the\u00a0grand finalist\u00a0where\u00a0scores\u00a0is 8.9 (57) \u2013 7.12 (54)\nA: SELECT Grand Finalist FROM 1-1139835-3 WHERE Scores = '8.9 (57) \u2013 7.12 (54)'"}
 863 | {"text": "table: 1-1139835-1\ncolumns: Year, Winners, Grand Finalist, Scores, Venue, Crowd, Margin, Season Result\nQ: what was the crowd when the scores are 10.12 (72) \u2013 8.11 (59)?\nA: SELECT MAX Crowd FROM 1-1139835-1 WHERE Scores = '10.12 (72) \u2013 8.11 (59)'"}
 864 | {"text": "table: 1-1139835-1\ncolumns: Year, Winners, Grand Finalist, Scores, Venue, Crowd, Margin, Season Result\nQ: what is the venue where the scores are 15.13 (103) \u2013 8.4 (52)?\nA: SELECT Venue FROM 1-1139835-1 WHERE Scores = '15.13 (103) \u2013 8.4 (52)'"}
 865 | {"text": "table: 1-1139835-1\ncolumns: Year, Winners, Grand Finalist, Scores, Venue, Crowd, Margin, Season Result\nQ: what is the venue where the margin is 4?\nA: SELECT Venue FROM 1-1139835-1 WHERE Margin = 4"}
 866 | {"text": "table: 1-1139835-1\ncolumns: Year, Winners, Grand Finalist, Scores, Venue, Crowd, Margin, Season Result\nQ: what is the crowd when the grand finalist was south melbourne?\nA: SELECT Crowd FROM 1-1139835-1 WHERE Grand Finalist = 'South Melbourne'"}
 867 | {"text": "table: 1-1140067-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: What was the date for monaco grand prix?\nA: SELECT Date FROM 1-1140067-2 WHERE Race = 'Monaco Grand Prix'"}
 868 | {"text": "table: 1-1140067-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: What was the date for the pole position of alain prost?\nA: SELECT Date FROM 1-1140067-2 WHERE Pole Position = 'Alain Prost'"}
 869 | {"text": "table: 1-1140067-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: What is the race winer of the portuguese grand prix?\nA: SELECT Race Winner FROM 1-1140067-2 WHERE Race = 'Portuguese Grand Prix'"}
 870 | {"text": "table: 1-1140074-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: what's the\u00a0race winner\u00a0with\u00a0date\u00a0being 12 june\nA: SELECT Race Winner FROM 1-1140074-2 WHERE Date = '12 June'"}
 871 | {"text": "table: 1-1140074-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: what's the\u00a0constructor\u00a0with\u00a0location\u00a0being hockenheimring\nA: SELECT Constructor FROM 1-1140074-2 WHERE Location = 'Hockenheimring'"}
 872 | {"text": "table: 1-1140074-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: what's the\u00a0race winner\u00a0with\u00a0location\u00a0being jacarepagu\u00e1\nA: SELECT Race Winner FROM 1-1140074-2 WHERE Location = 'Jacarepagu\u00e1'"}
 873 | {"text": "table: 1-1140074-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: what's the total number of\u00a0race winner\u00a0with\u00a0rnd\u00a0being 10\nA: SELECT COUNT Race Winner FROM 1-1140074-2 WHERE Rnd = 10"}
 874 | {"text": "table: 1-1140074-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: what's the\u00a0pole position\u00a0with\u00a0location\u00a0being hockenheimring\nA: SELECT Pole Position FROM 1-1140074-2 WHERE Location = 'Hockenheimring'"}
 875 | {"text": "table: 1-1140074-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: what's the\u00a0report\u00a0with\u00a0rnd\u00a0being 4\nA: SELECT Report FROM 1-1140074-2 WHERE Rnd = 4"}
 876 | {"text": "table: 1-1139835-9\ncolumns: Season, Premier, Runner Up, Score, Venue, Attendance, Premiership\nQ: What venue has an attendance of 30824 at Essendon in 1984?\nA: SELECT Venue FROM 1-1139835-9 WHERE Premier = 'Essendon' AND Attendance = 30824"}
 877 | {"text": "table: 1-1139835-9\ncolumns: Season, Premier, Runner Up, Score, Venue, Attendance, Premiership\nQ: What other venue was a runner up to Hawthorn?\nA: SELECT Venue FROM 1-1139835-9 WHERE Runner Up = 'Hawthorn'"}
 878 | {"text": "table: 1-1139835-9\ncolumns: Season, Premier, Runner Up, Score, Venue, Attendance, Premiership\nQ: What is the other premiership when the runner up wis Geelong?\nA: SELECT Premiership FROM 1-1139835-9 WHERE Runner Up = 'Geelong'"}
 879 | {"text": "table: 1-1139835-9\ncolumns: Season, Premier, Runner Up, Score, Venue, Attendance, Premiership\nQ: Who are all the runner ups when the score is 9.12 (66) \u2013 5.6 (36)?\nA: SELECT Runner Up FROM 1-1139835-9 WHERE Score = '9.12 (66) \u2013 5.6 (36)'"}
 880 | {"text": "table: 1-1140073-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: Who had the fastest lap in the race where Patrick Tambay was on the pole?\nA: SELECT Fastest Lap FROM 1-1140073-2 WHERE Pole Position = 'Patrick Tambay'"}
 881 | {"text": "table: 1-1140073-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: What race had Nelson Piquet on the pole and was in N\u00fcrburgring?\nA: SELECT Race FROM 1-1140073-2 WHERE Pole Position = 'Nelson Piquet' AND Location = 'N\u00fcrburgring'"}
 882 | {"text": "table: 1-1140073-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: How many rounds did Patrick Tambay record the fastest lap?\nA: SELECT COUNT Rnd FROM 1-1140073-2 WHERE Fastest Lap = 'Patrick Tambay'"}
 883 | {"text": "table: 1-1140073-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: Which race is located in kyalami?\nA: SELECT Race FROM 1-1140073-2 WHERE Location = 'Kyalami'"}
 884 | {"text": "table: 1-1140077-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: What is the fastest lap with pole position of gilles villeneuve?\nA: SELECT Fastest Lap FROM 1-1140077-2 WHERE Pole Position = 'Gilles Villeneuve'"}
 885 | {"text": "table: 1-1140077-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: Who did the fastest lap in the dutch grand prix?\nA: SELECT Fastest Lap FROM 1-1140077-2 WHERE Race = 'Dutch Grand Prix'"}
 886 | {"text": "table: 1-1140077-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: Who did the fastest lap with the race winner john watson?\nA: SELECT Fastest Lap FROM 1-1140077-2 WHERE Race Winner = 'John Watson'"}
 887 | {"text": "table: 1-1140076-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: What is the constructor for 9 May?\nA: SELECT Constructor FROM 1-1140076-2 WHERE Date = '9 May'"}
 888 | {"text": "table: 1-1140076-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: What is the pole position for the race with the fastest lap by Nelson Piquet and the constructor is Ferrari?\nA: SELECT Pole Position FROM 1-1140076-2 WHERE Fastest Lap = 'Nelson Piquet' AND Constructor = 'Ferrari'"}
 889 | {"text": "table: 1-1140076-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: What is the report listed for the race in San Marino Grand Prix?\nA: SELECT Report FROM 1-1140076-2 WHERE Race = 'San Marino Grand Prix'"}
 890 | {"text": "table: 1-1140076-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: Who was the constructor in the location Monza?\nA: SELECT Constructor FROM 1-1140076-2 WHERE Location = 'Monza'"}
 891 | {"text": "table: 1-1140076-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: How many races had the pole position Alain Prost and the race winner Keke Rosberg?\nA: SELECT COUNT Race FROM 1-1140076-2 WHERE Pole Position = 'Alain Prost' AND Race Winner = 'Keke Rosberg'"}
 892 | {"text": "table: 1-1140080-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: what's the\u00a0report\u00a0with\u00a0location\u00a0 \u00f6sterreichring\nA: SELECT Report FROM 1-1140080-2 WHERE Location = '\u00d6sterreichring'"}
 893 | {"text": "table: 1-1140080-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: what's the\u00a0report\u00a0with\u00a0race\u00a0argentine grand prix\nA: SELECT Report FROM 1-1140080-2 WHERE Race = 'Argentine Grand Prix'"}
 894 | {"text": "table: 1-1140080-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: what's the minimum\u00a0rnd\u00a0with\u00a0race\u00a0 italian grand prix\nA: SELECT MIN Rnd FROM 1-1140080-2 WHERE Race = 'Italian Grand Prix'"}
 895 | {"text": "table: 1-1140080-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: what's the total number of\u00a0report\u00a0with\u00a0date\u00a0 29 april\nA: SELECT COUNT Report FROM 1-1140080-2 WHERE Date = '29 April'"}
 896 | {"text": "table: 1-1140080-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: what's the\u00a0race winner\u00a0with\u00a0constructor\u00a0 renault\nA: SELECT Race Winner FROM 1-1140080-2 WHERE Constructor = 'Renault'"}
 897 | {"text": "table: 1-1140080-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: what's the\u00a0date\u00a0with\u00a0rnd\u00a0 1\nA: SELECT Date FROM 1-1140080-2 WHERE Rnd = 1"}
 898 | {"text": "table: 1-1140083-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: How many days is the Monaco Grand Prix?\nA: SELECT COUNT Date FROM 1-1140083-2 WHERE Race = 'Monaco Grand Prix'"}
 899 | {"text": "table: 1-1140083-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: How many rounds were won with James Hunt as pole position and John Watson as  fastest lap?\nA: SELECT COUNT Rnd FROM 1-1140083-2 WHERE Pole Position = 'James Hunt' AND Fastest Lap = 'John Watson'"}
 900 | {"text": "table: 1-1140083-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: The Dijon-prenois had how many fastest laps?\nA: SELECT COUNT Fastest Lap FROM 1-1140083-2 WHERE Location = 'Dijon-Prenois'"}
 901 | {"text": "table: 1-1140083-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: What was the constructor for round 15?\nA: SELECT Constructor FROM 1-1140083-2 WHERE Rnd = 15"}
 902 | {"text": "table: 1-1140088-6\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: Who won the Brands Hatch circuit?\nA: SELECT Winning driver FROM 1-1140088-6 WHERE Circuit = 'Brands Hatch'"}
 903 | {"text": "table: 1-1140088-6\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: Who constructed the I Italian Republic Grand Prix?\nA: SELECT Constructor FROM 1-1140088-6 WHERE Race Name = 'I Italian Republic Grand Prix'"}
 904 | {"text": "table: 1-1140088-6\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What race was held at Oulton Park?\nA: SELECT Race Name FROM 1-1140088-6 WHERE Circuit = 'Oulton Park'"}
 905 | {"text": "table: 1-1140088-6\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: Did the I Brazilian Grand Prix have a report?\nA: SELECT Report FROM 1-1140088-6 WHERE Race Name = 'I Brazilian Grand Prix'"}
 906 | {"text": "table: 1-1140085-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: what is the race where the pole position is niki lauda and the date is 27 april?\nA: SELECT Race FROM 1-1140085-2 WHERE Pole Position = 'Niki Lauda' AND Date = '27 April'"}
 907 | {"text": "table: 1-1140085-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: what is the date where the constructor is ferrari and the location is anderstorp?\nA: SELECT Date FROM 1-1140085-2 WHERE Constructor = 'Ferrari' AND Location = 'Anderstorp'"}
 908 | {"text": "table: 1-1140085-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: how many times is the pole position niki lauda and the race is monaco grand prix?\nA: SELECT COUNT Rnd FROM 1-1140085-2 WHERE Pole Position = 'Niki Lauda' AND Race = 'Monaco Grand Prix'"}
 909 | {"text": "table: 1-1140085-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: what is the report where the location is kyalami?\nA: SELECT Report FROM 1-1140085-2 WHERE Location = 'Kyalami'"}
 910 | {"text": "table: 1-1140085-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: who is the pole position for the rnd 3\nA: SELECT Pole Position FROM 1-1140085-2 WHERE Rnd = 3"}
 911 | {"text": "table: 1-1140085-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: what is the race where the fastest lap is by jean-pierre jarier?\nA: SELECT Race FROM 1-1140085-2 WHERE Fastest Lap = 'Jean-Pierre Jarier'"}
 912 | {"text": "table: 1-1140090-6\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What circuit did Clay Regazzoni win?\nA: SELECT Circuit FROM 1-1140090-6 WHERE Winning driver = 'Clay Regazzoni'"}
 913 | {"text": "table: 1-1140090-6\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What was the date when Chris Amon won?\nA: SELECT Date FROM 1-1140090-6 WHERE Winning driver = 'Chris Amon'"}
 914 | {"text": "table: 1-1140090-6\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What circuit is the Vi Rhein-Pokalrennen race in?\nA: SELECT Circuit FROM 1-1140090-6 WHERE Race Name = 'VI Rhein-Pokalrennen'"}
 915 | {"text": "table: 1-1140103-6\ncolumns: #, Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What date is listed at place 13\nA: SELECT Date FROM 1-1140103-6 WHERE # = 13"}
 916 | {"text": "table: 1-1140103-6\ncolumns: #, Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What date has a solitudering circuit\nA: SELECT Date FROM 1-1140103-6 WHERE Circuit = 'Solitudering'"}
 917 | {"text": "table: 1-1140103-6\ncolumns: #, Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: How many dates have silverstone circuit\nA: SELECT COUNT Date FROM 1-1140103-6 WHERE Circuit = 'Silverstone'"}
 918 | {"text": "table: 1-1140103-6\ncolumns: #, Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: How many constructors are listed for the XVI BRDC international trophy race\nA: SELECT COUNT Constructor FROM 1-1140103-6 WHERE Race Name = 'XVI BRDC International Trophy'"}
 919 | {"text": "table: 1-1140105-6\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What is the name of the circuit in which the race name is ii danish grand prix?\nA: SELECT Circuit FROM 1-1140105-6 WHERE Race Name = 'II Danish Grand Prix'"}
 920 | {"text": "table: 1-1140105-6\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What is te name of the constructors dated 26 march?\nA: SELECT Constructor FROM 1-1140105-6 WHERE Date = '26 March'"}
 921 | {"text": "table: 1-1140105-6\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What is the total amount of circuts dated 22 april?\nA: SELECT COUNT Circuit FROM 1-1140105-6 WHERE Date = '22 April'"}
 922 | {"text": "table: 1-1140105-6\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: what is the name of the constructor that has the circuit zeltweg airfield?\nA: SELECT Constructor FROM 1-1140105-6 WHERE Circuit = 'Zeltweg Airfield'"}
 923 | {"text": "table: 1-1140105-6\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What is the name of the winning driver where the circuit name is posillipo?\nA: SELECT Winning driver FROM 1-1140105-6 WHERE Circuit = 'Posillipo'"}
 924 | {"text": "table: 1-1140105-6\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What is the name of the circuit where the race xi Syracuse grand prix was held?\nA: SELECT Circuit FROM 1-1140105-6 WHERE Race Name = 'XI Syracuse Grand Prix'"}
 925 | {"text": "table: 1-1140111-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What kind of report is for the Pau circuit?\nA: SELECT Report FROM 1-1140111-5 WHERE Circuit = 'Pau'"}
 926 | {"text": "table: 1-1140111-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: How many different kinds of reports are there for races that Juan Manuel Fangio won?\nA: SELECT COUNT Report FROM 1-1140111-5 WHERE Winning driver = 'Juan Manuel Fangio'"}
 927 | {"text": "table: 1-1140111-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: Who constructed the Syracuse circuit?\nA: SELECT Constructor FROM 1-1140111-5 WHERE Circuit = 'Syracuse'"}
 928 | {"text": "table: 1-1140116-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What is the name of the race in the Modena circuit?\nA: SELECT Race Name FROM 1-1140116-5 WHERE Circuit = 'Modena'"}
 929 | {"text": "table: 1-1140116-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What is the race name in the Monza circuit?\nA: SELECT Race Name FROM 1-1140116-5 WHERE Circuit = 'Monza'"}
 930 | {"text": "table: 1-1140116-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: When does V Madgwick Cup take place?\nA: SELECT Date FROM 1-1140116-5 WHERE Race Name = 'V Madgwick Cup'"}
 931 | {"text": "table: 1-1140116-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: Which driver won the race xiv el\u00e4intarhanajot?\nA: SELECT Winning driver FROM 1-1140116-5 WHERE Race Name = 'XIV El\u00e4intarhanajot'"}
 932 | {"text": "table: 1-1140116-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: Who won the Modena circuit?\nA: SELECT Winning driver FROM 1-1140116-5 WHERE Circuit = 'Modena'"}
 933 | {"text": "table: 1-1140113-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: How many constructors won the III Redex Trophy?\nA: SELECT COUNT Constructor FROM 1-1140113-5 WHERE Race Name = 'III RedeX Trophy'"}
 934 | {"text": "table: 1-1140113-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What was the report of Mike Hawthorn's winning race?\nA: SELECT Report FROM 1-1140113-5 WHERE Winning driver = 'Mike Hawthorn'"}
 935 | {"text": "table: 1-1140113-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What was the name of the race in Bordeaux?\nA: SELECT Race Name FROM 1-1140113-5 WHERE Circuit = 'Bordeaux'"}
 936 | {"text": "table: 1-1140117-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What is the report for the race name V Ulster Trophy?\nA: SELECT Report FROM 1-1140117-5 WHERE Race Name = 'V Ulster Trophy'"}
 937 | {"text": "table: 1-1140117-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What is the constructor for the Silverstone circuit?\nA: SELECT Constructor FROM 1-1140117-5 WHERE Circuit = 'Silverstone'"}
 938 | {"text": "table: 1-1140117-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What's the report for the Silverstone circuit?\nA: SELECT Report FROM 1-1140117-5 WHERE Circuit = 'Silverstone'"}
 939 | {"text": "table: 1-1140117-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What's the report for the race name, XIII Grand Prix de l'Albigeois?\nA: SELECT Report FROM 1-1140117-5 WHERE Race Name = 'XIII Grand Prix de l'Albigeois'"}
 940 | {"text": "table: 1-1140117-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: Which date did the race name XII Pau Grand Prix take place on?\nA: SELECT Date FROM 1-1140117-5 WHERE Race Name = 'XII Pau Grand Prix'"}
 941 | {"text": "table: 1-1140117-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: Who was the winning driver for the goodwood circuit?\nA: SELECT Winning driver FROM 1-1140117-5 WHERE Circuit = 'Goodwood'"}
 942 | {"text": "table: 1-11411026-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: How many millions of U.S. viewers whatched episodes written by Krystal Houghton?\nA: SELECT U.S. viewers (millions) FROM 1-11411026-2 WHERE Written by = 'Krystal Houghton'"}
 943 | {"text": "table: 1-11411026-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: How many titles were directed in series 79?\nA: SELECT COUNT Directed by FROM 1-11411026-2 WHERE No. in series = 79"}
 944 | {"text": "table: 1-11411026-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: Who wrote an episode watched by 19.01 million US viewers?\nA: SELECT Written by FROM 1-11411026-2 WHERE U.S. viewers (millions) = '19.01'"}
 945 | {"text": "table: 1-11404452-1\ncolumns: Series #, Episode title, Writer(s), Director, U.S. viewers (millions), Original air date\nQ: What are the titles of the episodes where Rodman Flender is the director?\nA: SELECT Episode title FROM 1-11404452-1 WHERE Director = 'Rodman Flender'"}
 946 | {"text": "table: 1-11404452-1\ncolumns: Series #, Episode title, Writer(s), Director, U.S. viewers (millions), Original air date\nQ: What is the original air date of the Jamie Babbit directed episode?\nA: SELECT Original air date FROM 1-11404452-1 WHERE Director = 'Jamie Babbit'"}
 947 | {"text": "table: 1-11404452-1\ncolumns: Series #, Episode title, Writer(s), Director, U.S. viewers (millions), Original air date\nQ: What is the original air date when there were 12.81 million u.s viewers?\nA: SELECT Original air date FROM 1-11404452-1 WHERE U.S. viewers (millions) = '12.81'"}
 948 | {"text": "table: 1-11404452-1\ncolumns: Series #, Episode title, Writer(s), Director, U.S. viewers (millions), Original air date\nQ: When did Shelia Lawrence join the series?\nA: SELECT MIN Series # FROM 1-11404452-1 WHERE Writer(s) = 'Shelia Lawrence'"}
 949 | {"text": "table: 1-11404452-1\ncolumns: Series #, Episode title, Writer(s), Director, U.S. viewers (millions), Original air date\nQ: Who was the director when there were 13.66 million u.s viewers?\nA: SELECT Director FROM 1-11404452-1 WHERE U.S. viewers (millions) = '13.66'"}
 950 | {"text": "table: 1-1143966-1\ncolumns: Season, Games, Won, Lost, Tied, Points, Pct %, Goals for, Goals against, Standing\nQ: Name the percentage where the amount won was 25\nA: SELECT Pct % FROM 1-1143966-1 WHERE Won = 25"}
 951 | {"text": "table: 1-1143966-1\ncolumns: Season, Games, Won, Lost, Tied, Points, Pct %, Goals for, Goals against, Standing\nQ: How many games were won with 2nd oha was standing and there were 62 games?\nA: SELECT Won FROM 1-1143966-1 WHERE Standing = '2nd OHA' AND Games = 62"}
 952 | {"text": "table: 1-11447995-2\ncolumns: Ward, Bello, Ben-Tahir, Doucet, Furtenbacher, Gauthier, Haydon, Larter, Lawrance, Libweshya, Liscumb\nQ: What is the Liscumb when Gauthier is 34?\nA: SELECT Liscumb FROM 1-11447995-2 WHERE Gauthier = '34'"}
 953 | {"text": "table: 1-11447995-2\ncolumns: Ward, Bello, Ben-Tahir, Doucet, Furtenbacher, Gauthier, Haydon, Larter, Lawrance, Libweshya, Liscumb\nQ: What is the Bello when Ben-Tahir is 296?\nA: SELECT Bello FROM 1-11447995-2 WHERE Ben-Tahir = '296'"}
 954 | {"text": "table: 1-11447995-2\ncolumns: Ward, Bello, Ben-Tahir, Doucet, Furtenbacher, Gauthier, Haydon, Larter, Lawrance, Libweshya, Liscumb\nQ: What is Ben-Tahir when Bello is 51?\nA: SELECT Ben-Tahir FROM 1-11447995-2 WHERE Bello = '51'"}
 955 | {"text": "table: 1-11447995-2\ncolumns: Ward, Bello, Ben-Tahir, Doucet, Furtenbacher, Gauthier, Haydon, Larter, Lawrance, Libweshya, Liscumb\nQ: What is Haydon when Larter is 11 and Libweshya is 4?\nA: SELECT Haydon FROM 1-11447995-2 WHERE Larter = '11' AND Libweshya = '4'"}
 956 | {"text": "table: 1-11447995-2\ncolumns: Ward, Bello, Ben-Tahir, Doucet, Furtenbacher, Gauthier, Haydon, Larter, Lawrance, Libweshya, Liscumb\nQ: What is Liscumb when Haydon is 1632?\nA: SELECT Liscumb FROM 1-11447995-2 WHERE Haydon = '1632'"}
 957 | {"text": "table: 1-11447995-2\ncolumns: Ward, Bello, Ben-Tahir, Doucet, Furtenbacher, Gauthier, Haydon, Larter, Lawrance, Libweshya, Liscumb\nQ: What is Doucet when Lawrance is 36?\nA: SELECT Doucet FROM 1-11447995-2 WHERE Lawrance = '36'"}
 958 | {"text": "table: 1-11449590-2\ncolumns: Week, Date, Opponent, Result, Kickoff [a ], Game site, TV, Attendance, Record\nQ: Which tv had the date december 7, 1986?\nA: SELECT TV FROM 1-11449590-2 WHERE Date = 'December 7, 1986'"}
 959 | {"text": "table: 1-11449590-2\ncolumns: Week, Date, Opponent, Result, Kickoff [a ], Game site, TV, Attendance, Record\nQ: Which kickoff has the opponent at new orleans saints?\nA: SELECT Kickoff [a ] FROM 1-11449590-2 WHERE Opponent = 'at New Orleans Saints'"}
 960 | {"text": "table: 1-11464746-1\ncolumns: House Name, Composition, Named after, Founded, Colours\nQ: How many houses are green?\nA: SELECT COUNT House Name FROM 1-11464746-1 WHERE Colours = 'Green'"}
 961 | {"text": "table: 1-11464746-1\ncolumns: House Name, Composition, Named after, Founded, Colours\nQ: What year was the house named gongola made?\nA: SELECT Founded FROM 1-11464746-1 WHERE House Name = 'Gongola'"}
 962 | {"text": "table: 1-11464746-1\ncolumns: House Name, Composition, Named after, Founded, Colours\nQ: What is the name of the green house?\nA: SELECT House Name FROM 1-11464746-1 WHERE Colours = 'Green'"}
 963 | {"text": "table: 1-11464746-1\ncolumns: House Name, Composition, Named after, Founded, Colours\nQ: What is the green house made of?\nA: SELECT Composition FROM 1-11464746-1 WHERE Colours = 'Green'"}
 964 | {"text": "table: 1-11464746-1\ncolumns: House Name, Composition, Named after, Founded, Colours\nQ: What is the benue house made of?\nA: SELECT Composition FROM 1-11464746-1 WHERE House Name = 'Benue'"}
 965 | {"text": "table: 1-11465521-2\ncolumns: Week, Date, Opponent, Result, Kickoff [a ], Game site, TV, Attendance, Record\nQ: In which week was the game against a team with a record of 3-6 played?\nA: SELECT COUNT Week FROM 1-11465521-2 WHERE Record = '3-6'"}
 966 | {"text": "table: 1-11465521-2\ncolumns: Week, Date, Opponent, Result, Kickoff [a ], Game site, TV, Attendance, Record\nQ: Which channel had the game against the Minnesota Vikings?\nA: SELECT TV FROM 1-11465521-2 WHERE Opponent = 'Minnesota Vikings'"}
 967 | {"text": "table: 1-11465521-2\ncolumns: Week, Date, Opponent, Result, Kickoff [a ], Game site, TV, Attendance, Record\nQ: How many opponents were there at the game with 64,087 people in attendance?\nA: SELECT COUNT Opponent FROM 1-11465521-2 WHERE Attendance = '64,087'"}
 968 | {"text": "table: 1-11452830-2\ncolumns: Week, Date, Opponent, Result, Kickoff [a ], Game site, TV, Attendance, Record\nQ: Where was the game played when the team's record was 1-3? \nA: SELECT Game site FROM 1-11452830-2 WHERE Record = '1-3'"}
 969 | {"text": "table: 1-11452830-2\ncolumns: Week, Date, Opponent, Result, Kickoff [a ], Game site, TV, Attendance, Record\nQ: How many times was there a kickoff in the September 4, 1988 game? \nA: SELECT COUNT Kickoff [a ] FROM 1-11452830-2 WHERE Date = 'September 4, 1988'"}
 970 | {"text": "table: 1-11452830-2\ncolumns: Week, Date, Opponent, Result, Kickoff [a ], Game site, TV, Attendance, Record\nQ: How many crowds watched the game where the record was 1-3? \nA: SELECT COUNT Attendance FROM 1-11452830-2 WHERE Record = '1-3'"}
 971 | {"text": "table: 1-1149495-1\ncolumns: Series, Year, Winner, Runner-up, Third place, Fourth place, Fifth place, Sixth place, Host\nQ: What year finished with Daniel Zueras as the runner-up?\nA: SELECT COUNT Year FROM 1-1149495-1 WHERE Runner-up = 'Daniel Zueras'"}
 972 | {"text": "table: 1-1149495-1\ncolumns: Series, Year, Winner, Runner-up, Third place, Fourth place, Fifth place, Sixth place, Host\nQ: How many people hosted the show in the year when Chenoa  ended up in fourth place?\nA: SELECT COUNT Host FROM 1-1149495-1 WHERE Fourth place = 'Chenoa'"}
 973 | {"text": "table: 1-1149495-1\ncolumns: Series, Year, Winner, Runner-up, Third place, Fourth place, Fifth place, Sixth place, Host\nQ: How many fourth places were there in 2003?\nA: SELECT COUNT Fourth place FROM 1-1149495-1 WHERE Year = '2003'"}
 974 | {"text": "table: 1-1147705-1\ncolumns: model, max. motive power, max. torque at rpm, engine displacement, engine type, engine configuration & notes 0-100km/h\nQ: What is the engine type when the max torque at rpm is n\u00b7m ( lbf\u00b7ft ) @ 4,800 Answers:?\nA: SELECT engine type FROM 1-1147705-1 WHERE max. torque at rpm = 'N\u00b7m ( lbf\u00b7ft ) @ 4,800'"}
 975 | {"text": "table: 1-1147705-1\ncolumns: model, max. motive power, max. torque at rpm, engine displacement, engine type, engine configuration & notes 0-100km/h\nQ: What is the engine configuration $notes 0-100km/h for the engine type b5244 t2?\nA: SELECT engine configuration & notes 0-100km/h FROM 1-1147705-1 WHERE engine type = 'B5244 T2'"}
 976 | {"text": "table: 1-1147705-1\ncolumns: model, max. motive power, max. torque at rpm, engine displacement, engine type, engine configuration & notes 0-100km/h\nQ: What is the engine displacement for the engine type b5254 t?\nA: SELECT engine displacement FROM 1-1147705-1 WHERE engine type = 'B5254 T'"}
 977 | {"text": "table: 1-1147705-1\ncolumns: model, max. motive power, max. torque at rpm, engine displacement, engine type, engine configuration & notes 0-100km/h\nQ: How many have are model 2.4 awd?\nA: SELECT COUNT engine type FROM 1-1147705-1 WHERE model = '2.4 AWD'"}
 978 | {"text": "table: 1-1147705-1\ncolumns: model, max. motive power, max. torque at rpm, engine displacement, engine type, engine configuration & notes 0-100km/h\nQ: How many engine b5204 t3?\nA: SELECT COUNT engine displacement FROM 1-1147705-1 WHERE engine type = 'B5204 T3'"}
 979 | {"text": "table: 1-1147705-1\ncolumns: model, max. motive power, max. torque at rpm, engine displacement, engine type, engine configuration & notes 0-100km/h\nQ: How many engine b5234 t3?\nA: SELECT COUNT model FROM 1-1147705-1 WHERE engine type = 'B5234 T3'"}
 980 | {"text": "table: 1-1147701-4\ncolumns: Model name, Power (ps), Torque (Nm@rpm), Displacement (cm\u00b3), Engine code, Comment\nQ:  how many\u00a0power (ps)\u00a0with\u00a0torque (nm@rpm)\u00a0being 240@2200-5000\nA: SELECT COUNT Power (ps) FROM 1-1147701-4 WHERE Torque (Nm@rpm) = '240@2200-5000'"}
 981 | {"text": "table: 1-1147701-4\ncolumns: Model name, Power (ps), Torque (Nm@rpm), Displacement (cm\u00b3), Engine code, Comment\nQ: what's the\u00a0comment\u00a0with\u00a0model name\u00a0being 2.4 (2001-2007)\nA: SELECT Comment FROM 1-1147701-4 WHERE Model name = '2.4 (2001-2007)'"}
 982 | {"text": "table: 1-1147701-4\ncolumns: Model name, Power (ps), Torque (Nm@rpm), Displacement (cm\u00b3), Engine code, Comment\nQ: what's the\u00a0model name\u00a0with\u00a0engine code\u00a0being b5204 t5\nA: SELECT Model name FROM 1-1147701-4 WHERE Engine code = 'B5204 T5'"}
 983 | {"text": "table: 1-1147701-4\ncolumns: Model name, Power (ps), Torque (Nm@rpm), Displacement (cm\u00b3), Engine code, Comment\nQ: what's the\u00a0dbeingplacement (cm\u00b3)\u00a0with\u00a0torque (nm@rpm)\u00a0being 350@1800-6000\nA: SELECT Displacement (cm\u00b3) FROM 1-1147701-4 WHERE Torque (Nm@rpm) = '350@1800-6000'"}
 984 | {"text": "table: 1-1147701-4\ncolumns: Model name, Power (ps), Torque (Nm@rpm), Displacement (cm\u00b3), Engine code, Comment\nQ: what's the\u00a0model name\u00a0with\u00a0torque (nm@rpm)\u00a0being 230@4500\nA: SELECT Model name FROM 1-1147701-4 WHERE Torque (Nm@rpm) = '230@4500'"}
 985 | {"text": "table: 1-1147701-4\ncolumns: Model name, Power (ps), Torque (Nm@rpm), Displacement (cm\u00b3), Engine code, Comment\nQ: what's the\u00a0model name\u00a0with\u00a0engine code\u00a0being b5254 t4\nA: SELECT Model name FROM 1-1147701-4 WHERE Engine code = 'B5254 T4'"}
 986 | {"text": "table: 1-1147701-5\ncolumns: Model name, Power (ps), Torque (Nm@rpm), Displacement (cm\u00b3), Engine code, Comment\nQ: Name the torque of the engine is d5244 t5\nA: SELECT Torque (Nm@rpm) FROM 1-1147701-5 WHERE Engine code = 'D5244 T5'"}
 987 | {"text": "table: 1-1147701-5\ncolumns: Model name, Power (ps), Torque (Nm@rpm), Displacement (cm\u00b3), Engine code, Comment\nQ: What is the model of the engine d5244 t?\nA: SELECT Model name FROM 1-1147701-5 WHERE Engine code = 'D5244 T'"}
 988 | {"text": "table: 1-1147701-5\ncolumns: Model name, Power (ps), Torque (Nm@rpm), Displacement (cm\u00b3), Engine code, Comment\nQ: What is the model of the enginge d5252 t?\nA: SELECT Model name FROM 1-1147701-5 WHERE Engine code = 'D5252 T'"}
 989 | {"text": "table: 1-1147701-5\ncolumns: Model name, Power (ps), Torque (Nm@rpm), Displacement (cm\u00b3), Engine code, Comment\nQ: What is the model of the engine d5244 t7?\nA: SELECT Model name FROM 1-1147701-5 WHERE Engine code = 'D5244 T7'"}
 990 | {"text": "table: 1-11545282-11\ncolumns: Player, No., Nationality, Position, Years for Jazz, School/Club Team\nQ: What is the position of number 47?\nA: SELECT Position FROM 1-11545282-11 WHERE No. = '47'"}
 991 | {"text": "table: 1-11545282-11\ncolumns: Player, No., Nationality, Position, Years for Jazz, School/Club Team\nQ: Name the position of Turkey\nA: SELECT Position FROM 1-11545282-11 WHERE Nationality = 'Turkey'"}
 992 | {"text": "table: 1-11545282-11\ncolumns: Player, No., Nationality, Position, Years for Jazz, School/Club Team\nQ: Who is player number 51?\nA: SELECT Player FROM 1-11545282-11 WHERE No. = '51'"}
 993 | {"text": "table: 1-11545282-11\ncolumns: Player, No., Nationality, Position, Years for Jazz, School/Club Team\nQ: What is the position for the years 1998-99\nA: SELECT Position FROM 1-11545282-11 WHERE Years for Jazz = '1998-99'"}
 994 | {"text": "table: 1-11545282-11\ncolumns: Player, No., Nationality, Position, Years for Jazz, School/Club Team\nQ: How many positions are for creighton?\nA: SELECT COUNT Position FROM 1-11545282-11 WHERE School/Club Team = 'Creighton'"}
 995 | {"text": "table: 1-11545282-12\ncolumns: Player, No., Nationality, Position, Years for Jazz, School/Club Team\nQ: Which player is from Marshall and played 1974-75?\nA: SELECT Player FROM 1-11545282-12 WHERE Years for Jazz = '1974-75' AND School/Club Team = 'Marshall'"}
 996 | {"text": "table: 1-11545282-12\ncolumns: Player, No., Nationality, Position, Years for Jazz, School/Club Team\nQ: Which country is the player that went to Oregon?\nA: SELECT Nationality FROM 1-11545282-12 WHERE School/Club Team = 'Oregon'"}
 997 | {"text": "table: 1-11545282-12\ncolumns: Player, No., Nationality, Position, Years for Jazz, School/Club Team\nQ: Which country is Jim Les from?\nA: SELECT Nationality FROM 1-11545282-12 WHERE Player = 'Jim Les'"}
 998 | {"text": "table: 1-11545282-12\ncolumns: Player, No., Nationality, Position, Years for Jazz, School/Club Team\nQ: Which number is the player from Minnesota?\nA: SELECT MAX No. FROM 1-11545282-12 WHERE School/Club Team = 'Minnesota'"}
 999 | {"text": "table: 1-11545282-18\ncolumns: Player, No., Nationality, Position, Years for Jazz, School/Club Team\nQ: Which player played for years 2000-02\nA: SELECT Player FROM 1-11545282-18 WHERE Years for Jazz = '2000-02'"}
1000 | {"text": "table: 1-11545282-18\ncolumns: Player, No., Nationality, Position, Years for Jazz, School/Club Team\nQ: Which school is Kirk Snyder from?\nA: SELECT School/Club Team FROM 1-11545282-18 WHERE Player = 'Kirk Snyder'"}
1001 | 


--------------------------------------------------------------------------------
/Data/lora/valid.jsonl:
--------------------------------------------------------------------------------
  1 | {"text": "table: 1-10015132-11\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What position does the player who played for butler cc (ks) play?\nA: SELECT Position FROM 1-10015132-11 WHERE School/Club Team = 'Butler CC (KS)'"}
  2 | {"text": "table: 1-10015132-11\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: How many schools did player number 3 play at?\nA: SELECT COUNT School/Club Team FROM 1-10015132-11 WHERE No. = '3'"}
  3 | {"text": "table: 1-10015132-11\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What school did player number 21 play for?\nA: SELECT School/Club Team FROM 1-10015132-11 WHERE No. = '21'"}
  4 | {"text": "table: 1-10015132-11\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: Who is the player that wears number 42?\nA: SELECT Player FROM 1-10015132-11 WHERE No. = '42'"}
  5 | {"text": "table: 1-10015132-11\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What player played guard for toronto in 1996-97?\nA: SELECT Player FROM 1-10015132-11 WHERE Position = 'Guard' AND Years in Toronto = '1996-97'"}
  6 | {"text": "table: 1-10015132-9\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: Who are all of the players on the Westchester High School club team?\nA: SELECT Player FROM 1-10015132-9 WHERE School/Club Team = 'Westchester High School'"}
  7 | {"text": "table: 1-10015132-9\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What school/club team is Amir Johnson on?\nA: SELECT School/Club Team FROM 1-10015132-9 WHERE Player = 'Amir Johnson'"}
  8 | {"text": "table: 1-10015132-9\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What are the total amount of numbers on the Toronto team in 2005-06?\nA: SELECT COUNT No. FROM 1-10015132-9 WHERE Years in Toronto = '2005-06'"}
  9 | {"text": "table: 1-10015132-9\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What are the total number of positions on the Toronto team in 2006-07?\nA: SELECT COUNT Position FROM 1-10015132-9 WHERE Years in Toronto = '2006-07'"}
 10 | {"text": "table: 1-10015132-9\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What are the nationality of the players on the Fresno State school/club team?\nA: SELECT Nationality FROM 1-10015132-9 WHERE School/Club Team = 'Fresno State'"}
 11 | {"text": "table: 1-10015132-9\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What school/club team is Trey Johnson on?\nA: SELECT School/Club Team FROM 1-10015132-9 WHERE Player = 'Trey Johnson'"}
 12 | {"text": "table: 1-10026563-1\ncolumns: Entered office as Head of State or Government, Began time as senior G8 leader, Ended time as senior G8 leader, Person, Office\nQ: When did Jacques Chirac stop being a G8 leader?\nA: SELECT Ended time as senior G8 leader FROM 1-10026563-1 WHERE Person = 'Jacques Chirac'"}
 13 | {"text": "table: 1-10026563-1\ncolumns: Entered office as Head of State or Government, Began time as senior G8 leader, Ended time as senior G8 leader, Person, Office\nQ: When did the Prime Minister of Italy take office?\nA: SELECT Entered office as Head of State or Government FROM 1-10026563-1 WHERE Office = 'Prime Minister of Italy'"}
 14 | {"text": "table: 1-1008653-1\ncolumns: Country ( exonym ), Capital ( exonym ), Country ( endonym ), Capital ( endonym ), Official or native language(s) (alphabet/script)\nQ: What is the English name of the country whose official native language is Dutch Papiamento?\nA: SELECT Country ( exonym ) FROM 1-1008653-1 WHERE Official or native language(s) (alphabet/script) = 'Dutch Papiamento'"}
 15 | {"text": "table: 1-1008653-1\ncolumns: Country ( exonym ), Capital ( exonym ), Country ( endonym ), Capital ( endonym ), Official or native language(s) (alphabet/script)\nQ: What official or native languages are spoken in the country whose capital city is Canberra?\nA: SELECT Official or native language(s) (alphabet/script) FROM 1-1008653-1 WHERE Capital ( exonym ) = 'Canberra'"}
 16 | {"text": "table: 1-1008653-1\ncolumns: Country ( exonym ), Capital ( exonym ), Country ( endonym ), Capital ( endonym ), Official or native language(s) (alphabet/script)\nQ: What is the local name given to the city of Canberra?\nA: SELECT Capital ( endonym ) FROM 1-1008653-1 WHERE Capital ( exonym ) = 'Canberra'"}
 17 | {"text": "table: 1-1008653-1\ncolumns: Country ( exonym ), Capital ( exonym ), Country ( endonym ), Capital ( endonym ), Official or native language(s) (alphabet/script)\nQ: What is the local name given to the capital of Anguilla?\nA: SELECT Capital ( endonym ) FROM 1-1008653-1 WHERE Country ( endonym ) = 'Anguilla'"}
 18 | {"text": "table: 1-1008653-1\ncolumns: Country ( exonym ), Capital ( exonym ), Country ( endonym ), Capital ( endonym ), Official or native language(s) (alphabet/script)\nQ: What is the English name given to the city of St. John's?\nA: SELECT Capital ( exonym ) FROM 1-1008653-1 WHERE Capital ( endonym ) = 'St. John's'"}
 19 | {"text": "table: 1-1008653-1\ncolumns: Country ( exonym ), Capital ( exonym ), Country ( endonym ), Capital ( endonym ), Official or native language(s) (alphabet/script)\nQ: How many capital cities does Australia have?\nA: SELECT COUNT Capital ( endonym ) FROM 1-1008653-1 WHERE Country ( endonym ) = 'Australia'"}
 20 | {"text": "table: 1-10088101-1\ncolumns: No. in set, No. in series, Title, Directed by, Written by, Original air date, Production code\nQ: The episode with production code 9abx02 was originally aired on what date?\nA: SELECT Original air date FROM 1-10088101-1 WHERE Production code = '9ABX02'"}
 21 | {"text": "table: 1-10088101-1\ncolumns: No. in set, No. in series, Title, Directed by, Written by, Original air date, Production code\nQ: What is the episode number that has production code 8abx15?\nA: SELECT MIN No. in series FROM 1-10088101-1 WHERE Production code = '8ABX15'"}
 22 | {"text": "table: 1-10295819-2\ncolumns: Player, Highest singles ranking, Highest doubles ranking, First year played, Years played, Ties played, Total W\u2013L, Singles W\u2013L, Doubles W\u2013L\nQ: Name the minimum tiesplayed for 6 years\nA: SELECT MIN Ties played FROM 1-10295819-2 WHERE Years played = 6"}
 23 | {"text": "table: 1-10342194-3\ncolumns: District, Total amount of trees, Prevailing types, %, Amount of old trees, Amount of trees, that require replacement\nQ: What is the amount of trees, that require replacement when prevailing types, % is pine \u2014 29.37 poplar \u2014 26.12 acer negundo \u2014 13.2?\nA: SELECT Amount of trees, that require replacement FROM 1-10342194-3 WHERE Prevailing types, % = 'Pine \u2014 29.37 Poplar \u2014 26.12 Acer negundo \u2014 13.2'"}
 24 | {"text": "table: 1-10342194-3\ncolumns: District, Total amount of trees, Prevailing types, %, Amount of old trees, Amount of trees, that require replacement\nQ: What is the amount of trees, that require replacement when district is leninsky?\nA: SELECT Amount of trees, that require replacement FROM 1-10342194-3 WHERE District = 'Leninsky'"}
 25 | {"text": "table: 1-10342194-3\ncolumns: District, Total amount of trees, Prevailing types, %, Amount of old trees, Amount of trees, that require replacement\nQ: What is the district when the total amount of trees is smaller than 150817.6878461314 and amount of old trees is 1,928 (1.89%)?\nA: SELECT District FROM 1-10342194-3 WHERE Total amount of trees < 150817.6878461314 AND Amount of old trees = '1,928 (1.89%)'"}
 26 | {"text": "table: 1-10342194-3\ncolumns: District, Total amount of trees, Prevailing types, %, Amount of old trees, Amount of trees, that require replacement\nQ: What is the amount of trees, that require replacement when the district is motovilikhinsky?\nA: SELECT Amount of trees, that require replacement FROM 1-10342194-3 WHERE District = 'Motovilikhinsky'"}
 27 | {"text": "table: 1-10342194-3\ncolumns: District, Total amount of trees, Prevailing types, %, Amount of old trees, Amount of trees, that require replacement\nQ: What is the total amount of trees when district is leninsky?\nA: SELECT MAX Total amount of trees FROM 1-10342194-3 WHERE District = 'Leninsky'"}
 28 | {"text": "table: 1-10342194-3\ncolumns: District, Total amount of trees, Prevailing types, %, Amount of old trees, Amount of trees, that require replacement\nQ: What is the district when prevailing types, % is acer negundo \u2014 30.22 tilia \u2014 18.6 poplar \u2014 15.23?\nA: SELECT District FROM 1-10342194-3 WHERE Prevailing types, % = 'Acer negundo \u2014 30.22 Tilia \u2014 18.6 Poplar \u2014 15.23'"}
 29 | {"text": "table: 1-10429820-13\ncolumns: Iowa State vs., Overall Record, in Ames, at Opponents Venue, at Neutral Site, Last 5 Meetings, Last 10 Meetings, Current Streak, Since Beginning of Big 12\nQ: When the value of \"since beginning of big 12\" is synonymous with its' category, what are the in Ames values?\nA: SELECT in Ames FROM 1-10429820-13 WHERE Since Beginning of Big 12 = 'Since Beginning of Big 12'"}
 30 | {"text": "table: 1-1046170-5\ncolumns: Year, Division, League, Regular Season, Playoffs, U.S. Open Cup\nQ: what's the\u00a0u.s. open cup status\u00a0for regular season\u00a0of 4th, atlantic division \nA: SELECT U.S. Open Cup FROM 1-1046170-5 WHERE Regular Season = '4th, Atlantic Division'"}
 31 | {"text": "table: 1-1046170-5\ncolumns: Year, Division, League, Regular Season, Playoffs, U.S. Open Cup\nQ: how many division  did not qualify for u.s. open cup in 2003\nA: SELECT Division FROM 1-1046170-5 WHERE U.S. Open Cup = 'Did Not Qualify' AND Year = 2003"}
 32 | {"text": "table: 1-1046170-5\ncolumns: Year, Division, League, Regular Season, Playoffs, U.S. Open Cup\nQ: which round is u.s. open cup division semifinals\nA: SELECT U.S. Open Cup FROM 1-1046170-5 WHERE Playoffs = 'Division Semifinals'"}
 33 | {"text": "table: 1-1046170-5\ncolumns: Year, Division, League, Regular Season, Playoffs, U.S. Open Cup\nQ: what are all the playoffs for regular season is 1st, atlantic division\nA: SELECT Playoffs FROM 1-1046170-5 WHERE Regular Season = '1st, Atlantic Division'"}
 34 | {"text": "table: 1-1046170-5\ncolumns: Year, Division, League, Regular Season, Playoffs, U.S. Open Cup\nQ: what are all the playoffs for u.s. open cup in 1st round\nA: SELECT Playoffs FROM 1-1046170-5 WHERE U.S. Open Cup = '1st Round'"}
 35 | {"text": "table: 1-1061075-1\ncolumns: Season, Competition, Round, Opponents, 1st leg, 2nd leg, Aggregate\nQ: what is the total number of\u00a02nd leg\u00a0where\u00a0aggregate\u00a0is 7-2\nA: SELECT COUNT 2nd leg FROM 1-1061075-1 WHERE Aggregate = '7-2'"}
 36 | {"text": "table: 1-1061075-1\ncolumns: Season, Competition, Round, Opponents, 1st leg, 2nd leg, Aggregate\nQ:  what's the\u00a0aggregate\u00a0where\u00a01st leg\u00a0is 3\u20132\nA: SELECT Aggregate FROM 1-1061075-1 WHERE 1st leg = '3\u20132'"}
 37 | {"text": "table: 1-1061075-1\ncolumns: Season, Competition, Round, Opponents, 1st leg, 2nd leg, Aggregate\nQ:  what's the\u00a0competition\u00a0where\u00a0aggregate\u00a0is 4\u20137\nA: SELECT Competition FROM 1-1061075-1 WHERE Aggregate = '4\u20137'"}
 38 | {"text": "table: 1-1061075-1\ncolumns: Season, Competition, Round, Opponents, 1st leg, 2nd leg, Aggregate\nQ:  what's the\u00a0competition\u00a0where\u00a01st leg\u00a0is 4-1 (h)\nA: SELECT Competition FROM 1-1061075-1 WHERE 1st leg = '4-1 (h)'"}
 39 | {"text": "table: 1-1061075-1\ncolumns: Season, Competition, Round, Opponents, 1st leg, 2nd leg, Aggregate\nQ: what is the total number of\u00a0round\u00a0where\u00a0opponents\u00a0is haugar\nA: SELECT COUNT Round FROM 1-1061075-1 WHERE Opponents = 'Haugar'"}
 40 | {"text": "table: 1-1061075-1\ncolumns: Season, Competition, Round, Opponents, 1st leg, 2nd leg, Aggregate\nQ:  what's the\u00a01st leg\u00a0where\u00a0opponents\u00a0is galatasaray\nA: SELECT 1st leg FROM 1-1061075-1 WHERE Opponents = 'Galatasaray'"}
 41 | {"text": "table: 1-10706961-2\ncolumns: Rd, Name, Pole Position, Fastest Lap, Winning driver, Winning team, Report\nQ: What is the highest Rd that Tom Sneva had the pole position in?\nA: SELECT MAX Rd FROM 1-10706961-2 WHERE Pole Position = 'Tom Sneva'"}
 42 | {"text": "table: 1-10706961-2\ncolumns: Rd, Name, Pole Position, Fastest Lap, Winning driver, Winning team, Report\nQ: How many winning drivers were there in the race that had a fastest lap time of 56.920?\nA: SELECT COUNT Winning driver FROM 1-10706961-2 WHERE Fastest Lap = '56.920'"}
 43 | {"text": "table: 1-10706961-2\ncolumns: Rd, Name, Pole Position, Fastest Lap, Winning driver, Winning team, Report\nQ: How many reports are there in the race that Forsythe Racing won and Teo Fabi had the pole position in?\nA: SELECT COUNT Report FROM 1-10706961-2 WHERE Winning team = 'Forsythe Racing' AND Pole Position = 'Teo Fabi'"}
 44 | {"text": "table: 1-10706961-2\ncolumns: Rd, Name, Pole Position, Fastest Lap, Winning driver, Winning team, Report\nQ: Which Rd took place at the Indianapolis 500?\nA: SELECT Rd FROM 1-10706961-2 WHERE Name = 'Indianapolis 500'"}
 45 | {"text": "table: 1-10706961-2\ncolumns: Rd, Name, Pole Position, Fastest Lap, Winning driver, Winning team, Report\nQ: Which teams won when Bobby Rahal was their winning driver?\nA: SELECT Winning team FROM 1-10706961-2 WHERE Winning driver = 'Bobby Rahal'"}
 46 | {"text": "table: 1-10706961-2\ncolumns: Rd, Name, Pole Position, Fastest Lap, Winning driver, Winning team, Report\nQ: What was the fastest lap time in the Escort Radar Warning 200?\nA: SELECT Fastest Lap FROM 1-10706961-2 WHERE Name = 'Escort Radar Warning 200'"}
 47 | {"text": "table: 1-10707176-2\ncolumns: Rnd, Race Name, Circuit, City/Location, Date, Pole position, Winning driver, Winning team, Report\nQ: What report was there for the porsche north america?\nA: SELECT Report FROM 1-10707176-2 WHERE Winning team = 'Porsche North America'"}
 48 | {"text": "table: 1-10707176-2\ncolumns: Rnd, Race Name, Circuit, City/Location, Date, Pole position, Winning driver, Winning team, Report\nQ: What rnds were there for the phoenix international raceway?\nA: SELECT Rnd FROM 1-10707176-2 WHERE Circuit = 'Phoenix International Raceway'"}
 49 | {"text": "table: 1-10707176-2\ncolumns: Rnd, Race Name, Circuit, City/Location, Date, Pole position, Winning driver, Winning team, Report\nQ: Who was the pole position for the rnd equalling 12?\nA: SELECT Pole position FROM 1-10707176-2 WHERE Rnd = '12'"}
 50 | {"text": "table: 1-10707176-2\ncolumns: Rnd, Race Name, Circuit, City/Location, Date, Pole position, Winning driver, Winning team, Report\nQ: How many reports were the for the cleveland burke lakefront airport circut?\nA: SELECT COUNT Report FROM 1-10707176-2 WHERE Circuit = 'Cleveland Burke Lakefront Airport'"}
 51 | {"text": "table: 1-10707176-2\ncolumns: Rnd, Race Name, Circuit, City/Location, Date, Pole position, Winning driver, Winning team, Report\nQ: How many winning drivers were the for the rnd equalling 5?\nA: SELECT COUNT Winning driver FROM 1-10707176-2 WHERE Rnd = '5'"}
 52 | {"text": "table: 1-10706879-3\ncolumns: Rd, Name, Pole Position, Fastest Lap, Winning driver, Winning team, Report\nQ: The race tony bettenhausen 200 has what smallest rd?\nA: SELECT MIN Rd FROM 1-10706879-3 WHERE Name = 'Tony Bettenhausen 200'"}
 53 | {"text": "table: 1-10706879-3\ncolumns: Rd, Name, Pole Position, Fastest Lap, Winning driver, Winning team, Report\nQ: The winning team of the race, los angeles times 500 is who?\nA: SELECT Winning team FROM 1-10706879-3 WHERE Name = 'Los Angeles Times 500'"}
 54 | {"text": "table: 1-10706879-3\ncolumns: Rd, Name, Pole Position, Fastest Lap, Winning driver, Winning team, Report\nQ: How many winning drivers in the kraco twin 125 (r2) race were there?\nA: SELECT COUNT Winning driver FROM 1-10706879-3 WHERE Name = 'Kraco Twin 125 (R2)'"}
 55 | {"text": "table: 1-10706879-3\ncolumns: Rd, Name, Pole Position, Fastest Lap, Winning driver, Winning team, Report\nQ: What are the races that johnny rutherford has won?\nA: SELECT Name FROM 1-10706879-3 WHERE Winning driver = 'Johnny Rutherford'"}
 56 | {"text": "table: 1-10706879-3\ncolumns: Rd, Name, Pole Position, Fastest Lap, Winning driver, Winning team, Report\nQ: How many fastest laps were there for a rd that equals 10?\nA: SELECT COUNT Fastest Lap FROM 1-10706879-3 WHERE Rd = 10"}
 57 | {"text": "table: 1-10712301-5\ncolumns: Region, Operator, Licence award date, On air date, Closure date\nQ: What is the license award date for North East England?\nA: SELECT Licence award date FROM 1-10712301-5 WHERE Region = 'North East England'"}
 58 | {"text": "table: 1-10733530-3\ncolumns: Nation, Population (thousands), Internet subscriptions (2000) (thousands of users), Internet subscriptions (2008) (thousands of users), % growth (2000\u20132008), % Internet users\nQ: What is the percentage of growth in 2000-2008 in ethiopia?\nA: SELECT % growth (2000\u20132008) FROM 1-10733530-3 WHERE Nation = 'Ethiopia'"}
 59 | {"text": "table: 1-10733530-3\ncolumns: Nation, Population (thousands), Internet subscriptions (2000) (thousands of users), Internet subscriptions (2008) (thousands of users), % growth (2000\u20132008), % Internet users\nQ: Name the total number of percentage growth 2000-2008 of uganda?\nA: SELECT COUNT % growth (2000\u20132008) FROM 1-10733530-3 WHERE Nation = 'Uganda'"}
 60 | {"text": "table: 1-10733530-3\ncolumns: Nation, Population (thousands), Internet subscriptions (2000) (thousands of users), Internet subscriptions (2008) (thousands of users), % growth (2000\u20132008), % Internet users\nQ: What is the maximum percentage grown 2000-2008 in burundi\nA: SELECT MAX % growth (2000\u20132008) FROM 1-10733530-3 WHERE Nation = 'Burundi'"}
 61 | {"text": "table: 1-10798421-1\ncolumns: Village (German), Village (Slovenian), Number of people 1991, Percent of Slovenes 1991, Percent of Slovenes 1951\nQ: Provide me with the names of all the villages (German) that has 76.3% of Slovenes in 1951.\nA: SELECT Village (German) FROM 1-10798421-1 WHERE Percent of Slovenes 1951 = '76.3%'"}
 62 | {"text": "table: 1-10798421-1\ncolumns: Village (German), Village (Slovenian), Number of people 1991, Percent of Slovenes 1991, Percent of Slovenes 1951\nQ: Give me the minimum number of people in 1991 with 92.5% of Slovenes in 1991.\nA: SELECT MIN Number of people 1991 FROM 1-10798421-1 WHERE Percent of Slovenes 1991 = '92.5%'"}
 63 | {"text": "table: 1-10798421-1\ncolumns: Village (German), Village (Slovenian), Number of people 1991, Percent of Slovenes 1991, Percent of Slovenes 1951\nQ: Provide me with the name of all the village (German) that are part of the village (Slovenian) with sele srednji kot. \nA: SELECT Village (German) FROM 1-10798421-1 WHERE Village (Slovenian) = 'Sele Srednji Kot'"}
 64 | {"text": "table: 1-10798421-1\ncolumns: Village (German), Village (Slovenian), Number of people 1991, Percent of Slovenes 1991, Percent of Slovenes 1951\nQ: Provide me with the name of all the village (German) that are part of the village (Slovenian) with sele borovnica.\nA: SELECT Village (German) FROM 1-10798421-1 WHERE Village (Slovenian) = 'Sele Borovnica'"}
 65 | {"text": "table: 1-10798421-1\ncolumns: Village (German), Village (Slovenian), Number of people 1991, Percent of Slovenes 1991, Percent of Slovenes 1951\nQ: Provide me with the name of the village (German) where there is 96.9% Slovenes in 1951. \nA: SELECT Village (German) FROM 1-10798421-1 WHERE Percent of Slovenes 1951 = '96.9%'"}
 66 | {"text": "table: 1-10798421-1\ncolumns: Village (German), Village (Slovenian), Number of people 1991, Percent of Slovenes 1991, Percent of Slovenes 1951\nQ: Provide with the names of the village (German) that is part of village (Slovenian) with sele srednji kot.\nA: SELECT Village (German) FROM 1-10798421-1 WHERE Village (Slovenian) = 'Sele Srednji Kot'"}
 67 | {"text": "table: 1-10812293-3\ncolumns: Game, Date, Team, Score, High points, High rebounds, High assists, Location Attendance, Record\nQ: What was the score of the game on November 12?\nA: SELECT Score FROM 1-10812293-3 WHERE Date = 'November 12'"}
 68 | {"text": "table: 1-10812293-3\ncolumns: Game, Date, Team, Score, High points, High rebounds, High assists, Location Attendance, Record\nQ: Who had high assists when they played against San Antonio?\nA: SELECT High assists FROM 1-10812293-3 WHERE Team = 'San Antonio'"}
 69 | {"text": "table: 1-10812293-3\ncolumns: Game, Date, Team, Score, High points, High rebounds, High assists, Location Attendance, Record\nQ: Who scored the most points in game 4?\nA: SELECT High points FROM 1-10812293-3 WHERE Game = 4"}
 70 | {"text": "table: 1-10812293-3\ncolumns: Game, Date, Team, Score, High points, High rebounds, High assists, Location Attendance, Record\nQ: Where was the game on November 20?\nA: SELECT Location Attendance FROM 1-10812293-3 WHERE Date = 'November 20'"}
 71 | {"text": "table: 1-10935205-1\ncolumns: No. in season, No. in series, Title, Canadian airdate, US airdate, Production code\nQ: The canadian airdate of 11 february 2008 applied to what series number?\nA: SELECT COUNT No. in series FROM 1-10935205-1 WHERE Canadian airdate = '11 February 2008'"}
 72 | {"text": "table: 1-10935205-1\ncolumns: No. in season, No. in series, Title, Canadian airdate, US airdate, Production code\nQ: The U.S. airdate of 4 april 2008 had a production code of what?\nA: SELECT MAX Production code FROM 1-10935205-1 WHERE US airdate = '4 April 2008'"}
 73 | {"text": "table: 1-10935205-1\ncolumns: No. in season, No. in series, Title, Canadian airdate, US airdate, Production code\nQ: The episode titled \"don't stop believin'\" was what highest number of the season?\nA: SELECT MAX No. in season FROM 1-10935205-1 WHERE Title = '\"Don't Stop Believin'\"'"}
 74 | {"text": "table: 1-10935205-1\ncolumns: No. in season, No. in series, Title, Canadian airdate, US airdate, Production code\nQ: The U.S. airdate of 8 august 2008 also had canadian airdates of what?\nA: SELECT Canadian airdate FROM 1-10935205-1 WHERE US airdate = '8 August 2008'"}
 75 | {"text": "table: 1-10935205-1\ncolumns: No. in season, No. in series, Title, Canadian airdate, US airdate, Production code\nQ: The canadian airdate of 17 march 2008 had how many numbers in the season?\nA: SELECT COUNT No. in season FROM 1-10935205-1 WHERE Canadian airdate = '17 March 2008'"}
 76 | {"text": "table: 1-10935205-1\ncolumns: No. in season, No. in series, Title, Canadian airdate, US airdate, Production code\nQ: For the episode(s) aired in the U.S. on 4 april 2008, what were the names?\nA: SELECT Title FROM 1-10935205-1 WHERE US airdate = '4 April 2008'"}
 77 | {"text": "table: 1-10953197-5\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: Who directed the episode \"Great Sexpectations (2)\"?\nA: SELECT Director FROM 1-10953197-5 WHERE Title = '\"Great Sexpectations (2)\"'"}
 78 | {"text": "table: 1-10975034-2\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: Which player from the 2004 CFL draft attended Wilfrid Laurier?\nA: SELECT Player FROM 1-10975034-2 WHERE College = 'Wilfrid Laurier'"}
 79 | {"text": "table: 1-10975034-2\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What position does Christian Leibl-Cote play?\nA: SELECT Position FROM 1-10975034-2 WHERE Player = 'Christian Leibl-Cote'"}
 80 | {"text": "table: 1-10975034-2\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What is the pick number for Northwestern college?\nA: SELECT MAX Pick # FROM 1-10975034-2 WHERE College = 'Northwestern'"}
 81 | {"text": "table: 1-10992-3\ncolumns: No, City district (Stadtteil), Area in km\u00b2, Population, Foreign nationals, Foreign nationals in %, Area district (Ortsbezirk)\nQ: How many foreigners in percentage terms had a population of 4.911?\nA: SELECT COUNT Foreign nationals in % FROM 1-10992-3 WHERE Population = '4.911'"}
 82 | {"text": "table: 1-10992-3\ncolumns: No, City district (Stadtteil), Area in km\u00b2, Population, Foreign nationals, Foreign nationals in %, Area district (Ortsbezirk)\nQ: What is the number of the city district of stadtteil where foreigners are 5.162?\nA: SELECT COUNT City district (Stadtteil) FROM 1-10992-3 WHERE Foreign nationals = '5.162'"}
 83 | {"text": "table: 1-10992-3\ncolumns: No, City district (Stadtteil), Area in km\u00b2, Population, Foreign nationals, Foreign nationals in %, Area district (Ortsbezirk)\nQ: What is the city where the number is 47?\nA: SELECT City district (Stadtteil) FROM 1-10992-3 WHERE No = '47'"}
 84 | {"text": "table: 1-11044765-1\ncolumns: School, Mascot, Location, League, Enrollment\nQ: Which leagues have Raiders as their mascot?\nA: SELECT League FROM 1-11044765-1 WHERE Mascot = 'Raiders'"}
 85 | {"text": "table: 1-11044765-1\ncolumns: School, Mascot, Location, League, Enrollment\nQ: Which leagues is the Galena school in?\nA: SELECT League FROM 1-11044765-1 WHERE School = 'Galena'"}
 86 | {"text": "table: 1-11044765-1\ncolumns: School, Mascot, Location, League, Enrollment\nQ: What city and state is the Lancers mascot located?\nA: SELECT Location FROM 1-11044765-1 WHERE Mascot = 'Lancers'"}
 87 | {"text": "table: 1-11044765-1\ncolumns: School, Mascot, Location, League, Enrollment\nQ: What city and state are the miners located in?\nA: SELECT Location FROM 1-11044765-1 WHERE Mascot = 'Miners'"}
 88 | {"text": "table: 1-11044765-1\ncolumns: School, Mascot, Location, League, Enrollment\nQ: Which school has the Raiders as their mascot?\nA: SELECT School FROM 1-11044765-1 WHERE Mascot = 'Raiders'"}
 89 | {"text": "table: 1-1121352-2\ncolumns: No., Date, Tournament, Winning score, To par, Margin of victory, Runner(s)-up\nQ: Where was the tournament dated nov 3, 2002?\nA: SELECT Tournament FROM 1-1121352-2 WHERE Date = 'Nov 3, 2002'"}
 90 | {"text": "table: 1-1121352-2\ncolumns: No., Date, Tournament, Winning score, To par, Margin of victory, Runner(s)-up\nQ: Where is the margin of victory dated mar 28, 2004?\nA: SELECT Margin of victory FROM 1-1121352-2 WHERE Date = 'Mar 28, 2004'"}
 91 | {"text": "table: 1-1121352-2\ncolumns: No., Date, Tournament, Winning score, To par, Margin of victory, Runner(s)-up\nQ: What is the to par dated may 4, 2003?\nA: SELECT To par FROM 1-1121352-2 WHERE Date = 'May 4, 2003'"}
 92 | {"text": "table: 1-1121352-2\ncolumns: No., Date, Tournament, Winning score, To par, Margin of victory, Runner(s)-up\nQ: What date were the runner ups pat hurst juli inkster?\nA: SELECT Date FROM 1-1121352-2 WHERE Runner(s)-up = 'Pat Hurst Juli Inkster'"}
 93 | {"text": "table: 1-11210576-4\ncolumns: Character, Fate, Actor, First Episode, Final Episode, Duration, Final Episode Count\nQ: what's the total number of\u00a0final epbeingode count\u00a0with\u00a0character\u00a0being rick stetler\nA: SELECT COUNT Final Episode Count FROM 1-11210576-4 WHERE Character = 'Rick Stetler'"}
 94 | {"text": "table: 1-11210576-4\ncolumns: Character, Fate, Actor, First Episode, Final Episode, Duration, Final Episode Count\nQ: what are all the actor where first episode is \"ambush\"\nA: SELECT Actor FROM 1-11210576-4 WHERE First Episode = '\"Ambush\"'"}
 95 | {"text": "table: 1-11210576-4\ncolumns: Character, Fate, Actor, First Episode, Final Episode, Duration, Final Episode Count\nQ: what's the\u00a0character\u00a0with\u00a0fate\u00a0being deceased: knife wound\nA: SELECT Character FROM 1-11210576-4 WHERE Fate = 'Deceased: Knife Wound'"}
 96 | {"text": "table: 1-11210576-4\ncolumns: Character, Fate, Actor, First Episode, Final Episode, Duration, Final Episode Count\nQ: what's the total number of\u00a0final epbeingode count\u00a0with\u00a0first epbeingode\u00a0being \"l.a.\"\nA: SELECT COUNT Final Episode Count FROM 1-11210576-4 WHERE First Episode = '\"L.A.\"'"}
 97 | {"text": "table: 1-11210576-4\ncolumns: Character, Fate, Actor, First Episode, Final Episode, Duration, Final Episode Count\nQ: what's the\u00a0actor\u00a0with\u00a0character\u00a0being judge joseph ratner\nA: SELECT Actor FROM 1-11210576-4 WHERE Character = 'Judge Joseph Ratner'"}
 98 | {"text": "table: 1-11210576-4\ncolumns: Character, Fate, Actor, First Episode, Final Episode, Duration, Final Episode Count\nQ: what's the\u00a0first epbeingode\u00a0with\u00a0final epbeingode\u00a0being \"rio\"\nA: SELECT First Episode FROM 1-11210576-4 WHERE Final Episode = '\"Rio\"'"}
 99 | {"text": "table: 1-11214772-2\ncolumns: Year, Champion, Score, Runner-Up, Location, Semi-Finalist #1, Semi-Finalist #2\nQ: Which team was the second semi finalist in 2007?\nA: SELECT Semi-Finalist #2 FROM 1-11214772-2 WHERE Year = 2007"}
100 | {"text": "table: 1-11214772-2\ncolumns: Year, Champion, Score, Runner-Up, Location, Semi-Finalist #1, Semi-Finalist #2\nQ: How many teams were listed as runner up in 2005 and there the first semi finalist was Western Carolina?\nA: SELECT COUNT Runner-Up FROM 1-11214772-2 WHERE Semi-Finalist #1 = 'Western Carolina' AND Year = 2005"}
101 | 


--------------------------------------------------------------------------------
/Data/lora/wikisql.py:
--------------------------------------------------------------------------------
  1 | # Copyright © 2023 Apple Inc.
  2 | 
  3 | """
  4 | Code to preprocess the WikiSQL dataset adapted from
  5 | https://github.com/salesforce/WikiSQL and
  6 | https://huggingface.co/sqllama/sqllama-V0/blob/main/wikisql.ipynb .
  7 | """
  8 | 
  9 | 
 10 | import json
 11 | import os
 12 | 
 13 | 
 14 | def load():
 15 |     """
 16 |     Load all three splits of the WikiSQL dataset.
 17 |     """
 18 |     return (WikiSQL(dn) for dn in ["train", "dev", "test"])
 19 | 
 20 | 
 21 | class WikiSQL:
 22 |     def __init__(self, dataset, save_dir="/tmp"):
 23 |         valid_sets = ("train", "dev", "test")
 24 |         if dataset not in valid_sets:
 25 |             raise ValueError(f"Dataset must be in {valid_sets}, got {dataset}")
 26 |         data_dir = os.path.join(save_dir, "wikisql")
 27 |         self._maybe_download(data_dir)
 28 | 
 29 |         self._parse_tables(os.path.join(data_dir, f"data/{dataset}.tables.jsonl"))
 30 |         self._parse_queries(os.path.join(data_dir, f"data/{dataset}.jsonl"))
 31 | 
 32 |     def _maybe_download(self, data_dir):
 33 |         if not os.path.exists(data_dir):
 34 |             import io
 35 |             import tarfile
 36 |             from urllib import request
 37 | 
 38 |             url = "https://raw.githubusercontent.com/salesforce/WikiSQL/master/data.tar.bz2"
 39 |             r = request.urlopen(url)
 40 |             with tarfile.open(fileobj=io.BytesIO(r.read())) as tf:
 41 |                 tf.extractall(data_dir)
 42 | 
 43 |     def _parse_tables(self, tables):
 44 |         self._tables = {}
 45 |         with open(tables) as f:
 46 |             for line in f:
 47 |                 table = json.loads(line)
 48 |                 self._tables[table["id"]] = {
 49 |                     "columns": table["header"],
 50 |                     "types": table["types"],
 51 |                     "desc": f"table: {table['id']}\ncolumns: {', '.join(table['header'])}",
 52 |                 }
 53 | 
 54 |     def _parse_queries(self, queries):
 55 |         self._queries = []
 56 |         with open(queries) as f:
 57 |             for line in f:
 58 |                 query = json.loads(line)
 59 |                 table = self._tables[query["table_id"]]
 60 |                 question = query["question"]
 61 |                 answer = self.query_to_text(
 62 |                     query["sql"], query["table_id"], table["columns"], table["types"]
 63 |                 )
 64 |                 self._queries.append(
 65 |                     f"<s>{table['desc']}\nQ: {question}\nA: {answer}</s>"
 66 |                 )
 67 | 
 68 |     def query_to_text(self, query, table, columns, types):
 69 |         aggregation_ops = ["", "MAX", "MIN", "COUNT", "SUM", "AVG"]
 70 |         condition_ops = ["=", ">", "<", "OP"]
 71 |         column = columns[query["sel"]]
 72 |         aggregation = (aggregation_ops[query["agg"]] + " ") if query["agg"] > 0 else ""
 73 |         sql = f"SELECT {aggregation}{column} FROM {table}"
 74 | 
 75 |         conditions = query["conds"]
 76 |         if conditions:
 77 |             cs = []
 78 |             for i, o, v in conditions:
 79 |                 column = columns[i]
 80 |                 op = condition_ops[o]
 81 | 
 82 |                 if types[i] == "text":
 83 |                     value = f"'{v}'"
 84 |                 else:
 85 |                     value = v
 86 |                 cs.append(f"{column} {op} {value}")
 87 | 
 88 |             sql += " WHERE " + " AND ".join(cs)
 89 | 
 90 |         return sql
 91 | 
 92 |     def __getitem__(self, idx):
 93 |         return self._queries[idx]
 94 | 
 95 |     def __len__(self):
 96 |         return len(self._queries)
 97 | 
 98 | 
 99 | if __name__ == "__main__":
100 |     datanames = ["train", "dev", "test"]
101 |     sizes = [56355, 8421, 15878]
102 |     for dataname, size in zip(datanames, sizes):
103 |         len(WikiSQL(dataname)) == size, f"Wrong {dataname} set size."
104 | 
105 |     # Write the sets to jsonl
106 |     import json
107 | 
108 |     train, dev, test = load()
109 |     datasets = [
110 |         (train, "train", 1000),
111 |         (dev, "valid", 100),
112 |         (test, "test", 100),
113 |     ]
114 |     for dataset, name, size in datasets:
115 |         with open(f"data/{name}.jsonl", "w") as fid:
116 |             for e, t in zip(range(size), dataset):
117 |                 # Strip the <s>, </s> since the tokenizer adds them
118 |                 json.dump({"text": t[3:-4]}, fid)
119 |                 fid.write("\n")
120 | 


--------------------------------------------------------------------------------
/LICENSE:
--------------------------------------------------------------------------------
 1 | MIT License
 2 | 
 3 | Copyright (c) 2024 ml-explore
 4 | 
 5 | Permission is hereby granted, free of charge, to any person obtaining a copy
 6 | of this software and associated documentation files (the "Software"), to deal
 7 | in the Software without restriction, including without limitation the rights
 8 | to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 9 | copies of the Software, and to permit persons to whom the Software is
10 | furnished to do so, subject to the following conditions:
11 | 
12 | The above copyright notice and this permission notice shall be included in all
13 | copies or substantial portions of the Software.
14 | 
15 | THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
16 | IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
17 | FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
18 | AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
19 | LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
20 | OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
21 | SOFTWARE.
22 | 


--------------------------------------------------------------------------------
/Libraries/Embedders/BaseConfiguration.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2025 Apple Inc.
  2 | 
  3 | import Foundation
  4 | 
  5 | /// Base ``LanguageModel`` configuration -- provides `modelType`
  6 | /// and `quantization` (used in loading the model).
  7 | ///
  8 | /// This is used by ``ModelFactory/load(hub:configuration:progressHandler:)``
  9 | /// to determine the type of model to load.
 10 | public struct BaseConfiguration: Codable, Sendable {
 11 |     public let modelType: String
 12 | 
 13 |     public struct Quantization: Codable, Sendable, Equatable {
 14 |         public init(groupSize: Int, bits: Int) {
 15 |             self.groupSize = groupSize
 16 |             self.bits = bits
 17 |         }
 18 | 
 19 |         public let groupSize: Int
 20 |         public let bits: Int
 21 | 
 22 |         public var asTuple: (Int, Int) { (groupSize, bits) }
 23 | 
 24 |         enum CodingKeys: String, CodingKey {
 25 |             case groupSize = "group_size"
 26 |             case bits = "bits"
 27 |         }
 28 |     }
 29 | 
 30 |     /// handling instructions for ``PerLayerQuantization``
 31 |     public enum QuantizationOption: Sendable {
 32 |         case skip
 33 |         case quantize(Quantization)
 34 |     }
 35 | 
 36 |     /// Per-layer ``Quantization`` values with optional default.
 37 |     public struct PerLayerQuantization: Sendable {
 38 |         public var quantization: Quantization? = nil
 39 |         public var perLayerQuantization: [String: QuantizationOption]
 40 | 
 41 |         public init(
 42 |             quantization: BaseConfiguration.Quantization? = nil,
 43 |             perLayerQuantization: [String: BaseConfiguration.QuantizationOption]
 44 |         ) {
 45 |             self.quantization = quantization
 46 |             self.perLayerQuantization = perLayerQuantization
 47 |         }
 48 | 
 49 |         /// The quantization to apply for the given layer name or nil for no quantization.
 50 |         public func quantization(layer: String) -> Quantization? {
 51 |             if let perLayer = perLayerQuantization[layer] {
 52 |                 switch perLayer {
 53 |                 case .skip:
 54 |                     return nil
 55 |                 case .quantize(let quantization):
 56 |                     return quantization
 57 |                 }
 58 |             } else {
 59 |                 return quantization
 60 |             }
 61 |         }
 62 |     }
 63 | 
 64 |     /// Special codable to support a mixed key: Int / key: Quantization
 65 |     /// structure for hereogenous quantization, e.g.
 66 |     ///
 67 |     /// ```
 68 |     /// "quantization": {
 69 |     ///     "group_size": 64,
 70 |     ///     "bits": 4,
 71 |     ///     "model.embed_tokens": {
 72 |     ///         "group_size": 32,
 73 |     ///         "bits": 4
 74 |     ///     },
 75 |     ///     "model.layers.0.self_attn.q_norm": false,
 76 |     /// ```
 77 |     ///
 78 |     /// This mixed type structure requires manual decoding.
 79 |     struct QuantizationContainer: Codable, Sendable {
 80 |         var quantization: Quantization
 81 |         var perLayerQuantization: PerLayerQuantization
 82 | 
 83 |         // based on Dictionary's coding key
 84 |         internal struct _DictionaryCodingKey: CodingKey {
 85 |             internal let stringValue: String
 86 |             internal let intValue: Int?
 87 | 
 88 |             internal init(stringValue: String) {
 89 |                 self.stringValue = stringValue
 90 |                 self.intValue = Int(stringValue)
 91 |             }
 92 | 
 93 |             internal init(intValue: Int) {
 94 |                 self.stringValue = "\(intValue)"
 95 |                 self.intValue = intValue
 96 |             }
 97 |         }
 98 | 
 99 |         init(from decoder: any Decoder) throws {
100 |             // handle the embedded Quantization
101 |             self.quantization = try Quantization(from: decoder)
102 | 
103 |             // and the interleaved per-layer values
104 |             var perLayerQuantization = [String: QuantizationOption]()
105 |             let container = try decoder.container(keyedBy: _DictionaryCodingKey.self)
106 |             for key in container.allKeys {
107 |                 switch key.stringValue {
108 |                 case Quantization.CodingKeys.groupSize.rawValue: continue
109 |                 case Quantization.CodingKeys.bits.rawValue: continue
110 | 
111 |                 default:
112 |                     if let f = try? container.decode(Bool.self, forKey: key) {
113 |                         if !f {
114 |                             perLayerQuantization[key.stringValue] = .skip
115 |                         }
116 |                     } else {
117 |                         perLayerQuantization[key.stringValue] = .quantize(
118 |                             try container.decode(Quantization.self, forKey: key))
119 |                     }
120 |                 }
121 |             }
122 |             self.perLayerQuantization = PerLayerQuantization(
123 |                 quantization: quantization, perLayerQuantization: perLayerQuantization)
124 |         }
125 | 
126 |         func encode(to encoder: any Encoder) throws {
127 |             try quantization.encode(to: encoder)
128 | 
129 |             var container = encoder.container(keyedBy: _DictionaryCodingKey.self)
130 |             for (key, value) in perLayerQuantization.perLayerQuantization {
131 |                 switch value {
132 |                 case .skip:
133 |                     try container.encode(false, forKey: .init(stringValue: key))
134 |                 case .quantize(let q):
135 |                     try container.encode(q, forKey: .init(stringValue: key))
136 |                 }
137 |             }
138 |         }
139 |     }
140 | 
141 |     var quantizationContainer: QuantizationContainer?
142 | 
143 |     @available(*, deprecated, message: "Please use perLayerQuantization instead")
144 |     public var quantization: Quantization? {
145 |         quantizationContainer?.quantization
146 |     }
147 | 
148 |     public var perLayerQuantization: PerLayerQuantization? {
149 |         quantizationContainer?.perLayerQuantization
150 |     }
151 | 
152 |     enum CodingKeys: String, CodingKey {
153 |         case modelType = "model_type"
154 |         case quantizationContainer = "quantization"
155 |     }
156 | }
157 | 


--------------------------------------------------------------------------------
/Libraries/Embedders/Bert.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import MLX
  4 | import MLXNN
  5 | 
  6 | extension MLXArray {
  7 |     public static func arange(_ size: Int) -> MLXArray {
  8 |         return MLXArray(Array(0 ..< size))
  9 |     }
 10 | }
 11 | 
 12 | private class BertEmbedding: Module {
 13 | 
 14 |     let typeVocabularySize: Int
 15 |     @ModuleInfo(key: "word_embeddings") var wordEmbeddings: Embedding
 16 |     @ModuleInfo(key: "norm") var norm: LayerNorm
 17 |     @ModuleInfo(key: "token_type_embeddings") var tokenTypeEmbeddings: Embedding?
 18 |     @ModuleInfo(key: "position_embeddings") var positionEmbeddings: Embedding
 19 | 
 20 |     init(_ config: BertConfiguration) {
 21 |         typeVocabularySize = config.typeVocabularySize
 22 |         _wordEmbeddings.wrappedValue = Embedding(
 23 |             embeddingCount: config.vocabularySize, dimensions: config.embedDim)
 24 |         _norm.wrappedValue = LayerNorm(
 25 |             dimensions: config.embedDim, eps: config.layerNormEps)
 26 |         if config.typeVocabularySize > 0 {
 27 |             _tokenTypeEmbeddings.wrappedValue = Embedding(
 28 |                 embeddingCount: config.typeVocabularySize,
 29 |                 dimensions: config.embedDim)
 30 |         }
 31 |         _positionEmbeddings.wrappedValue = Embedding(
 32 |             embeddingCount: config.maxPositionEmbeddings,
 33 |             dimensions: config.embedDim)
 34 | 
 35 |     }
 36 | 
 37 |     func callAsFunction(
 38 |         _ inputIds: MLXArray,
 39 |         positionIds: MLXArray? = nil,
 40 |         tokenTypeIds: MLXArray? = nil
 41 |     ) -> MLXArray {
 42 |         let posIds = positionIds ?? broadcast(MLXArray.arange(inputIds.dim(1)), to: inputIds.shape)
 43 |         var words = wordEmbeddings(inputIds) + positionEmbeddings(posIds)
 44 |         if let tokenTypeIds, let tokenTypeEmbeddings {
 45 |             words += tokenTypeEmbeddings(tokenTypeIds)
 46 |         }
 47 |         return norm(words)
 48 |     }
 49 | }
 50 | 
 51 | private class TransformerBlock: Module {
 52 |     let attention: MultiHeadAttention
 53 |     @ModuleInfo(key: "ln1") var preLayerNorm: LayerNorm
 54 |     @ModuleInfo(key: "ln2") var postLayerNorm: LayerNorm
 55 |     @ModuleInfo(key: "linear1") var up: Linear
 56 |     @ModuleInfo(key: "linear2") var down: Linear
 57 | 
 58 |     init(_ config: BertConfiguration) {
 59 |         attention = MultiHeadAttention(
 60 |             dimensions: config.embedDim, numHeads: config.numHeads, bias: true)
 61 |         _preLayerNorm.wrappedValue = LayerNorm(
 62 |             dimensions: config.embedDim, eps: config.layerNormEps)
 63 |         _postLayerNorm.wrappedValue = LayerNorm(
 64 |             dimensions: config.embedDim, eps: config.layerNormEps)
 65 |         _up.wrappedValue = Linear(config.embedDim, config.interDim)
 66 |         _down.wrappedValue = Linear(config.interDim, config.embedDim)
 67 |     }
 68 | 
 69 |     func callAsFunction(_ inputs: MLXArray, mask: MLXArray? = nil) -> MLXArray {
 70 |         let attentionOut = attention(inputs, keys: inputs, values: inputs, mask: mask)
 71 |         let preNorm = preLayerNorm(inputs + attentionOut)
 72 | 
 73 |         let mlpOut = down(gelu(up(preNorm)))
 74 |         return postLayerNorm(mlpOut + preNorm)
 75 |     }
 76 | }
 77 | 
 78 | private class Encoder: Module {
 79 |     let layers: [TransformerBlock]
 80 |     init(_ config: BertConfiguration) {
 81 |         precondition(config.vocabularySize > 0)
 82 |         layers = (0 ..< config.numLayers).map { _ in TransformerBlock(config) }
 83 |     }
 84 |     func callAsFunction(_ inputs: MLXArray, attentionMask: MLXArray? = nil) -> MLXArray {
 85 |         var outputs = inputs
 86 |         for layer in layers {
 87 |             outputs = layer(outputs, mask: attentionMask)
 88 |         }
 89 |         return outputs
 90 |     }
 91 | }
 92 | 
 93 | private class LMHead: Module {
 94 |     @ModuleInfo(key: "dense") var dense: Linear
 95 |     @ModuleInfo(key: "ln") var layerNorm: LayerNorm
 96 |     @ModuleInfo(key: "decoder") var decoder: Linear
 97 | 
 98 |     init(_ config: BertConfiguration) {
 99 |         _dense.wrappedValue = Linear(
100 |             config.embedDim, config.embedDim, bias: true)
101 |         _layerNorm.wrappedValue = LayerNorm(
102 |             dimensions: config.embedDim, eps: config.layerNormEps)
103 |         _decoder.wrappedValue = Linear(
104 |             config.embedDim, config.vocabularySize, bias: true)
105 |     }
106 |     func callAsFunction(_ inputs: MLXArray) -> MLXArray {
107 |         return decoder(layerNorm(silu(dense(inputs))))
108 |     }
109 | }
110 | 
111 | public class BertModel: Module, EmbeddingModel {
112 |     @ModuleInfo(key: "lm_head") fileprivate var lmHead: LMHead?
113 |     @ModuleInfo(key: "embeddings") fileprivate var embedder: BertEmbedding
114 |     let pooler: Linear?
115 |     fileprivate let encoder: Encoder
116 |     public var vocabularySize: Int
117 | 
118 |     public init(
119 |         _ config: BertConfiguration, lmHead: Bool = false
120 |     ) {
121 |         precondition(config.vocabularySize > 0)
122 |         vocabularySize = config.vocabularySize
123 |         encoder = Encoder(config)
124 |         _embedder.wrappedValue = BertEmbedding(config)
125 | 
126 |         if lmHead {
127 |             _lmHead.wrappedValue = LMHead(config)
128 |             self.pooler = nil
129 |         } else {
130 |             pooler = Linear(config.embedDim, config.embedDim)
131 |             _lmHead.wrappedValue = nil
132 |         }
133 |     }
134 | 
135 |     public func callAsFunction(
136 |         _ inputs: MLXArray, positionIds: MLXArray? = nil, tokenTypeIds: MLXArray? = nil,
137 |         attentionMask: MLXArray? = nil
138 |     )
139 |         -> EmbeddingModelOutput
140 |     {
141 |         var inp = inputs
142 |         if inp.ndim == 1 {
143 |             inp = inp.reshaped(1, -1)
144 |         }
145 |         var mask = attentionMask
146 |         if mask != nil {
147 |             mask = mask!.asType(embedder.wordEmbeddings.weight.dtype).expandedDimensions(axes: [
148 |                 1, 2,
149 |             ]).log()
150 |         }
151 |         let outputs = encoder(
152 |             embedder(inp, positionIds: positionIds, tokenTypeIds: tokenTypeIds),
153 |             attentionMask: mask)
154 |         if let lmHead {
155 |             return EmbeddingModelOutput(hiddenStates: lmHead(outputs), pooledOutput: nil)
156 |         } else {
157 |             return EmbeddingModelOutput(
158 |                 hiddenStates: outputs, pooledOutput: tanh(pooler!(outputs[0..., 0])))
159 |         }
160 |     }
161 | 
162 |     public func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {
163 |         weights.reduce(into: [:]) { result, item in
164 |             var key = item.key.replacingOccurrences(of: ".layer.", with: ".layers.")
165 |             key = key.replacingOccurrences(of: ".self.key.", with: ".key_proj.")
166 |             key = key.replacingOccurrences(of: ".self.query.", with: ".query_proj.")
167 |             key = key.replacingOccurrences(of: ".self.value.", with: ".value_proj.")
168 |             key = key.replacingOccurrences(
169 |                 of: ".attention.output.dense.", with: ".attention.out_proj.")
170 |             key = key.replacingOccurrences(of: ".attention.output.LayerNorm.", with: ".ln1.")
171 |             key = key.replacingOccurrences(of: ".output.LayerNorm.", with: ".ln2.")
172 |             key = key.replacingOccurrences(of: ".intermediate.dense.", with: ".linear1.")
173 |             key = key.replacingOccurrences(of: ".output.dense.", with: ".linear2.")
174 |             key = key.replacingOccurrences(of: ".LayerNorm.", with: ".norm.")
175 |             key = key.replacingOccurrences(of: "pooler.dense.", with: "pooler.")
176 |             key = key.replacingOccurrences(
177 |                 of:
178 |                     "cls.predictions.transform.dense.",
179 |                 with: "lm_head.dense.")
180 |             key = key.replacingOccurrences(
181 |                 of:
182 |                     "cls.predictions.transform.LayerNorm.",
183 |                 with: "lm_head.ln.")
184 |             key = key.replacingOccurrences(
185 |                 of:
186 |                     "cls.predictions.decoder",
187 |                 with: "lm_head.decoder")
188 |             key = key.replacingOccurrences(
189 |                 of: "cls.predictions.transform.norm.weight",
190 |                 with: "lm_head.ln.weight")
191 |             key = key.replacingOccurrences(
192 |                 of: "cls.predictions.transform.norm.bias",
193 |                 with: "lm_head.ln.bias")
194 |             key = key.replacingOccurrences(of: "cls.predictions.bias", with: "lm_head.decoder.bias")
195 |             key = key.replacingOccurrences(of: "bert.", with: "")
196 |             result[key] = item.value
197 |         }.filter { key, _ in key != "embeddings.position_ids" }
198 |     }
199 | }
200 | 
201 | public class DistilBertModel: BertModel {
202 |     public override func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {
203 |         weights.reduce(into: [:]) { result, item in
204 |             var key = item.key.replacingOccurrences(of: ".layer.", with: ".layers.")
205 |             key = key.replacingOccurrences(of: "transformer.", with: "encoder.")
206 |             key = key.replacingOccurrences(of: "embeddings.LayerNorm", with: "embeddings.norm")
207 |             key = key.replacingOccurrences(of: ".attention.q_lin.", with: ".attention.query_proj.")
208 |             key = key.replacingOccurrences(of: ".attention.k_lin.", with: ".attention.key_proj.")
209 |             key = key.replacingOccurrences(of: ".attention.v_lin.", with: ".attention.value_proj.")
210 |             key = key.replacingOccurrences(of: ".attention.out_lin.", with: ".attention.out_proj.")
211 |             key = key.replacingOccurrences(of: ".sa_layer_norm.", with: ".ln1.")
212 |             key = key.replacingOccurrences(of: ".ffn.lin1.", with: ".linear1.")
213 |             key = key.replacingOccurrences(of: ".ffn.lin2.", with: ".linear2.")
214 |             key = key.replacingOccurrences(of: ".output_layer_norm.", with: ".ln2.")
215 |             key = key.replacingOccurrences(of: "vocab_transform", with: "lm_head.dense")
216 |             key = key.replacingOccurrences(of: "vocab_layer_norm", with: "lm_head.ln")
217 |             key = key.replacingOccurrences(of: "vocab_projector", with: "lm_head.decoder")
218 |             key = key.replacingOccurrences(of: "distilbert.", with: "")
219 |             result[key] = item.value
220 |         }.filter { key, _ in key != "embeddings.position_ids" }
221 |     }
222 | }
223 | 
224 | public struct BertConfiguration: Decodable, Sendable {
225 |     var layerNormEps: Float = 1e-12
226 |     var maxTrainedPositions: Int = 2048
227 |     var embedDim: Int = 768
228 |     var numHeads: Int = 12
229 |     var interDim: Int = 3072
230 |     var numLayers: Int = 12
231 |     var typeVocabularySize: Int = 2
232 |     var vocabularySize: Int = 30528
233 |     var maxPositionEmbeddings: Int = 0
234 |     var modelType: String
235 | 
236 |     enum CodingKeys: String, CodingKey {
237 |         case layerNormEps = "layer_norm_eps"
238 |         case maxTrainedPositions = "max_trained_positions"
239 |         case vocabularySize = "vocab_size"
240 |         case maxPositionEmbeddings = "max_position_embeddings"
241 |         case modelType = "model_type"
242 |     }
243 | 
244 |     enum BertCodingKeys: String, CodingKey {
245 |         case embedDim = "hidden_size"
246 |         case numHeads = "num_attention_heads"
247 |         case interDim = "intermediate_size"
248 |         case numLayers = "num_hidden_layers"
249 |         case typeVocabularySize = "type_vocab_size"
250 |     }
251 | 
252 |     enum DistilBertCodingKeys: String, CodingKey {
253 |         case embedDim = "dim"
254 |         case numLayers = "n_layers"
255 |         case numHeads = "n_heads"
256 |         case interDim = "hidden_dim"
257 |     }
258 | 
259 |     public init(from decoder: Decoder) throws {
260 |         let container: KeyedDecodingContainer<CodingKeys> =
261 |             try decoder.container(
262 |                 keyedBy: CodingKeys.self)
263 |         layerNormEps =
264 |             try container.decodeIfPresent(
265 |                 Float.self,
266 |                 forKey: CodingKeys.layerNormEps.self)
267 |             ?? 1e-12
268 |         maxTrainedPositions =
269 |             try container.decodeIfPresent(
270 |                 Int.self,
271 |                 forKey: CodingKeys.maxTrainedPositions
272 |                     .self) ?? 2048
273 |         vocabularySize =
274 |             try container.decodeIfPresent(
275 |                 Int.self,
276 |                 forKey: CodingKeys.vocabularySize.self)
277 |             ?? 30528
278 |         maxPositionEmbeddings =
279 |             try container.decodeIfPresent(
280 |                 Int.self,
281 |                 forKey: CodingKeys.maxPositionEmbeddings
282 |                     .self) ?? 0
283 |         modelType = try container.decode(String.self, forKey: CodingKeys.modelType.self)
284 | 
285 |         if modelType == "distilbert" {
286 |             let distilBertConfig: KeyedDecodingContainer<DistilBertCodingKeys> =
287 |                 try decoder.container(
288 |                     keyedBy: DistilBertCodingKeys.self)
289 |             embedDim =
290 |                 try distilBertConfig.decodeIfPresent(
291 |                     Int.self,
292 |                     forKey: DistilBertCodingKeys.embedDim.self) ?? 768
293 |             numHeads =
294 |                 try distilBertConfig.decodeIfPresent(
295 |                     Int.self,
296 |                     forKey: DistilBertCodingKeys.numHeads.self) ?? 12
297 |             interDim =
298 |                 try distilBertConfig.decodeIfPresent(
299 |                     Int.self, forKey: DistilBertCodingKeys.interDim.self)
300 |                 ?? 3072
301 |             numLayers =
302 |                 try distilBertConfig.decodeIfPresent(
303 |                     Int.self,
304 |                     forKey: DistilBertCodingKeys.numLayers.self) ?? 12
305 |             typeVocabularySize = 0
306 |         } else {
307 |             let bertConfig: KeyedDecodingContainer<BertCodingKeys> = try decoder.container(
308 |                 keyedBy: BertCodingKeys.self)
309 | 
310 |             embedDim =
311 |                 try bertConfig.decodeIfPresent(
312 |                     Int.self,
313 |                     forKey: BertCodingKeys.embedDim.self) ?? 768
314 |             numHeads =
315 |                 try bertConfig.decodeIfPresent(
316 |                     Int.self,
317 |                     forKey: BertCodingKeys.numHeads.self) ?? 12
318 |             interDim =
319 |                 try bertConfig.decodeIfPresent(
320 |                     Int.self, forKey: BertCodingKeys.interDim.self)
321 |                 ?? 3072
322 |             numLayers =
323 |                 try bertConfig.decodeIfPresent(
324 |                     Int.self,
325 |                     forKey: BertCodingKeys.numLayers.self) ?? 12
326 |             typeVocabularySize =
327 |                 try bertConfig.decodeIfPresent(
328 |                     Int.self,
329 |                     forKey: BertCodingKeys.typeVocabularySize
330 |                         .self) ?? 2
331 |         }
332 |     }
333 | }
334 | 


--------------------------------------------------------------------------------
/Libraries/Embedders/Configuration.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import Foundation
  4 | 
  5 | public enum StringOrNumber: Codable, Equatable, Sendable {
  6 |     case string(String)
  7 |     case float(Float)
  8 | 
  9 |     public init(from decoder: Decoder) throws {
 10 |         let values = try decoder.singleValueContainer()
 11 | 
 12 |         if let v = try? values.decode(Float.self) {
 13 |             self = .float(v)
 14 |         } else {
 15 |             let v = try values.decode(String.self)
 16 |             self = .string(v)
 17 |         }
 18 |     }
 19 | 
 20 |     public func encode(to encoder: Encoder) throws {
 21 |         var container = encoder.singleValueContainer()
 22 |         switch self {
 23 |         case .string(let v): try container.encode(v)
 24 |         case .float(let v): try container.encode(v)
 25 |         }
 26 |     }
 27 | }
 28 | 
 29 | private class ModelTypeRegistry: @unchecked Sendable {
 30 | 
 31 |     // Note: Using NSLock as we have very small (just dictionary get/set)
 32 |     // critical sections and expect no contention. This allows the methods
 33 |     // to remain synchronous.
 34 |     private let lock = NSLock()
 35 | 
 36 |     private var creators: [String: @Sendable (URL) throws -> EmbeddingModel] = [
 37 |         "bert": {
 38 |             url in
 39 |             let configuration = try JSONDecoder().decode(
 40 |                 BertConfiguration.self, from: Data(contentsOf: url))
 41 |             let model = BertModel(configuration)
 42 |             return model
 43 |         },
 44 |         "roberta": {
 45 |             url in
 46 |             let configuration = try JSONDecoder().decode(
 47 |                 BertConfiguration.self, from: Data(contentsOf: url))
 48 |             let model = BertModel(configuration)
 49 |             return model
 50 |         },
 51 |         "xlm-roberta": {
 52 |             url in
 53 |             let configuration = try JSONDecoder().decode(
 54 |                 BertConfiguration.self, from: Data(contentsOf: url))
 55 |             let model = BertModel(configuration)
 56 |             return model
 57 |         },
 58 |         "distilbert": {
 59 |             url in
 60 |             let configuration = try JSONDecoder().decode(
 61 |                 BertConfiguration.self, from: Data(contentsOf: url))
 62 |             let model = BertModel(configuration)
 63 |             return model
 64 |         },
 65 |         "nomic_bert": {
 66 |             url in
 67 |             let configuration = try JSONDecoder().decode(
 68 |                 NomicBertConfiguration.self, from: Data(contentsOf: url))
 69 |             let model = NomicBertModel(configuration)
 70 |             return model
 71 |         },
 72 |     ]
 73 | 
 74 |     public func registerModelType(
 75 |         _ type: String, creator: @Sendable @escaping (URL) throws -> EmbeddingModel
 76 |     ) {
 77 |         lock.withLock {
 78 |             creators[type] = creator
 79 |         }
 80 |     }
 81 | 
 82 |     public func createModel(configuration: URL, rawValue: String) throws -> EmbeddingModel {
 83 |         let creator = lock.withLock {
 84 |             creators[rawValue]
 85 |         }
 86 |         guard let creator else {
 87 |             throw EmbedderError(message: "Unsupported model type.")
 88 |         }
 89 |         return try creator(configuration)
 90 |     }
 91 | 
 92 | }
 93 | 
 94 | private let modelTypeRegistry = ModelTypeRegistry()
 95 | 
 96 | public struct ModelType: RawRepresentable, Codable, Sendable {
 97 |     public let rawValue: String
 98 | 
 99 |     public init(rawValue: String) {
100 |         self.rawValue = rawValue
101 |     }
102 | 
103 |     public static func registerModelType(
104 |         _ type: String, creator: @Sendable @escaping (URL) throws -> EmbeddingModel
105 |     ) {
106 |         modelTypeRegistry.registerModelType(type, creator: creator)
107 |     }
108 | 
109 |     public func createModel(configuration: URL) throws -> EmbeddingModel {
110 |         try modelTypeRegistry.createModel(configuration: configuration, rawValue: rawValue)
111 |     }
112 | }
113 | 


--------------------------------------------------------------------------------
/Libraries/Embedders/EmbeddingModel.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import Foundation
  4 | @preconcurrency import Hub
  5 | import MLX
  6 | import MLXNN
  7 | import Tokenizers
  8 | 
  9 | /// Container for models that guarantees single threaded access.
 10 | ///
 11 | /// Wrap models used by e.g. the UI in a ModelContainer. Callers can access
 12 | /// the model and/or tokenizer:
 13 | ///
 14 | /// ```swift
 15 | /// let promptTokens = await modelContainer.perform { _, tokenizer in
 16 | ///     tokenizer.encode(text: prompt)
 17 | /// }
 18 | /// ```
 19 | ///
 20 | /// or:
 21 | ///
 22 | /// ```swift
 23 | /// let result = await modelContainer.perform { model, tokenizer in
 24 | ///     LLM.generate(
 25 | ///         promptTokens: promptTokens, parameters: generateParameters, model: model,
 26 | ///         tokenizer: tokenizer, extraEOSTokens: modelConfiguration.extraEOSTokens
 27 | ///     ) { tokens in
 28 | ///     ...
 29 | ///     }
 30 | /// }
 31 | /// ```
 32 | public actor ModelContainer {
 33 |     let model: EmbeddingModel
 34 |     let tokenizer: Tokenizer
 35 |     let pooler: Pooling
 36 | 
 37 |     public init(
 38 |         model: EmbeddingModel, tokenizer: Tokenizer, pooler: Pooling = Pooling(strategy: .none)
 39 |     ) {
 40 |         self.model = model
 41 |         self.tokenizer = tokenizer
 42 |         self.pooler = pooler
 43 |     }
 44 | 
 45 |     /// build the model and tokenizer without passing non-sendable data over isolation barriers
 46 |     public init(
 47 |         hub: HubApi, modelDirectory: URL, configuration: ModelConfiguration
 48 |     ) async throws {
 49 |         self.model = try loadSynchronous(modelDirectory: modelDirectory)
 50 | 
 51 |         let (tokenizerConfig, tokenizerData) = try await loadTokenizerConfig(
 52 |             configuration: configuration, hub: hub)
 53 |         self.tokenizer = try PreTrainedTokenizer(
 54 |             tokenizerConfig: tokenizerConfig, tokenizerData: tokenizerData)
 55 |         self.pooler = loadPooling(modelDirectory: modelDirectory)  //?? Pooling(strategy: .none)
 56 |     }
 57 | 
 58 |     /// Perform an action on the model and/or tokenizer. Callers _must_ eval any `MLXArray` before returning as
 59 |     /// `MLXArray` is not `Sendable`.
 60 |     public func perform<R>(_ action: @Sendable (EmbeddingModel, Tokenizer, Pooling) throws -> R)
 61 |         rethrows
 62 |         -> R
 63 |     {
 64 |         try action(model, tokenizer, pooler)
 65 |     }
 66 | }
 67 | 
 68 | extension Module {
 69 | 
 70 |     /// Compute the number of parameters in a possibly quantized model
 71 |     public func numParameters() -> Int {
 72 |         return leafModules().flattenedValues().map {
 73 |             mod -> Int in
 74 |             if let qlin = mod as? QuantizedLinear {
 75 |                 return qlin.scales.size * qlin.groupSize
 76 |             } else if let qemb = mod as? QuantizedEmbedding {
 77 |                 return qemb.scales.size * qemb.groupSize
 78 |             } else {
 79 |                 return mod.parameters().flattenedValues().reduce(
 80 |                     0,
 81 |                     {
 82 |                         $0 + $1.size
 83 |                     })
 84 |             }
 85 |         }.reduce(0, +)
 86 |     }
 87 | }
 88 | 
 89 | public struct EmbeddingModelOutput {
 90 |     let hiddenStates: MLXArray?
 91 |     let pooledOutput: MLXArray?
 92 | }
 93 | 
 94 | public protocol EmbeddingModel: Module {
 95 |     var vocabularySize: Int { get }
 96 |     func callAsFunction(
 97 |         _ inputs: MLXArray, positionIds: MLXArray?, tokenTypeIds: MLXArray?,
 98 |         attentionMask: MLXArray?
 99 |     ) -> EmbeddingModelOutput
100 |     /// Optionally preprocess the weights and modify / remove values as needed.
101 |     func sanitize(weights: [String: MLXArray]) -> [String: MLXArray]
102 | }
103 | 
104 | extension EmbeddingModel {
105 |     func callAsFunction(
106 |         _ inputs: MLXArray, positionIds: MLXArray? = nil, tokenTypeIds: MLXArray? = nil,
107 |         attentionMask: MLXArray? = nil
108 |     ) -> EmbeddingModelOutput {
109 |         return callAsFunction(
110 |             inputs, positionIds: positionIds, tokenTypeIds: tokenTypeIds,
111 |             attentionMask: attentionMask)
112 |     }
113 | }
114 | 


--------------------------------------------------------------------------------
/Libraries/Embedders/Load.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import Foundation
  4 | @preconcurrency import Hub
  5 | import MLX
  6 | import MLXNN
  7 | import Tokenizers
  8 | 
  9 | struct EmbedderError: Error {
 10 |     let message: String
 11 | }
 12 | 
 13 | func prepareModelDirectory(
 14 |     hub: HubApi, configuration: ModelConfiguration,
 15 |     progressHandler: @Sendable @escaping (Progress) -> Void
 16 | ) async throws -> URL {
 17 |     do {
 18 |         switch configuration.id {
 19 |         case .id(let id):
 20 |             // download the model weights
 21 |             let repo = Hub.Repo(id: id)
 22 |             let modelFiles = ["*.safetensors", "config.json"]
 23 |             return try await hub.snapshot(
 24 |                 from: repo, matching: modelFiles, progressHandler: progressHandler)
 25 | 
 26 |         case .directory(let directory):
 27 |             return directory
 28 |         }
 29 |     } catch Hub.HubClientError.authorizationRequired {
 30 |         // an authorizationRequired means (typically) that the named repo doesn't exist on
 31 |         // on the server so retry with local only configuration
 32 |         return configuration.modelDirectory(hub: hub)
 33 |     } catch {
 34 |         let nserror = error as NSError
 35 |         if nserror.domain == NSURLErrorDomain && nserror.code == NSURLErrorNotConnectedToInternet {
 36 |             // Error Domain=NSURLErrorDomain Code=-1009 "The Internet connection appears to be offline."
 37 |             // fall back to the local directory
 38 |             return configuration.modelDirectory(hub: hub)
 39 |         } else {
 40 |             throw error
 41 |         }
 42 |     }
 43 | }
 44 | 
 45 | /// Load and return the model and tokenizer
 46 | public func load(
 47 |     hub: HubApi = HubApi(), configuration: ModelConfiguration,
 48 |     progressHandler: @Sendable @escaping (Progress) -> Void = { _ in }
 49 | ) async throws -> (EmbeddingModel, Tokenizer) {
 50 |     let modelDirectory = try await prepareModelDirectory(
 51 |         hub: hub, configuration: configuration, progressHandler: progressHandler)
 52 |     let model = try loadSynchronous(modelDirectory: modelDirectory)
 53 |     let tokenizer = try await loadTokenizer(configuration: configuration, hub: hub)
 54 | 
 55 |     return (model, tokenizer)
 56 | }
 57 | 
 58 | func loadSynchronous(modelDirectory: URL) throws -> EmbeddingModel {
 59 |     // create the model (no weights loaded)
 60 |     let configurationURL = modelDirectory.appending(component: "config.json")
 61 |     let baseConfig = try JSONDecoder().decode(
 62 |         BaseConfiguration.self, from: Data(contentsOf: configurationURL))
 63 | 
 64 |     let modelType = ModelType(rawValue: baseConfig.modelType)
 65 |     let model = try modelType.createModel(configuration: configurationURL)
 66 | 
 67 |     // load the weights
 68 |     var weights = [String: MLXArray]()
 69 |     let enumerator = FileManager.default.enumerator(
 70 |         at: modelDirectory, includingPropertiesForKeys: nil)!
 71 |     for case let url as URL in enumerator {
 72 |         if url.pathExtension == "safetensors" {
 73 |             let w = try loadArrays(url: url)
 74 |             for (key, value) in w {
 75 |                 weights[key] = value
 76 |             }
 77 |         }
 78 |     }
 79 | 
 80 |     // per-model cleanup
 81 |     weights = model.sanitize(weights: weights)
 82 | 
 83 |     // quantize if needed
 84 |     if let perLayerQuantization = baseConfig.perLayerQuantization {
 85 |         quantize(model: model) { path, module in
 86 |             if weights["\(path).scales"] != nil {
 87 |                 return perLayerQuantization.quantization(layer: path)?.asTuple
 88 |             } else {
 89 |                 return nil
 90 |             }
 91 |         }
 92 |     }
 93 | 
 94 |     if let quantization = baseConfig.quantization {
 95 |         quantize(model: model, groupSize: quantization.groupSize, bits: quantization.bits) {
 96 |             path, module in
 97 |             weights["\(path).scales"] != nil
 98 |         }
 99 |     }
100 | 
101 |     // apply the loaded weights
102 |     let parameters = ModuleParameters.unflattened(weights)
103 |     try model.update(parameters: parameters, verify: [.all])
104 | 
105 |     eval(model)
106 | 
107 |     return model
108 | }
109 | 
110 | /// Load and return the model and tokenizer wrapped in a ``ModelContainer`` (provides
111 | /// thread safe access).
112 | public func loadModelContainer(
113 |     hub: HubApi = HubApi(), configuration: ModelConfiguration,
114 |     progressHandler: @Sendable @escaping (Progress) -> Void = { _ in }
115 | ) async throws -> ModelContainer {
116 |     let modelDirectory = try await prepareModelDirectory(
117 |         hub: hub, configuration: configuration, progressHandler: progressHandler)
118 |     return try await ModelContainer(
119 |         hub: hub, modelDirectory: modelDirectory, configuration: configuration)
120 | }
121 | 


--------------------------------------------------------------------------------
/Libraries/Embedders/Models.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import Foundation
  4 | import Hub
  5 | 
  6 | /// Registry of models and any overrides that go with them, e.g. prompt augmentation.
  7 | /// If asked for an unknown configuration this will use the model/tokenizer as is.
  8 | ///
  9 | /// The Python tokenizers have a very rich set of implementations and configuration. The
 10 | /// swift-tokenizers code handles a good chunk of that and this is a place to augment that
 11 | /// implementation, if needed.
 12 | public struct ModelConfiguration: Sendable {
 13 | 
 14 |     public enum Identifier: Sendable {
 15 |         case id(String)
 16 |         case directory(URL)
 17 |     }
 18 | 
 19 |     public var id: Identifier
 20 | 
 21 |     public var name: String {
 22 |         switch id {
 23 |         case .id(let string):
 24 |             string
 25 |         case .directory(let url):
 26 |             url.deletingLastPathComponent().lastPathComponent + "/" + url.lastPathComponent
 27 |         }
 28 |     }
 29 | 
 30 |     /// pull the tokenizer from an alternate id
 31 |     public let tokenizerId: String?
 32 | 
 33 |     /// overrides for TokenizerModel/knownTokenizers -- useful before swift-transformers is updated
 34 |     public let overrideTokenizer: String?
 35 | 
 36 |     public init(
 37 |         id: String, tokenizerId: String? = nil, overrideTokenizer: String? = nil
 38 |     ) {
 39 |         self.id = .id(id)
 40 |         self.tokenizerId = tokenizerId
 41 |         self.overrideTokenizer = overrideTokenizer
 42 |     }
 43 | 
 44 |     public init(
 45 |         directory: URL, tokenizerId: String? = nil, overrideTokenizer: String? = nil
 46 |     ) {
 47 |         self.id = .directory(directory)
 48 |         self.tokenizerId = tokenizerId
 49 |         self.overrideTokenizer = overrideTokenizer
 50 |     }
 51 | 
 52 |     public func modelDirectory(hub: HubApi = HubApi()) -> URL {
 53 |         switch id {
 54 |         case .id(let id):
 55 |             // download the model weights and config
 56 |             let repo = Hub.Repo(id: id)
 57 |             return hub.localRepoLocation(repo)
 58 | 
 59 |         case .directory(let directory):
 60 |             return directory
 61 |         }
 62 |     }
 63 | 
 64 |     @MainActor
 65 |     public static var registry = [String: ModelConfiguration]()
 66 | 
 67 |     @MainActor
 68 |     public static func register(configurations: [ModelConfiguration]) {
 69 |         bootstrap()
 70 | 
 71 |         for c in configurations {
 72 |             registry[c.name] = c
 73 |         }
 74 |     }
 75 | 
 76 |     @MainActor
 77 |     public static func configuration(id: String) -> ModelConfiguration {
 78 |         bootstrap()
 79 | 
 80 |         if let c = registry[id] {
 81 |             return c
 82 |         } else {
 83 |             return ModelConfiguration(id: id)
 84 |         }
 85 |     }
 86 | 
 87 |     @MainActor
 88 |     public static var models: some Collection<ModelConfiguration> & Sendable {
 89 |         bootstrap()
 90 |         return Self.registry.values
 91 |     }
 92 | }
 93 | 
 94 | extension ModelConfiguration {
 95 |     public static let bge_micro = ModelConfiguration(id: "TaylorAI/bge-micro-v2")
 96 |     public static let gte_tiny = ModelConfiguration(id: "TaylorAI/gte-tiny")
 97 |     public static let minilm_l6 = ModelConfiguration(id: "sentence-transformers/all-MiniLM-L6-v2")
 98 |     public static let snowflake_xs = ModelConfiguration(id: "Snowflake/snowflake-arctic-embed-xs")
 99 |     public static let minilm_l12 = ModelConfiguration(id: "sentence-transformers/all-MiniLM-L12-v2")
100 |     public static let bge_small = ModelConfiguration(id: "BAAI/bge-small-en-v1.5")
101 |     public static let multilingual_e5_small = ModelConfiguration(
102 |         id: "intfloat/multilingual-e5-small")
103 |     public static let bge_base = ModelConfiguration(id: "BAAI/bge-base-en-v1.5")
104 |     public static let nomic_text_v1 = ModelConfiguration(id: "nomic-ai/nomic-embed-text-v1")
105 |     public static let nomic_text_v1_5 = ModelConfiguration(id: "nomic-ai/nomic-embed-text-v1.5")
106 |     public static let bge_large = ModelConfiguration(id: "BAAI/bge-large-en-v1.5")
107 |     public static let snowflake_lg = ModelConfiguration(id: "Snowflake/snowflake-arctic-embed-l")
108 |     public static let bge_m3 = ModelConfiguration(id: "BAAI/bge-m3")
109 |     public static let mixedbread_large = ModelConfiguration(
110 |         id: "mixedbread-ai/mxbai-embed-large-v1")
111 | 
112 |     private enum BootstrapState: Sendable {
113 |         case idle
114 |         case bootstrapping
115 |         case bootstrapped
116 |     }
117 | 
118 |     @MainActor
119 |     static private var bootstrapState = BootstrapState.idle
120 | 
121 |     @MainActor
122 |     static func bootstrap() {
123 |         switch bootstrapState {
124 |         case .idle:
125 |             bootstrapState = .bootstrapping
126 |             register(configurations: [
127 |                 bge_micro,
128 |                 gte_tiny,
129 |                 minilm_l6,
130 |                 snowflake_xs,
131 |                 minilm_l12,
132 |                 bge_small,
133 |                 multilingual_e5_small,
134 |                 bge_base,
135 |                 nomic_text_v1,
136 |                 nomic_text_v1_5,
137 |                 bge_large,
138 |                 snowflake_lg,
139 |                 bge_m3,
140 |                 mixedbread_large,
141 |             ])
142 |             bootstrapState = .bootstrapped
143 | 
144 |         case .bootstrapping:
145 |             break
146 | 
147 |         case .bootstrapped:
148 |             break
149 |         }
150 |     }
151 | }
152 | 


--------------------------------------------------------------------------------
/Libraries/Embedders/NomicBert.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import Foundation
  4 | import MLX
  5 | import MLXNN
  6 | 
  7 | class NomicEmbedding: Module {
  8 | 
  9 |     let typeVocabularySize: Int
 10 |     @ModuleInfo(key: "word_embeddings") var wordEmbeddings: Embedding
 11 |     @ModuleInfo(key: "norm") var norm: LayerNorm
 12 |     @ModuleInfo(key: "token_type_embeddings") var tokenTypeEmbeddings: Embedding?
 13 |     @ModuleInfo(key: "position_embeddings") var positionEmbeddings: Embedding?
 14 | 
 15 |     init(_ config: NomicBertConfiguration) {
 16 |         typeVocabularySize = config.typeVocabularySize
 17 |         _wordEmbeddings.wrappedValue = Embedding(
 18 |             embeddingCount: config.vocabularySize, dimensions: config.embedDim)
 19 |         _norm.wrappedValue = LayerNorm(
 20 |             dimensions: config.embedDim, eps: config.layerNormEps)
 21 |         if config.typeVocabularySize > 0 {
 22 |             _tokenTypeEmbeddings.wrappedValue = Embedding(
 23 |                 embeddingCount: config.typeVocabularySize,
 24 |                 dimensions: config.embedDim)
 25 |         }
 26 |         if config.maxPositionEmbeddings > 0 {
 27 |             _positionEmbeddings.wrappedValue = Embedding(
 28 |                 embeddingCount: config.maxPositionEmbeddings,
 29 |                 dimensions: config.embedDim)
 30 |         }
 31 |     }
 32 | 
 33 |     func callAsFunction(
 34 |         _ inputIds: MLXArray, positionIds: MLXArray? = nil,
 35 |         tokenTypeIds: MLXArray? = nil
 36 |     ) -> MLXArray {
 37 |         var words = wordEmbeddings(inputIds)
 38 | 
 39 |         if let tokenTypeIds, let tokenTypeEmbeddings {
 40 |             words += tokenTypeEmbeddings(tokenTypeIds)
 41 |         }
 42 |         let positions =
 43 |             positionIds ?? broadcast(MLXArray.arange(inputIds.dim(1)), to: inputIds.shape)
 44 |         if let positionEmbeddings {
 45 |             words += positionEmbeddings(positions)
 46 |         }
 47 |         return norm(words)
 48 |     }
 49 | }
 50 | 
 51 | private class MLP: Module, UnaryLayer {
 52 |     @ModuleInfo(key: "fc11") var up: Linear
 53 |     @ModuleInfo(key: "fc12") var gate: Linear
 54 |     @ModuleInfo(key: "fc2") var down: Linear
 55 | 
 56 |     private static func scaledHiddenFeatures(config: NomicBertConfiguration)
 57 |         -> Int
 58 |     {
 59 |         let multipleOf = 256
 60 |         let hiddenFeatures: Int = config.MLPDim
 61 |         return (hiddenFeatures + multipleOf - 1) / multipleOf * multipleOf
 62 |     }
 63 | 
 64 |     init(_ config: NomicBertConfiguration) {
 65 |         let hiddenFeatures = MLP.scaledHiddenFeatures(config: config)
 66 |         _up.wrappedValue = Linear(
 67 |             config.embedDim, hiddenFeatures, bias: config.mlpFc1Bias)
 68 |         _gate.wrappedValue = Linear(
 69 |             config.embedDim, hiddenFeatures, bias: config.mlpFc1Bias)
 70 |         _down.wrappedValue = Linear(
 71 |             hiddenFeatures, config.embedDim, bias: config.mlpFc2Bias)
 72 |     }
 73 | 
 74 |     func callAsFunction(_ inputs: MLXArray) -> MLXArray {
 75 |         let activations = up(inputs) * silu(gate(inputs))
 76 |         return down(activations)
 77 |     }
 78 | }
 79 | 
 80 | func computeBaseFrequency(
 81 |     base: Float, dims: Int, ropeType: String,
 82 |     ropeScaling: [String: StringOrNumber]?
 83 | )
 84 |     -> Float
 85 | {
 86 |     if ropeType != "llama3" {
 87 |         return base
 88 |     }
 89 | 
 90 |     guard let ropeScaling = ropeScaling else {
 91 |         return base
 92 |     }
 93 | 
 94 |     guard case .float(let factor) = ropeScaling["factor"],
 95 |         case .float(let lowFreqFactor) = ropeScaling["low_freq_factor"]
 96 |             ?? .float(1.0),
 97 |         case .float(let highFreqFactor) = ropeScaling["high_freq_factor"]
 98 |             ?? .float(4.0),
 99 |         case .float(let oldContextLen) = ropeScaling[
100 |             "original_max_position_embeddings"]
101 |             ?? .float(8192)
102 |     else {
103 |         return base
104 |     }
105 | 
106 |     let lowFreqWavelen = oldContextLen / lowFreqFactor
107 |     let highFreqWavelen = oldContextLen / highFreqFactor
108 | 
109 |     let freqs = (0 ..< dims).compactMap { index -> Float? in
110 |         if index % 2 == 0 {
111 |             return pow(base, Float(index) / Float(dims))
112 |         }
113 |         return nil
114 |     }
115 | 
116 |     let newBaseFreqs = freqs.map { freq -> Float in
117 |         let wavelen = 2 * .pi / freq
118 |         let smooth = max(
119 |             0,
120 |             min(
121 |                 1,
122 |                 (wavelen - highFreqWavelen) / (lowFreqWavelen - highFreqWavelen)
123 |             ))
124 |         return freq * ((1 - smooth) * factor + smooth)
125 |     }
126 | 
127 |     return newBaseFreqs.reduce(0, +) / Float(newBaseFreqs.count)
128 | }
129 | 
130 | private class DynamicNTKScalingRoPE: Module {
131 |     let dims: Int
132 |     let maxPositionEmbeddings: Int?
133 |     let traditional: Bool
134 |     let base: Float
135 |     var scale: Float
136 |     let ropeType: String
137 |     let ropeScaling: [String: StringOrNumber]?
138 | 
139 |     init(
140 |         dims: Int, maxPositionEmbeddings: Int?, traditional: Bool = false,
141 |         base: Float = 10000, scale: Float = 1.0, ropeType: String = "default",
142 |         ropeScaling: [String: StringOrNumber]? = nil
143 |     ) {
144 |         self.dims = dims
145 |         self.maxPositionEmbeddings = maxPositionEmbeddings
146 |         self.traditional = traditional
147 |         self.base = computeBaseFrequency(
148 |             base: base, dims: dims, ropeType: ropeType, ropeScaling: ropeScaling
149 |         )
150 |         self.scale = scale
151 |         self.ropeType = ropeType
152 |         self.ropeScaling = ropeScaling
153 |     }
154 | 
155 |     func callAsFunction(_ x: MLXArray, offset: Int = 0) -> MLXArray {
156 |         let seqLen = x.dim(1) + offset
157 |         var base = self.base
158 |         if let maxPositionEmbeddings, seqLen > maxPositionEmbeddings {
159 |             let factorAdjustment =
160 |                 Float(seqLen) / Float(maxPositionEmbeddings) - 1
161 |             let dimensionRatio = Float(dims) / Float(Float(dims) - 2)
162 |             let adjustedScale =
163 |                 scale * pow(1 + factorAdjustment, dimensionRatio)
164 |             base *= adjustedScale
165 |         }
166 |         return MLXFast.RoPE(
167 |             x, dimensions: dims, traditional: traditional, base: base,
168 |             scale: scale, offset: offset)
169 |     }
170 | }
171 | 
172 | private class Attention: Module {
173 |     let numHeads: Int
174 |     let headDim: Int
175 | 
176 |     @ModuleInfo(key: "Wqkv") var wqkv: Linear
177 |     @ModuleInfo(key: "out_proj") var wo: Linear
178 | 
179 |     enum PositionalEncoding {
180 |         case rope(RoPE)
181 |         case dynamicNTKScalingRoPE(DynamicNTKScalingRoPE)
182 | 
183 |         func applyEncoding(_ x: MLXArray, offset: Int = 0) -> MLXArray {
184 |             switch self {
185 |             case .rope(let rope):
186 |                 return rope.callAsFunction(x, offset: offset)
187 |             case .dynamicNTKScalingRoPE(let dynamicNTKScalingRoPE):
188 |                 return dynamicNTKScalingRoPE.callAsFunction(x, offset: offset)
189 |             }
190 |         }
191 |     }
192 | 
193 |     let rope: PositionalEncoding
194 |     let rotaryEmbDim: Int
195 |     let normFactor: Float
196 | 
197 |     init(_ config: NomicBertConfiguration) {
198 |         _wqkv.wrappedValue = Linear(
199 |             config.embedDim, 3 * config.embedDim, bias: config.qkvProjBias)
200 |         _wo.wrappedValue = Linear(
201 |             config.embedDim, config.embedDim, bias: config.qkvProjBias)
202 |         numHeads = config.numHeads
203 |         headDim = config.embedDim / numHeads
204 |         rotaryEmbDim = Int(Float(headDim) * config.rotaryEmbFraction)
205 |         normFactor = sqrt(Float(headDim))
206 | 
207 |         if config.rotaryScalingFactor != nil {
208 |             rope = .dynamicNTKScalingRoPE(
209 |                 DynamicNTKScalingRoPE(
210 |                     dims: rotaryEmbDim,
211 |                     maxPositionEmbeddings: config.maxPositionEmbeddings,
212 |                     traditional: config.rotaryEmbInterleaved,
213 |                     base: config.rotaryEmbBase,
214 |                     scale: config.rotaryScalingFactor!))
215 |         } else {
216 |             rope = .rope(
217 |                 RoPE(
218 |                     dimensions: rotaryEmbDim,
219 |                     traditional: config.rotaryEmbInterleaved,
220 |                     base: config.rotaryEmbBase,
221 |                     scale: 1.0)
222 |             )
223 |         }
224 |     }
225 | 
226 |     func callAsFunction(_ inputs: MLXArray, mask: MLXArray? = nil) -> MLXArray {
227 |         let (B, L) = (inputs.dim(0), inputs.dim(1))
228 |         let queryPos = numHeads * headDim
229 |         let qkv = split(
230 |             wqkv(inputs), indices: [queryPos, queryPos * 2], axis: -1
231 |         )
232 |         var queries = qkv[0]
233 |         var keys = qkv[1]
234 |         var values = qkv[2]
235 | 
236 |         // prepare the queries, keys and values for the attention computation
237 |         queries = queries.reshaped(B, L, numHeads, -1).transposed(
238 |             0, 2, 1, 3)
239 |         keys = keys.reshaped(B, L, numHeads, -1).transposed(0, 2, 1, 3)
240 |         values = values.reshaped(B, L, numHeads, -1).transposed(0, 2, 1, 3)
241 | 
242 |         if rotaryEmbDim > 0 {
243 |             queries = rope.applyEncoding(queries)
244 |             keys = rope.applyEncoding(keys)
245 |         }
246 |         var scores = queries.matmul(keys.transposed(0, 1, 3, 2)) / normFactor
247 | 
248 |         if let mask {
249 |             scores = scores + mask
250 |         }
251 |         let probs = softmax(scores, axis: -1)
252 | 
253 |         let output = matmul(probs, values).transposed(0, 2, 1, 3).reshaped(B, L, -1)
254 |         return wo(output)
255 |     }
256 | }
257 | 
258 | private class TransformerBlock: Module {
259 |     @ModuleInfo(key: "attn") var attention: Attention
260 |     @ModuleInfo(key: "norm1") var postAttentionLayerNorm: LayerNorm
261 |     @ModuleInfo(key: "norm2") var outputLayerNorm: LayerNorm
262 |     @ModuleInfo(key: "mlp") var mlp: MLP
263 | 
264 |     init(_ config: NomicBertConfiguration) {
265 |         _attention.wrappedValue = Attention(config)
266 |         _mlp.wrappedValue = MLP(config)
267 |         _outputLayerNorm.wrappedValue = LayerNorm(
268 |             dimensions: config.embedDim, eps: config.layerNormEps)
269 |         _postAttentionLayerNorm.wrappedValue = LayerNorm(
270 |             dimensions: config.embedDim, eps: config.layerNormEps)
271 |     }
272 | 
273 |     func callAsFunction(_ inputs: MLXArray, mask: MLXArray? = nil) -> MLXArray {
274 |         let attentionOut = attention(inputs, mask: mask)
275 |         let addAndNorm = postAttentionLayerNorm(attentionOut + inputs)
276 |         let mlpOut = mlp(addAndNorm)
277 |         return outputLayerNorm(addAndNorm + mlpOut)
278 |     }
279 | }
280 | 
281 | private class LMHead: Module {
282 |     @ModuleInfo(key: "dense") var dense: Linear
283 |     @ModuleInfo(key: "ln") var layerNorm: LayerNorm
284 |     @ModuleInfo(key: "decoder") var decoder: Linear
285 | 
286 |     init(_ config: NomicBertConfiguration) {
287 |         _dense.wrappedValue = Linear(
288 |             config.embedDim, config.embedDim, bias: config.mlpFc1Bias)
289 |         _layerNorm.wrappedValue = LayerNorm(
290 |             dimensions: config.embedDim, eps: config.layerNormEps)
291 |         _decoder.wrappedValue = Linear(
292 |             config.embedDim, config.vocabularySize, bias: config.mlpFc1Bias)
293 |     }
294 |     func callAsFunction(_ inputs: MLXArray) -> MLXArray {
295 |         return decoder(layerNorm(silu(dense(inputs))))
296 |     }
297 | }
298 | 
299 | private class Encoder: Module {
300 | 
301 |     let layers: [TransformerBlock]
302 | 
303 |     init(
304 |         _ config: NomicBertConfiguration
305 |     ) {
306 |         precondition(config.vocabularySize > 0)
307 | 
308 |         layers = (0 ..< config.numLayers).map {
309 |             _ in TransformerBlock(config)
310 |         }
311 |     }
312 | 
313 |     func callAsFunction(_ inputs: MLXArray, attentionMask: MLXArray? = nil) -> MLXArray {
314 |         var outputs = inputs
315 |         for (index, layer) in layers.enumerated() {
316 |             outputs = layer(outputs, mask: attentionMask)
317 |         }
318 |         return outputs
319 |     }
320 | }
321 | 
322 | public class NomicBertModel: Module, EmbeddingModel {
323 |     @ModuleInfo(key: "lm_head") fileprivate var lmHead: LMHead?
324 |     @ModuleInfo(key: "embeddings") var embedder: NomicEmbedding
325 |     let pooler: Linear?
326 |     fileprivate let encoder: Encoder
327 |     public var vocabularySize: Int
328 | 
329 |     public init(
330 |         _ config: NomicBertConfiguration, pooler: Bool = true,
331 |         lmHead: Bool = false
332 |     ) {
333 |         precondition(config.vocabularySize > 0)
334 |         vocabularySize = config.vocabularySize
335 |         encoder = Encoder(config)
336 |         _embedder.wrappedValue = NomicEmbedding(config)
337 | 
338 |         if pooler {
339 |             self.pooler = Linear(config.embedDim, config.embedDim)
340 |         } else {
341 |             self.pooler = nil
342 |         }
343 |         if lmHead {
344 |             _lmHead.wrappedValue = LMHead(config)
345 |         }
346 |     }
347 | 
348 |     public func callAsFunction(
349 |         _ inputs: MLXArray, positionIds: MLXArray? = nil, tokenTypeIds: MLXArray? = nil,
350 |         attentionMask: MLXArray? = nil
351 |     )
352 |         -> EmbeddingModelOutput
353 |     {
354 |         var inp = inputs
355 |         if inp.ndim == 1 {
356 |             inp = inp.reshaped(1, -1)
357 |         }
358 |         var mask = attentionMask
359 |         if mask != nil {
360 |             mask = mask!.asType(embedder.wordEmbeddings.weight.dtype).expandedDimensions(axes: [
361 |                 1, 2,
362 |             ]).log()
363 |         }
364 |         let outputs = encoder(
365 |             embedder(
366 |                 inp, positionIds: positionIds, tokenTypeIds: tokenTypeIds),
367 |             attentionMask: mask)
368 |         if let lmHead {
369 |             return EmbeddingModelOutput(hiddenStates: lmHead(outputs), pooledOutput: nil)
370 |         }
371 |         if let pooler {
372 |             return EmbeddingModelOutput(
373 |                 hiddenStates: outputs, pooledOutput: tanh(pooler(outputs[0..., 0])))
374 |         }
375 |         return EmbeddingModelOutput(hiddenStates: outputs, pooledOutput: nil)
376 |     }
377 | 
378 |     public func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {
379 |         weights.reduce(into: [:]) { result, item in
380 |             var key = item.key.replacingOccurrences(
381 |                 of: "emb_ln", with: "embeddings.norm")
382 |             key = key.replacingOccurrences(of: "bert.", with: "")
383 |             key = key.replacingOccurrences(
384 |                 of: "cls.predictions.transform.dense.", with: "lm_head.dense.")
385 |             key = key.replacingOccurrences(
386 |                 of: "cls.predictions.transform.LayerNorm.", with: "lm_head.ln.")
387 |             key = key.replacingOccurrences(
388 |                 of: "cls.predictions.decoder", with: "lm_head.decoder")
389 |             key = key.replacingOccurrences(of: "pooler.dense.", with: "pooler.")
390 |             result[key] = item.value
391 |         }
392 |     }
393 | }
394 | 
395 | public struct NomicBertConfiguration: Decodable, Sendable {
396 |     var layerNormEps: Float = 1e-12
397 |     var maxTrainedPositions: Int = 2048
398 |     var mlpFc1Bias: Bool = false
399 |     var mlpFc2Bias: Bool = false
400 |     var embedDim: Int = 768
401 |     var numHeads: Int = 12
402 |     var MLPDim: Int = 3072
403 |     var numLayers: Int = 12
404 |     var qkvProjBias: Bool = false
405 |     var rotaryEmbBase: Float = 1000
406 |     var rotaryEmbFraction: Float = 1.0
407 |     var rotaryEmbInterleaved: Bool = false
408 |     var rotaryEmbScaleBase: Float?
409 |     var rotaryScalingFactor: Float?
410 |     var typeVocabularySize: Int = 2
411 |     var vocabularySize: Int = 30528
412 |     var maxPositionEmbeddings: Int = 0
413 | 
414 |     enum CodingKeys: String, CodingKey {
415 |         case layerNormEps = "layer_norm_epsilon"
416 |         case maxTrainedPositions = "max_trained_positions"
417 |         case mlpFc1Bias = "mlp_fc1_bias"
418 |         case mlpFc2Bias = "mlp_fc2_bias"
419 |         case embedDim = "n_embd"
420 |         case numHeads = "n_head"
421 |         case MLPDim = "n_inner"
422 |         case numLayers = "n_layer"
423 |         case qkvProjBias = "qkv_proj_bias"
424 |         case rotaryEmbBase = "rotary_emb_base"
425 |         case rotaryEmbFraction = "rotary_emb_fraction"
426 |         case rotaryEmbInterleaved = "rotary_emb_interleaved"
427 |         case rotaryEmbScaleBase = "rotary_emb_scale_base"
428 |         case rotaryScalingFactor = "rotary_scaling_factor"
429 |         case typeVocabularySize = "type_vocab_size"
430 |         case useCache = "use_cache"
431 |         case vocabularySize = "vocab_size"
432 |         case maxPositionEmbeddings = "max_position_embeddings"
433 |     }
434 | 
435 |     public init(from decoder: Decoder) throws {
436 |         let container: KeyedDecodingContainer<NomicBertConfiguration.CodingKeys> =
437 |             try decoder.container(
438 |                 keyedBy: NomicBertConfiguration.CodingKeys.self)
439 |         layerNormEps =
440 |             try container.decodeIfPresent(
441 |                 Float.self,
442 |                 forKey: NomicBertConfiguration.CodingKeys.layerNormEps.self)
443 |             ?? 1e-12
444 |         maxTrainedPositions =
445 |             try container.decodeIfPresent(
446 |                 Int.self,
447 |                 forKey: NomicBertConfiguration.CodingKeys.maxTrainedPositions
448 |                     .self) ?? 2048
449 |         mlpFc1Bias =
450 |             try container.decodeIfPresent(
451 |                 Bool.self,
452 |                 forKey: NomicBertConfiguration.CodingKeys.mlpFc1Bias.self)
453 |             ?? false
454 |         mlpFc2Bias =
455 |             try container.decodeIfPresent(
456 |                 Bool.self,
457 |                 forKey: NomicBertConfiguration.CodingKeys.mlpFc2Bias.self)
458 |             ?? false
459 |         embedDim =
460 |             try container.decodeIfPresent(
461 |                 Int.self,
462 |                 forKey: NomicBertConfiguration.CodingKeys.embedDim.self) ?? 768
463 |         numHeads =
464 |             try container.decodeIfPresent(
465 |                 Int.self,
466 |                 forKey: NomicBertConfiguration.CodingKeys.numHeads.self) ?? 12
467 |         MLPDim =
468 |             try container.decodeIfPresent(
469 |                 Int.self, forKey: NomicBertConfiguration.CodingKeys.MLPDim.self)
470 |             ?? 3072
471 |         numLayers =
472 |             try container.decodeIfPresent(
473 |                 Int.self,
474 |                 forKey: NomicBertConfiguration.CodingKeys.numLayers.self) ?? 12
475 |         qkvProjBias =
476 |             try container.decodeIfPresent(
477 |                 Bool.self,
478 |                 forKey: NomicBertConfiguration.CodingKeys.qkvProjBias.self)
479 |             ?? false
480 |         rotaryEmbBase =
481 |             try container.decodeIfPresent(
482 |                 Float.self,
483 |                 forKey: NomicBertConfiguration.CodingKeys.rotaryEmbBase.self)
484 |             ?? 1000
485 |         rotaryEmbFraction =
486 |             try container.decodeIfPresent(
487 |                 Float.self,
488 |                 forKey: NomicBertConfiguration.CodingKeys.rotaryEmbFraction.self
489 |             ) ?? 1.0
490 |         rotaryEmbInterleaved =
491 |             try container.decodeIfPresent(
492 |                 Bool.self,
493 |                 forKey: NomicBertConfiguration.CodingKeys.rotaryEmbInterleaved
494 |                     .self) ?? false
495 |         rotaryEmbScaleBase =
496 |             try container.decodeIfPresent(
497 |                 Float.self,
498 |                 forKey: NomicBertConfiguration.CodingKeys.rotaryEmbScaleBase)
499 |             ?? nil
500 |         rotaryScalingFactor =
501 |             try container.decodeIfPresent(
502 |                 Float.self,
503 |                 forKey: NomicBertConfiguration.CodingKeys.rotaryScalingFactor)
504 |             ?? nil
505 |         typeVocabularySize =
506 |             try container.decodeIfPresent(
507 |                 Int.self,
508 |                 forKey: NomicBertConfiguration.CodingKeys.typeVocabularySize
509 |                     .self) ?? 2
510 |         vocabularySize =
511 |             try container.decodeIfPresent(
512 |                 Int.self,
513 |                 forKey: NomicBertConfiguration.CodingKeys.vocabularySize.self)
514 |             ?? 30528
515 |         maxPositionEmbeddings =
516 |             try container.decodeIfPresent(
517 |                 Int.self,
518 |                 forKey: NomicBertConfiguration.CodingKeys.maxPositionEmbeddings
519 |                     .self) ?? 0
520 |     }
521 | }
522 | 


--------------------------------------------------------------------------------
/Libraries/Embedders/Pooling.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import Foundation
  4 | import MLX
  5 | import MLXLinalg
  6 | import MLXNN
  7 | 
  8 | public struct PoolingConfiguration: Codable {
  9 |     public let dimension: Int
 10 |     public let poolingModeClsToken: Bool
 11 |     public let poolingModeMeanTokens: Bool
 12 |     public let poolingModeMaxTokens: Bool
 13 |     public let poolingModeLastToken: Bool
 14 | 
 15 |     enum CodingKeys: String, CodingKey {
 16 |         case dimension = "word_embedding_dimension"
 17 |         case poolingModeClsToken = "pooling_mode_cls_token"
 18 |         case poolingModeMeanTokens = "pooling_mode_mean_tokens"
 19 |         case poolingModeMaxTokens = "pooling_mode_max_tokens"
 20 |         case poolingModeLastToken = "pooling_mode_lasttoken"
 21 |     }
 22 | }
 23 | 
 24 | func loadPooling(modelDirectory: URL) -> Pooling {
 25 |     let configurationURL = modelDirectory.appending(components: "1_Pooling", "config.json")
 26 |     guard
 27 |         let poolingConfig = try? JSONDecoder().decode(
 28 |             PoolingConfiguration.self, from: Data(contentsOf: configurationURL))
 29 |     else {
 30 |         return Pooling(strategy: .none)
 31 |     }
 32 | 
 33 |     return Pooling(config: poolingConfig)
 34 | }
 35 | 
 36 | public class Pooling: Module {
 37 |     public enum Strategy {
 38 |         case mean
 39 |         case cls
 40 |         case first
 41 |         case last
 42 |         case max
 43 |         case none
 44 |     }
 45 |     let strategy: Strategy
 46 |     let dimension: Int?
 47 | 
 48 |     public init(
 49 |         strategy: Strategy, dimension: Int? = nil
 50 |     ) {
 51 |         self.strategy = strategy
 52 |         self.dimension = dimension
 53 |     }
 54 | 
 55 |     public init(
 56 |         config: PoolingConfiguration
 57 |     ) {
 58 |         dimension = config.dimension
 59 |         if config.poolingModeClsToken {
 60 |             strategy = .cls
 61 |         } else if config.poolingModeMeanTokens {
 62 |             strategy = .mean
 63 |         } else if config.poolingModeMaxTokens {
 64 |             strategy = .max
 65 |         } else if config.poolingModeLastToken {
 66 |             strategy = .last
 67 |         } else {
 68 |             strategy = .first
 69 |         }
 70 |     }
 71 | 
 72 |     public func callAsFunction(
 73 |         _ inputs: EmbeddingModelOutput, mask: MLXArray? = nil, normalize: Bool = false,
 74 |         applyLayerNorm: Bool = false
 75 |     ) -> MLXArray {
 76 |         let _mask = mask ?? MLXArray.ones(Array(inputs.hiddenStates?.shape[0 ..< 2] ?? [0]))
 77 | 
 78 |         var pooled: MLXArray
 79 |         switch self.strategy {
 80 |         case .mean:
 81 |             pooled =
 82 |                 sum(
 83 |                     inputs.hiddenStates! * _mask.expandedDimensions(axes: [-1]),
 84 |                     axis: 1)
 85 |                 / sum(_mask, axis: -1, keepDims: true)
 86 |         case .max:
 87 |             pooled = MLX.max(
 88 |                 inputs.hiddenStates! * _mask.expandedDimensions(axes: [-1]), axis: 1)
 89 |         case .first:
 90 |             pooled = inputs.hiddenStates![0..., 0, 0...]
 91 |         case .last:
 92 |             pooled = inputs.hiddenStates![0..., -1, 0...]
 93 |         case .cls:
 94 |             pooled =
 95 |                 inputs.pooledOutput
 96 |                 ?? inputs.hiddenStates![0..., 0, 0...]
 97 |         case .none:
 98 |             pooled = inputs.pooledOutput ?? inputs.hiddenStates!
 99 |         }
100 |         if applyLayerNorm {
101 |             pooled = MLXFast.layerNorm(pooled, eps: 1e-5)
102 |         }
103 |         if let dimension {
104 |             pooled = pooled[0..., 0 ..< dimension]
105 |         }
106 |         if normalize {
107 |             pooled = pooled / norm(pooled, axis: -1, keepDims: true)
108 |         }
109 |         return pooled
110 |     }
111 | }
112 | 


--------------------------------------------------------------------------------
/Libraries/Embedders/README.md:
--------------------------------------------------------------------------------
 1 | #  MLXEmbedders
 2 | 
 3 | This directory contains ports of popular Encoders / Embedding Models. 
 4 | 
 5 | ## Usage Example
 6 | 
 7 | ```swift
 8 | 
 9 | let modelContainer = try await MLXEmbedders.loadModelContainer(
10 |     configuration: ModelConfiguration.nomic_text_v1_5)
11 | let result = await modelContainer.perform {
12 |     (model: EmbeddingModel, tokenizer, pooling) -> [[Float]] in
13 |     let inputs = [
14 |         "search_query: Animals in Tropical Climates.",
15 |         "search_document: Elephants",
16 |         "search_document: Horses",
17 |         "search_document: Polar Bears",
18 |     ].map {
19 |         tokenizer.encode(text: $0, addSpecialTokens: true)
20 |     }
21 |     // Pad to longest
22 |     let maxLength = inputs.reduce(into: 16) { acc, elem in
23 |         acc = max(acc, elem.count)
24 |     }
25 | 
26 |     let padded = stacked(
27 |         inputs.map { elem in
28 |             MLXArray(
29 |                 elem
30 |                     + Array(
31 |                         repeating: tokenizer.eosTokenId ?? 0,
32 |                         count: maxLength - elem.count))
33 |         })
34 |     let mask = (padded .!= tokenizer.eosTokenId ?? 0)
35 |     let tokenTypes = MLXArray.zeros(like: padded)
36 |     let result = pooling(
37 |         model(padded, positionIds: nil, tokenTypeIds: tokenTypes, attentionMask: mask),
38 |         normalize: true, applyLayerNorm: true
39 |     ).eval()
40 |     return result.map { $0.asArray(Float.self) }
41 | }
42 | ```
43 | 
44 | 
45 | Ported to swift from [taylorai/mlx_embedding_models](https://github.com/taylorai/mlx_embedding_models/tree/main)
46 | 


--------------------------------------------------------------------------------
/Libraries/Embedders/Tokenizer.swift:
--------------------------------------------------------------------------------
 1 | // Copyright © 2024 Apple Inc.
 2 | 
 3 | import Foundation
 4 | import Hub
 5 | import Tokenizers
 6 | 
 7 | public func loadTokenizer(configuration: ModelConfiguration, hub: HubApi) async throws -> Tokenizer
 8 | {
 9 |     let (tokenizerConfig, tokenizerData) = try await loadTokenizerConfig(
10 |         configuration: configuration, hub: hub)
11 | 
12 |     return try PreTrainedTokenizer(
13 |         tokenizerConfig: tokenizerConfig, tokenizerData: tokenizerData)
14 | }
15 | 
16 | func loadTokenizerConfig(configuration: ModelConfiguration, hub: HubApi) async throws -> (
17 |     Config, Config
18 | ) {
19 |     // from AutoTokenizer.from() -- this lets us override parts of the configuration
20 |     let config: LanguageModelConfigurationFromHub
21 | 
22 |     switch configuration.id {
23 |     case .id(let id):
24 |         do {
25 |             // the load can fail (async when we try to use it)
26 |             let loaded = LanguageModelConfigurationFromHub(
27 |                 modelName: configuration.tokenizerId ?? id, hubApi: hub)
28 |             _ = try await loaded.tokenizerConfig
29 |             config = loaded
30 |         } catch {
31 |             let nserror = error as NSError
32 |             if nserror.domain == NSURLErrorDomain
33 |                 && nserror.code == NSURLErrorNotConnectedToInternet
34 |             {
35 |                 // Internet connection appears to be offline -- fall back to loading from
36 |                 // the local directory
37 |                 config = LanguageModelConfigurationFromHub(
38 |                     modelFolder: configuration.modelDirectory(hub: hub), hubApi: hub)
39 |             } else {
40 |                 throw error
41 |             }
42 |         }
43 |     case .directory(let directory):
44 |         config = LanguageModelConfigurationFromHub(modelFolder: directory, hubApi: hub)
45 |     }
46 | 
47 |     guard let tokenizerConfig = try await config.tokenizerConfig else {
48 |         throw EmbedderError(message: "missing config")
49 |     }
50 |     let tokenizerData = try await config.tokenizerData
51 |     return (tokenizerConfig, tokenizerData)
52 | }
53 | 


--------------------------------------------------------------------------------
/Libraries/MLXLLM/Documentation.docc/Documentation.md:
--------------------------------------------------------------------------------
 1 | # ``MLXLLM``
 2 | 
 3 | Example implementations of various Large Language Models (LLMs).
 4 | 
 5 | ## Other MLX Libraries Packages
 6 | 
 7 | - [MLXEmbedders](MLXEmbedders)
 8 | - [MLXLLM](MLXLLM)
 9 | - [MLXLMCommon](MLXLMCommon)
10 | - [MLXMNIST](MLXMNIST)
11 | - [MLXVLM](MLXVLM)
12 | - [StableDiffusion](StableDiffusion)
13 | 
14 | ## Quick Start
15 | 
16 | See <doc:evaluation>.
17 | 
18 | Using LLMs and VLMs is as easy as this:
19 | 
20 | ```swift
21 | let model = try await loadModel(id: "mlx-community/Qwen3-4B-4bit")
22 | let session = ChatSession(model)
23 | print(try await session.respond(to: "What are two things to see in San Francisco?")
24 | print(try await session.respond(to: "How about a great place to eat?")
25 | ```
26 | 
27 | More advanced APIs are available for those that need them, see <doc:using-model>.
28 | 
29 | ## Topics
30 | 
31 | - <doc:evaluation>
32 | - <doc:adding-model>
33 | - <doc:using-model>
34 | 
35 | ### Models
36 | 
37 | - ``CohereModel``
38 | - ``GemmaModel``
39 | - ``Gemma2Model``
40 | - ``InternLM2Model``
41 | - ``LlamaModel``
42 | - ``OpenELMModel``
43 | - ``PhiModel``
44 | - ``Phi3Model``
45 | - ``PhiMoEModel``
46 | - ``Qwen2Model``
47 | - ``Qwen3Model``
48 | - ``Starcoder2Model``
49 | - ``MiMoModel``
50 | - ``GLM4Model``
51 | - ``AceReason``
52 | 


--------------------------------------------------------------------------------
/Libraries/MLXLLM/Documentation.docc/adding-model.md:
--------------------------------------------------------------------------------
 1 | #  Adding a Model
 2 | 
 3 | If the model follows the typical LLM pattern you can add a new
 4 | model in a few steps.
 5 | 
 6 | - `config.json`, `tokenizer.json`, and `tokenizer_config.json`
 7 | - `*.safetensors`
 8 | 
 9 | You can follow the pattern of the models in the [Models](Models) directory
10 | and create a `.swift` file for your new model:
11 | 
12 | ## Create a Configuration
13 | 
14 | Create a configuration struct to match the `config.json` (any parameters needed).
15 | 
16 | ```swift
17 | public struct YourModelConfiguration: Codable, Sendable {
18 |     public let hiddenSize: Int
19 |     
20 |     // use this pattern for values that need defaults
21 |     public let _layerNormEps: Float?
22 |     public var layerNormEps: Float { _layerNormEps ?? 1e-6 }
23 |     
24 |     enum CodingKeys: String, CodingKey {
25 |         case hiddenSize = "hidden_size"
26 |         case _layerNormEps = "layer_norm_eps"
27 |     }
28 | }
29 | ```
30 | 
31 | ## Create the Model Class
32 | 
33 | Create the model class. The top-level public class should have a
34 | structure something like this:
35 | 
36 | ```swift
37 | public class YourModel: Module, LLMModel, KVCacheDimensionProvider, LoRAModel {
38 | 
39 |     public let kvHeads: [Int]
40 | 
41 |     @ModuleInfo var model: YourModelInner
42 | 
43 |     public func loraLinearLayers() -> LoRALinearLayers {
44 |         // TODO: modify as needed
45 |         model.layers.map { ($0.attention, ["q_proj", "v_proj"]) }
46 |     }
47 | 
48 |     public init(_ args: YourModelConfiguration) {
49 |         self.kvHeads = Array(repeating: args.kvHeads, count: args.hiddenLayers)
50 |         self.model = YourModelInner(args)
51 |     }
52 | 
53 |     public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {
54 |         // TODO: modify as needed
55 |         let out = model(inputs, cache: cache)
56 |         return model.embedTokens.asLinear(out)
57 |     }
58 | }
59 | ```
60 | 
61 | ## Register the Model
62 | 
63 | In [LLMModelFactory.swift](LLMModelFactory.swift) register the model type itself
64 | (this is independent of the model id):
65 | 
66 | ```swift
67 | public class ModelTypeRegistry: @unchecked Sendable {
68 | ...
69 |     private var creators: [String: @Sendable (URL) throws -> any LanguageModel] = [
70 |         "yourModel": create(YourModelConfiguration.self, YourModel.init),
71 | ```
72 | 
73 | Add a constant for the model in the `ModelRegistry` (not strictly required but useful
74 | for callers to refer to it in code):
75 | 
76 | ```swift
77 | public class ModelRegistry: @unchecked Sendable {
78 | ...
79 |     static public let yourModel_4bit = ModelConfiguration(
80 |         id: "mlx-community/YourModel-4bit",
81 |         defaultPrompt: "What is the gravity on Mars and the moon?"
82 |     )
83 | ```
84 | 
85 | and finally add it to the all list -- this will let users find the model
86 | configuration by id:
87 | 
88 | ```swift
89 |     private static func all() -> [ModelConfiguration] {
90 |         [
91 |             codeLlama13b4bit,
92 | ...
93 |             yourModel_4bit,
94 | ```
95 | 


--------------------------------------------------------------------------------
/Libraries/MLXLLM/Documentation.docc/evaluation.md:
--------------------------------------------------------------------------------
 1 | #  Evaluation
 2 | 
 3 | The simplified LLM/VLM API allows you to load a model and evaluate prompts with only a few lines of code.
 4 | 
 5 | For example, this loads a model and asks a question and a follow-on question:
 6 | 
 7 | ```swift
 8 | let model = try await loadModel(id: "mlx-community/Qwen3-4B-4bit")
 9 | let session = ChatSession(model)
10 | print(try await session.respond(to: "What are two things to see in San Francisco?")
11 | print(try await session.respond(to: "How about a great place to eat?")
12 | ```
13 | 
14 | The second question actually refers to information (the location) from the first
15 | question -- this context is maintained inside the ``ChatSession`` object.
16 | 
17 | If you need a one-shot prompt/response simply create a ``ChatSession``, evaluate
18 | the prompt and discard.  Multiple ``ChatSession`` instances could also be used
19 | (at the cost of the memory in the `KVCache`) to handle multiple streams of
20 | context.
21 | 
22 | ## Streaming Output
23 | 
24 | The previous example produced the entire response in one call.  Often
25 | users want to see the text as it is generated -- you can do this with
26 | a stream:
27 | 
28 | ```swift
29 | let model = try await loadModel(id: "mlx-community/Qwen3-4B-4bit")
30 | let session = ChatSession(model)
31 | 
32 | for try await item in session.streamResponse(to: "Why is the sky blue?") {
33 |     print(item, terminator: "")
34 | }
35 | print()
36 | ```
37 | 
38 | ## VLMs (Vision Language Models)
39 | 
40 | This same API supports VLMs as well.  Simply present the image or video
41 | to the ``ChatSession``:
42 | 
43 | ```swift
44 | let model = try await loadModel(id: "mlx-community/Qwen2.5-VL-3B-Instruct-4bit")
45 | let session = ChatSession(model)
46 | 
47 | let answer1 = try await session.respond(
48 |     to: "what kind of creature is in the picture?"
49 |     image: .url(URL(fileURLWithPath: "support/test.jpg"))
50 | )
51 | print(answer1)
52 | 
53 | // we can ask a followup question referring back to the previous image
54 | let answer2 = try await session.respond(
55 |     to: "What is behind the dog?"
56 | )
57 | print(answer2)
58 | ```
59 | 
60 | ## Advanced Usage
61 | 
62 | The ``ChatSession`` has a number of parameters you can supply when creating it:
63 | 
64 | - **instructions**: optional instructions to the chat session, e.g. describing what type of responses to give
65 |     - for example you might instruct the language model to respond in rhyme or
66 |         talking like a famous character from a movie
67 |     - or that the responses should be very brief
68 | - **generateParameters**: parameters that control the generation of output, e.g. token limits and temperature
69 |     - see ``GenerateParameters``
70 | - **processing**: optional media processing instructions
71 | 


--------------------------------------------------------------------------------
/Libraries/MLXLLM/Documentation.docc/using-model.md:
--------------------------------------------------------------------------------
  1 | #  Using a Model
  2 | 
  3 | Using a model is easy:  load the weights, tokenize and evaluate.
  4 | 
  5 | There is a high level API described in <doc:evaluation> and this documentation
  6 | describes the lower level API if you need more control.
  7 | 
  8 | ## Loading a Model
  9 | 
 10 | A model is typically loaded by using a `ModelFactory` and a `ModelConfiguration`:
 11 | 
 12 | ```swift
 13 | // e.g. LLMModelFactory.shared
 14 | let modelFactory: ModelFactory
 15 | 
 16 | // e.g. LLMRegistry.llama3_8B_4bit
 17 | let modelConfiguration: ModelConfiguration
 18 | 
 19 | let container = try await modelFactory.loadContainer(configuration: modelConfiguration)
 20 | ```
 21 | 
 22 | The `container` provides an isolation context (an `actor`) to run inference in the model.
 23 | 
 24 | Predefined `ModelConfiguration` instances are provided as static variables
 25 | on the `ModelRegistry` types or they can be created:
 26 | 
 27 | ```swift
 28 | let modelConfiguration = ModelConfiguration(id: "mlx-community/llama3_8B_4bit")
 29 | ```
 30 | 
 31 | The flow inside the `ModelFactory` goes like this:
 32 | 
 33 | ```swift
 34 | public class LLMModelFactory: ModelFactory {
 35 | 
 36 |     public func _load(
 37 |         hub: HubApi, configuration: ModelConfiguration,
 38 |         progressHandler: @Sendable @escaping (Progress) -> Void
 39 |     ) async throws -> ModelContext {
 40 |         // download the weight and config using HubApi
 41 |         // load the base configuration
 42 |         // using the typeRegistry create a model (random weights)
 43 |         // load the weights, apply quantization as needed, update the model
 44 |             // calls model.sanitize() for weight preparation
 45 |         // load the tokenizer
 46 |         // (vlm) load the processor configuration, create the processor
 47 |     }
 48 | }
 49 | ```
 50 | 
 51 | Callers with specialized requirements can use these individual components to manually
 52 | load models, if needed.
 53 | 
 54 | ## Evaluation Flow
 55 | 
 56 | - Load the Model
 57 | - UserInput
 58 | - LMInput
 59 | - generate()
 60 |     - NaiveStreamingDetokenizer
 61 |     - TokenIterator
 62 | 
 63 | ## Evaluating a Model
 64 | 
 65 | Once a model is loaded you can evaluate a prompt or series of
 66 | messages. Minimally you need to prepare the user input:
 67 | 
 68 | ```swift
 69 | let prompt = "Describe the image in English"
 70 | var input = UserInput(prompt: prompt, images: image.map { .url($0) })
 71 | input.processing.resize = .init(width: 256, height: 256)
 72 | ```
 73 | 
 74 | This example shows adding some images and processing instructions -- if
 75 | model accepts text only then these parts can be omitted. The inference
 76 | calls are the same.
 77 | 
 78 | Assuming you are using a `ModelContainer` (an actor that holds
 79 | a `ModelContext`, which is the bundled set of types that implement a
 80 | model), the first step is to convert the `UserInput` into the
 81 | `LMInput` (LanguageModel Input):
 82 | 
 83 | ```swift
 84 | let generateParameters: GenerateParameters
 85 | let input: UserInput
 86 | 
 87 | let result = try await modelContainer.perform { [input] context in
 88 |     let input = try context.processor.prepare(input: input)
 89 | 
 90 | ```
 91 | 
 92 | Given that `input` we can call `generate()` to produce a stream
 93 | of tokens. In this example we use a `NaiveStreamingDetokenizer`
 94 | to assist in converting a stream of tokens into text and print it.
 95 | The stream is stopped after we hit a maximum number of tokens:
 96 | 
 97 | ```
 98 |     var detokenizer = NaiveStreamingDetokenizer(tokenizer: context.tokenizer)
 99 | 
100 |     return try MLXLMCommon.generate(
101 |         input: input, parameters: generateParameters, context: context
102 |     ) { tokens in
103 | 
104 |         if let last = tokens.last {
105 |             detokenizer.append(token: last)
106 |         }
107 | 
108 |         if let new = detokenizer.next() {
109 |             print(new, terminator: "")
110 |             fflush(stdout)
111 |         }
112 | 
113 |         if tokens.count >= maxTokens {
114 |             return .stop
115 |         } else {
116 |             return .more
117 |         }
118 |     }
119 | }
120 | ```
121 | 


--------------------------------------------------------------------------------
/Libraries/MLXLLM/LLMModel.swift:
--------------------------------------------------------------------------------
 1 | // Copyright © 2024 Apple Inc.
 2 | 
 3 | import MLX
 4 | import MLXLMCommon
 5 | import Tokenizers
 6 | 
 7 | /// Marker protocol for LLMModels
 8 | public protocol LLMModel: LanguageModel, LoRAModel {
 9 | 
10 |     /// Models can implement this is they need a custom `MessageGenerator`.
11 |     ///
12 |     /// The default implementation returns `DefaultMessageGenerator`.
13 |     func messageGenerator(tokenizer: Tokenizer) -> MessageGenerator
14 | }
15 | 
16 | extension LLMModel {
17 | 
18 |     /// Default prepare step for ``LLMModel``.
19 |     ///
20 |     /// This will evaluate the prompt in chunks until there is a small amount of
21 |     /// tokens left to feed into the `TokenIterator`.
22 |     public func prepare(_ input: LMInput, cache: [KVCache], windowSize: Int?) throws
23 |         -> PrepareResult
24 |     {
25 |         let prefillStepSize = windowSize ?? 512
26 |         var y = input.text
27 |         var state: LMOutput.State? = nil
28 | 
29 |         // prepare the prompt in chunks if larger than the prefill size
30 |         while y.tokens.size > prefillStepSize {
31 |             let input = y[.newAxis, ..<prefillStepSize]
32 |             let result = self(input, cache: cache.isEmpty ? nil : cache, state: state)
33 |             eval(cache)
34 |             y = y[prefillStepSize...]
35 |         }
36 | 
37 |         return .tokens(y)
38 |     }
39 | 
40 |     public func messageGenerator(tokenizer: Tokenizer) -> MessageGenerator {
41 |         DefaultMessageGenerator()
42 |     }
43 | }
44 | 


--------------------------------------------------------------------------------
/Libraries/MLXLLM/LLMModelFactory.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import Foundation
  4 | import Hub
  5 | import MLX
  6 | import MLXLMCommon
  7 | import Tokenizers
  8 | 
  9 | /// Creates a function that loads a configuration file and instantiates a model with the proper configuration
 10 | private func create<C: Codable, M>(
 11 |     _ configurationType: C.Type, _ modelInit: @escaping (C) -> M
 12 | ) -> (URL) throws -> M {
 13 |     { url in
 14 |         let configuration = try JSONDecoder().decode(
 15 |             C.self, from: Data(contentsOf: url))
 16 |         return modelInit(configuration)
 17 |     }
 18 | }
 19 | 
 20 | /// Registry of model type, e.g 'llama', to functions that can instantiate the model from configuration.
 21 | ///
 22 | /// Typically called via ``LLMModelFactory/load(hub:configuration:progressHandler:)``.
 23 | public class LLMTypeRegistry: ModelTypeRegistry, @unchecked Sendable {
 24 | 
 25 |     /// Shared instance with default model types.
 26 |     public static let shared: LLMTypeRegistry = .init(creators: all())
 27 | 
 28 |     /// All predefined model types.
 29 |     private static func all() -> [String: @Sendable (URL) throws -> any LanguageModel] {
 30 |         [
 31 |             "mistral": create(LlamaConfiguration.self, LlamaModel.init),
 32 |             "llama": create(LlamaConfiguration.self, LlamaModel.init),
 33 |             "phi": create(PhiConfiguration.self, PhiModel.init),
 34 |             "phi3": create(Phi3Configuration.self, Phi3Model.init),
 35 |             "phimoe": create(PhiMoEConfiguration.self, PhiMoEModel.init),
 36 |             "gemma": create(GemmaConfiguration.self, GemmaModel.init),
 37 |             "gemma2": create(Gemma2Configuration.self, Gemma2Model.init),
 38 |             "qwen2": create(Qwen2Configuration.self, Qwen2Model.init),
 39 |             "qwen3": create(Qwen3Configuration.self, Qwen3Model.init),
 40 |             "qwen3_moe": create(Qwen3MoEConfiguration.self, Qwen3MoEModel.init),
 41 |             "starcoder2": create(Starcoder2Configuration.self, Starcoder2Model.init),
 42 |             "cohere": create(CohereConfiguration.self, CohereModel.init),
 43 |             "openelm": create(OpenElmConfiguration.self, OpenELMModel.init),
 44 |             "internlm2": create(InternLM2Configuration.self, InternLM2Model.init),
 45 |             "gemma3_text": create(Gemma3TextConfiguration.self, Gemma3TextModel.init),
 46 |             "granite": create(GraniteConfiguration.self, GraniteModel.init),
 47 |             "mimo": create(MiMoConfiguration.self, MiMoModel.init),
 48 |             "glm4": create(GLM4Configuration.self, GLM4Model.init),
 49 |             "acereason": create(Qwen2Configuration.self, Qwen2Model.init),
 50 |         ]
 51 |     }
 52 | 
 53 | }
 54 | 
 55 | /// Registry of models and any overrides that go with them, e.g. prompt augmentation.
 56 | /// If asked for an unknown configuration this will use the model/tokenizer as-is.
 57 | ///
 58 | /// The Python tokenizers have a very rich set of implementations and configuration. The
 59 | /// swift-tokenizers code handles a good chunk of that and this is a place to augment that
 60 | /// implementation, if needed.
 61 | public class LLMRegistry: AbstractModelRegistry, @unchecked Sendable {
 62 | 
 63 |     /// Shared instance with default model configurations.
 64 |     public static let shared = LLMRegistry(modelConfigurations: all())
 65 | 
 66 |     static public let smolLM_135M_4bit = ModelConfiguration(
 67 |         id: "mlx-community/SmolLM-135M-Instruct-4bit",
 68 |         defaultPrompt: "Tell me about the history of Spain."
 69 |     )
 70 | 
 71 |     static public let mistralNeMo4bit = ModelConfiguration(
 72 |         id: "mlx-community/Mistral-Nemo-Instruct-2407-4bit",
 73 |         defaultPrompt: "Explain quaternions."
 74 |     )
 75 | 
 76 |     static public let mistral7B4bit = ModelConfiguration(
 77 |         id: "mlx-community/Mistral-7B-Instruct-v0.3-4bit",
 78 |         defaultPrompt: "Describe the Swift language."
 79 |     )
 80 | 
 81 |     static public let codeLlama13b4bit = ModelConfiguration(
 82 |         id: "mlx-community/CodeLlama-13b-Instruct-hf-4bit-MLX",
 83 |         overrideTokenizer: "PreTrainedTokenizer",
 84 |         defaultPrompt: "func sortArray(_ array: [Int]) -> String { <FILL_ME> }"
 85 |     )
 86 | 
 87 |     static public let deepSeekR1_7B_4bit = ModelConfiguration(
 88 |         id: "mlx-community/DeepSeek-R1-Distill-Qwen-7B-4bit",
 89 |         defaultPrompt: "Is 9.9 greater or 9.11?"
 90 |     )
 91 | 
 92 |     static public let phi4bit = ModelConfiguration(
 93 |         id: "mlx-community/phi-2-hf-4bit-mlx",
 94 |         // https://www.promptingguide.ai/models/phi-2
 95 |         defaultPrompt: "Why is the sky blue?"
 96 |     )
 97 | 
 98 |     static public let phi3_5_4bit = ModelConfiguration(
 99 |         id: "mlx-community/Phi-3.5-mini-instruct-4bit",
100 |         defaultPrompt: "What is the gravity on Mars and the moon?",
101 |         extraEOSTokens: ["<|end|>"]
102 |     )
103 | 
104 |     static public let phi3_5MoE = ModelConfiguration(
105 |         id: "mlx-community/Phi-3.5-MoE-instruct-4bit",
106 |         defaultPrompt: "What is the gravity on Mars and the moon?",
107 |         extraEOSTokens: ["<|end|>"]
108 |     ) {
109 |         prompt in
110 |         "<|user|>\n\(prompt)<|end|>\n<|assistant|>\n"
111 |     }
112 | 
113 |     static public let gemma2bQuantized = ModelConfiguration(
114 |         id: "mlx-community/quantized-gemma-2b-it",
115 |         overrideTokenizer: "PreTrainedTokenizer",
116 |         // https://www.promptingguide.ai/models/gemma
117 |         defaultPrompt: "what is the difference between lettuce and cabbage?"
118 |     )
119 | 
120 |     static public let gemma_2_9b_it_4bit = ModelConfiguration(
121 |         id: "mlx-community/gemma-2-9b-it-4bit",
122 |         overrideTokenizer: "PreTrainedTokenizer",
123 |         // https://www.promptingguide.ai/models/gemma
124 |         defaultPrompt: "What is the difference between lettuce and cabbage?"
125 |     )
126 | 
127 |     static public let gemma_2_2b_it_4bit = ModelConfiguration(
128 |         id: "mlx-community/gemma-2-2b-it-4bit",
129 |         overrideTokenizer: "PreTrainedTokenizer",
130 |         // https://www.promptingguide.ai/models/gemma
131 |         defaultPrompt: "What is the difference between lettuce and cabbage?"
132 |     )
133 | 
134 |     static public let qwen205b4bit = ModelConfiguration(
135 |         id: "mlx-community/Qwen1.5-0.5B-Chat-4bit",
136 |         overrideTokenizer: "PreTrainedTokenizer",
137 |         defaultPrompt: "why is the sky blue?"
138 |     )
139 | 
140 |     static public let qwen2_5_7b = ModelConfiguration(
141 |         id: "mlx-community/Qwen2.5-7B-Instruct-4bit",
142 |         defaultPrompt: "Why is the sky blue?"
143 |     )
144 | 
145 |     static public let qwen2_5_1_5b = ModelConfiguration(
146 |         id: "mlx-community/Qwen2.5-1.5B-Instruct-4bit",
147 |         defaultPrompt: "Why is the sky blue?"
148 |     )
149 | 
150 |     static public let qwen3_0_6b_4bit = ModelConfiguration(
151 |         id: "mlx-community/Qwen3-0.6B-4bit",
152 |         defaultPrompt: "Why is the sky blue?"
153 |     )
154 | 
155 |     static public let qwen3_1_7b_4bit = ModelConfiguration(
156 |         id: "mlx-community/Qwen3-1.7B-4bit",
157 |         defaultPrompt: "Why is the sky blue?"
158 |     )
159 | 
160 |     static public let qwen3_4b_4bit = ModelConfiguration(
161 |         id: "mlx-community/Qwen3-4B-4bit",
162 |         defaultPrompt: "Why is the sky blue?"
163 |     )
164 | 
165 |     static public let qwen3_8b_4bit = ModelConfiguration(
166 |         id: "mlx-community/Qwen3-8B-4bit",
167 |         defaultPrompt: "Why is the sky blue?"
168 |     )
169 | 
170 |     static public let qwen3MoE_30b_a3b_4bit = ModelConfiguration(
171 |         id: "mlx-community/Qwen3-30B-A3B-4bit",
172 |         defaultPrompt: "Why is the sky blue?"
173 |     )
174 | 
175 |     static public let openelm270m4bit = ModelConfiguration(
176 |         id: "mlx-community/OpenELM-270M-Instruct",
177 |         // https://huggingface.co/apple/OpenELM
178 |         defaultPrompt: "Once upon a time there was"
179 |     )
180 | 
181 |     static public let llama3_1_8B_4bit = ModelConfiguration(
182 |         id: "mlx-community/Meta-Llama-3.1-8B-Instruct-4bit",
183 |         defaultPrompt: "What is the difference between a fruit and a vegetable?"
184 |     )
185 | 
186 |     static public let llama3_8B_4bit = ModelConfiguration(
187 |         id: "mlx-community/Meta-Llama-3-8B-Instruct-4bit",
188 |         defaultPrompt: "What is the difference between a fruit and a vegetable?"
189 |     )
190 | 
191 |     static public let llama3_2_1B_4bit = ModelConfiguration(
192 |         id: "mlx-community/Llama-3.2-1B-Instruct-4bit",
193 |         defaultPrompt: "What is the difference between a fruit and a vegetable?"
194 |     )
195 | 
196 |     static public let llama3_2_3B_4bit = ModelConfiguration(
197 |         id: "mlx-community/Llama-3.2-3B-Instruct-4bit",
198 |         defaultPrompt: "What is the difference between a fruit and a vegetable?"
199 |     )
200 | 
201 |     static public let gemma3_1B_qat_4bit = ModelConfiguration(
202 |         id: "mlx-community/gemma-3-1b-it-qat-4bit",
203 |         defaultPrompt: "What is the difference between a fruit and a vegetable?",
204 |         extraEOSTokens: ["<end_of_turn>"]
205 |     )
206 | 
207 |     static public let granite3_3_2b_4bit = ModelConfiguration(
208 |         id: "mlx-community/granite-3.3-2b-instruct-4bit",
209 |         defaultPrompt: ""
210 |     )
211 | 
212 |     static public let mimo_7b_sft_4bit = ModelConfiguration(
213 |         id: "mlx-community/MiMo-7B-SFT-4bit",
214 |         defaultPrompt: "Why is the sky blue?"
215 |     )
216 | 
217 |     static public let glm4_9b_4bit = ModelConfiguration(
218 |         id: "mlx-community/GLM-4-9B-0414-4bit",
219 |         defaultPrompt: "Why is the sky blue?"
220 |     )
221 | 
222 |     static public let acereason_7b_4bit = ModelConfiguration(
223 |         id: "mlx-community/AceReason-Nemotron-7B-4bit",
224 |         defaultPrompt: ""
225 |     )
226 | 
227 |     private static func all() -> [ModelConfiguration] {
228 |         [
229 |             codeLlama13b4bit,
230 |             deepSeekR1_7B_4bit,
231 |             gemma2bQuantized,
232 |             gemma_2_2b_it_4bit,
233 |             gemma_2_9b_it_4bit,
234 |             granite3_3_2b_4bit,
235 |             llama3_1_8B_4bit,
236 |             llama3_2_1B_4bit,
237 |             llama3_2_3B_4bit,
238 |             llama3_8B_4bit,
239 |             mistral7B4bit,
240 |             mistralNeMo4bit,
241 |             openelm270m4bit,
242 |             phi3_5MoE,
243 |             phi3_5_4bit,
244 |             phi4bit,
245 |             qwen205b4bit,
246 |             qwen2_5_7b,
247 |             qwen2_5_1_5b,
248 |             qwen3_0_6b_4bit,
249 |             qwen3_1_7b_4bit,
250 |             qwen3_4b_4bit,
251 |             qwen3_8b_4bit,
252 |             qwen3MoE_30b_a3b_4bit,
253 |             smolLM_135M_4bit,
254 |             gemma3_1B_qat_4bit,
255 |             mimo_7b_sft_4bit,
256 |             glm4_9b_4bit,
257 |             acereason_7b_4bit,
258 |         ]
259 |     }
260 | 
261 | }
262 | 
263 | @available(*, deprecated, renamed: "LLMRegistry", message: "Please use LLMRegistry directly.")
264 | public typealias ModelRegistry = LLMRegistry
265 | 
266 | private struct LLMUserInputProcessor: UserInputProcessor {
267 | 
268 |     let tokenizer: Tokenizer
269 |     let configuration: ModelConfiguration
270 |     let messageGenerator: MessageGenerator
271 | 
272 |     internal init(
273 |         tokenizer: any Tokenizer, configuration: ModelConfiguration,
274 |         messageGenerator: MessageGenerator
275 |     ) {
276 |         self.tokenizer = tokenizer
277 |         self.configuration = configuration
278 |         self.messageGenerator = messageGenerator
279 |     }
280 | 
281 |     func prepare(input: UserInput) throws -> LMInput {
282 |         let messages = messageGenerator.generate(from: input)
283 |         do {
284 |             let promptTokens = try tokenizer.applyChatTemplate(
285 |                 messages: messages, tools: input.tools, additionalContext: input.additionalContext)
286 | 
287 |             return LMInput(tokens: MLXArray(promptTokens))
288 |         } catch TokenizerError.missingChatTemplate {
289 |             print(
290 |                 "No chat template was included or provided, so converting messages to simple text format. This is not optimal for model performance, so applications should provide a chat template if none is included with the model."
291 |             )
292 |             let prompt =
293 |                 messages
294 |                 .compactMap { $0["content"] as? String }
295 |                 .joined(separator: "\n\n")
296 |             let promptTokens = tokenizer.encode(text: prompt)
297 |             return LMInput(tokens: MLXArray(promptTokens))
298 |         }
299 |     }
300 | }
301 | 
302 | /// Factory for creating new LLMs.
303 | ///
304 | /// Callers can use the `shared` instance or create a new instance if custom configuration
305 | /// is required.
306 | ///
307 | /// ```swift
308 | /// let modelContainer = try await LLMModelFactory.shared.loadContainer(
309 | ///     configuration: LLMRegistry.llama3_8B_4bit)
310 | /// ```
311 | public class LLMModelFactory: ModelFactory {
312 | 
313 |     public init(typeRegistry: ModelTypeRegistry, modelRegistry: AbstractModelRegistry) {
314 |         self.typeRegistry = typeRegistry
315 |         self.modelRegistry = modelRegistry
316 |     }
317 | 
318 |     /// Shared instance with default behavior.
319 |     public static let shared = LLMModelFactory(
320 |         typeRegistry: LLMTypeRegistry.shared, modelRegistry: LLMRegistry.shared)
321 | 
322 |     /// registry of model type, e.g. configuration value `llama` -> configuration and init methods
323 |     public let typeRegistry: ModelTypeRegistry
324 | 
325 |     /// registry of model id to configuration, e.g. `mlx-community/Llama-3.2-3B-Instruct-4bit`
326 |     public let modelRegistry: AbstractModelRegistry
327 | 
328 |     public func _load(
329 |         hub: HubApi, configuration: ModelConfiguration,
330 |         progressHandler: @Sendable @escaping (Progress) -> Void
331 |     ) async throws -> sending ModelContext {
332 |         // download weights and config
333 |         let modelDirectory = try await downloadModel(
334 |             hub: hub, configuration: configuration, progressHandler: progressHandler)
335 | 
336 |         // Load the generic config to understand which model and how to load the weights
337 |         let configurationURL = modelDirectory.appending(component: "config.json")
338 | 
339 |         let baseConfig: BaseConfiguration
340 |         do {
341 |             baseConfig = try JSONDecoder().decode(
342 |                 BaseConfiguration.self, from: Data(contentsOf: configurationURL))
343 |         } catch let error as DecodingError {
344 |             throw ModelFactoryError.configurationDecodingError(
345 |                 configurationURL.lastPathComponent, configuration.name, error)
346 |         }
347 | 
348 |         let model: LanguageModel
349 |         do {
350 |             model = try typeRegistry.createModel(
351 |                 configuration: configurationURL, modelType: baseConfig.modelType)
352 |         } catch let error as DecodingError {
353 |             throw ModelFactoryError.configurationDecodingError(
354 |                 configurationURL.lastPathComponent, configuration.name, error)
355 |         }
356 | 
357 |         // apply the weights to the bare model
358 |         try loadWeights(
359 |             modelDirectory: modelDirectory, model: model,
360 |             perLayerQuantization: baseConfig.perLayerQuantization)
361 | 
362 |         let tokenizer = try await loadTokenizer(configuration: configuration, hub: hub)
363 | 
364 |         let messageGenerator =
365 |             if let model = model as? LLMModel {
366 |                 model.messageGenerator(tokenizer: tokenizer)
367 |             } else {
368 |                 DefaultMessageGenerator()
369 |             }
370 | 
371 |         let processor = LLMUserInputProcessor(
372 |             tokenizer: tokenizer, configuration: configuration,
373 |             messageGenerator: messageGenerator)
374 | 
375 |         return .init(
376 |             configuration: configuration, model: model, processor: processor, tokenizer: tokenizer)
377 |     }
378 | 
379 | }
380 | 
381 | public class TrampolineModelFactory: NSObject, ModelFactoryTrampoline {
382 |     public static func modelFactory() -> (any MLXLMCommon.ModelFactory)? {
383 |         LLMModelFactory.shared
384 |     }
385 | }
386 | 


--------------------------------------------------------------------------------
/Libraries/MLXLLM/Lora+Data.swift:
--------------------------------------------------------------------------------
 1 | // Copyright © 2024 Apple Inc.
 2 | 
 3 | import Foundation
 4 | 
 5 | enum LoRADataError: LocalizedError {
 6 |     case fileNotFound(URL, String)
 7 | 
 8 |     var errorDescription: String? {
 9 |         switch self {
10 |         case .fileNotFound(let directory, let name):
11 |             return String(
12 |                 localized: "Could not find data file '\(name)' in directory '\(directory.path())'.")
13 |         }
14 |     }
15 | }
16 | 
17 | /// Load a LoRA data file.
18 | ///
19 | /// Given a directory and a base name, e.g. `train`, this will load a `.jsonl` or `.txt` file
20 | /// if possible.
21 | public func loadLoRAData(directory: URL, name: String) throws -> [String] {
22 |     let extensions = ["jsonl", "txt"]
23 | 
24 |     for ext in extensions {
25 |         let url = directory.appending(component: "\(name).\(ext)")
26 |         if FileManager.default.fileExists(atPath: url.path()) {
27 |             return try loadLoRAData(url: url)
28 |         }
29 |     }
30 | 
31 |     throw LoRADataError.fileNotFound(directory, name)
32 | }
33 | 
34 | /// Load a .txt or .jsonl file and return the contents
35 | public func loadLoRAData(url: URL) throws -> [String] {
36 |     switch url.pathExtension {
37 |     case "jsonl":
38 |         return try loadJSONL(url: url)
39 | 
40 |     case "txt":
41 |         return try loadLines(url: url)
42 | 
43 |     default:
44 |         fatalError("Unable to load data file, unknown type: \(url)")
45 | 
46 |     }
47 | }
48 | 
49 | func loadJSONL(url: URL) throws -> [String] {
50 | 
51 |     struct Line: Codable {
52 |         let text: String?
53 |     }
54 | 
55 |     return try String(contentsOf: url)
56 |         .components(separatedBy: .newlines)
57 |         .filter {
58 |             $0.first == "{"
59 |         }
60 |         .compactMap {
61 |             try JSONDecoder().decode(Line.self, from: $0.data(using: .utf8)!).text
62 |         }
63 | }
64 | 
65 | func loadLines(url: URL) throws -> [String] {
66 |     try String(contentsOf: url)
67 |         .components(separatedBy: .newlines)
68 |         .filter { !$0.isEmpty }
69 | }
70 | 


--------------------------------------------------------------------------------
/Libraries/MLXLLM/LoraTrain.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import Foundation
  4 | import MLX
  5 | import MLXLMCommon
  6 | import MLXNN
  7 | import MLXOptimizers
  8 | import Tokenizers
  9 | 
 10 | /// Equivalent to `lora.py/iterate_batches()`. Used internally by ``LoRATrain``.
 11 | struct LoRABatchIterator: Sequence, IteratorProtocol {
 12 | 
 13 |     let dataset: [String]
 14 |     let batchSize: Int
 15 |     let tokenizer: Tokenizer
 16 | 
 17 |     let train: Bool
 18 | 
 19 |     var indices: [Int]
 20 |     var index = 0
 21 | 
 22 |     public init(dataset: [String], tokenizer: Tokenizer, batchSize: Int, train: Bool) {
 23 |         self.dataset = dataset
 24 |         self.batchSize = batchSize
 25 |         self.tokenizer = tokenizer
 26 |         self.train = train
 27 | 
 28 |         self.indices = Array(0 ..< dataset.count)
 29 |         if train {
 30 |             indices.shuffle()
 31 |         }
 32 |     }
 33 | 
 34 |     mutating public func next() -> (MLXArray, MLXArray, MLXArray)? {
 35 |         if index >= indices.count {
 36 |             if !train {
 37 |                 return nil
 38 |             }
 39 | 
 40 |             indices.shuffle()
 41 |             index = 0
 42 |         }
 43 | 
 44 |         let endIndex = Swift.min(index + batchSize, indices.count)
 45 | 
 46 |         let batch = (index ..< endIndex)
 47 |             .map { tokenizer.encode(text: dataset[indices[$0]]) }
 48 |         let lengths = batch.map { $0.count }
 49 |         let maxLength = lengths.max() ?? 0
 50 | 
 51 |         if maxLength > 2048 {
 52 |             print(
 53 |                 """
 54 |                 [WARNING] Some sequences are longer than 2048 tokens.
 55 |                 Consider pre-splitting your data to save memory.
 56 |                 """)
 57 |         }
 58 | 
 59 |         // pad to the max length
 60 |         let batchArray = MLXArray.zeros([lengths.count, maxLength], type: Int32.self)
 61 |         for (j, (b, l)) in zip(batch, lengths).enumerated() {
 62 |             batchArray[j, 0 ..< l] = MLXArray(b)
 63 |         }
 64 | 
 65 |         index = endIndex
 66 | 
 67 |         return (batchArray[0..., .stride(to: -1)], batchArray[0..., 1...], MLXArray(lengths))
 68 |     }
 69 | }
 70 | 
 71 | /// Collection of functions for adding LoRA adapters to an LLM model, training, fusing and saving/loading weights.
 72 | ///
 73 | /// The typical flow for training is:
 74 | ///
 75 | /// ```swift
 76 | /// // load the base model and tokenizer
 77 | /// let (model, tokenizer) = try await LLM.load(configuration: ModelConfiguration.mistral7B4bit)
 78 | ///
 79 | /// // add LoRALinear adapter layers
 80 | /// LoRATrain.convert(model: model, layers: Array(model.loraLinearLayers().suffix(4)))
 81 | ///
 82 | /// // optionally load LoRA weights
 83 | /// try LoRATrain.loadLoRAWeights(model: model, url: ...)
 84 | ///
 85 | /// // load the train/validation data
 86 | /// let train = try loadLoRAData(directory: data, name: "train")
 87 | /// let valid = try loadLoRAData(directory: data, name: "valid")
 88 | ///
 89 | /// // train
 90 | /// let optimizer = Adam(learningRate: 1e-5)
 91 | /// try await LoRATrain.train(
 92 | ///     model: model, train: train, validate: valid, optimizer: optimizer, tokenizer: tokenizer,
 93 | ///     parameters: LoRATrain.Parameters()
 94 | /// ) { progress in
 95 | ///     print(progress)
 96 | ///     return .more
 97 | /// }
 98 | /// ```
 99 | ///
100 | /// At this point the model will be trained and you could do one of the following:
101 | ///
102 | /// - ``saveLoRAWeights(model:url:)`` -- write the LoRA weights to a file
103 | /// - ``fuse(model:layers:deQuantize:)`` -- fuse the LoRA weights and convert back into the original model
104 | ///     architecture. These weights can be saved and reloaded with normal model handling code.
105 | /// - ``evaluate(model:dataset:loss:tokenizer:batchSize:batchCount:)``-- compute the test loss
106 | ///     againts a test dataset
107 | /// - use the in memory model as a normal `LLMModel` and evaluate a prompt
108 | ///
109 | public enum LoRATrain {
110 | 
111 |     public typealias LoraLossFunction = (Module, MLXArray, MLXArray, MLXArray) -> (
112 |         MLXArray, MLXArray
113 |     )
114 | 
115 |     /// LoRA training parameters
116 |     public struct Parameters: Sendable {
117 |         /// number of prompts to evaluate per iteration
118 |         public var batchSize = 4
119 | 
120 |         /// number of iterations to train for
121 |         public var iterations = 1000
122 | 
123 |         /// number of training steps between loss reporting
124 |         public var stepsPerReport = 10
125 | 
126 |         /// number of steps between validations
127 |         public var stepsPerEval = 100
128 | 
129 |         /// number of validations batches, `0` uses the entire validation set
130 |         public var validationBatches = 10
131 | 
132 |         /// save the model every N iterations
133 |         public var saveEvery = 100
134 | 
135 |         /// save path for the adapter `.safetensors`
136 |         public var adapterURL: URL?
137 | 
138 |         public init(
139 |             batchSize: Int = 4, iterations: Int = 1000, stepsPerReport: Int = 10,
140 |             stepsPerEval: Int = 100, validationBatches: Int = 10, saveEvery: Int = 100,
141 |             adapterURL: URL? = nil
142 |         ) {
143 |             self.batchSize = batchSize
144 |             self.iterations = iterations
145 |             self.stepsPerReport = stepsPerReport
146 |             self.stepsPerEval = stepsPerEval
147 |             self.validationBatches = validationBatches
148 |             self.saveEvery = saveEvery
149 |             self.adapterURL = adapterURL
150 |         }
151 |     }
152 | 
153 |     /// Freeze the model layers and replace the indicated modules (Linear) that should be
154 |     /// converted to ``LoRALinear`` and remain trainable.
155 |     ///
156 |     /// Once a model has had the LoRA adapters applied, adapter weights can be loaded
157 |     /// (if available):
158 |     ///
159 |     /// ```swift
160 |     /// try LoRATrain.loadLoRAWeights(model: model, url: args.adapter)
161 |     /// ```
162 |     ///
163 |     /// At this point the model is ready for one or more of the following:
164 |     ///
165 |     /// - training with ``train(model:train:validate:optimizer:loss:tokenizer:parameters:progress:)``
166 |     /// - loss evaluation with ``evaluate(model:dataset:loss:tokenizer:batchSize:batchCount:)``
167 |     /// - fusing with ``fuse(model:layers:deQuantize:)``
168 |     /// - text generation with ``generate(promptTokens:parameters:model:tokenizer:additionalEOSTokens:didGenerate:)``
169 |     ///     - note that this is just using normal model text generation
170 |     ///
171 |     /// - Parameters:
172 |     ///   - model: model to convert
173 |     ///   - layers: number of suffix layers to convert
174 |     public static func convert(model: Module, layers: LoRALinearLayers) {
175 |         model.freeze()
176 | 
177 |         for (layer, keys) in layers {
178 |             var update = ModuleChildren()
179 |             let children = layer.children()
180 |             for key in keys {
181 |                 if let item = children[key], case .value(let child) = item {
182 |                     if let linear = child as? Linear {
183 |                         update[key] = .value(LoRALinear.from(linear: linear))
184 |                     } else {
185 |                         print("\(key) on \(layer) is not Linear")
186 |                     }
187 |                 } else {
188 |                     print("failed to find key \(key) on \(layer)")
189 |                 }
190 |             }
191 |             layer.update(modules: update)
192 |         }
193 |     }
194 | 
195 |     /// Fuses the LoRA adapters back into the model weights.
196 |     ///
197 |     /// This produces a model in the original format with `Linear` or `QuantizedLinear` layer
198 |     /// weights that incorporate the LoRA adapter.
199 |     ///
200 |     /// - Parameters:
201 |     ///   - model: model to convert
202 |     ///   - deQuantize: if `true` will convert `QuantizedLinear` back into `Linear`
203 |     public static func fuse(model: Module, layers: LoRALinearLayers, deQuantize: Bool = false) {
204 |         for (layer, keys) in layers {
205 |             var update = ModuleChildren()
206 |             let children = layer.children()
207 |             for key in keys {
208 |                 if let item = children[key], case .value(let child) = item {
209 |                     if let lora = child as? LoRALayer {
210 |                         update[key] = .value(lora.fused())
211 |                     }
212 |                 }
213 |             }
214 |             if !update.isEmpty {
215 |                 layer.update(modules: update)
216 |             }
217 |         }
218 |     }
219 | 
220 |     public static func loss(model: Module, inputs: MLXArray, targets: MLXArray, lengths: MLXArray)
221 |         -> (
222 |             MLXArray, MLXArray
223 |         )
224 |     {
225 |         // def loss(model, inputs, targets, lengths):
226 | 
227 |         // run model on inputs
228 |         let model = model as! any LLMModel
229 |         let logits = model(inputs, cache: nil).asType(.float32)
230 | 
231 |         // mask padding tokens
232 |         let lengthMask = MLXArray(0 ..< inputs.dim(1))[.newAxis, 0...] .< lengths[0..., .newAxis]
233 | 
234 |         // calculate the loss
235 |         let ntoks = lengthMask.sum()
236 |         let ce = (crossEntropy(logits: logits, targets: targets) * lengthMask).sum() / ntoks
237 |         return (ce, ntoks)
238 |     }
239 | 
240 |     /// Evaluate the model and dataset and return the loss over the entire dataset.
241 |     ///
242 |     /// - Parameters:
243 |     ///   - model: the model to evaluate
244 |     ///   - dataset: the dataset
245 |     ///   - loss: loss function
246 |     ///   - tokenizer: tokenizer
247 |     ///   - batchSize: number of items from the dataset to evaluate at once
248 |     ///   - batchCount: number of batch elements to evaluate, 0 for all
249 |     /// - Returns: the loss over the enumerate data
250 |     ///
251 |     /// ### See Also
252 |     /// - ``loadLoRAData(directory:name:)``
253 |     public static func evaluate(
254 |         model: Module, dataset: [String], loss: LoraLossFunction = loss, tokenizer: Tokenizer,
255 |         batchSize: Int, batchCount: Int
256 |     ) -> Float {
257 |         var allLosses = [Float]()
258 |         var tokenCount = 0
259 | 
260 |         for (iteration, (inputs, targets, lengths)) in LoRABatchIterator(
261 |             dataset: dataset, tokenizer: tokenizer, batchSize: batchSize, train: false
262 |         ).enumerated() {
263 |             let (losses, tokens) = loss(model, inputs, targets, lengths)
264 |             allLosses.append((losses * tokens).item(Float.self))
265 |             tokenCount += tokens.item(Int.self)
266 | 
267 |             if batchCount != 0 && iteration + 1 >= batchCount {
268 |                 break
269 |             }
270 |         }
271 | 
272 |         return (sum(MLXArray(allLosses), stream: .cpu) / tokenCount).item(Float.self)
273 |     }
274 | 
275 |     /// Given a model with LoRA adaptors applied, load adapter weights from a `.safetensors` file.
276 |     ///
277 |     /// ### See Also
278 |     /// - ``convert(model:layers:)``
279 |     /// - ``saveLoRAWeights(model:url:)``
280 |     public static func loadLoRAWeights(model: Module, url: URL) throws {
281 |         let weights = try ModuleParameters.unflattened(loadArrays(url: url))
282 |         try model.update(parameters: weights, verify: .noUnusedKeys)
283 |         eval(model)
284 |     }
285 | 
286 |     /// Given a model with LoRA adaptors applied, write adapter weights to a `.safetensors` file.
287 |     ///
288 |     /// ### See Also
289 |     /// - ``convert(model:layers:)``
290 |     /// - ``loadLoRAWeights(model:url:)``
291 |     public static func saveLoRAWeights(model: Module, url: URL) throws {
292 |         let parameters = Dictionary(
293 |             uniqueKeysWithValues: model.trainableParameters().flattened())
294 |         try save(arrays: parameters, url: url)
295 |     }
296 | 
297 |     public enum Progress: CustomStringConvertible, Sendable {
298 |         case train(
299 |             iteration: Int, trainingLoss: Float, iterationsPerSecond: Double,
300 |             tokensPerSecond: Double)
301 |         case validation(iteration: Int, validationLoss: Float, validationTime: Double)
302 |         case save(iteration: Int, url: URL)
303 | 
304 |         public var description: String {
305 |             switch self {
306 |             case .train(
307 |                 let iteration, let trainingLoss, let iterationsPerSecond, let tokensPerSecond):
308 |                 "Iteration \(iteration + 1): training loss \(trainingLoss.formatted()), "
309 |                     + "iterations/sec \(iterationsPerSecond.formatted()), "
310 |                     + "Tokens/sec \(tokensPerSecond.formatted())"
311 |             case .validation(let iteration, let validationLoss, let validationTime):
312 |                 "Iteration \(iteration + 1): "
313 |                     + "validation loss \(validationLoss.formatted()), "
314 |                     + "validation time \(validationTime.formatted())s"
315 |             case .save(let iteration, let url):
316 |                 "Iteration \(iteration + 1): saved weights to \(url.path())"
317 |             }
318 |         }
319 |     }
320 | 
321 |     public enum ProgressDisposition: Sendable {
322 |         case stop
323 |         case more
324 |     }
325 | 
326 |     /// Train (or continue training) LoRA weights.
327 |     ///
328 |     /// - Parameters:
329 |     ///   - model: model to train
330 |     ///   - train: training dataset
331 |     ///   - validate: validate dataset
332 |     ///   - optimizer: optimizer used in training
333 |     ///   - loss: loss function
334 |     ///   - tokenizer: tokenizer
335 |     ///   - parameters: training parameters
336 |     ///   - progress: progress callback
337 |     public static func train(
338 |         model: Module, train: [String], validate: [String], optimizer: Optimizer,
339 |         loss: @escaping LoraLossFunction = loss, tokenizer: Tokenizer, parameters: Parameters,
340 |         progress: (Progress) -> ProgressDisposition
341 |     ) throws {
342 |         // def train(model, train_set, val_set, optimizer, loss, tokenizer, args)
343 | 
344 |         let lossValueGrad = valueAndGrad(model: model) { model, arrays in
345 |             let (ce, ntoks) = loss(model, arrays[0], arrays[1], arrays[2])
346 |             return [ce, ntoks]
347 |         }
348 | 
349 |         var losses = [Float]()
350 |         var tokenCount = 0
351 | 
352 |         var start = Date.timeIntervalSinceReferenceDate
353 | 
354 |         for (iteration, (inputs, targets, lengths)) in LoRABatchIterator(
355 |             dataset: train, tokenizer: tokenizer, batchSize: parameters.batchSize, train: true
356 |         ).enumerated() {
357 |             // forward and backward pass
358 |             let (resultArray, grad) = lossValueGrad(model, [inputs, targets, lengths])
359 |             let lvalue = resultArray[0]
360 |             let tokens = resultArray[1]
361 | 
362 |             // model update
363 |             optimizer.update(model: model, gradients: grad)
364 |             eval(model, optimizer, lvalue)
365 | 
366 |             // record loss
367 |             losses.append(lvalue.item(Float.self))
368 |             tokenCount += tokens.item(Int.self)
369 | 
370 |             // report training loss
371 |             if (iteration + 1) % parameters.stepsPerReport == 0 {
372 |                 let trainingLoss = MLXArray(losses).mean(stream: .cpu).item(Float.self)
373 |                 let now = Date.timeIntervalSinceReferenceDate
374 | 
375 |                 let iterationsPerSecond = Double(parameters.stepsPerReport) / (now - start)
376 |                 let tokensPerSecond = Double(tokenCount) / (now - start)
377 | 
378 |                 if progress(
379 |                     .train(
380 |                         iteration: iteration, trainingLoss: trainingLoss,
381 |                         iterationsPerSecond: iterationsPerSecond, tokensPerSecond: tokensPerSecond))
382 |                     == .stop
383 |                 {
384 |                     break
385 |                 }
386 | 
387 |                 losses.removeAll()
388 |                 tokenCount = 0
389 |                 start = Date.timeIntervalSinceReferenceDate
390 |             }
391 | 
392 |             // report validation loss
393 |             if iteration == 0 || (iteration + 1) % parameters.stepsPerEval == 0 {
394 |                 let validationStart = Date.timeIntervalSinceReferenceDate
395 |                 let validationLoss = evaluate(
396 |                     model: model, dataset: validate, loss: loss, tokenizer: tokenizer,
397 |                     batchSize: parameters.batchSize, batchCount: parameters.validationBatches)
398 |                 let now = Date.timeIntervalSinceReferenceDate
399 | 
400 |                 if progress(
401 |                     .validation(
402 |                         iteration: iteration, validationLoss: validationLoss,
403 |                         validationTime: now - validationStart)) == .stop
404 |                 {
405 |                     break
406 |                 }
407 | 
408 |                 start = Date.timeIntervalSinceReferenceDate
409 |             }
410 | 
411 |             // save adapter weights if needed
412 |             if let adapterURL = parameters.adapterURL, (iteration + 1) % parameters.saveEvery == 0 {
413 |                 try saveLoRAWeights(model: model, url: adapterURL)
414 | 
415 |                 if progress(.save(iteration: iteration, url: adapterURL)) == .stop {
416 |                     break
417 |                 }
418 | 
419 |                 start = Date.timeIntervalSinceReferenceDate
420 |             }
421 | 
422 |             if iteration + 1 >= parameters.iterations {
423 |                 break
424 |             }
425 |         }
426 |     }
427 | }
428 | 


--------------------------------------------------------------------------------
/Libraries/MLXLLM/Models/Cohere.swift:
--------------------------------------------------------------------------------
  1 | import Foundation
  2 | import MLX
  3 | import MLXLMCommon
  4 | import MLXNN
  5 | 
  6 | // port of https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/models/cohere.py
  7 | 
  8 | private class Attention: Module {
  9 | 
 10 |     let args: CohereConfiguration
 11 |     let scale: Float
 12 | 
 13 |     @ModuleInfo(key: "q_proj") var wq: Linear
 14 |     @ModuleInfo(key: "k_proj") var wk: Linear
 15 |     @ModuleInfo(key: "v_proj") var wv: Linear
 16 |     @ModuleInfo(key: "o_proj") var wo: Linear
 17 | 
 18 |     let rope: RoPE
 19 | 
 20 |     public init(_ args: CohereConfiguration) {
 21 |         self.args = args
 22 | 
 23 |         let dim = args.hiddenSize
 24 |         let heads = args.attentionHeads
 25 |         let kvHeads = args.kvHeads
 26 | 
 27 |         let headDim = args.hiddenSize / heads
 28 |         self.scale = pow(Float(headDim), -0.5)
 29 | 
 30 |         self._wq.wrappedValue = Linear(dim, heads * headDim, bias: false)
 31 |         self._wk.wrappedValue = Linear(dim, kvHeads * headDim, bias: false)
 32 |         self._wv.wrappedValue = Linear(dim, kvHeads * headDim, bias: false)
 33 |         self._wo.wrappedValue = Linear(heads * headDim, dim, bias: false)
 34 | 
 35 |         self.rope = RoPE(
 36 |             dimensions: headDim, traditional: args.ropeTraditional, base: args.ropeTheta)
 37 |     }
 38 | 
 39 |     public func callAsFunction(
 40 |         _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
 41 |     ) -> MLXArray {
 42 |         let (B, L) = (x.dim(0), x.dim(1))
 43 | 
 44 |         var queries = wq(x)
 45 |         var keys = wk(x)
 46 |         var values = wv(x)
 47 | 
 48 |         // prepare the queries, keys and values for the attention computation
 49 |         queries = queries.reshaped(B, L, args.attentionHeads, -1).transposed(0, 2, 1, 3)
 50 |         keys = keys.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)
 51 |         values = values.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)
 52 | 
 53 |         if let cache {
 54 |             queries = rope(queries, offset: cache.offset)
 55 |             keys = rope(keys, offset: cache.offset)
 56 |             (keys, values) = cache.update(keys: keys, values: values)
 57 |         } else {
 58 |             queries = rope(queries)
 59 |             keys = rope(keys)
 60 |         }
 61 | 
 62 |         let output = MLXFast.scaledDotProductAttention(
 63 |             queries: queries, keys: keys, values: values, scale: scale, mask: mask
 64 |         )
 65 |         .transposed(0, 2, 1, 3)
 66 |         .reshaped(B, L, -1)
 67 | 
 68 |         return wo(output)
 69 |     }
 70 | }
 71 | 
 72 | private class MLP: Module, UnaryLayer {
 73 | 
 74 |     @ModuleInfo(key: "gate_proj") var gate: Linear
 75 |     @ModuleInfo(key: "down_proj") var down: Linear
 76 |     @ModuleInfo(key: "up_proj") var up: Linear
 77 | 
 78 |     public init(dimensions: Int, hiddenDimensions: Int) {
 79 |         self._gate.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)
 80 |         self._up.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)
 81 |         self._down.wrappedValue = Linear(hiddenDimensions, dimensions, bias: false)
 82 | 
 83 |     }
 84 | 
 85 |     public func callAsFunction(_ x: MLXArray) -> MLXArray {
 86 |         down(silu(gate(x)) * up(x))
 87 |     }
 88 | }
 89 | 
 90 | private class TransformerBlock: Module {
 91 | 
 92 |     @ModuleInfo(key: "self_attn") var attention: Attention
 93 |     let mlp: MLP
 94 | 
 95 |     @ModuleInfo(key: "input_layernorm") var inputLayerNorm: LayerNorm
 96 | 
 97 |     public init(_ args: CohereConfiguration) {
 98 |         self._attention.wrappedValue = Attention(args)
 99 |         self.mlp = MLP(dimensions: args.hiddenSize, hiddenDimensions: args.intermediateSize)
100 |         self._inputLayerNorm.wrappedValue = LayerNorm(
101 |             dimensions: args.hiddenSize, eps: args.layerNormEps)
102 | 
103 |     }
104 | 
105 |     public func callAsFunction(
106 |         _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
107 |     ) -> MLXArray {
108 |         let h = inputLayerNorm(x)
109 |         let attnH = attention(h, mask: mask, cache: cache)
110 |         let ffH = mlp(h)
111 |         return attnH + ffH + x
112 |     }
113 | }
114 | 
115 | public class CohereModelInner: Module {
116 | 
117 |     @ModuleInfo(key: "embed_tokens") var embedTokens: Embedding
118 | 
119 |     fileprivate let layers: [TransformerBlock]
120 |     let norm: LayerNorm
121 | 
122 |     public init(_ args: CohereConfiguration) {
123 |         precondition(args.vocabularySize > 0)
124 | 
125 |         self._embedTokens.wrappedValue = Embedding(
126 |             embeddingCount: args.vocabularySize, dimensions: args.hiddenSize)
127 | 
128 |         self.layers = (0 ..< args.hiddenLayers)
129 |             .map { _ in
130 |                 TransformerBlock(args)
131 |             }
132 |         self.norm = LayerNorm(dimensions: args.hiddenSize, eps: args.layerNormEps)
133 |     }
134 | 
135 |     public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]? = nil) -> MLXArray {
136 |         var h = embedTokens(inputs)
137 | 
138 |         let mask = createAttentionMask(h: h, cache: cache)
139 | 
140 |         for (i, layer) in layers.enumerated() {
141 |             h = layer(h, mask: mask, cache: cache?[i])
142 |         }
143 | 
144 |         return norm(h)
145 |     }
146 | }
147 | 
148 | public class CohereModel: Module, LLMModel, KVCacheDimensionProvider {
149 | 
150 |     public let vocabularySize: Int
151 |     public let kvHeads: [Int]
152 | 
153 |     let model: CohereModelInner
154 |     let logitScale: Float
155 | 
156 |     public init(_ args: CohereConfiguration) {
157 |         self.vocabularySize = args.vocabularySize
158 |         self.kvHeads = (0 ..< args.hiddenLayers).map { _ in args.kvHeads }
159 |         self.model = CohereModelInner(args)
160 |         self.logitScale = args.logitScale
161 |     }
162 | 
163 |     public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {
164 |         var out = model(inputs, cache: cache)
165 |         out = model.embedTokens.asLinear(out)
166 |         out = out * self.logitScale
167 |         return out
168 |     }
169 | }
170 | 
171 | public struct CohereConfiguration: Codable, Sendable {
172 | 
173 |     var hiddenSize: Int
174 |     var hiddenLayers: Int
175 |     var intermediateSize: Int
176 |     var attentionHeads: Int
177 |     var layerNormEps: Float
178 |     var vocabularySize: Int
179 |     var kvHeads: Int
180 |     var ropeTheta: Float = 8000000.0
181 |     var ropeTraditional: Bool = true
182 |     var ropeScaling: [String: StringOrNumber]? = nil
183 |     var logitScale: Float
184 | 
185 |     enum CodingKeys: String, CodingKey {
186 |         case hiddenSize = "hidden_size"
187 |         case hiddenLayers = "num_hidden_layers"
188 |         case intermediateSize = "intermediate_size"
189 |         case attentionHeads = "num_attention_heads"
190 |         case kvHeads = "num_key_value_heads"
191 |         case ropeTheta = "rope_theta"
192 |         case vocabularySize = "vocab_size"
193 |         case layerNormEps = "layer_norm_eps"
194 |         case logitScale = "logit_scale"
195 |         case ropeTraditional = "rope_traditional"
196 |         case ropeScaling = "rope_scaling"
197 |     }
198 | 
199 |     public init(from decoder: Decoder) throws {
200 |         // custom implementation to handle optional keys with required values
201 |         let container: KeyedDecodingContainer<CohereConfiguration.CodingKeys> =
202 |             try decoder.container(
203 |                 keyedBy: CohereConfiguration.CodingKeys.self)
204 | 
205 |         self.hiddenSize = try container.decode(
206 |             Int.self, forKey: CohereConfiguration.CodingKeys.hiddenSize)
207 |         self.hiddenLayers = try container.decode(
208 |             Int.self, forKey: CohereConfiguration.CodingKeys.hiddenLayers)
209 |         self.intermediateSize = try container.decode(
210 |             Int.self, forKey: CohereConfiguration.CodingKeys.intermediateSize)
211 |         self.attentionHeads = try container.decode(
212 |             Int.self, forKey: CohereConfiguration.CodingKeys.attentionHeads)
213 |         self.layerNormEps = try container.decode(
214 |             Float.self, forKey: CohereConfiguration.CodingKeys.layerNormEps)
215 |         self.vocabularySize = try container.decode(
216 |             Int.self, forKey: CohereConfiguration.CodingKeys.vocabularySize)
217 |         self.kvHeads = try container.decode(
218 |             Int.self, forKey: CohereConfiguration.CodingKeys.kvHeads)
219 |         self.ropeTheta =
220 |             try container.decodeIfPresent(
221 |                 Float.self, forKey: CohereConfiguration.CodingKeys.ropeTheta)
222 |             ?? 8000000.0
223 |         self.ropeScaling = try container.decodeIfPresent(
224 |             [String: StringOrNumber].self, forKey: CohereConfiguration.CodingKeys.ropeScaling)
225 |         self.logitScale = try container.decode(
226 |             Float.self, forKey: CohereConfiguration.CodingKeys.logitScale)
227 |     }
228 | }
229 | 
230 | // MARK: - LoRA
231 | 
232 | extension CohereModel: LoRAModel {
233 |     public func loraLinearLayers() -> LoRALinearLayers {
234 |         model.layers.map { ($0.attention, ["q_proj", "v_proj"]) }
235 |     }
236 | }
237 | 


--------------------------------------------------------------------------------
/Libraries/MLXLLM/Models/GLM4.swift:
--------------------------------------------------------------------------------
  1 | //
  2 | //  GLM4.swift
  3 | //  LLM
  4 | //
  5 | //  Created by John Mai on 2025/5/1.
  6 | //
  7 | 
  8 | import Foundation
  9 | import MLX
 10 | import MLXLMCommon
 11 | import MLXNN
 12 | 
 13 | // port of https://github.com/ml-explore/mlx-lm/blob/main/mlx_lm/models/glm4.py
 14 | 
 15 | private class Attention: Module {
 16 |     let args: GLM4Configuration
 17 |     let scale: Float
 18 | 
 19 |     @ModuleInfo(key: "q_proj") var wq: Linear
 20 |     @ModuleInfo(key: "k_proj") var wk: Linear
 21 |     @ModuleInfo(key: "v_proj") var wv: Linear
 22 |     @ModuleInfo(key: "o_proj") var wo: Linear
 23 | 
 24 |     let rope: RoPE
 25 | 
 26 |     public init(_ args: GLM4Configuration) {
 27 |         self.args = args
 28 | 
 29 |         let headDim = args.headDim > 0 ? args.headDim : args.hiddenSize / args.attentionHeads
 30 |         self.scale = pow(Float(headDim), -0.5)
 31 | 
 32 |         _wq.wrappedValue = Linear(
 33 |             args.hiddenSize, args.attentionHeads * headDim, bias: args.attentionBias)
 34 |         _wk.wrappedValue = Linear(args.hiddenSize, args.kvHeads * headDim, bias: args.attentionBias)
 35 |         _wv.wrappedValue = Linear(args.hiddenSize, args.kvHeads * headDim, bias: args.attentionBias)
 36 |         _wo.wrappedValue = Linear(args.attentionHeads * headDim, args.hiddenSize, bias: false)
 37 | 
 38 |         self.rope = RoPE(
 39 |             dimensions: Int(Float(headDim) * args.partialRotaryFactor),
 40 |             traditional: args.ropeTraditional, base: args.ropeTheta)
 41 |     }
 42 | 
 43 |     public func callAsFunction(
 44 |         _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
 45 |     ) -> MLXArray {
 46 |         let (B, L) = (x.dim(0), x.dim(1))
 47 | 
 48 |         var queries = wq(x)
 49 |         var keys = wk(x)
 50 |         var values = wv(x)
 51 | 
 52 |         queries = queries.reshaped(B, L, args.attentionHeads, -1).transposed(0, 2, 1, 3)
 53 |         keys = keys.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)
 54 |         values = values.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)
 55 | 
 56 |         if let cache {
 57 |             queries = rope(queries, offset: cache.offset)
 58 |             keys = rope(keys, offset: cache.offset)
 59 |             (keys, values) = cache.update(
 60 |                 keys: keys, values: values)
 61 |         } else {
 62 |             queries = rope(queries)
 63 |             keys = rope(keys)
 64 |         }
 65 | 
 66 |         let output = MLXFast.scaledDotProductAttention(
 67 |             queries: queries, keys: keys, values: values, scale: scale,
 68 |             mask: mask
 69 |         )
 70 |         .transposed(0, 2, 1, 3)
 71 |         .reshaped(B, L, -1)
 72 | 
 73 |         return wo(output)
 74 |     }
 75 | }
 76 | 
 77 | private class MLP: Module, UnaryLayer {
 78 |     @ModuleInfo(key: "gate_up_proj") var gateUp: Linear
 79 |     @ModuleInfo(key: "down_proj") var down: Linear
 80 | 
 81 |     public init(_ args: GLM4Configuration) {
 82 |         _gateUp.wrappedValue = Linear(args.hiddenSize, 2 * args.intermediateSize, bias: false)
 83 |         _down.wrappedValue = Linear(args.intermediateSize, args.hiddenSize, bias: false)
 84 |     }
 85 | 
 86 |     public func callAsFunction(_ x: MLXArray) -> MLXArray {
 87 |         let x = gateUp(x)
 88 |         let chunks = split(x, parts: 2, axis: -1)
 89 |         return down(silu(chunks[0]) * chunks[1])
 90 |     }
 91 | }
 92 | 
 93 | private class GLM4DecoderLayer: Module {
 94 |     @ModuleInfo(key: "self_attn") var attention: Attention
 95 |     let mlp: MLP
 96 | 
 97 |     @ModuleInfo(key: "input_layernorm") var inputLayerNorm: RMSNorm
 98 |     @ModuleInfo(key: "post_attention_layernorm") var postAttentionLayerNorm: RMSNorm
 99 |     @ModuleInfo(key: "post_self_attn_layernorm") var postSelfAttnLayerNorm: RMSNorm
100 |     @ModuleInfo(key: "post_mlp_layernorm") var postMlpLayerNorm: RMSNorm
101 | 
102 |     public init(_ args: GLM4Configuration) {
103 |         _attention.wrappedValue = Attention(args)
104 |         self.mlp = MLP(args)
105 |         _inputLayerNorm.wrappedValue = RMSNorm(
106 |             dimensions: args.hiddenSize, eps: args.rmsNormEps)
107 |         _postAttentionLayerNorm.wrappedValue = RMSNorm(
108 |             dimensions: args.hiddenSize, eps: args.rmsNormEps)
109 |         _postSelfAttnLayerNorm.wrappedValue = RMSNorm(
110 |             dimensions: args.hiddenSize, eps: args.rmsNormEps)
111 |         _postMlpLayerNorm.wrappedValue = RMSNorm(
112 |             dimensions: args.hiddenSize, eps: args.rmsNormEps)
113 |     }
114 | 
115 |     public func callAsFunction(
116 |         _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
117 |     ) -> MLXArray {
118 |         var x =
119 |             x
120 |             + postSelfAttnLayerNorm(
121 |                 attention(inputLayerNorm(x), mask: mask, cache: cache)
122 |             )
123 |         let residual = x
124 |         x = postMlpLayerNorm(mlp(postAttentionLayerNorm(x))) + residual
125 |         return x
126 |     }
127 | }
128 | 
129 | private class GLM4ModelInner: Module {
130 |     @ModuleInfo(key: "embed_tokens") var embedTokens: Embedding
131 | 
132 |     fileprivate let layers: [GLM4DecoderLayer]
133 |     let norm: RMSNorm
134 | 
135 |     public init(_ args: GLM4Configuration) {
136 |         precondition(args.vocabularySize > 0)
137 | 
138 |         _embedTokens.wrappedValue = Embedding(
139 |             embeddingCount: args.vocabularySize, dimensions: args.hiddenSize)
140 | 
141 |         self.layers = (0 ..< args.hiddenLayers)
142 |             .map { _ in
143 |                 GLM4DecoderLayer(args)
144 |             }
145 |         self.norm = RMSNorm(dimensions: args.hiddenSize, eps: args.rmsNormEps)
146 |     }
147 | 
148 |     public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]? = nil) -> MLXArray {
149 |         var h = embedTokens(inputs)
150 | 
151 |         let mask = createAttentionMask(h: h, cache: cache)
152 | 
153 |         for (i, layer) in layers.enumerated() {
154 |             h = layer(h, mask: mask, cache: cache?[i])
155 |         }
156 | 
157 |         return norm(h)
158 |     }
159 | }
160 | 
161 | public class GLM4Model: Module, LLMModel, KVCacheDimensionProvider {
162 |     public let vocabularySize: Int
163 |     public let kvHeads: [Int]
164 | 
165 |     private let model: GLM4ModelInner
166 |     let configuration: GLM4Configuration
167 |     let modelType: String
168 | 
169 |     @ModuleInfo(key: "lm_head") var lmHead: Linear
170 | 
171 |     public init(_ args: GLM4Configuration) {
172 |         self.configuration = args
173 |         self.vocabularySize = args.vocabularySize
174 |         self.kvHeads = (0 ..< args.hiddenLayers).map { _ in args.kvHeads }
175 |         self.modelType = args.modelType
176 |         self.model = GLM4ModelInner(args)
177 | 
178 |         _lmHead.wrappedValue = Linear(args.hiddenSize, args.vocabularySize, bias: false)
179 |     }
180 | 
181 |     public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {
182 |         let out = model(inputs, cache: cache)
183 |         return lmHead(out)
184 |     }
185 | 
186 |     public func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {
187 |         var weights = weights
188 | 
189 |         if configuration.tieWordEmbeddings {
190 |             weights["lm_head.weight"] = nil
191 |         }
192 | 
193 |         return weights
194 |     }
195 | }
196 | 
197 | public struct GLM4Configuration: Codable, Sendable {
198 |     var hiddenSize: Int
199 |     var hiddenLayers: Int
200 |     var intermediateSize: Int
201 |     var attentionHeads: Int
202 |     var attentionBias: Bool
203 |     var headDim: Int
204 |     var rmsNormEps: Float
205 |     var vocabularySize: Int
206 |     var kvHeads: Int
207 |     var partialRotaryFactor: Float
208 |     var ropeTheta: Float = 10000.0
209 |     var ropeTraditional: Bool = true
210 |     var tieWordEmbeddings = false
211 |     var maxPositionEmbeddings: Int = 32768
212 |     var modelType: String
213 | 
214 |     enum CodingKeys: String, CodingKey {
215 |         case hiddenSize = "hidden_size"
216 |         case hiddenLayers = "num_hidden_layers"
217 |         case intermediateSize = "intermediate_size"
218 |         case attentionHeads = "num_attention_heads"
219 |         case attentionBias = "attention_bias"
220 |         case headDim = "head_dim"
221 |         case rmsNormEps = "rms_norm_eps"
222 |         case vocabularySize = "vocab_size"
223 |         case kvHeads = "num_key_value_heads"
224 |         case partialRotaryFactor = "partial_rotary_factor"
225 |         case ropeTheta = "rope_theta"
226 |         case ropeTraditional = "rope_traditional"
227 |         case tieWordEmbeddings = "tie_word_embeddings"
228 |         case maxPositionEmbeddings = "max_position_embeddings"
229 |         case modelType = "model_type"
230 |     }
231 | 
232 |     public init(from decoder: Decoder) throws {
233 |         let container: KeyedDecodingContainer<GLM4Configuration.CodingKeys> =
234 |             try decoder.container(
235 |                 keyedBy: GLM4Configuration.CodingKeys.self)
236 | 
237 |         self.modelType = try container.decode(
238 |             String.self, forKey: GLM4Configuration.CodingKeys.modelType)
239 |         self.hiddenSize = try container.decode(
240 |             Int.self, forKey: GLM4Configuration.CodingKeys.hiddenSize)
241 |         self.hiddenLayers = try container.decode(
242 |             Int.self, forKey: GLM4Configuration.CodingKeys.hiddenLayers)
243 |         self.intermediateSize = try container.decode(
244 |             Int.self, forKey: GLM4Configuration.CodingKeys.intermediateSize)
245 |         self.attentionHeads = try container.decode(
246 |             Int.self, forKey: GLM4Configuration.CodingKeys.attentionHeads)
247 |         self.attentionBias = try container.decode(
248 |             Bool.self, forKey: GLM4Configuration.CodingKeys.attentionBias)
249 |         self.headDim = try container.decode(
250 |             Int.self, forKey: GLM4Configuration.CodingKeys.headDim)
251 |         self.rmsNormEps = try container.decode(
252 |             Float.self, forKey: GLM4Configuration.CodingKeys.rmsNormEps)
253 |         self.vocabularySize = try container.decode(
254 |             Int.self, forKey: GLM4Configuration.CodingKeys.vocabularySize)
255 |         self.kvHeads = try container.decode(Int.self, forKey: GLM4Configuration.CodingKeys.kvHeads)
256 |         self.partialRotaryFactor = try container.decode(
257 |             Float.self, forKey: GLM4Configuration.CodingKeys.partialRotaryFactor)
258 |         self.ropeTheta =
259 |             try container.decodeIfPresent(
260 |                 Float.self, forKey: GLM4Configuration.CodingKeys.ropeTheta)
261 |             ?? 10000.0
262 |         self.ropeTraditional =
263 |             try container.decodeIfPresent(
264 |                 Bool.self, forKey: GLM4Configuration.CodingKeys.ropeTraditional)
265 |             ?? true
266 |         self.tieWordEmbeddings =
267 |             try container.decodeIfPresent(Bool.self, forKey: .tieWordEmbeddings) ?? false
268 |         self.maxPositionEmbeddings =
269 |             try container.decodeIfPresent(Int.self, forKey: .maxPositionEmbeddings) ?? 32768
270 |     }
271 | }
272 | 
273 | // MARK: - LoRA
274 | 
275 | extension GLM4Model: LoRAModel {
276 |     public func loraLinearLayers() -> LoRALinearLayers {
277 |         model.layers.map { ($0.attention, ["q_proj", "v_proj"]) }
278 |     }
279 | }
280 | 


--------------------------------------------------------------------------------
/Libraries/MLXLLM/Models/Gemma.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import Foundation
  4 | import MLX
  5 | import MLXLMCommon
  6 | import MLXNN
  7 | import Tokenizers
  8 | 
  9 | // Port of https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/models/gemma.py
 10 | 
 11 | // Specialized norm for Gemma
 12 | private class RMSNorm: Module, UnaryLayer {
 13 |     let weight: MLXArray
 14 |     let eps: Float
 15 | 
 16 |     public init(dimensions: Int, eps: Float = 1e-5) {
 17 |         self.weight = MLXArray.ones([dimensions])
 18 |         self.eps = eps
 19 |     }
 20 | 
 21 |     public func callAsFunction(_ x: MLXArray) -> MLXArray {
 22 |         return MLXFast.rmsNorm(x, weight: 1.0 + self.weight, eps: self.eps)
 23 |     }
 24 | }
 25 | 
 26 | private class Attention: Module {
 27 |     let args: GemmaConfiguration
 28 |     let nHeads: Int
 29 |     let nKVHeads: Int
 30 |     let headDim: Int
 31 |     let scale: Float
 32 | 
 33 |     @ModuleInfo(key: "q_proj") var wq: Linear
 34 |     @ModuleInfo(key: "k_proj") var wk: Linear
 35 |     @ModuleInfo(key: "v_proj") var wv: Linear
 36 |     @ModuleInfo(key: "o_proj") var wo: Linear
 37 | 
 38 |     let rope: RoPE
 39 | 
 40 |     public init(_ args: GemmaConfiguration) {
 41 |         self.args = args
 42 | 
 43 |         let dim = args.hiddenSize
 44 |         self.nHeads = args.attentionHeads
 45 |         self.nKVHeads = args.kvHeads
 46 |         self.headDim = args.headDimensions
 47 |         self.scale = pow(Float(headDim), -0.5)
 48 | 
 49 |         self._wq.wrappedValue = Linear(dim, nHeads * headDim, bias: false)
 50 |         self._wk.wrappedValue = Linear(dim, nKVHeads * headDim, bias: false)
 51 |         self._wv.wrappedValue = Linear(dim, nKVHeads * headDim, bias: false)
 52 |         self._wo.wrappedValue = Linear(nHeads * headDim, dim, bias: false)
 53 | 
 54 |         self.rope = RoPE(
 55 |             dimensions: headDim, traditional: args.ropeTraditional, base: args.ropeTheta)
 56 |     }
 57 | 
 58 |     public func callAsFunction(
 59 |         _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
 60 |     ) -> MLXArray {
 61 |         let (B, L) = (x.dim(0), x.dim(1))
 62 | 
 63 |         var queries = wq(x)
 64 |         var keys = wk(x)
 65 |         var values = wv(x)
 66 | 
 67 |         // Prepare the queries, keys and values for the attention computation
 68 |         queries = queries.reshaped(B, L, nHeads, -1).transposed(0, 2, 1, 3)
 69 |         keys = keys.reshaped(B, L, nKVHeads, -1).transposed(0, 2, 1, 3)
 70 |         values = values.reshaped(B, L, nKVHeads, -1).transposed(0, 2, 1, 3)
 71 | 
 72 |         if let cache {
 73 |             queries = rope(queries, offset: cache.offset)
 74 |             keys = rope(keys, offset: cache.offset)
 75 |             (keys, values) = cache.update(keys: keys, values: values)
 76 |         } else {
 77 |             queries = rope(queries)
 78 |             keys = rope(keys)
 79 |         }
 80 | 
 81 |         let output = MLXFast.scaledDotProductAttention(
 82 |             queries: queries, keys: keys, values: values, scale: scale, mask: mask
 83 |         )
 84 |         .transposed(0, 2, 1, 3)
 85 |         .reshaped(B, L, -1)
 86 | 
 87 |         return wo(output)
 88 |     }
 89 | }
 90 | 
 91 | private class MLP: Module, UnaryLayer {
 92 |     @ModuleInfo(key: "gate_proj") var gate: Linear
 93 |     @ModuleInfo(key: "down_proj") var down: Linear
 94 |     @ModuleInfo(key: "up_proj") var up: Linear
 95 | 
 96 |     public init(dimensions: Int, hiddenDimensions: Int) {
 97 |         self._gate.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)
 98 |         self._down.wrappedValue = Linear(hiddenDimensions, dimensions, bias: false)
 99 |         self._up.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)
100 |     }
101 | 
102 |     public func callAsFunction(_ x: MLXArray) -> MLXArray {
103 |         down(gelu(gate(x)) * up(x))
104 |     }
105 | }
106 | 
107 | private class TransformerBlock: Module {
108 |     @ModuleInfo(key: "self_attn") var attention: Attention
109 |     let mlp: MLP
110 | 
111 |     @ModuleInfo(key: "input_layernorm") var inputLayerNorm: Gemma.RMSNorm
112 |     @ModuleInfo(key: "post_attention_layernorm") var postAttentionLayerNorm: Gemma.RMSNorm
113 | 
114 |     public init(_ args: GemmaConfiguration) {
115 |         self._attention.wrappedValue = Attention(args)
116 |         self.mlp = MLP(dimensions: args.hiddenSize, hiddenDimensions: args.intermediateSize)
117 |         self._inputLayerNorm.wrappedValue = Gemma.RMSNorm(
118 |             dimensions: args.hiddenSize, eps: args.rmsNormEps)
119 |         self._postAttentionLayerNorm.wrappedValue = Gemma.RMSNorm(
120 |             dimensions: args.hiddenSize, eps: args.rmsNormEps)
121 |     }
122 | 
123 |     public func callAsFunction(
124 |         _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
125 |     ) -> MLXArray {
126 |         var r = attention(inputLayerNorm(x), mask: mask, cache: cache)
127 |         let h = x + r
128 |         r = mlp(postAttentionLayerNorm(h))
129 |         return h + r
130 |     }
131 | }
132 | 
133 | private class GemmaModelInner: Module {
134 |     let args: GemmaConfiguration
135 |     let vocabularySize: Int
136 |     let numHiddenLayers: Int
137 | 
138 |     @ModuleInfo(key: "embed_tokens") var embedTokens: Embedding
139 |     fileprivate let layers: [TransformerBlock]
140 |     fileprivate let norm: Gemma.RMSNorm
141 | 
142 |     public init(_ args: GemmaConfiguration) {
143 |         precondition(args.vocabularySize > 0)
144 | 
145 |         self.args = args
146 |         self.vocabularySize = args.vocabularySize
147 |         self.numHiddenLayers = args.hiddenLayers
148 | 
149 |         self._embedTokens.wrappedValue = Embedding(
150 |             embeddingCount: args.vocabularySize, dimensions: args.hiddenSize)
151 | 
152 |         self.layers = (0 ..< args.hiddenLayers)
153 |             .map { _ in
154 |                 TransformerBlock(args)
155 |             }
156 |         self.norm = Gemma.RMSNorm(dimensions: args.hiddenSize, eps: args.rmsNormEps)
157 |     }
158 | 
159 |     public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]? = nil) -> MLXArray {
160 |         var h = embedTokens(inputs)
161 |         h = h * pow(Float(args.hiddenSize), 0.5)
162 | 
163 |         let mask = createAttentionMask(h: h, cache: cache)
164 | 
165 |         for (i, layer) in layers.enumerated() {
166 |             h = layer(h, mask: mask, cache: cache?[i])
167 |         }
168 | 
169 |         return norm(h)
170 |     }
171 | }
172 | 
173 | public class GemmaModel: Module, LLMModel, KVCacheDimensionProvider {
174 |     public let vocabularySize: Int
175 |     public let kvHeads: [Int]
176 | 
177 |     let modelType: String
178 |     private let model: GemmaModelInner
179 | 
180 |     public init(_ args: GemmaConfiguration) {
181 |         self.modelType = args.modelType
182 |         self.vocabularySize = args.vocabularySize
183 |         self.kvHeads = Array(repeating: args.kvHeads, count: args.hiddenLayers)
184 |         self.model = GemmaModelInner(args)
185 |     }
186 | 
187 |     public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {
188 |         let out = model(inputs, cache: cache)
189 |         return model.embedTokens.asLinear(out)
190 |     }
191 | 
192 |     public func messageGenerator(tokenizer: any Tokenizer) -> any MessageGenerator {
193 |         NoSystemMessageGenerator()
194 |     }
195 | }
196 | 
197 | public struct GemmaConfiguration: Codable, Sendable {
198 |     var modelType: String
199 |     var hiddenSize: Int
200 |     var hiddenLayers: Int
201 |     var intermediateSize: Int
202 |     var attentionHeads: Int
203 |     var headDimensions: Int
204 |     var rmsNormEps: Float
205 |     var vocabularySize: Int
206 |     var kvHeads: Int
207 |     private let _ropeTheta: Float?
208 |     public var ropeTheta: Float { _ropeTheta ?? 10_000 }
209 |     private let _ropeTraditional: Bool?
210 |     public var ropeTraditional: Bool { _ropeTraditional ?? false }
211 | 
212 |     enum CodingKeys: String, CodingKey {
213 |         case modelType = "model_type"
214 |         case hiddenSize = "hidden_size"
215 |         case hiddenLayers = "num_hidden_layers"
216 |         case intermediateSize = "intermediate_size"
217 |         case attentionHeads = "num_attention_heads"
218 |         case headDimensions = "head_dim"
219 |         case rmsNormEps = "rms_norm_eps"
220 |         case vocabularySize = "vocab_size"
221 |         case kvHeads = "num_key_value_heads"
222 |         case _ropeTheta = "rope_theta"
223 |         case _ropeTraditional = "rope_traditional"
224 |     }
225 | }
226 | 
227 | // MARK: - LoRA
228 | 
229 | extension GemmaModel: LoRAModel {
230 |     public func loraLinearLayers() -> LoRALinearLayers {
231 |         model.layers.map { ($0.attention, ["q_proj", "v_proj"]) }
232 |     }
233 | }
234 | 


--------------------------------------------------------------------------------
/Libraries/MLXLLM/Models/Gemma2.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import Foundation
  4 | import MLX
  5 | import MLXLMCommon
  6 | import MLXNN
  7 | import Tokenizers
  8 | 
  9 | // Port of https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/models/gemma2.py
 10 | 
 11 | private class Attention: Module {
 12 |     let args: Gemma2Configuration
 13 |     let scale: Float
 14 |     let logitSoftCap: Float
 15 |     let headDim: Int
 16 |     let nHeads: Int
 17 |     let nKVHeads: Int
 18 |     let repeats: Int
 19 | 
 20 |     @ModuleInfo(key: "q_proj") var wq: Linear
 21 |     @ModuleInfo(key: "k_proj") var wk: Linear
 22 |     @ModuleInfo(key: "v_proj") var wv: Linear
 23 |     @ModuleInfo(key: "o_proj") var wo: Linear
 24 | 
 25 |     let rope: RoPE
 26 | 
 27 |     public init(_ args: Gemma2Configuration) {
 28 |         self.args = args
 29 | 
 30 |         let dim = args.hiddenSize
 31 |         self.nHeads = args.attentionHeads
 32 |         self.nKVHeads = args.kvHeads
 33 |         self.repeats = args.attentionHeads / args.kvHeads
 34 |         self.headDim = args.headDimensions
 35 | 
 36 |         self.scale = 1.0 / pow(Float(args.queryPreAttnScalar), 0.5)
 37 | 
 38 |         self._wq.wrappedValue = Linear(dim, nHeads * headDim, bias: false)
 39 |         self._wk.wrappedValue = Linear(dim, nKVHeads * headDim, bias: false)
 40 |         self._wv.wrappedValue = Linear(dim, nKVHeads * headDim, bias: false)
 41 |         self._wo.wrappedValue = Linear(nHeads * headDim, dim, bias: false)
 42 |         self.logitSoftCap = args.attnLogitSoftcapping
 43 |         self.rope = RoPE(
 44 |             dimensions: headDim, traditional: args.ropeTraditional, base: args.ropeTheta)
 45 |     }
 46 | 
 47 |     public func callAsFunction(
 48 |         _ x: MLXArray, mask: MLXArray?, cache: KVCache?
 49 |     ) -> MLXArray {
 50 |         let (B, L) = (x.dim(0), x.dim(1))
 51 |         var queries = wq(x)
 52 |         var keys = wk(x)
 53 |         var values = wv(x)
 54 |         queries = queries.reshaped(B, L, nHeads, -1).transposed(0, 2, 1, 3)
 55 |         keys = keys.reshaped(B, L, nKVHeads, -1).transposed(0, 2, 1, 3)
 56 |         values = values.reshaped(B, L, nKVHeads, -1).transposed(0, 2, 1, 3)
 57 | 
 58 |         if let cache {
 59 |             queries = rope(queries, offset: cache.offset)
 60 |             keys = rope(keys, offset: cache.offset)
 61 |             (keys, values) = cache.update(keys: keys, values: values)
 62 |         } else {
 63 |             queries = rope(queries)
 64 |             keys = rope(keys)
 65 |         }
 66 | 
 67 |         queries = queries * self.scale
 68 | 
 69 |         if repeats > 1 {
 70 |             queries = queries.reshaped([B, nKVHeads, repeats, L, headDim])
 71 |             keys = expandedDimensions(keys, axes: [2])
 72 |             values = expandedDimensions(values, axes: [2])
 73 |         }
 74 | 
 75 |         var scores = matmul(queries, keys.swappedAxes(-1, -2))
 76 |         scores = tanh(scores / logitSoftCap) * logitSoftCap
 77 | 
 78 |         if let mask {
 79 |             scores = scores + mask
 80 |         }
 81 |         scores = softmax(scores, axis: -1, precise: true)
 82 |         var output = matmul(scores, values)
 83 |         if repeats > 1 {
 84 |             output = output.reshaped([B, nHeads, L, headDim])
 85 |         }
 86 |         output = output.transposed(0, 2, 1, 3).reshaped(B, L, -1)
 87 |         return wo(output)
 88 |     }
 89 | }
 90 | 
 91 | private class MLP: Module, UnaryLayer {
 92 |     @ModuleInfo(key: "gate_proj") var gate: Linear
 93 |     @ModuleInfo(key: "down_proj") var down: Linear
 94 |     @ModuleInfo(key: "up_proj") var up: Linear
 95 | 
 96 |     public init(dimensions: Int, hiddenDimensions: Int) {
 97 |         self._gate.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)
 98 |         self._down.wrappedValue = Linear(hiddenDimensions, dimensions, bias: false)
 99 |         self._up.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)
100 |     }
101 | 
102 |     public func callAsFunction(_ x: MLXArray) -> MLXArray {
103 |         down(gelu(gate(x)) * up(x))
104 |     }
105 | }
106 | 
107 | // Minimal changes from Gemma TransformerBlock
108 | private class TransformerBlock: Module {
109 |     @ModuleInfo(key: "self_attn") var attention: Attention
110 |     let mlp: MLP
111 | 
112 |     @ModuleInfo(key: "input_layernorm") var inputLayerNorm: Gemma.RMSNorm
113 |     @ModuleInfo(key: "pre_feedforward_layernorm") var preFeedforwardLayerNorm: Gemma.RMSNorm
114 |     @ModuleInfo(key: "post_feedforward_layernorm") var postFeedforwardLayerNorm: Gemma.RMSNorm
115 |     @ModuleInfo(key: "post_attention_layernorm") var postAttentionLayerNorm: Gemma.RMSNorm
116 | 
117 |     public init(_ args: Gemma2Configuration) {
118 |         self._attention.wrappedValue = Attention(args)
119 |         self.mlp = MLP(dimensions: args.hiddenSize, hiddenDimensions: args.intermediateSize)
120 |         self._inputLayerNorm.wrappedValue = Gemma.RMSNorm(
121 |             dimensions: args.hiddenSize, eps: args.rmsNormEps)
122 |         self._preFeedforwardLayerNorm.wrappedValue = Gemma.RMSNorm(
123 |             dimensions: args.hiddenSize, eps: args.rmsNormEps)
124 |         self._postFeedforwardLayerNorm.wrappedValue = Gemma.RMSNorm(
125 |             dimensions: args.hiddenSize, eps: args.rmsNormEps)
126 |         self._postAttentionLayerNorm.wrappedValue = Gemma.RMSNorm(
127 |             dimensions: args.hiddenSize, eps: args.rmsNormEps)
128 |     }
129 | 
130 |     public func callAsFunction(
131 |         _ x: MLXArray, mask: MLXArray?, cache: KVCache?
132 |     ) -> MLXArray {
133 |         var r = attention(inputLayerNorm(x), mask: mask, cache: cache)
134 |         let h = x + postAttentionLayerNorm(r)
135 |         r = mlp(preFeedforwardLayerNorm(h))
136 |         let out = h + postFeedforwardLayerNorm(r)
137 |         return out
138 |     }
139 | }
140 | 
141 | // Uses Gemma2TransformerBlock, otherwise same as GemmaModelInner
142 | private class ModelInner: Module {
143 |     @ModuleInfo(key: "embed_tokens") var embedTokens: Embedding
144 | 
145 |     fileprivate let layers: [TransformerBlock]
146 |     fileprivate let norm: Gemma.RMSNorm
147 | 
148 |     let hiddenScale: Float
149 | 
150 |     public init(_ args: Gemma2Configuration) {
151 |         precondition(args.vocabularySize > 0)
152 | 
153 |         self._embedTokens.wrappedValue = Embedding(
154 |             embeddingCount: args.vocabularySize, dimensions: args.hiddenSize)
155 | 
156 |         self.hiddenScale = pow(Float(args.hiddenSize), 0.5)
157 | 
158 |         self.layers = (0 ..< args.hiddenLayers)
159 |             .map { _ in
160 |                 TransformerBlock(args)
161 |             }
162 |         self.norm = Gemma.RMSNorm(dimensions: args.hiddenSize, eps: args.rmsNormEps)
163 |     }
164 | 
165 |     public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]? = nil) -> MLXArray {
166 |         var h = embedTokens(inputs)
167 |         h = h * hiddenScale
168 | 
169 |         let mask: MLXArray? = createAttentionMask(h: h, cache: cache)
170 | 
171 |         for (i, layer) in layers.enumerated() {
172 |             h = layer(h, mask: mask, cache: cache?[i])
173 |         }
174 | 
175 |         return norm(h)
176 |     }
177 | }
178 | 
179 | // Uses Gemma2ModelInner, otherwise same as GemmaModel
180 | public class Gemma2Model: Module, LLMModel, KVCacheDimensionProvider {
181 |     public let vocabularySize: Int
182 |     public let kvHeads: [Int]
183 | 
184 |     private let model: ModelInner
185 |     let logitSoftCap: Float
186 | 
187 |     public init(_ args: Gemma2Configuration) {
188 |         self.vocabularySize = args.vocabularySize
189 |         self.kvHeads = Array(repeating: args.kvHeads, count: args.hiddenLayers)
190 |         self.model = ModelInner(args)
191 |         self.logitSoftCap = args.finalLogitSoftcapping
192 |     }
193 | 
194 |     public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {
195 |         var out = model(inputs, cache: cache)
196 |         out = model.embedTokens.asLinear(out)
197 |         out = tanh(out / logitSoftCap) * logitSoftCap
198 |         return out
199 |     }
200 | 
201 |     public func messageGenerator(tokenizer: any Tokenizer) -> any MessageGenerator {
202 |         NoSystemMessageGenerator()
203 |     }
204 | }
205 | 
206 | public struct Gemma2Configuration: Codable {
207 |     var hiddenSize: Int
208 |     var hiddenLayers: Int
209 |     var intermediateSize: Int
210 |     var attentionHeads: Int
211 |     var headDimensions: Int
212 |     var rmsNormEps: Float
213 |     var vocabularySize: Int
214 |     var kvHeads: Int
215 |     var ropeTheta: Float = 10_000
216 |     var ropeTraditional: Bool = false
217 |     var attnLogitSoftcapping: Float = 50.0
218 |     var finalLogitSoftcapping: Float = 30.0
219 |     var queryPreAttnScalar: Float = 144.0
220 | 
221 |     enum CodingKeys: String, CodingKey {
222 |         case hiddenSize = "hidden_size"
223 |         case hiddenLayers = "num_hidden_layers"
224 |         case intermediateSize = "intermediate_size"
225 |         case attentionHeads = "num_attention_heads"
226 |         case headDimensions = "head_dim"
227 |         case rmsNormEps = "rms_norm_eps"
228 |         case vocabularySize = "vocab_size"
229 |         case kvHeads = "num_key_value_heads"
230 |         case ropeTheta = "rope_theta"
231 |         case ropeTraditional = "rope_traditional"
232 |         case attnLogitSoftcapping = "attn_logit_softcapping"
233 |         case finalLogitSoftcapping = "final_logit_softcapping"
234 |         case queryPreAttnScalar = "query_pre_attn_scalar"
235 |     }
236 | 
237 |     public init(from decoder: Swift.Decoder) throws {
238 |         // Custom implementation to handle optional keys with required values
239 |         let container: KeyedDecodingContainer<CodingKeys> = try decoder.container(
240 |             keyedBy: CodingKeys.self)
241 | 
242 |         self.hiddenSize = try container.decode(
243 |             Int.self, forKey: CodingKeys.hiddenSize)
244 |         self.hiddenLayers = try container.decode(
245 |             Int.self, forKey: CodingKeys.hiddenLayers)
246 |         self.intermediateSize = try container.decode(
247 |             Int.self, forKey: CodingKeys.intermediateSize)
248 |         self.attentionHeads = try container.decode(
249 |             Int.self, forKey: CodingKeys.attentionHeads)
250 |         self.headDimensions = try container.decode(
251 |             Int.self, forKey: CodingKeys.headDimensions)
252 |         self.rmsNormEps = try container.decode(
253 |             Float.self, forKey: CodingKeys.rmsNormEps)
254 |         self.vocabularySize = try container.decode(
255 |             Int.self, forKey: CodingKeys.vocabularySize)
256 |         self.kvHeads = try container.decode(Int.self, forKey: CodingKeys.kvHeads)
257 |         self.ropeTheta =
258 |             try container.decodeIfPresent(Float.self, forKey: CodingKeys.ropeTheta)
259 |             ?? 10_000
260 |         self.ropeTraditional =
261 |             try container.decodeIfPresent(
262 |                 Bool.self, forKey: CodingKeys.ropeTraditional) ?? false
263 |         self.attnLogitSoftcapping = try container.decode(
264 |             Float.self, forKey: CodingKeys.attnLogitSoftcapping)
265 |         self.finalLogitSoftcapping = try container.decode(
266 |             Float.self, forKey: CodingKeys.finalLogitSoftcapping)
267 |         self.queryPreAttnScalar = try container.decode(
268 |             Float.self, forKey: CodingKeys.queryPreAttnScalar)
269 |     }
270 | }
271 | 
272 | // MARK: - LoRA
273 | 
274 | extension Gemma2Model: LoRAModel {
275 |     public func loraLinearLayers() -> LoRALinearLayers {
276 |         model.layers.map { ($0.attention, ["q_proj", "v_proj"]) }
277 |     }
278 | }
279 | 


--------------------------------------------------------------------------------
/Libraries/MLXLLM/Models/Gemma3Text.swift:
--------------------------------------------------------------------------------
  1 | //
  2 | //  Gemma3Text.swift
  3 | //  mlx-swift-examples
  4 | //
  5 | //  Created by Anthony DePasquale on 14.03.2025.
  6 | //
  7 | 
  8 | // Based on https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/models/gemma3_text.py
  9 | 
 10 | import Foundation
 11 | import MLX
 12 | import MLXFast
 13 | import MLXLLM
 14 | import MLXLMCommon
 15 | import MLXNN
 16 | 
 17 | public struct Gemma3TextConfiguration: Codable {
 18 |     let modelType: String
 19 |     let hiddenSize: Int
 20 |     let hiddenLayers: Int
 21 |     let intermediateSize: Int
 22 |     let attentionHeads: Int
 23 |     let headDim: Int
 24 |     let rmsNormEps: Float
 25 |     let vocabularySize: Int
 26 |     let kvHeads: Int
 27 |     let ropeGlobalBaseFreq: Float
 28 |     let ropeLocalBaseFreq: Float
 29 |     let ropeTraditional: Bool
 30 |     let queryPreAttnScalar: Float
 31 |     let slidingWindow: Int
 32 |     let slidingWindowPattern: Int
 33 | 
 34 |     enum CodingKeys: String, CodingKey {
 35 |         case modelType = "model_type"
 36 |         case hiddenSize = "hidden_size"
 37 |         case hiddenLayers = "num_hidden_layers"
 38 |         case intermediateSize = "intermediate_size"
 39 |         case attentionHeads = "num_attention_heads"
 40 |         case headDim = "head_dim"
 41 |         case rmsNormEps = "rms_norm_eps"
 42 |         case vocabularySize = "vocab_size"
 43 |         case kvHeads = "num_key_value_heads"
 44 |         case ropeGlobalBaseFreq = "rope_global_base_freq"
 45 |         case ropeLocalBaseFreq = "rope_local_base_freq"
 46 |         case ropeTraditional = "rope_traditional"
 47 |         case queryPreAttnScalar = "query_pre_attn_scalar"
 48 |         case slidingWindow = "sliding_window"
 49 |         case slidingWindowPattern = "sliding_window_pattern"
 50 |     }
 51 | 
 52 |     public init(from decoder: Decoder) throws {
 53 |         let container = try decoder.container(keyedBy: CodingKeys.self)
 54 | 
 55 |         modelType = try container.decode(String.self, forKey: .modelType)
 56 |         hiddenSize = try container.decode(Int.self, forKey: .hiddenSize)
 57 |         hiddenLayers = try container.decode(Int.self, forKey: .hiddenLayers)
 58 |         intermediateSize = try container.decode(Int.self, forKey: .intermediateSize)
 59 |         attentionHeads = try container.decodeIfPresent(Int.self, forKey: .attentionHeads) ?? 4
 60 |         headDim = try container.decodeIfPresent(Int.self, forKey: .headDim) ?? 256
 61 |         rmsNormEps = try container.decodeIfPresent(Float.self, forKey: .rmsNormEps) ?? 1.0e-6
 62 |         vocabularySize = try container.decodeIfPresent(Int.self, forKey: .vocabularySize) ?? 262144
 63 |         kvHeads = try container.decodeIfPresent(Int.self, forKey: .kvHeads) ?? 1
 64 |         ropeGlobalBaseFreq =
 65 |             try container.decodeIfPresent(Float.self, forKey: .ropeGlobalBaseFreq) ?? 1_000_000.0
 66 |         ropeLocalBaseFreq =
 67 |             try container.decodeIfPresent(Float.self, forKey: .ropeLocalBaseFreq) ?? 10_000.0
 68 |         ropeTraditional =
 69 |             try container.decodeIfPresent(Bool.self, forKey: .ropeTraditional) ?? false
 70 |         queryPreAttnScalar =
 71 |             try container.decodeIfPresent(Float.self, forKey: .queryPreAttnScalar) ?? 256
 72 |         slidingWindow = try container.decodeIfPresent(Int.self, forKey: .slidingWindow) ?? 512
 73 |         slidingWindowPattern =
 74 |             try container.decodeIfPresent(Int.self, forKey: .slidingWindowPattern) ?? 6
 75 |     }
 76 | }
 77 | 
 78 | private class Attention: Module {
 79 |     let nHeads: Int
 80 |     let nKVHeads: Int
 81 |     let repeats: Int
 82 |     let headDim: Int
 83 |     let layerIdx: Int
 84 |     let scale: Float
 85 |     let isSliding: Bool
 86 |     let slidingWindow: Int
 87 |     let slidingWindowPattern: Int
 88 | 
 89 |     @ModuleInfo(key: "q_proj") var queryProj: Linear
 90 |     @ModuleInfo(key: "k_proj") var keyProj: Linear
 91 |     @ModuleInfo(key: "v_proj") var valueProj: Linear
 92 |     @ModuleInfo(key: "o_proj") var outputProj: Linear
 93 | 
 94 |     @ModuleInfo(key: "q_norm") var queryNorm: Gemma.RMSNorm
 95 |     @ModuleInfo(key: "k_norm") var keyNorm: Gemma.RMSNorm
 96 | 
 97 |     @ModuleInfo var rope: RoPE
 98 | 
 99 |     init(_ config: Gemma3TextConfiguration, layerIdx: Int) {
100 |         let dim = config.hiddenSize
101 |         self.nHeads = config.attentionHeads
102 |         self.nKVHeads = config.kvHeads
103 |         self.repeats = nHeads / nKVHeads
104 |         self.headDim = config.headDim
105 |         self.layerIdx = layerIdx
106 |         self.slidingWindow = config.slidingWindow
107 |         self.slidingWindowPattern = config.slidingWindowPattern
108 | 
109 |         self.scale = pow(config.queryPreAttnScalar, -0.5)
110 | 
111 |         self._queryProj.wrappedValue = Linear(dim, nHeads * headDim, bias: false)
112 |         self._keyProj.wrappedValue = Linear(dim, nKVHeads * headDim, bias: false)
113 |         self._valueProj.wrappedValue = Linear(dim, nKVHeads * headDim, bias: false)
114 |         self._outputProj.wrappedValue = Linear(nHeads * headDim, dim, bias: false)
115 | 
116 |         self._queryNorm.wrappedValue = Gemma.RMSNorm(
117 |             dimensions: headDim, eps: config.rmsNormEps)
118 |         self._keyNorm.wrappedValue = Gemma.RMSNorm(dimensions: headDim, eps: config.rmsNormEps)
119 | 
120 |         self.isSliding = (layerIdx + 1) % config.slidingWindowPattern != 0
121 | 
122 |         let baseFreq = isSliding ? config.ropeLocalBaseFreq : config.ropeGlobalBaseFreq
123 |         self._rope.wrappedValue = RoPE(
124 |             dimensions: headDim,
125 |             traditional: config.ropeTraditional,
126 |             base: baseFreq
127 |         )
128 | 
129 |         super.init()
130 |     }
131 | 
132 |     func callAsFunction(
133 |         _ x: MLXArray,
134 |         mask: MLXFast.ScaledDotProductAttentionMaskMode,
135 |         cache: KVCache? = nil
136 |     ) -> MLXArray {
137 |         let (B, L, _) = (x.dim(0), x.dim(1), x.dim(2))
138 | 
139 |         var queries = queryProj(x)
140 |         var keys = keyProj(x)
141 |         var values = valueProj(x)
142 | 
143 |         queries = queries.reshaped(B, L, nHeads, -1).transposed(0, 2, 1, 3)
144 |         keys = keys.reshaped(B, L, nKVHeads, -1).transposed(0, 2, 1, 3)
145 |         values = values.reshaped(B, L, nKVHeads, -1).transposed(0, 2, 1, 3)
146 | 
147 |         queries = queryNorm(queries)
148 |         keys = keyNorm(keys)
149 | 
150 |         if let cache {
151 |             queries = rope(queries, offset: cache.offset)
152 |             keys = rope(keys, offset: cache.offset)
153 |             (keys, values) = cache.update(keys: keys, values: values)
154 |         } else {
155 |             queries = rope(queries)
156 |             keys = rope(keys)
157 |         }
158 | 
159 |         // Sliding window masking
160 |         var finalMask = mask
161 |         if case .array(let maskArray) = mask {
162 |             let keySeqLen = keys.shape[2]
163 |             if maskArray.shape.last! != keySeqLen {
164 |                 let slicedMask = maskArray[.ellipsis, (-keySeqLen)...]
165 |                 finalMask = .array(slicedMask)
166 |             }
167 |         }
168 | 
169 |         var output = MLXFast.scaledDotProductAttention(
170 |             queries: queries,
171 |             keys: keys,
172 |             values: values,
173 |             scale: scale,
174 |             mask: finalMask
175 |         )
176 |         output = output.transposed(0, 2, 1, 3).reshaped(B, L, -1)
177 |         return outputProj(output)
178 |     }
179 | }
180 | 
181 | private class MLP: Module {
182 |     @ModuleInfo(key: "gate_proj") var gateProj: Linear
183 |     @ModuleInfo(key: "down_proj") var downProj: Linear
184 |     @ModuleInfo(key: "up_proj") var upProj: Linear
185 | 
186 |     init(dimensions: Int, hiddenDimensions: Int) {
187 |         self._gateProj.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)
188 |         self._downProj.wrappedValue = Linear(hiddenDimensions, dimensions, bias: false)
189 |         self._upProj.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)
190 |         super.init()
191 |     }
192 | 
193 |     func callAsFunction(_ x: MLXArray) -> MLXArray {
194 |         return downProj(geluApproximate(gateProj(x)) * upProj(x))
195 |     }
196 | }
197 | 
198 | private class TransformerBlock: Module {
199 |     @ModuleInfo(key: "self_attn") var selfAttention: Attention
200 |     @ModuleInfo var mlp: MLP
201 |     @ModuleInfo(key: "input_layernorm") var inputLayerNorm: Gemma.RMSNorm
202 |     @ModuleInfo(key: "post_attention_layernorm") var postAttentionLayerNorm: Gemma.RMSNorm
203 |     @ModuleInfo(key: "pre_feedforward_layernorm") var preFeedforwardLayerNorm: Gemma.RMSNorm
204 |     @ModuleInfo(key: "post_feedforward_layernorm") var postFeedforwardLayerNorm: Gemma.RMSNorm
205 | 
206 |     let numAttentionHeads: Int
207 |     let hiddenSize: Int
208 |     let layerIdx: Int
209 | 
210 |     init(_ config: Gemma3TextConfiguration, layerIdx: Int) {
211 |         self.numAttentionHeads = config.attentionHeads
212 |         self.hiddenSize = config.hiddenSize
213 |         self.layerIdx = layerIdx
214 | 
215 |         self._selfAttention.wrappedValue = Attention(config, layerIdx: layerIdx)
216 |         self.mlp = MLP(dimensions: config.hiddenSize, hiddenDimensions: config.intermediateSize)
217 | 
218 |         self._inputLayerNorm.wrappedValue = Gemma.RMSNorm(
219 |             dimensions: config.hiddenSize, eps: config.rmsNormEps)
220 |         self._postAttentionLayerNorm.wrappedValue = Gemma.RMSNorm(
221 |             dimensions: config.hiddenSize, eps: config.rmsNormEps)
222 |         self._preFeedforwardLayerNorm.wrappedValue = Gemma.RMSNorm(
223 |             dimensions: config.hiddenSize, eps: config.rmsNormEps)
224 |         self._postFeedforwardLayerNorm.wrappedValue = Gemma.RMSNorm(
225 |             dimensions: config.hiddenSize, eps: config.rmsNormEps)
226 | 
227 |         super.init()
228 |     }
229 | 
230 |     func callAsFunction(
231 |         _ x: MLXArray,
232 |         mask: MLXFast.ScaledDotProductAttentionMaskMode,
233 |         cache: KVCache? = nil
234 |     ) -> MLXArray {
235 |         let inputNorm = inputLayerNorm(x)
236 |         let r = selfAttention(inputNorm, mask: mask, cache: cache)
237 |         let attnNorm = postAttentionLayerNorm(r)
238 |         let h = Gemma.clipResidual(x, attnNorm)
239 |         let preMLPNorm = preFeedforwardLayerNorm(h)
240 |         let r2 = mlp(preMLPNorm)
241 |         let postMLPNorm = postFeedforwardLayerNorm(r2)
242 |         let out = Gemma.clipResidual(h, postMLPNorm)
243 |         return out
244 |     }
245 | }
246 | 
247 | private class Gemma3Model: Module {
248 |     @ModuleInfo(key: "embed_tokens") var embedTokens: Embedding
249 |     @ModuleInfo var layers: [TransformerBlock]
250 |     @ModuleInfo var norm: Gemma.RMSNorm
251 | 
252 |     let config: Gemma3TextConfiguration
253 | 
254 |     init(_ config: Gemma3TextConfiguration) {
255 |         self.config = config
256 | 
257 |         self._embedTokens.wrappedValue = Embedding(
258 |             embeddingCount: config.vocabularySize,
259 |             dimensions: config.hiddenSize
260 |         )
261 | 
262 |         self._layers.wrappedValue = (0 ..< config.hiddenLayers).map { layerIdx in
263 |             TransformerBlock(config, layerIdx: layerIdx)
264 |         }
265 | 
266 |         self.norm = Gemma.RMSNorm(dimensions: config.hiddenSize, eps: config.rmsNormEps)
267 | 
268 |         super.init()
269 |     }
270 | 
271 |     func callAsFunction(
272 |         _ inputs: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode? = nil,
273 |         cache: [KVCache?]? = nil
274 |     )
275 |         -> MLXArray
276 |     {
277 |         var h: MLXArray
278 |         h = embedTokens(inputs)
279 |         let scale = MLXArray(sqrt(Float(config.hiddenSize)), dtype: .bfloat16)
280 |         h = h * scale.asType(h.dtype)
281 |         var layerCache = cache
282 |         if layerCache == nil {
283 |             layerCache = Array(repeating: nil as KVCache?, count: layers.count)
284 |         }
285 |         // Create attention masks
286 |         var fullMask: MLXFast.ScaledDotProductAttentionMaskMode = .none
287 |         var slidingWindowMask: MLXFast.ScaledDotProductAttentionMaskMode = .none
288 |         if mask == nil {
289 |             let j = config.slidingWindowPattern
290 |             let globalLayerCache: [KVCache]
291 |             if j > 0 && j <= (layerCache?.count ?? 0), let globalCache = layerCache?[j - 1] {
292 |                 globalLayerCache = [globalCache]
293 |             } else {
294 |                 globalLayerCache = []
295 |             }
296 |             fullMask = createAttentionMask(h: h, cache: globalLayerCache)
297 |             let allCaches = layerCache?.compactMap { $0 } ?? []
298 |             slidingWindowMask = createAttentionMask(h: h, cache: allCaches)
299 |         }
300 |         for (i, layer) in layers.enumerated() {
301 |             let isGlobal = (i % config.slidingWindowPattern == config.slidingWindowPattern - 1)
302 | 
303 |             let localMask: MLXFast.ScaledDotProductAttentionMaskMode
304 |             if let mask {
305 |                 localMask = mask
306 |             } else if isGlobal {
307 |                 localMask = fullMask
308 |             } else {
309 |                 localMask = slidingWindowMask
310 |             }
311 |             h = layer(h, mask: localMask, cache: layerCache?[i])
312 |         }
313 |         return norm(h)
314 |     }
315 | }
316 | 
317 | public class Gemma3TextModel: Module, LLMModel {
318 | 
319 |     @ModuleInfo private var model: Gemma3Model
320 |     @ModuleInfo(key: "lm_head") var lmHead: Linear
321 | 
322 |     public let config: Gemma3TextConfiguration
323 |     public var vocabularySize: Int { config.vocabularySize }
324 | 
325 |     public init(_ config: Gemma3TextConfiguration) {
326 |         self.config = config
327 |         self.model = Gemma3Model(config)
328 |         self._lmHead.wrappedValue = Linear(config.hiddenSize, config.vocabularySize, bias: false)
329 |         super.init()
330 |     }
331 | 
332 |     public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]? = nil) -> MLXArray {
333 |         var out = model(inputs, mask: nil, cache: cache)
334 |         out = lmHead(out)
335 |         return out
336 |     }
337 | 
338 |     public func sanitize(weights: [String: MLXArray])
339 |         -> [String: MLXArray]
340 |     {
341 |         var processedWeights = weights
342 |         if processedWeights["lm_head.weight"] == nil {
343 |             if let embedWeight = processedWeights["model.embed_tokens.weight"] {
344 |                 processedWeights["lm_head.weight"] = embedWeight
345 |             }
346 |         }
347 |         return processedWeights
348 |     }
349 | 
350 |     public func newCache(parameters: GenerateParameters? = nil) -> [KVCache] {
351 |         var caches = [KVCache]()
352 |         let slidingWindow = config.slidingWindow
353 |         let slidingWindowPattern = config.slidingWindowPattern
354 | 
355 |         for i in 0 ..< config.hiddenLayers {
356 |             let isGlobalLayer = (i % slidingWindowPattern == slidingWindowPattern - 1)
357 | 
358 |             if isGlobalLayer {
359 |                 // For global layers, use standard cache but with reasonable step size for long sequences
360 |                 let cache = StandardKVCache()
361 |                 cache.step = 1024  // Larger step size for efficiency with long sequences
362 |                 caches.append(cache)
363 |             } else {
364 |                 // For sliding window layers, use rotating cache
365 |                 caches.append(
366 |                     RotatingKVCache(maxSize: slidingWindow, keep: 0)
367 |                 )
368 |             }
369 |         }
370 | 
371 |         return caches
372 |     }
373 | 
374 |     /// Handles prompt processing for sequences
375 |     public func prepare(
376 |         _ input: LMInput, cache: [KVCache], windowSize: Int? = nil
377 |     ) throws -> PrepareResult {
378 |         let promptTokens = input.text.tokens
379 |         let promptCount = promptTokens.shape[0]
380 | 
381 |         guard promptCount > 0 else {
382 |             print("Warning: Preparing with empty prompt tokens.")
383 |             let emptyToken = MLXArray(Int32(0))[0 ..< 0]
384 |             return .tokens(.init(tokens: emptyToken))
385 |         }
386 | 
387 |         return .tokens(input.text)
388 |     }
389 | }
390 | 
391 | extension Gemma3TextModel: LoRAModel {
392 |     public func loraLinearLayers() -> LoRALinearLayers {
393 |         model.layers.map { ($0.selfAttention, ["q_proj", "v_proj"]) }
394 |     }
395 | }
396 | 


--------------------------------------------------------------------------------
/Libraries/MLXLLM/Models/Granite.swift:
--------------------------------------------------------------------------------
  1 | //
  2 | //  Granite.swift
  3 | //  mlx-swift-examples
  4 | //
  5 | //  Created by Sachin Desai on 4/25/25.
  6 | //
  7 | 
  8 | // Port of https://github.com/ml-explore/mlx-lm/blob/main/mlx_lm/models/granite.py
  9 | 
 10 | import Foundation
 11 | import MLX
 12 | import MLXLMCommon
 13 | import MLXNN
 14 | 
 15 | private class Attention: Module {
 16 |     let args: GraniteConfiguration
 17 |     let scale: Float
 18 | 
 19 |     @ModuleInfo(key: "q_proj") var wq: Linear
 20 |     @ModuleInfo(key: "k_proj") var wk: Linear
 21 |     @ModuleInfo(key: "v_proj") var wv: Linear
 22 |     @ModuleInfo(key: "o_proj") var wo: Linear
 23 | 
 24 |     let rope: RoPE
 25 | 
 26 |     public init(_ args: GraniteConfiguration) {
 27 |         self.args = args
 28 | 
 29 |         let dim = args.hiddenSize
 30 |         let nHeads = args.attentionHeads
 31 |         let nKvHeads = args.kvHeads
 32 |         let headDim = dim / nHeads
 33 | 
 34 |         self.scale = args.attentionMultiplier
 35 |         let attentionBias = args.attentionBias
 36 | 
 37 |         self._wq.wrappedValue = Linear(dim, nHeads * headDim, bias: attentionBias)
 38 |         self._wk.wrappedValue = Linear(dim, nKvHeads * headDim, bias: attentionBias)
 39 |         self._wv.wrappedValue = Linear(dim, nKvHeads * headDim, bias: attentionBias)
 40 |         self._wo.wrappedValue = Linear(nHeads * headDim, dim, bias: attentionBias)
 41 | 
 42 |         let ropeScale: Float
 43 |         if let ropeScaling = args.ropeScaling, ropeScaling["type"] == .string("linear"),
 44 |             let factor = ropeScaling["factor"]
 45 |         {
 46 |             if let v = factor.asFloat() {
 47 |                 ropeScale = 1 / v
 48 |             } else {
 49 |                 fatalError("ropeScaling.factor must be a float")
 50 |             }
 51 |         } else {
 52 |             ropeScale = 1
 53 |         }
 54 |         rope = RoPE(dimensions: headDim, traditional: false, base: args.ropeTheta, scale: ropeScale)
 55 |     }
 56 | 
 57 |     public func callAsFunction(
 58 |         _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
 59 |     ) -> MLXArray {
 60 |         let (B, L) = (x.dim(0), x.dim(1))
 61 | 
 62 |         var queries = wq(x)
 63 |         var keys = wk(x)
 64 |         var values = wv(x)
 65 | 
 66 |         // prepare the queries, keys and values for the attention computation
 67 |         queries = queries.reshaped(B, L, args.attentionHeads, -1).transposed(0, 2, 1, 3)
 68 |         keys = keys.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)
 69 |         values = values.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)
 70 | 
 71 |         if let cache {
 72 |             queries = rope(queries, offset: cache.offset)
 73 |             keys = rope(keys, offset: cache.offset)
 74 |             (keys, values) = cache.update(keys: keys, values: values)
 75 |         } else {
 76 |             queries = rope(queries)
 77 |             keys = rope(keys)
 78 |         }
 79 | 
 80 |         let output = MLXFast.scaledDotProductAttention(
 81 |             queries: queries, keys: keys, values: values, scale: self.scale, mask: mask
 82 |         )
 83 |         .transposed(0, 2, 1, 3)
 84 |         .reshaped(B, L, -1)
 85 | 
 86 |         return wo(output)
 87 |     }
 88 | }
 89 | 
 90 | private class MLP: Module, UnaryLayer {
 91 |     @ModuleInfo(key: "gate_proj") var gate: Linear
 92 |     @ModuleInfo(key: "down_proj") var down: Linear
 93 |     @ModuleInfo(key: "up_proj") var up: Linear
 94 | 
 95 |     public init(_ args: GraniteConfiguration) {
 96 |         let dim = args.hiddenSize
 97 |         let hiddenDim = args.intermediateSize
 98 |         let mlpBias = args.mlpBias
 99 | 
100 |         self._gate.wrappedValue = Linear(dim, hiddenDim, bias: mlpBias)
101 |         self._down.wrappedValue = Linear(hiddenDim, dim, bias: mlpBias)
102 |         self._up.wrappedValue = Linear(dim, hiddenDim, bias: mlpBias)
103 |     }
104 | 
105 |     public func callAsFunction(_ x: MLXArray) -> MLXArray {
106 |         down(silu(gate(x)) * up(x))
107 |     }
108 | }
109 | 
110 | private class TransformerBlock: Module {
111 |     @ModuleInfo(key: "self_attn") var attention: Attention
112 |     let mlp: MLP
113 | 
114 |     @ModuleInfo(key: "input_layernorm") var inputLayerNorm: RMSNorm
115 |     @ModuleInfo(key: "post_attention_layernorm") var postAttentionLayerNorm: RMSNorm
116 | 
117 |     let residualMultiplier: Float
118 | 
119 |     public init(_ args: GraniteConfiguration) {
120 |         let attentionHeads = args.attentionHeads
121 |         let hiddenSize = args.hiddenSize
122 | 
123 |         self._attention.wrappedValue = Attention(args)
124 |         self.mlp = MLP(args)
125 | 
126 |         self._inputLayerNorm.wrappedValue = RMSNorm(
127 |             dimensions: hiddenSize, eps: args.rmsNormEps)
128 |         self._postAttentionLayerNorm.wrappedValue = RMSNorm(
129 |             dimensions: hiddenSize, eps: args.rmsNormEps)
130 | 
131 |         self.residualMultiplier = args.residualMultiplier
132 |     }
133 | 
134 |     public func callAsFunction(
135 |         _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
136 |     ) -> MLXArray {
137 |         var r = attention(inputLayerNorm(x), mask: mask, cache: cache)
138 |         let h = x + r * residualMultiplier
139 |         r = mlp(postAttentionLayerNorm(h))
140 |         let out = h + r * residualMultiplier
141 |         return out
142 |     }
143 | }
144 | 
145 | private class GraniteModelInner: Module {
146 |     @ModuleInfo(key: "embed_tokens") var embedTokens: Embedding
147 |     fileprivate let layers: [TransformerBlock]
148 |     let norm: RMSNorm
149 |     let embeddingMultiplier: Float
150 | 
151 |     public init(_ args: GraniteConfiguration) {
152 |         precondition(args.vocabularySize > 0)
153 | 
154 |         self._embedTokens.wrappedValue = Embedding(
155 |             embeddingCount: args.vocabularySize, dimensions: args.hiddenSize)
156 |         self.layers = (0 ..< args.hiddenLayers)
157 |             .map { _ in
158 |                 TransformerBlock(args)
159 |             }
160 | 
161 |         self.norm = RMSNorm(dimensions: args.hiddenSize, eps: args.rmsNormEps)
162 |         self.embeddingMultiplier = args.embeddingMultiplier
163 |     }
164 | 
165 |     public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]? = nil) -> MLXArray {
166 |         var h = embedTokens(inputs) * embeddingMultiplier
167 | 
168 |         let mask = createAttentionMask(h: h, cache: cache)
169 | 
170 |         for (i, layer) in layers.enumerated() {
171 |             h = layer(h, mask: mask, cache: cache?[i])
172 |         }
173 | 
174 |         return norm(h)
175 |     }
176 | }
177 | 
178 | public class GraniteModel: Module, LLMModel, KVCacheDimensionProvider {
179 |     public let vocabularySize: Int
180 |     public let kvHeads: [Int]
181 |     let logitsScaling: Float
182 | 
183 |     private let model: GraniteModelInner
184 |     let configuration: GraniteConfiguration
185 | 
186 |     @ModuleInfo(key: "lm_head") var lmHead: Linear?
187 | 
188 |     public init(_ args: GraniteConfiguration) {
189 |         self.configuration = args
190 |         self.vocabularySize = args.vocabularySize
191 |         self.kvHeads = (0 ..< args.hiddenLayers).map { _ in args.kvHeads }
192 | 
193 |         self.model = GraniteModelInner(args)
194 | 
195 |         if !args.tieWordEmbeddings {
196 |             self._lmHead.wrappedValue = Linear(args.hiddenSize, args.vocabularySize, bias: false)
197 |         }
198 |         self.logitsScaling = args.logitsScaling
199 |     }
200 | 
201 |     public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {
202 |         var out = model(inputs, cache: cache)
203 |         if let lmHead {
204 |             out = lmHead(out)
205 |         } else {
206 |             out = model.embedTokens.asLinear(out)
207 |         }
208 | 
209 |         return out / logitsScaling
210 |     }
211 | }
212 | 
213 | public struct GraniteConfiguration: Codable, Sendable {
214 |     var hiddenSize: Int
215 |     var hiddenLayers: Int
216 |     var intermediateSize: Int
217 |     var attentionHeads: Int
218 |     var rmsNormEps: Float
219 |     var vocabularySize: Int
220 |     var logitsScaling: Float
221 |     var attentionMultiplier: Float
222 |     var embeddingMultiplier: Float
223 |     var residualMultiplier: Float
224 |     var maxPositionEmbeddings: Int
225 |     var kvHeads: Int
226 |     var attentionBias: Bool
227 |     var mlpBias: Bool
228 |     var ropeTheta: Float
229 |     var ropeTraditional: Bool = false
230 |     var ropeScaling: [String: StringOrNumber]? = nil
231 |     var tieWordEmbeddings: Bool = true
232 | 
233 |     enum CodingKeys: String, CodingKey {
234 |         case hiddenSize = "hidden_size"
235 |         case hiddenLayers = "num_hidden_layers"
236 |         case intermediateSize = "intermediate_size"
237 |         case attentionHeads = "num_attention_heads"
238 |         case rmsNormEps = "rms_norm_eps"
239 |         case vocabularySize = "vocab_size"
240 |         case logitsScaling = "logits_scaling"
241 |         case attentionMultiplier = "attention_multiplier"
242 |         case embeddingMultiplier = "embedding_multiplier"
243 |         case residualMultiplier = "residual_multiplier"
244 |         case maxPositionEmbeddings = "max_position_embeddings"
245 |         case kvHeads = "num_key_value_heads"
246 |         case attentionBias = "attention_bias"
247 |         case mlpBias = "mlp_bias"
248 |         case ropeTheta = "rope_theta"
249 |         case ropeScaling = "rope_scaling"
250 |         case tieWordEmbeddings = "tie_word_embeddings"
251 |     }
252 | 
253 |     public init(from decoder: Decoder) throws {
254 |         let container: KeyedDecodingContainer<GraniteConfiguration.CodingKeys> =
255 |             try decoder.container(keyedBy: GraniteConfiguration.CodingKeys.self)
256 | 
257 |         self.hiddenSize = try container.decode(Int.self, forKey: .hiddenSize)
258 |         self.hiddenLayers = try container.decode(Int.self, forKey: .hiddenLayers)
259 |         self.intermediateSize = try container.decode(Int.self, forKey: .intermediateSize)
260 |         self.attentionHeads = try container.decode(Int.self, forKey: .attentionHeads)
261 |         self.rmsNormEps = try container.decode(Float.self, forKey: .rmsNormEps)
262 |         self.vocabularySize = try container.decode(Int.self, forKey: .vocabularySize)
263 |         self.logitsScaling = try container.decode(Float.self, forKey: .logitsScaling)
264 |         self.attentionMultiplier = try container.decode(Float.self, forKey: .attentionMultiplier)
265 |         self.embeddingMultiplier = try container.decode(Float.self, forKey: .embeddingMultiplier)
266 |         self.residualMultiplier = try container.decode(Float.self, forKey: .residualMultiplier)
267 |         self.maxPositionEmbeddings = try container.decode(Int.self, forKey: .maxPositionEmbeddings)
268 |         self.kvHeads = try container.decode(Int.self, forKey: .kvHeads)
269 |         self.attentionBias = try container.decode(Bool.self, forKey: .attentionBias)
270 |         self.mlpBias = try container.decode(Bool.self, forKey: .mlpBias) ?? false
271 |         self.ropeTheta = try container.decodeIfPresent(Float.self, forKey: .ropeTheta) ?? 10000000.0
272 |         self.ropeScaling = try container.decodeIfPresent(
273 |             [String: StringOrNumber].self, forKey: .ropeScaling)
274 |         self.tieWordEmbeddings = try container.decode(Bool.self, forKey: .tieWordEmbeddings)
275 |     }
276 | }
277 | 
278 | // MARK: - LoRA
279 | 
280 | extension GraniteModel: LoRAModel {
281 |     public func loraLinearLayers() -> LoRALinearLayers {
282 |         model.layers.map { ($0.attention, ["q_proj", "v_proj"]) }
283 |     }
284 | }
285 | 


--------------------------------------------------------------------------------
/Libraries/MLXLLM/Models/Internlm2.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import Foundation
  4 | import MLX
  5 | import MLXLMCommon
  6 | import MLXNN
  7 | 
  8 | // Port of https://github.com/maiqingqiang/mlx-examples/blob/main/llms/mlx_lm/models/internlm2.py
  9 | 
 10 | private class DynamicNTKScalingRoPE: Module {
 11 |     let dims: Int
 12 |     let maxPositionEmbeddings: Int
 13 |     let traditional: Bool
 14 |     let originalBase: Float
 15 |     var scale: Float
 16 | 
 17 |     init(
 18 |         dims: Int, maxPositionEmbeddings: Int = 2048, traditional: Bool = false,
 19 |         base: Float = 10000, scale: Float = 1.0
 20 |     ) {
 21 |         self.dims = dims
 22 |         self.maxPositionEmbeddings = maxPositionEmbeddings
 23 |         self.traditional = traditional
 24 |         self.originalBase = base
 25 |         self.scale = scale
 26 |     }
 27 | 
 28 |     func callAsFunction(_ x: MLXArray, offset: Int = 0) -> MLXArray {
 29 |         let seqLen = x.dim(1) + offset
 30 |         var base = originalBase
 31 |         if seqLen > maxPositionEmbeddings {
 32 |             base *= pow(
 33 |                 (scale * Float(seqLen) / Float(maxPositionEmbeddings)) - (scale - 1),
 34 |                 Float(dims) / Float(dims - 2))
 35 |         }
 36 |         return MLXFast.RoPE(
 37 |             x, dimensions: dims, traditional: traditional, base: base, scale: scale, offset: offset)
 38 |     }
 39 | }
 40 | 
 41 | private class Attention: Module {
 42 |     let args: InternLM2Configuration
 43 |     let scale: Float
 44 | 
 45 |     let heads: Int
 46 |     let kvHeads: Int
 47 |     let kvGroups: Int
 48 |     let headDim: Int
 49 | 
 50 |     @ModuleInfo(key: "wqkv") var wqkv: Linear
 51 |     @ModuleInfo(key: "wo") var wo: Linear
 52 | 
 53 |     let rope: DynamicNTKScalingRoPE
 54 | 
 55 |     init(_ args: InternLM2Configuration) {
 56 |         self.args = args
 57 | 
 58 |         let dim = args.hiddenSize
 59 |         self.heads = args.attentionHeads
 60 |         self.kvHeads = args.kvHeads
 61 |         self.kvGroups = args.kvGroups
 62 | 
 63 |         self.headDim = args.hiddenSize / self.heads
 64 |         self.scale = pow(Float(headDim), -0.5)
 65 | 
 66 |         self._wqkv.wrappedValue = Linear(
 67 |             dim, (self.heads + 2 * self.kvHeads) * self.headDim, bias: args.bias)
 68 |         self._wo.wrappedValue = Linear(self.heads * self.headDim, dim, bias: args.bias)
 69 | 
 70 |         let ropeScale: Float
 71 |         if let ropeScaling = args.ropeScaling, ropeScaling["type"] == .string("linear"),
 72 |             let factor = ropeScaling["factor"]
 73 |         {
 74 |             if let v = factor.asFloat() {
 75 |                 ropeScale = 1 / v
 76 |             } else {
 77 |                 fatalError("ropeScaling.factor must be a float")
 78 |             }
 79 |         } else {
 80 |             ropeScale = 1
 81 |         }
 82 | 
 83 |         self.rope = DynamicNTKScalingRoPE(
 84 |             dims: self.headDim,
 85 |             maxPositionEmbeddings: args.maxPositionEmbeddings,
 86 |             traditional: args.ropeTraditional,
 87 |             base: args.ropeTheta,
 88 |             scale: ropeScale
 89 |         )
 90 |     }
 91 | 
 92 |     func callAsFunction(
 93 |         _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
 94 |     ) -> MLXArray {
 95 |         let (B, L) = (x.dim(0), x.dim(1))
 96 | 
 97 |         var qkvStates = wqkv(x)
 98 |         qkvStates = qkvStates.reshaped(B, L, -1, 2 + self.kvGroups, self.headDim)
 99 |         var queries = qkvStates[.ellipsis, ..<self.kvGroups, 0...]
100 |         queries = queries.reshaped(B, L, -1, self.headDim)
101 |         var keys = qkvStates[.ellipsis, -2, 0...]
102 |         var values = qkvStates[.ellipsis, -1, 0...]
103 | 
104 |         // prepare the queries, keys and values for the attention computation
105 |         queries = queries.reshaped(B, L, args.attentionHeads, -1).transposed(0, 2, 1, 3)
106 |         keys = keys.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)
107 |         values = values.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)
108 | 
109 |         if let cache {
110 |             queries = rope(queries, offset: cache.offset)
111 |             keys = rope(keys, offset: cache.offset)
112 |             (keys, values) = cache.update(keys: keys, values: values)
113 |         } else {
114 |             queries = rope(queries)
115 |             keys = rope(keys)
116 |         }
117 | 
118 |         let output = MLXFast.scaledDotProductAttention(
119 |             queries: queries, keys: keys, values: values, scale: scale, mask: mask
120 |         )
121 |         .transposed(0, 2, 1, 3)
122 |         .reshaped(B, L, -1)
123 | 
124 |         return wo(output)
125 |     }
126 | }
127 | 
128 | private class MLP: Module, UnaryLayer {
129 |     @ModuleInfo(key: "w1") var w1: Linear
130 |     @ModuleInfo(key: "w2") var w2: Linear
131 |     @ModuleInfo(key: "w3") var w3: Linear
132 | 
133 |     init(dim: Int, hiddenDim: Int) {
134 |         self._w1.wrappedValue = Linear(dim, hiddenDim, bias: false)
135 |         self._w2.wrappedValue = Linear(hiddenDim, dim, bias: false)
136 |         self._w3.wrappedValue = Linear(dim, hiddenDim, bias: false)
137 |     }
138 | 
139 |     func callAsFunction(_ x: MLXArray) -> MLXArray {
140 |         return w2(silu(w1(x)) * w3(x))
141 |     }
142 | }
143 | 
144 | private class TransformerBlock: Module {
145 |     @ModuleInfo(key: "attention") var attention: Attention
146 |     @ModuleInfo(key: "feed_forward") var feedForward: MLP
147 | 
148 |     @ModuleInfo(key: "attention_norm") var attentionNorm: RMSNorm
149 |     @ModuleInfo(key: "ffn_norm") var ffnNorm: RMSNorm
150 | 
151 |     init(_ args: InternLM2Configuration) {
152 |         self._attention.wrappedValue = Attention(args)
153 |         self._feedForward.wrappedValue = MLP(dim: args.hiddenSize, hiddenDim: args.intermediateSize)
154 |         self._attentionNorm.wrappedValue = RMSNorm(
155 |             dimensions: args.hiddenSize, eps: args.rmsNormEps)
156 |         self._ffnNorm.wrappedValue = RMSNorm(dimensions: args.hiddenSize, eps: args.rmsNormEps)
157 |     }
158 | 
159 |     func callAsFunction(
160 |         _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
161 |     ) -> MLXArray {
162 |         var r = attention(attentionNorm(x), mask: mask, cache: cache)
163 |         let h = x + r
164 |         r = feedForward(ffnNorm(h))
165 |         let out = h + r
166 |         return out
167 |     }
168 | }
169 | 
170 | private class InternLM2ModelInner: Module {
171 |     @ModuleInfo(key: "tok_embeddings") var tokEmbeddings: Embedding
172 | 
173 |     let layers: [TransformerBlock]
174 |     let norm: RMSNorm
175 | 
176 |     init(_ args: InternLM2Configuration) {
177 |         precondition(args.vocabularySize > 0)
178 | 
179 |         self._tokEmbeddings.wrappedValue = Embedding(
180 |             embeddingCount: args.vocabularySize, dimensions: args.hiddenSize)
181 | 
182 |         self.layers = (0 ..< args.hiddenLayers).map { _ in TransformerBlock(args) }
183 |         self.norm = RMSNorm(dimensions: args.hiddenSize, eps: args.rmsNormEps)
184 |     }
185 | 
186 |     func callAsFunction(_ inputs: MLXArray, cache: [KVCache]? = nil) -> MLXArray {
187 |         var h = tokEmbeddings(inputs)
188 | 
189 |         let mask = createAttentionMask(h: h, cache: cache)
190 | 
191 |         for (i, layer) in layers.enumerated() {
192 |             h = layer(h, mask: mask, cache: cache?[i])
193 |         }
194 | 
195 |         return norm(h)
196 |     }
197 | }
198 | 
199 | public class InternLM2Model: Module, LLMModel, KVCacheDimensionProvider {
200 |     public let vocabularySize: Int
201 |     public let kvHeads: [Int]
202 | 
203 |     fileprivate let model: InternLM2ModelInner
204 | 
205 |     @ModuleInfo(key: "output") var output: Linear?
206 | 
207 |     public init(_ args: InternLM2Configuration) {
208 |         self.vocabularySize = args.vocabularySize
209 |         self.kvHeads = (0 ..< args.hiddenLayers).map { _ in args.kvHeads }
210 |         self.model = InternLM2ModelInner(args)
211 |         if !args.tieWordEmbeddings {
212 |             self._output.wrappedValue = Linear(args.hiddenSize, args.vocabularySize, bias: false)
213 |         }
214 |     }
215 | 
216 |     public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {
217 |         let out = model(inputs, cache: cache)
218 |         if let output {
219 |             return output(out)
220 |         } else {
221 |             return model.tokEmbeddings.asLinear(out)
222 |         }
223 |     }
224 | 
225 |     public func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {
226 |         // Remove unused precomputed rotary frequencies
227 |         weights.filter {
228 |             !$0.key.contains("attention.rope.inv_freq")
229 |         }
230 |     }
231 | }
232 | 
233 | extension InternLM2Model: LoRAModel {
234 |     public func loraLinearLayers() -> LoRALinearLayers {
235 |         model.layers.map { ($0.attention, ["q_proj", "v_proj"]) }
236 |     }
237 | }
238 | 
239 | public struct InternLM2Configuration: Codable, Sendable {
240 |     var hiddenSize: Int
241 |     var hiddenLayers: Int
242 |     var intermediateSize: Int
243 |     var attentionHeads: Int
244 |     var rmsNormEps: Float
245 |     var vocabularySize: Int
246 |     var kvHeads: Int
247 |     var maxPositionEmbeddings: Int = 32768
248 |     var ropeTheta: Float = 10000
249 |     var ropeTraditional: Bool = false
250 |     var ropeScaling: [String: StringOrNumber]?
251 |     var tieWordEmbeddings: Bool = false
252 |     var bias: Bool = true
253 | 
254 |     var kvGroups: Int {
255 |         attentionHeads / kvHeads
256 |     }
257 | 
258 |     enum CodingKeys: String, CodingKey {
259 |         case hiddenSize = "hidden_size"
260 |         case hiddenLayers = "num_hidden_layers"
261 |         case intermediateSize = "intermediate_size"
262 |         case attentionHeads = "num_attention_heads"
263 |         case rmsNormEps = "rms_norm_eps"
264 |         case vocabularySize = "vocab_size"
265 |         case kvHeads = "num_key_value_heads"
266 |         case maxPositionEmbeddings = "max_position_embeddings"
267 |         case ropeTheta = "rope_theta"
268 |         case ropeTraditional = "rope_traditional"
269 |         case ropeScaling = "rope_scaling"
270 |         case tieWordEmbeddings = "tie_word_embeddings"
271 |         case bias = "bias"
272 |     }
273 | 
274 |     public init(from decoder: Decoder) throws {
275 |         let container = try decoder.container(keyedBy: CodingKeys.self)
276 | 
277 |         hiddenSize = try container.decode(Int.self, forKey: .hiddenSize)
278 |         hiddenLayers = try container.decode(Int.self, forKey: .hiddenLayers)
279 |         intermediateSize = try container.decode(Int.self, forKey: .intermediateSize)
280 |         attentionHeads = try container.decode(Int.self, forKey: .attentionHeads)
281 |         rmsNormEps = try container.decode(Float.self, forKey: .rmsNormEps)
282 |         vocabularySize = try container.decode(Int.self, forKey: .vocabularySize)
283 |         kvHeads = try container.decodeIfPresent(Int.self, forKey: .kvHeads) ?? attentionHeads
284 |         maxPositionEmbeddings = try container.decode(Int.self, forKey: .maxPositionEmbeddings)
285 |         if let ropeTheta = try container.decodeIfPresent(Float.self, forKey: .ropeTheta) {
286 |             self.ropeTheta = ropeTheta
287 |         }
288 |         if let ropeTraditional = try container.decodeIfPresent(Bool.self, forKey: .ropeTraditional)
289 |         {
290 |             self.ropeTraditional = ropeTraditional
291 |         }
292 |         ropeScaling = try container.decodeIfPresent(
293 |             [String: StringOrNumber].self, forKey: .ropeScaling)
294 |         if let tieWordEmbeddings = try container.decodeIfPresent(
295 |             Bool.self, forKey: .tieWordEmbeddings)
296 |         {
297 |             self.tieWordEmbeddings = tieWordEmbeddings
298 |         }
299 |         if let bias = try container.decodeIfPresent(Bool.self, forKey: .bias) {
300 |             self.bias = bias
301 |         }
302 | 
303 |         if let ropeScaling {
304 |             let requiredKeys: Set<String> = ["factor", "type"]
305 |             let keys = Set(ropeScaling.keys)
306 |             if !requiredKeys.isSubset(of: keys) {
307 |                 throw DecodingError.dataCorruptedError(
308 |                     forKey: .ropeScaling, in: container,
309 |                     debugDescription: "rope_scaling must contain keys \(requiredKeys)"
310 |                 )
311 |             }
312 |             if let type = ropeScaling["type"],
313 |                 type != .string("linear") && type != .string("dynamic")
314 |             {
315 |                 throw DecodingError.dataCorruptedError(
316 |                     forKey: .ropeScaling, in: container,
317 |                     debugDescription:
318 |                         "rope_scaling 'type' currently only supports 'linear' or 'dynamic'"
319 |                 )
320 |             }
321 |         }
322 |     }
323 | }
324 | 


--------------------------------------------------------------------------------
/Libraries/MLXLLM/Models/Llama.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import Foundation
  4 | import MLX
  5 | import MLXLMCommon
  6 | import MLXNN
  7 | import Tokenizers
  8 | 
  9 | // port of https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/models/llama.py
 10 | 
 11 | func computeBaseFrequency(
 12 |     base: Float, dims: Int, ropeType: String, ropeScaling: [String: StringOrNumber]?
 13 | )
 14 |     -> Float
 15 | {
 16 |     if ropeType != "llama3" {
 17 |         return base
 18 |     }
 19 | 
 20 |     guard let ropeScaling = ropeScaling else {
 21 |         return base
 22 |     }
 23 | 
 24 |     guard case .float(let factor) = ropeScaling["factor"],
 25 |         case .float(let lowFreqFactor) = ropeScaling["low_freq_factor"] ?? .float(1.0),
 26 |         case .float(let highFreqFactor) = ropeScaling["high_freq_factor"] ?? .float(4.0),
 27 |         case .float(let oldContextLen) = ropeScaling["original_max_position_embeddings"]
 28 |             ?? .float(8192)
 29 |     else {
 30 |         return base
 31 |     }
 32 | 
 33 |     let lowFreqWavelen = oldContextLen / lowFreqFactor
 34 |     let highFreqWavelen = oldContextLen / highFreqFactor
 35 | 
 36 |     let freqs = (0 ..< dims).compactMap { index -> Float? in
 37 |         if index % 2 == 0 {
 38 |             return pow(base, Float(index) / Float(dims))
 39 |         }
 40 |         return nil
 41 |     }
 42 | 
 43 |     let newBaseFreqs = freqs.map { freq -> Float in
 44 |         let wavelen = 2 * .pi / freq
 45 |         let smooth = max(
 46 |             0, min(1, (wavelen - highFreqWavelen) / (lowFreqWavelen - highFreqWavelen)))
 47 |         return freq * ((1 - smooth) * factor + smooth)
 48 |     }
 49 | 
 50 |     return newBaseFreqs.reduce(0, +) / Float(newBaseFreqs.count)
 51 | }
 52 | 
 53 | private class DynamicNTKScalingRoPE: Module {
 54 |     let dims: Int
 55 |     let maxPositionEmbeddings: Int
 56 |     let traditional: Bool
 57 |     var base: Float?
 58 |     let scale: Float
 59 |     let ropeType: String
 60 |     let ropeScaling: [String: StringOrNumber]?
 61 |     var freqs: MLXArray?
 62 | 
 63 |     init(
 64 |         dims: Int,
 65 |         maxPositionEmbeddings: Int?,
 66 |         traditional: Bool = false,
 67 |         base: Float = 10000,
 68 |         scale: Float = 1.0,
 69 |         ropeType: String = "default",
 70 |         ropeScaling: [String: StringOrNumber]? = nil
 71 |     ) {
 72 |         self.dims = dims
 73 |         self.maxPositionEmbeddings = maxPositionEmbeddings ?? 2048
 74 |         self.traditional = traditional
 75 |         self.base = base
 76 |         self.scale = scale
 77 |         self.ropeType = ropeType
 78 |         self.ropeScaling = ropeScaling
 79 |         super.init()
 80 |         computeFreqs()
 81 |     }
 82 | 
 83 |     private func computeFreqs() {
 84 |         if ropeType != "llama3" {
 85 |             freqs = nil
 86 |             return
 87 |         }
 88 | 
 89 |         guard let ropeScaling = ropeScaling,
 90 |             case .float(let factor) = ropeScaling["factor"],
 91 |             case .float(let lowFreqFactor) = ropeScaling["low_freq_factor"] ?? .float(1.0),
 92 |             case .float(let highFreqFactor) = ropeScaling["high_freq_factor"] ?? .float(4.0),
 93 |             case .float(let oldContextLen) = ropeScaling["original_max_position_embeddings"]
 94 |                 ?? .float(8192),
 95 |             let base
 96 |         else {
 97 |             freqs = nil
 98 |             return
 99 |         }
100 | 
101 |         let lowFreqWavelen = oldContextLen / lowFreqFactor
102 |         let highFreqWavelen = oldContextLen / highFreqFactor
103 | 
104 |         let indices = MLXArray(stride(from: 0, to: dims, by: 2))
105 |         var frequencies = MLX.pow(base, indices / Float(dims))
106 |         let wavelens = 2 * Float.pi * frequencies
107 | 
108 |         frequencies = MLX.where(
109 |             wavelens .> MLXArray(lowFreqWavelen), frequencies * factor, frequencies)
110 |         let isMediumFreq = MLX.logicalAnd(
111 |             wavelens .> MLXArray(highFreqWavelen),
112 |             wavelens .< MLXArray(lowFreqWavelen)
113 |         )
114 |         let smoothFactors =
115 |             (oldContextLen / wavelens - lowFreqFactor) / (highFreqFactor - lowFreqFactor)
116 |         let smoothFreqs = frequencies / ((1 - smoothFactors) / factor + smoothFactors)
117 | 
118 |         freqs = MLX.where(isMediumFreq, smoothFreqs, frequencies)
119 |         self.base = nil
120 |     }
121 | 
122 |     func callAsFunction(_ x: MLXArray, offset: Int = 0) -> MLXArray {
123 |         MLXFast.RoPE(
124 |             x,
125 |             dimensions: dims,
126 |             traditional: traditional,
127 |             base: base,
128 |             scale: scale,
129 |             offset: offset,
130 |             freqs: freqs
131 |         )
132 |     }
133 | }
134 | 
135 | private class Attention: Module {
136 | 
137 |     let args: LlamaConfiguration
138 |     let scale: Float
139 | 
140 |     @ModuleInfo(key: "q_proj") var wq: Linear
141 |     @ModuleInfo(key: "k_proj") var wk: Linear
142 |     @ModuleInfo(key: "v_proj") var wv: Linear
143 |     @ModuleInfo(key: "o_proj") var wo: Linear
144 | 
145 |     let rope: DynamicNTKScalingRoPE
146 | 
147 |     init(_ args: LlamaConfiguration) {
148 |         self.args = args
149 | 
150 |         let dim = args.hiddenSize
151 |         let heads = args.attentionHeads
152 |         let kvHeads = args.kvHeads
153 | 
154 |         let headDim = args.resolvedHeadDimensions
155 |         self.scale = pow(Float(headDim), -0.5)
156 | 
157 |         self._wq.wrappedValue = Linear(dim, heads * headDim, bias: args.attentionBias)
158 |         self._wk.wrappedValue = Linear(dim, kvHeads * headDim, bias: args.attentionBias)
159 |         self._wv.wrappedValue = Linear(dim, kvHeads * headDim, bias: args.attentionBias)
160 |         self._wo.wrappedValue = Linear(heads * headDim, dim, bias: args.attentionBias)
161 | 
162 |         self.rope = DynamicNTKScalingRoPE(
163 |             dims: headDim,
164 |             maxPositionEmbeddings: args.maxPositionEmbeddings,
165 |             traditional: args.ropeTraditional,
166 |             base: args.ropeTheta,
167 |             scale: 1.0,
168 |             ropeType: {
169 |                 if case .string(let value) = args.ropeScaling?["type"] {
170 |                     return value
171 |                 } else {
172 |                     return "default"
173 |                 }
174 |             }(),
175 |             ropeScaling: args.ropeScaling)
176 |     }
177 | 
178 |     func callAsFunction(
179 |         _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
180 |     ) -> MLXArray {
181 |         let (B, L) = (x.dim(0), x.dim(1))
182 | 
183 |         var queries = wq(x)
184 |         var keys = wk(x)
185 |         var values = wv(x)
186 | 
187 |         // Prepare the queries, keys and values for the attention computation
188 |         queries = queries.reshaped(B, L, args.attentionHeads, -1).transposed(0, 2, 1, 3)
189 |         keys = keys.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)
190 |         values = values.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)
191 | 
192 |         if let cache {
193 |             queries = rope(queries, offset: cache.offset)
194 |             keys = rope(keys, offset: cache.offset)
195 |             (keys, values) = cache.update(keys: keys, values: values)
196 |         } else {
197 |             queries = rope(queries)
198 |             keys = rope(keys)
199 |         }
200 | 
201 |         let output = MLXFast.scaledDotProductAttention(
202 |             queries: queries, keys: keys, values: values, scale: scale, mask: mask
203 |         )
204 |         .transposed(0, 2, 1, 3)
205 |         .reshaped(B, L, -1)
206 | 
207 |         return wo(output)
208 |     }
209 | }
210 | 
211 | private class MLP: Module, UnaryLayer {
212 | 
213 |     @ModuleInfo(key: "gate_proj") var gate: Linear
214 |     @ModuleInfo(key: "down_proj") var down: Linear
215 |     @ModuleInfo(key: "up_proj") var up: Linear
216 | 
217 |     init(_ args: LlamaConfiguration) {
218 |         self._gate.wrappedValue = Linear(args.hiddenSize, args.intermediateSize, bias: args.mlpBias)
219 |         self._down.wrappedValue = Linear(args.intermediateSize, args.hiddenSize, bias: args.mlpBias)
220 |         self._up.wrappedValue = Linear(args.hiddenSize, args.intermediateSize, bias: args.mlpBias)
221 |     }
222 | 
223 |     func callAsFunction(_ x: MLXArray) -> MLXArray {
224 |         let activation = silu(gate(x))
225 |         return down(activation * up(x))
226 |     }
227 | }
228 | 
229 | private class TransformerBlock: Module {
230 |     @ModuleInfo(key: "self_attn") var attention: Attention
231 |     @ModuleInfo(key: "mlp") var mlp: MLP
232 | 
233 |     @ModuleInfo(key: "input_layernorm") var inputLayerNorm: RMSNorm
234 |     @ModuleInfo(key: "post_attention_layernorm") var postAttentionLayerNorm: RMSNorm
235 | 
236 |     init(_ args: LlamaConfiguration) {
237 |         self._attention.wrappedValue = Attention(args)
238 |         self._mlp.wrappedValue = MLP(args)
239 |         self._inputLayerNorm.wrappedValue = RMSNorm(
240 |             dimensions: args.hiddenSize, eps: args.rmsNormEps)
241 |         self._postAttentionLayerNorm.wrappedValue = RMSNorm(
242 |             dimensions: args.hiddenSize, eps: args.rmsNormEps)
243 |     }
244 | 
245 |     func callAsFunction(
246 |         _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
247 |     ) -> MLXArray {
248 |         var r = attention(inputLayerNorm(x), mask: mask, cache: cache)
249 |         let h = x + r
250 |         r = mlp(postAttentionLayerNorm(h))
251 |         let out = h + r
252 |         return out
253 |     }
254 | }
255 | 
256 | private class LlamaModelInner: Module {
257 | 
258 |     @ModuleInfo(key: "embed_tokens") var embedTokens: Embedding
259 | 
260 |     let layers: [TransformerBlock]
261 |     let norm: RMSNorm
262 | 
263 |     init(_ args: LlamaConfiguration) {
264 |         precondition(args.vocabularySize > 0)
265 | 
266 |         self._embedTokens.wrappedValue = Embedding(
267 |             embeddingCount: args.vocabularySize, dimensions: args.hiddenSize)
268 | 
269 |         self.layers = (0 ..< args.hiddenLayers).map { _ in TransformerBlock(args) }
270 |         self.norm = RMSNorm(dimensions: args.hiddenSize, eps: args.rmsNormEps)
271 |     }
272 | 
273 |     func callAsFunction(_ inputs: MLXArray, cache: [KVCache]? = nil) -> MLXArray {
274 |         var h = embedTokens(inputs)
275 | 
276 |         let mask = createAttentionMask(h: h, cache: cache)
277 | 
278 |         for (i, layer) in layers.enumerated() {
279 |             h = layer(h, mask: mask, cache: cache?[i])
280 |         }
281 | 
282 |         return norm(h)
283 |     }
284 | }
285 | 
286 | /// Model for Llama and Mistral model types.
287 | public class LlamaModel: Module, LLMModel, KVCacheDimensionProvider {
288 | 
289 |     public let vocabularySize: Int
290 |     public let kvHeads: [Int]
291 | 
292 |     fileprivate let model: LlamaModelInner
293 | 
294 |     @ModuleInfo(key: "lm_head") var lmHead: Linear?
295 | 
296 |     public init(_ args: LlamaConfiguration) {
297 |         self.vocabularySize = args.vocabularySize
298 |         self.kvHeads = (0 ..< args.hiddenLayers).map { _ in args.kvHeads }
299 |         self.model = LlamaModelInner(args)
300 |         if !args.tieWordEmbeddings {
301 |             self._lmHead.wrappedValue = Linear(args.hiddenSize, args.vocabularySize, bias: false)
302 |         }
303 |     }
304 | 
305 |     public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {
306 |         let out = model(inputs, cache: cache)
307 |         if let lmHead {
308 |             return lmHead(out)
309 |         } else {
310 |             return model.embedTokens.asLinear(out)
311 |         }
312 |     }
313 | 
314 |     public func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {
315 |         // Remove unused precomputed rotary frequencies
316 |         weights.filter {
317 |             !$0.key.contains("self_attn.rotary_emb.inv_freq")
318 |         }
319 |     }
320 | 
321 |     public func messageGenerator(tokenizer: any Tokenizer) -> any MessageGenerator {
322 |         // some models allow the system role and some do not -- this is enforced
323 |         // by the chat template (code).
324 |         do {
325 |             let probe = [
326 |                 [
327 |                     "role": "system",
328 |                     "content": "test",
329 |                 ]
330 |             ]
331 |             _ = try tokenizer.applyChatTemplate(messages: probe)
332 |             return DefaultMessageGenerator()
333 |         } catch {
334 |             return NoSystemMessageGenerator()
335 |         }
336 |     }
337 | }
338 | 
339 | public struct LlamaConfiguration: Codable, Sendable {
340 | 
341 |     var hiddenSize: Int
342 |     var hiddenLayers: Int
343 |     var intermediateSize: Int
344 |     var attentionHeads: Int
345 |     var headDimensions: Int?
346 |     var rmsNormEps: Float
347 |     var vocabularySize: Int
348 |     var kvHeads: Int
349 |     var maxPositionEmbeddings: Int?
350 |     var ropeTheta: Float = 10_000
351 |     var ropeTraditional: Bool = false
352 |     var ropeScaling: [String: StringOrNumber]?
353 |     var tieWordEmbeddings: Bool = true
354 |     var attentionBias: Bool = false
355 |     var mlpBias: Bool = false
356 | 
357 |     public init(
358 |         hiddenSize: Int, hiddenLayers: Int, intermediateSize: Int, attentionHeads: Int,
359 |         headDimensions: Int? = nil, rmsNormEps: Float, vocabularySize: Int, kvHeads: Int,
360 |         maxPositionEmbeddings: Int? = nil, ropeTheta: Float = 10_000, ropeTraditional: Bool = false,
361 |         ropeScaling: [String: StringOrNumber]? = nil, tieWordEmbeddings: Bool = true,
362 |         attentionBias: Bool = false, mlpBias: Bool = false
363 |     ) {
364 |         self.hiddenSize = hiddenSize
365 |         self.hiddenLayers = hiddenLayers
366 |         self.intermediateSize = intermediateSize
367 |         self.attentionHeads = attentionHeads
368 |         self.headDimensions = headDimensions
369 |         self.rmsNormEps = rmsNormEps
370 |         self.vocabularySize = vocabularySize
371 |         self.kvHeads = kvHeads
372 |         self.maxPositionEmbeddings = maxPositionEmbeddings
373 |         self.ropeTheta = ropeTheta
374 |         self.ropeTraditional = ropeTraditional
375 |         self.ropeScaling = ropeScaling
376 |         self.tieWordEmbeddings = tieWordEmbeddings
377 |         self.attentionBias = attentionBias
378 |         self.mlpBias = mlpBias
379 |     }
380 | 
381 |     var resolvedHeadDimensions: Int {
382 |         headDimensions ?? (hiddenSize / attentionHeads)
383 |     }
384 | 
385 |     enum CodingKeys: String, CodingKey {
386 |         case hiddenSize = "hidden_size"
387 |         case hiddenLayers = "num_hidden_layers"
388 |         case intermediateSize = "intermediate_size"
389 |         case attentionHeads = "num_attention_heads"
390 |         case headDimensions = "head_dim"
391 |         case rmsNormEps = "rms_norm_eps"
392 |         case vocabularySize = "vocab_size"
393 |         case kvHeads = "num_key_value_heads"
394 |         case maxPositionEmbeddings = "max_position_embeddings"
395 |         case ropeTheta = "rope_theta"
396 |         case ropeTraditional = "rope_traditional"
397 |         case ropeScaling = "rope_scaling"
398 |         case tieWordEmbeddings = "tie_word_embeddings"
399 |         case attentionBias = "attention_bias"
400 |         case mlpBias = "mlp_bias"
401 |     }
402 | 
403 |     public init(from decoder: Swift.Decoder) throws {
404 |         let container = try decoder.container(keyedBy: CodingKeys.self)
405 | 
406 |         hiddenSize = try container.decode(Int.self, forKey: .hiddenSize)
407 |         hiddenLayers = try container.decode(Int.self, forKey: .hiddenLayers)
408 |         intermediateSize = try container.decode(Int.self, forKey: .intermediateSize)
409 |         attentionHeads = try container.decode(Int.self, forKey: .attentionHeads)
410 |         headDimensions = try container.decodeIfPresent(Int.self, forKey: .headDimensions)
411 |         rmsNormEps = try container.decode(Float.self, forKey: .rmsNormEps)
412 |         vocabularySize = try container.decode(Int.self, forKey: .vocabularySize)
413 |         kvHeads = try container.decodeIfPresent(Int.self, forKey: .kvHeads) ?? attentionHeads
414 |         maxPositionEmbeddings = try container.decodeIfPresent(
415 |             Int.self, forKey: .maxPositionEmbeddings)
416 |         if let ropeTheta = try container.decodeIfPresent(Float.self, forKey: .ropeTheta) {
417 |             self.ropeTheta = ropeTheta
418 |         }
419 |         if let ropeTraditional = try container.decodeIfPresent(Bool.self, forKey: .ropeTraditional)
420 |         {
421 |             self.ropeTraditional = ropeTraditional
422 |         }
423 |         ropeScaling = try container.decodeIfPresent(
424 |             [String: StringOrNumber].self, forKey: .ropeScaling)
425 |         if let tieWordEmbeddings = try container.decodeIfPresent(
426 |             Bool.self, forKey: .tieWordEmbeddings)
427 |         {
428 |             self.tieWordEmbeddings = tieWordEmbeddings
429 |         }
430 |         if let attentionBias = try container.decodeIfPresent(Bool.self, forKey: .attentionBias) {
431 |             self.attentionBias = attentionBias
432 |         }
433 |         if let mlpBias = try container.decodeIfPresent(Bool.self, forKey: .mlpBias) {
434 |             self.mlpBias = mlpBias
435 |         }
436 | 
437 |         if let ropeScaling {
438 |             if ropeScaling["factor"] == nil {
439 |                 throw DecodingError.dataCorruptedError(
440 |                     forKey: .ropeScaling, in: container,
441 |                     debugDescription: "rope_scaling must contain 'factor'")
442 |             }
443 |             if let ropeType = ropeScaling["type"] ?? ropeScaling["rope_type"] {
444 |                 if case .string = ropeType {
445 |                     let options = [
446 |                         StringOrNumber.string("linear"), StringOrNumber.string("dynamic"),
447 |                         StringOrNumber.string("llama3"),
448 |                     ]
449 |                     if !options.contains(ropeType) {
450 |                         throw DecodingError.dataCorruptedError(
451 |                             forKey: .ropeScaling, in: container,
452 |                             debugDescription:
453 |                                 "rope_scaling 'type' currently only supports 'linear', 'dynamic', or 'llama3'"
454 |                         )
455 |                     }
456 |                 }
457 |             } else {
458 |                 throw DecodingError.dataCorruptedError(
459 |                     forKey: .ropeScaling, in: container,
460 |                     debugDescription: "rope_scaling must contain either 'type' or 'rope_type'")
461 |             }
462 |         }
463 |     }
464 | }
465 | 
466 | // MARK: - LoRA
467 | 
468 | extension LlamaModel: LoRAModel {
469 |     public func loraLinearLayers() -> LoRALinearLayers {
470 |         model.layers.map { ($0.attention, ["q_proj", "v_proj"]) }
471 |     }
472 | }
473 | 


--------------------------------------------------------------------------------
/Libraries/MLXLLM/Models/MiMo.swift:
--------------------------------------------------------------------------------
  1 | //
  2 | //  MiMo.swift
  3 | //  LLM
  4 | //
  5 | //  Created by John Mai on 2025/5/3.
  6 | //
  7 | 
  8 | import Foundation
  9 | import MLX
 10 | import MLXLMCommon
 11 | import MLXNN
 12 | 
 13 | private class Attention: Module {
 14 |     let args: MiMoConfiguration
 15 |     let scale: Float
 16 | 
 17 |     @ModuleInfo(key: "q_proj") var wq: Linear
 18 |     @ModuleInfo(key: "k_proj") var wk: Linear
 19 |     @ModuleInfo(key: "v_proj") var wv: Linear
 20 |     @ModuleInfo(key: "o_proj") var wo: Linear
 21 | 
 22 |     let rope: RoPE
 23 | 
 24 |     public init(_ args: MiMoConfiguration) {
 25 |         self.args = args
 26 | 
 27 |         let dim = args.hiddenSize
 28 |         let heads = args.attentionHeads
 29 |         let kvHeads = args.kvHeads
 30 | 
 31 |         let headDim = args.hiddenSize / heads
 32 |         self.scale = pow(Float(headDim), -0.5)
 33 | 
 34 |         _wq.wrappedValue = Linear(dim, heads * headDim, bias: true)
 35 |         _wk.wrappedValue = Linear(dim, kvHeads * headDim, bias: true)
 36 |         _wv.wrappedValue = Linear(dim, kvHeads * headDim, bias: true)
 37 |         _wo.wrappedValue = Linear(heads * headDim, dim, bias: false)
 38 | 
 39 |         let ropeScale: Float
 40 |         if let ropeScaling = args.ropeScaling, ropeScaling["type"] == .string("linear"),
 41 |             let factor = ropeScaling["factor"]
 42 |         {
 43 |             if let v = factor.asFloat() {
 44 |                 ropeScale = 1 / v
 45 |             } else {
 46 |                 fatalError("ropeScaling.factor must be a float")
 47 |             }
 48 |         } else {
 49 |             ropeScale = 1
 50 |         }
 51 | 
 52 |         self.rope = RoPE(
 53 |             dimensions: headDim, traditional: args.ropeTraditional, base: args.ropeTheta,
 54 |             scale: ropeScale)
 55 |     }
 56 | 
 57 |     public func callAsFunction(
 58 |         _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
 59 |     ) -> MLXArray {
 60 |         let (B, L) = (x.dim(0), x.dim(1))
 61 | 
 62 |         var queries = wq(x)
 63 |         var keys = wk(x)
 64 |         var values = wv(x)
 65 | 
 66 |         // prepare the queries, keys and values for the attention computation
 67 |         queries = queries.reshaped(B, L, args.attentionHeads, -1).transposed(0, 2, 1, 3)
 68 |         keys = keys.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)
 69 |         values = values.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)
 70 | 
 71 |         if let cache {
 72 |             queries = rope(queries, offset: cache.offset)
 73 |             keys = rope(keys, offset: cache.offset)
 74 |             (keys, values) = cache.update(keys: keys, values: values)
 75 |         } else {
 76 |             queries = rope(queries)
 77 |             keys = rope(keys)
 78 |         }
 79 | 
 80 |         let output = MLXFast.scaledDotProductAttention(
 81 |             queries: queries, keys: keys, values: values, scale: scale, mask: mask
 82 |         )
 83 |         .transposed(0, 2, 1, 3)
 84 |         .reshaped(B, L, -1)
 85 | 
 86 |         return wo(output)
 87 |     }
 88 | }
 89 | 
 90 | private class MLP: Module, UnaryLayer {
 91 |     @ModuleInfo(key: "gate_proj") var gate: Linear
 92 |     @ModuleInfo(key: "down_proj") var down: Linear
 93 |     @ModuleInfo(key: "up_proj") var up: Linear
 94 | 
 95 |     public init(dimensions: Int, hiddenDimensions: Int) {
 96 |         _gate.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)
 97 |         _down.wrappedValue = Linear(hiddenDimensions, dimensions, bias: false)
 98 |         _up.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)
 99 |     }
100 | 
101 |     public func callAsFunction(_ x: MLXArray) -> MLXArray {
102 |         down(silu(gate(x)) * up(x))
103 |     }
104 | }
105 | 
106 | private class TransformerBlock: Module {
107 |     @ModuleInfo(key: "self_attn") var attention: Attention
108 |     let mlp: MLP
109 | 
110 |     @ModuleInfo(key: "input_layernorm") var inputLayerNorm: RMSNorm
111 |     @ModuleInfo(key: "post_attention_layernorm") var postAttentionLayerNorm: RMSNorm
112 | 
113 |     public init(_ args: MiMoConfiguration) {
114 |         _attention.wrappedValue = Attention(args)
115 |         self.mlp = MLP(dimensions: args.hiddenSize, hiddenDimensions: args.intermediateSize)
116 |         _inputLayerNorm.wrappedValue = RMSNorm(
117 |             dimensions: args.hiddenSize, eps: args.rmsNormEps)
118 |         _postAttentionLayerNorm.wrappedValue = RMSNorm(
119 |             dimensions: args.hiddenSize, eps: args.rmsNormEps)
120 |     }
121 | 
122 |     public func callAsFunction(
123 |         _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
124 |     ) -> MLXArray {
125 |         var r = attention(inputLayerNorm(x), mask: mask, cache: cache)
126 |         let h = x + r
127 |         r = mlp(postAttentionLayerNorm(h))
128 |         let out = h + r
129 |         return out
130 |     }
131 | }
132 | 
133 | private class MiMoModelInner: Module {
134 |     @ModuleInfo(key: "embed_tokens") var embedTokens: Embedding
135 | 
136 |     fileprivate let layers: [TransformerBlock]
137 |     let norm: RMSNorm
138 | 
139 |     let numNextnPredictLayers: Int
140 | 
141 |     public init(_ args: MiMoConfiguration) {
142 |         precondition(args.vocabularySize > 0)
143 | 
144 |         _embedTokens.wrappedValue = Embedding(
145 |             embeddingCount: args.vocabularySize, dimensions: args.hiddenSize)
146 | 
147 |         self.layers = (0 ..< args.hiddenLayers).map { _ in
148 |             TransformerBlock(args)
149 |         }
150 |         self.norm = RMSNorm(dimensions: args.hiddenSize, eps: args.rmsNormEps)
151 |         self.numNextnPredictLayers = args.numNextnPredictLayers
152 |     }
153 | 
154 |     public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]? = nil) -> MLXArray {
155 |         var h = embedTokens(inputs)
156 | 
157 |         let mask = createAttentionMask(h: h, cache: cache)
158 | 
159 |         for (i, layer) in layers.enumerated() {
160 |             h = layer(h, mask: mask, cache: cache?[i])
161 |         }
162 | 
163 |         return norm(h)
164 |     }
165 | }
166 | 
167 | public class MiMoModel: Module, LLMModel, KVCacheDimensionProvider {
168 |     public let vocabularySize: Int
169 |     public let kvHeads: [Int]
170 | 
171 |     private let model: MiMoModelInner
172 |     let configuration: MiMoConfiguration
173 | 
174 |     @ModuleInfo(key: "lm_head") var lmHead: Linear?
175 | 
176 |     public init(_ args: MiMoConfiguration) {
177 |         self.configuration = args
178 |         self.vocabularySize = args.vocabularySize
179 |         self.kvHeads = (0 ..< args.hiddenLayers).map { _ in args.kvHeads }
180 |         self.model = MiMoModelInner(args)
181 | 
182 |         if !args.tieWordEmbeddings {
183 |             _lmHead.wrappedValue = Linear(args.hiddenSize, args.vocabularySize, bias: false)
184 |         }
185 |     }
186 | 
187 |     public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]? = nil) -> MLXArray {
188 |         let out = model(inputs, cache: cache)
189 | 
190 |         if let lmHead = lmHead {
191 |             return lmHead(out)
192 |         } else {
193 |             return model.embedTokens.asLinear(out)
194 |         }
195 |     }
196 | 
197 |     public func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {
198 |         var weights = weights
199 | 
200 |         if configuration.tieWordEmbeddings {
201 |             weights.removeValue(forKey: "lm_head.weight")
202 |         }
203 | 
204 |         // Remove unused precomputed rotary freqs and mtp_layers
205 |         return weights.filter { key, _ in
206 |             !key.contains("self_attn.rotary_emb.inv_freq") && !key.hasPrefix("model.mtp_layers.")
207 |         }
208 |     }
209 | }
210 | 
211 | public struct MiMoConfiguration: Codable, Sendable {
212 |     var hiddenSize: Int
213 |     var hiddenLayers: Int
214 |     var intermediateSize: Int
215 |     var attentionHeads: Int
216 |     var rmsNormEps: Float
217 |     var vocabularySize: Int
218 |     var kvHeads: Int
219 |     var maxPositionEmbeddings: Int
220 |     var ropeTheta: Float
221 |     var ropeTraditional: Bool
222 |     var ropeScaling: [String: StringOrNumber]?
223 |     var tieWordEmbeddings: Bool
224 |     var numNextnPredictLayers: Int
225 | 
226 |     enum CodingKeys: String, CodingKey {
227 |         case hiddenSize = "hidden_size"
228 |         case hiddenLayers = "num_hidden_layers"
229 |         case intermediateSize = "intermediate_size"
230 |         case attentionHeads = "num_attention_heads"
231 |         case rmsNormEps = "rms_norm_eps"
232 |         case vocabularySize = "vocab_size"
233 |         case kvHeads = "num_key_value_heads"
234 |         case maxPositionEmbeddings = "max_position_embeddings"
235 |         case ropeTheta = "rope_theta"
236 |         case ropeTraditional = "rope_traditional"
237 |         case ropeScaling = "rope_scaling"
238 |         case tieWordEmbeddings = "tie_word_embeddings"
239 |         case numNextnPredictLayers = "num_nextn_predict_layers"
240 |     }
241 | 
242 |     public init(from decoder: Decoder) throws {
243 |         let container = try decoder.container(keyedBy: CodingKeys.self)
244 | 
245 |         self.hiddenSize = try container.decode(Int.self, forKey: .hiddenSize)
246 |         self.hiddenLayers = try container.decode(Int.self, forKey: .hiddenLayers)
247 |         self.intermediateSize = try container.decode(Int.self, forKey: .intermediateSize)
248 |         self.attentionHeads = try container.decode(Int.self, forKey: .attentionHeads)
249 |         self.rmsNormEps = try container.decode(Float.self, forKey: .rmsNormEps)
250 |         self.vocabularySize = try container.decode(Int.self, forKey: .vocabularySize)
251 |         self.kvHeads = try container.decode(Int.self, forKey: .kvHeads)
252 |         self.maxPositionEmbeddings =
253 |             try container.decodeIfPresent(Int.self, forKey: .maxPositionEmbeddings) ?? 32768
254 |         self.ropeTheta = try container.decodeIfPresent(Float.self, forKey: .ropeTheta) ?? 10000.0
255 |         self.ropeTraditional =
256 |             try container.decodeIfPresent(Bool.self, forKey: .ropeTraditional) ?? false
257 |         self.ropeScaling = try container.decodeIfPresent(
258 |             [String: StringOrNumber].self, forKey: .ropeScaling)
259 |         self.tieWordEmbeddings =
260 |             try container.decodeIfPresent(Bool.self, forKey: .tieWordEmbeddings) ?? false
261 |         self.numNextnPredictLayers =
262 |             try container.decodeIfPresent(Int.self, forKey: .numNextnPredictLayers) ?? 2
263 |     }
264 | }
265 | 
266 | // MARK: - LoRA
267 | 
268 | extension MiMoModel: LoRAModel {
269 |     public func loraLinearLayers() -> LoRALinearLayers {
270 |         model.layers.map { ($0.attention, ["q_proj", "v_proj"]) }
271 |     }
272 | }
273 | 


--------------------------------------------------------------------------------
/Libraries/MLXLLM/Models/OpenELM.swift:
--------------------------------------------------------------------------------
  1 | //
  2 | //  OpenELM.swift
  3 | //  LLM
  4 | //
  5 | //  Created by Sachin Desai on 2024/4/27.
  6 | //
  7 | 
  8 | import Foundation
  9 | import MLX
 10 | import MLXLMCommon
 11 | import MLXNN
 12 | 
 13 | func computeHeads(modelDim: Int, headDim: Int) -> Int {
 14 |     assert(modelDim % headDim == 0, "modelDim must be divisible by headDim")
 15 |     return modelDim / headDim
 16 | }
 17 | 
 18 | func makeDivisible(_ v: Float, divisor: Int = 8, minValue: Float? = nil) -> Int {
 19 |     let minVal = minValue ?? Float(divisor)
 20 |     var roundDown = max(minVal, Float(Int((v + Float(divisor) / 2) / Float(divisor)) * divisor))
 21 | 
 22 |     if roundDown < 0.9 * v {
 23 |         roundDown += Float(divisor)
 24 |     }
 25 |     return Int(roundDown)
 26 | }
 27 | 
 28 | private class MultiHeadCausalAttention: Module {
 29 |     let scale: Float
 30 |     let heads: Int
 31 |     let headDim: Int
 32 |     let kvHeads: Int
 33 | 
 34 |     @ModuleInfo(key: "qkv_proj") var qkvProj: Linear
 35 |     @ModuleInfo(key: "out_proj") var outProj: Linear
 36 | 
 37 |     @ModuleInfo(key: "q_norm") var qNorm: RMSNorm?
 38 |     @ModuleInfo(key: "k_norm") var kNorm: RMSNorm?
 39 | 
 40 |     let rope: RoPE
 41 | 
 42 |     public init(_ args: OpenElmConfiguration, layerId: Int) {
 43 |         self.headDim = args.headDimensions
 44 |         let modelDim = args.modelDim
 45 | 
 46 |         self.heads = args.numQueryHeads[layerId]
 47 |         self.kvHeads = args.kvHeads[layerId]
 48 |         self.scale = pow(Float(headDim), -0.5)
 49 | 
 50 |         let opSize = (heads + (kvHeads * 2)) * headDim
 51 |         self._qkvProj.wrappedValue = Linear(modelDim, opSize, bias: false)
 52 |         self._outProj.wrappedValue = Linear(heads * headDim, modelDim, bias: false)
 53 | 
 54 |         if args.normalizeQkProjections {
 55 |             self._qNorm.wrappedValue = RMSNorm(dimensions: headDim, eps: args.rmsNormEps)
 56 |             self._kNorm.wrappedValue = RMSNorm(dimensions: headDim, eps: args.rmsNormEps)
 57 |         }
 58 | 
 59 |         self.rope = RoPE(
 60 |             dimensions: headDim, traditional: args.ropeTraditional, base: args.ropeTheta)
 61 |     }
 62 | 
 63 |     public func callAsFunction(
 64 |         _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
 65 |     ) -> MLXArray {
 66 |         let (B, L) = (x.dim(0), x.dim(1))
 67 |         let qkv = qkvProj(x).reshaped(B, L, heads + (kvHeads * 2), headDim).transposed(0, 2, 1, 3)
 68 | 
 69 |         let qkvSplit = split(qkv, indices: [heads, heads + kvHeads], axis: 1)
 70 |         var queries = qkvSplit[0]
 71 |         var keys = qkvSplit[1]
 72 |         var values = qkvSplit[2]
 73 | 
 74 |         if let qNorm, let kNorm {
 75 |             queries = qNorm(queries)
 76 |             keys = kNorm(keys)
 77 |         }
 78 | 
 79 |         if let cache {
 80 |             queries = rope(queries, offset: cache.offset)
 81 |             keys = rope(keys, offset: cache.offset)
 82 |             (keys, values) = cache.update(keys: keys, values: values)
 83 |         } else {
 84 |             queries = rope(queries)
 85 |             keys = rope(keys)
 86 |         }
 87 | 
 88 |         let output = MLXFast.scaledDotProductAttention(
 89 |             queries: queries, keys: keys, values: values, scale: scale, mask: mask
 90 |         ).transposed(0, 2, 1, 3).reshaped(B, L, heads * headDim)
 91 | 
 92 |         return outProj(output)
 93 |     }
 94 | }
 95 | 
 96 | private class FeedForwardNetwork: Module, UnaryLayer {
 97 |     @ModuleInfo var proj_1: Linear
 98 |     @ModuleInfo var proj_2: Linear
 99 | 
100 |     public init(_ args: OpenElmConfiguration, layedId: Int) {
101 |         let dim = args.modelDim
102 |         let ffnMultiplier = args.ffnMultipliers[layedId]
103 |         let intermediateDim = Int(
104 |             makeDivisible(Float(ffnMultiplier) * Float(dim), divisor: args.ffnDimDivisor))
105 | 
106 |         self.proj_1 = Linear(dim, 2 * intermediateDim, bias: false)
107 |         self.proj_2 = Linear(intermediateDim, dim, bias: false)
108 |     }
109 | 
110 |     public func callAsFunction(_ x: MLXArray) -> MLXArray {
111 |         let a = proj_1(x)
112 |         let b = split(a, parts: 2, axis: -1)
113 |         let gate = b[0]
114 |         let x = b[1]
115 |         return proj_2(silu(gate) * x)
116 |     }
117 | }
118 | 
119 | private class TransformerDecoderLayer: Module {
120 |     @ModuleInfo(key: "attn") var attn: MultiHeadCausalAttention
121 |     let ffn: FeedForwardNetwork
122 | 
123 |     @ModuleInfo(key: "ffn_norm") var ffnNorm: RMSNorm
124 |     @ModuleInfo(key: "attn_norm") var attnNorm: RMSNorm
125 | 
126 |     public init(_ args: OpenElmConfiguration, layerId: Int) {
127 |         let dim = args.modelDim
128 |         self._attn.wrappedValue = MultiHeadCausalAttention(args, layerId: layerId)
129 |         self.ffn = FeedForwardNetwork(args, layedId: layerId)
130 |         self._ffnNorm.wrappedValue = RMSNorm(dimensions: dim, eps: args.rmsNormEps)
131 |         self._attnNorm.wrappedValue = RMSNorm(dimensions: dim, eps: args.rmsNormEps)
132 |     }
133 | 
134 |     public func callAsFunction(
135 |         _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
136 |     ) -> MLXArray {
137 |         var r = attn(attnNorm(x), mask: mask, cache: cache)
138 |         let h = x + r
139 |         r = ffn(ffnNorm(h))
140 |         let out = h + r
141 |         return out
142 |     }
143 | }
144 | 
145 | class OpenELMModelInner: Module {
146 |     @ModuleInfo(key: "token_embeddings") var embedTokens: Embedding
147 | 
148 |     fileprivate let layers: [TransformerDecoderLayer]
149 |     fileprivate let norm: RMSNorm
150 | 
151 |     public init(_ args: OpenElmConfiguration) {
152 |         precondition(args.vocabularySize > 0)
153 | 
154 |         self._embedTokens.wrappedValue = Embedding(
155 |             embeddingCount: args.vocabularySize, dimensions: args.modelDim)
156 | 
157 |         self.layers = (0 ..< args.numTransformerLayers)
158 |             .map { layerId in
159 |                 TransformerDecoderLayer(args, layerId: layerId)
160 |             }
161 | 
162 |         self.norm = RMSNorm(dimensions: args.modelDim, eps: args.rmsNormEps)
163 |     }
164 | 
165 |     public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]? = nil) -> MLXArray {
166 |         var h = embedTokens(inputs)
167 |         let mask = createAttentionMask(h: h, cache: cache)
168 | 
169 |         for (i, layer) in layers.enumerated() {
170 |             h = layer(h, mask: mask, cache: cache?[i])
171 |         }
172 | 
173 |         return norm(h)
174 |     }
175 | }
176 | 
177 | public class OpenELMModel: Module, LLMModel, KVCacheDimensionProvider {
178 |     public let vocabularySize: Int
179 |     public let kvHeads: [Int]
180 | 
181 |     let transformer: OpenELMModelInner
182 | 
183 |     @ModuleInfo(key: "lm_head") var lmHead: Linear?
184 | 
185 |     public init(_ args: OpenElmConfiguration) {
186 |         self.vocabularySize = args.vocabularySize
187 |         self.kvHeads = args.kvHeads
188 | 
189 |         self.transformer = OpenELMModelInner(args)
190 |         if !args.shareInputOutputLayers {
191 |             self._lmHead.wrappedValue = Linear(
192 |                 args.numTransformerLayers, args.vocabularySize, bias: false)
193 |         }
194 |     }
195 | 
196 |     public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {
197 |         var out = transformer(inputs, cache: cache)
198 |         if let lmHead {
199 |             out = lmHead(out)
200 |         } else {
201 |             out = transformer.embedTokens.asLinear(out)
202 |         }
203 | 
204 |         return out
205 |     }
206 | }
207 | 
208 | public struct OpenElmConfiguration: Codable, Sendable {
209 |     var modelType: String
210 |     var headDimensions: Int
211 |     var numTransformerLayers: Int
212 |     var modelDim: Int
213 |     var vocabularySize: Int
214 |     var ffnDimDivisor: Int
215 |     var numQueryHeads: [Int] = []
216 |     var kvHeads: [Int] = []
217 |     var ffnWithGlu: Bool = true
218 |     var normalizeQkProjections: Bool = true
219 |     var shareInputOutputLayers: Bool = true
220 |     var rmsNormEps: Float = 1e-6
221 |     var ropeTheta: Float = 10_000
222 |     var ropeTraditional: Bool = false
223 |     var numGqaGroups: Int = 4
224 |     var ffnMultipliers: [Float] = [0.5, 4.0]
225 |     var qkvMultiplier: [Float] = [0.5, 1.0]
226 | 
227 |     enum CodingKeys: String, CodingKey {
228 |         case modelType = "model_type"
229 |         case headDimensions = "head_dim"
230 |         case numTransformerLayers = "num_transformer_layers"
231 |         case modelDim = "model_dim"
232 |         case vocabularySize = "vocab_size"
233 |         case ffnDimDivisor = "ffn_dim_divisor"
234 |         case ffnMultipliers = "ffn_multipliers"
235 |         case ffnWithGlu = "ffn_with_glu"
236 |         case normalizeQkProjections = "normalize_qk_projections"
237 |         case shareInputOutputLayers = "share_input_output_layers"
238 |     }
239 | 
240 |     public init(from decoder: Decoder) throws {
241 |         // custom implementation to handle optional keys with required values
242 |         let container: KeyedDecodingContainer<OpenElmConfiguration.CodingKeys> =
243 |             try decoder.container(
244 |                 keyedBy: OpenElmConfiguration.CodingKeys.self)
245 | 
246 |         self.modelType = try container.decode(
247 |             String.self, forKey: OpenElmConfiguration.CodingKeys.modelType)
248 |         self.headDimensions = try container.decode(
249 |             Int.self, forKey: OpenElmConfiguration.CodingKeys.headDimensions)
250 |         self.numTransformerLayers = try container.decode(
251 |             Int.self, forKey: OpenElmConfiguration.CodingKeys.numTransformerLayers)
252 | 
253 |         self.modelDim = try container.decode(
254 |             Int.self, forKey: OpenElmConfiguration.CodingKeys.modelDim)
255 |         self.vocabularySize = try container.decode(
256 |             Int.self, forKey: OpenElmConfiguration.CodingKeys.vocabularySize)
257 |         self.ffnDimDivisor = try container.decode(
258 |             Int.self, forKey: OpenElmConfiguration.CodingKeys.ffnDimDivisor)
259 | 
260 |         let qkvMultipliers = stride(
261 |             from: qkvMultiplier[0], through: qkvMultiplier[1],
262 |             by: (qkvMultiplier[1] - qkvMultiplier[0]) / Float(numTransformerLayers - 1)
263 |         )
264 |         .map { round($0 * 100) / 100 }
265 | 
266 |         let headMultipleOf = numGqaGroups
267 |         let queryDims = qkvMultipliers.map { a in
268 |             makeDivisible(Float(self.modelDim) * a, divisor: self.headDimensions * headMultipleOf)
269 |         }
270 | 
271 |         self.numQueryHeads = queryDims.map { qDim in
272 |             Int(computeHeads(modelDim: qDim, headDim: self.headDimensions))
273 |         }
274 | 
275 |         self.kvHeads = self.numQueryHeads.map { qHeads in
276 |             qHeads / numGqaGroups
277 |         }
278 | 
279 |         self.ffnMultipliers = stride(
280 |             from: ffnMultipliers[0], through: ffnMultipliers[1],
281 |             by: (ffnMultipliers[1] - ffnMultipliers[0]) / Float(numTransformerLayers - 1)
282 |         )
283 |         .map { round($0 * 100) / 100 }
284 | 
285 |         self.ffnWithGlu =
286 |             try container.decodeIfPresent(
287 |                 Bool.self, forKey: OpenElmConfiguration.CodingKeys.ffnWithGlu) ?? true
288 |         self.normalizeQkProjections =
289 |             try container.decodeIfPresent(
290 |                 Bool.self, forKey: OpenElmConfiguration.CodingKeys.normalizeQkProjections) ?? true
291 |         self.shareInputOutputLayers =
292 |             try container.decodeIfPresent(
293 |                 Bool.self, forKey: OpenElmConfiguration.CodingKeys.shareInputOutputLayers) ?? true
294 |     }
295 | }
296 | 
297 | // MARK: - LoRA
298 | 
299 | extension OpenELMModel: LoRAModel {
300 |     public func loraLinearLayers() -> LoRALinearLayers {
301 |         transformer.layers.map { ($0.attn, ["qkv_proj"]) }
302 |     }
303 | }
304 | 


--------------------------------------------------------------------------------
/Libraries/MLXLLM/Models/Phi.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import Foundation
  4 | import MLX
  5 | import MLXLMCommon
  6 | import MLXNN
  7 | 
  8 | // https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/models/phi.py
  9 | 
 10 | private class PhiAttention: Module {
 11 | 
 12 |     let args: PhiConfiguration
 13 |     let heads: Int
 14 |     let headDim: Int
 15 | 
 16 |     @ModuleInfo(key: "q_proj") var wq: Linear
 17 |     @ModuleInfo(key: "k_proj") var wk: Linear
 18 |     @ModuleInfo(key: "v_proj") var wv: Linear
 19 |     @ModuleInfo(key: "dense") var dense: Linear
 20 | 
 21 |     let rope: RoPE
 22 | 
 23 |     public init(_ args: PhiConfiguration) {
 24 |         self.args = args
 25 | 
 26 |         let hiddenSize = args.hiddenSize
 27 |         self.heads = args.attentionHeads
 28 |         self.headDim = args.hiddenSize / heads
 29 |         let kvHeads = args.kvHeads
 30 | 
 31 |         if headDim * heads != hiddenSize {
 32 |             fatalError("hidden_size must be divisible by num_heads")
 33 |         }
 34 | 
 35 |         self._wq.wrappedValue = Linear(hiddenSize, heads * headDim, bias: true)
 36 |         self._wk.wrappedValue = Linear(hiddenSize, kvHeads * headDim, bias: true)
 37 |         self._wv.wrappedValue = Linear(hiddenSize, kvHeads * headDim, bias: true)
 38 |         self._dense.wrappedValue = Linear(heads * headDim, hiddenSize, bias: true)
 39 | 
 40 |         self.rope = RoPE(
 41 |             dimensions: Int(args.partialRotaryFactor * Float(headDim)), traditional: false,
 42 |             base: args.ropeTheta)
 43 |     }
 44 | 
 45 |     public func callAsFunction(
 46 |         _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
 47 |     ) -> MLXArray {
 48 |         let (B, L) = (x.dim(0), x.dim(1))
 49 | 
 50 |         var queries = wq(x)
 51 |         var keys = wk(x)
 52 |         var values = wv(x)
 53 | 
 54 |         // prepare the queries, keys and values for the attention computation
 55 |         queries = queries.reshaped(B, L, heads, headDim).transposed(0, 2, 1, 3)
 56 |         keys = keys.reshaped(B, L, args.kvHeads, headDim).transposed(0, 2, 1, 3)
 57 |         values = values.reshaped(B, L, args.kvHeads, headDim).transposed(0, 2, 1, 3)
 58 | 
 59 |         // Add RoPE to the queries and keys and combine them with the cache
 60 |         if let cache {
 61 |             queries = rope(queries, offset: cache.offset)
 62 |             keys = rope(keys, offset: cache.offset)
 63 |             (keys, values) = cache.update(keys: keys, values: values)
 64 |         } else {
 65 |             queries = rope(queries)
 66 |             keys = rope(keys)
 67 |         }
 68 | 
 69 |         // Finally perform the attention computation
 70 |         let scale = sqrt(1 / Float(queries.dim(-1)))
 71 |         let output = MLXFast.scaledDotProductAttention(
 72 |             queries: queries.asType(.float32), keys: keys, values: values, scale: scale, mask: mask
 73 |         )
 74 |         .asType(values.dtype)
 75 |         .transposed(0, 2, 1, 3)
 76 |         .reshaped(B, L, -1)
 77 | 
 78 |         return dense(output)
 79 |     }
 80 | }
 81 | 
 82 | private class PhiMLP: Module, UnaryLayer {
 83 | 
 84 |     @ModuleInfo var fc1: Linear
 85 |     @ModuleInfo var fc2: Linear
 86 |     @ModuleInfo var act: GELU
 87 | 
 88 |     public init(_ config: PhiConfiguration) {
 89 |         self.fc1 = Linear(config.hiddenSize, config.intermediateSize)
 90 |         self.fc2 = Linear(config.intermediateSize, config.hiddenSize)
 91 |         self.act = GELU(approximation: .precise)
 92 |     }
 93 | 
 94 |     public func callAsFunction(_ x: MLXArray) -> MLXArray {
 95 |         fc2(act(fc1(x)))
 96 |     }
 97 | }
 98 | 
 99 | private class PhiDecoderLayer: Module {
100 | 
101 |     @ModuleInfo(key: "self_attn") var selfAttention: PhiAttention
102 |     @ModuleInfo(key: "input_layernorm") var inputLayerNorm: LayerNorm
103 |     var mlp: PhiMLP
104 | 
105 |     public init(_ config: PhiConfiguration) {
106 |         self._selfAttention.wrappedValue = PhiAttention(config)
107 |         self._inputLayerNorm.wrappedValue = LayerNorm(
108 |             dimensions: config.hiddenSize, eps: config.layerNormEps)
109 |         self.mlp = PhiMLP(config)
110 |     }
111 | 
112 |     public func callAsFunction(
113 |         _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
114 |     ) -> MLXArray {
115 |         let h = inputLayerNorm(x)
116 |         let attentionH = selfAttention(h, mask: mask, cache: cache)
117 |         let ffH = mlp(h)
118 |         return attentionH + ffH + x
119 |     }
120 | }
121 | 
122 | private class PhiModelInner: Module {
123 | 
124 |     @ModuleInfo(key: "embed_tokens") var embedTokens: Embedding
125 | 
126 |     @ModuleInfo var layers: [PhiDecoderLayer]
127 |     @ModuleInfo(key: "final_layernorm") var finalLayerNorm: LayerNorm
128 | 
129 |     public init(_ args: PhiConfiguration) {
130 |         self._embedTokens.wrappedValue = Embedding(
131 |             embeddingCount: args.vocabularySize, dimensions: args.hiddenSize)
132 | 
133 |         self.layers = (0 ..< args.hiddenLayers)
134 |             .map { _ in
135 |                 PhiDecoderLayer(args)
136 |             }
137 |         self._finalLayerNorm.wrappedValue = LayerNorm(
138 |             dimensions: args.hiddenSize, eps: args.layerNormEps)
139 |     }
140 | 
141 |     public func callAsFunction(
142 |         _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: [KVCache]? = nil
143 |     ) -> MLXArray {
144 |         var x = embedTokens(x)
145 | 
146 |         for (i, layer) in layers.enumerated() {
147 |             x = layer(x, mask: mask, cache: cache?[i])
148 |         }
149 | 
150 |         return finalLayerNorm(x)
151 |     }
152 | }
153 | 
154 | public class PhiModel: Module, LLMModel, KVCacheDimensionProvider {
155 | 
156 |     public let vocabularySize: Int
157 |     public let kvHeads: [Int]
158 | 
159 |     fileprivate let model: PhiModelInner
160 | 
161 |     @ModuleInfo(key: "lm_head") var lmHead: Linear
162 | 
163 |     public init(_ args: PhiConfiguration) {
164 |         self.vocabularySize = args.vocabularySize
165 |         self.kvHeads = (0 ..< args.hiddenLayers).map { _ in args.kvHeads }
166 |         self.model = PhiModelInner(args)
167 |         self._lmHead.wrappedValue = Linear(args.hiddenSize, args.vocabularySize, bias: true)
168 |     }
169 | 
170 |     public func callAsFunction(_ x: MLXArray, cache: [KVCache]?) -> MLXArray {
171 |         let mask = createAttentionMask(h: x, cache: cache)
172 | 
173 |         let y = model(x, mask: mask, cache: cache)
174 |         return lmHead(y)
175 |     }
176 | }
177 | 
178 | public struct PhiConfiguration: Codable, Sendable {
179 |     var maxPositionalEmbeddings = 2048
180 |     var vocabularySize = 51200
181 |     var hiddenSize = 2560
182 |     var attentionHeads = 32
183 |     var hiddenLayers = 32
184 |     var kvHeads = 32
185 |     var partialRotaryFactor: Float = 0.4
186 |     var intermediateSize = 10240
187 |     var layerNormEps: Float = 1e-5
188 |     var ropeTheta: Float = 10_000
189 | 
190 |     enum CodingKeys: String, CodingKey {
191 |         case maxPositionalEmbeddings = "max_position_embeddings"
192 |         case vocabularySize = "vocab_size"
193 |         case hiddenSize = "hidden_size"
194 |         case attentionHeads = "num_attention_heads"
195 |         case hiddenLayers = "num_hidden_layers"
196 |         case kvHeads = "num_key_value_heads"
197 |         case partialRotaryFactor = "partial_rotary_factor"
198 |         case intermediateSize = "intermediate_size"
199 |         case layerNormEps = "layer_norm_eps"
200 |         case ropeTheta = "rope_theta"
201 |     }
202 | 
203 |     public init(from decoder: Decoder) throws {
204 |         let container: KeyedDecodingContainer<PhiConfiguration.CodingKeys> = try decoder.container(
205 |             keyedBy: PhiConfiguration.CodingKeys.self)
206 | 
207 |         self.maxPositionalEmbeddings = try container.decode(
208 |             Int.self, forKey: PhiConfiguration.CodingKeys.maxPositionalEmbeddings)
209 |         self.vocabularySize = try container.decode(
210 |             Int.self, forKey: PhiConfiguration.CodingKeys.vocabularySize)
211 |         self.hiddenSize = try container.decode(
212 |             Int.self, forKey: PhiConfiguration.CodingKeys.hiddenSize)
213 |         self.attentionHeads = try container.decode(
214 |             Int.self, forKey: PhiConfiguration.CodingKeys.attentionHeads)
215 |         self.hiddenLayers = try container.decode(
216 |             Int.self, forKey: PhiConfiguration.CodingKeys.hiddenLayers)
217 |         self.kvHeads =
218 |             try container.decodeIfPresent(Int.self, forKey: PhiConfiguration.CodingKeys.kvHeads)
219 |             ?? attentionHeads
220 |         self.partialRotaryFactor = try container.decode(
221 |             Float.self, forKey: PhiConfiguration.CodingKeys.partialRotaryFactor)
222 |         self.intermediateSize = try container.decode(
223 |             Int.self, forKey: PhiConfiguration.CodingKeys.intermediateSize)
224 |         self.layerNormEps = try container.decode(
225 |             Float.self, forKey: PhiConfiguration.CodingKeys.layerNormEps)
226 |         self.ropeTheta =
227 |             try container.decodeIfPresent(Float.self, forKey: PhiConfiguration.CodingKeys.ropeTheta)
228 |             ?? 10_000
229 | 
230 |     }
231 | }
232 | 
233 | // MARK: - LoRA
234 | 
235 | extension PhiModel: LoRAModel {
236 |     public func loraLinearLayers() -> LoRALinearLayers {
237 |         model.layers.map { ($0.selfAttention, ["q_proj", "v_proj"]) }
238 |     }
239 | }
240 | 


--------------------------------------------------------------------------------
/Libraries/MLXLLM/Models/Phi3.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import Foundation
  4 | import MLX
  5 | import MLXLMCommon
  6 | import MLXNN
  7 | 
  8 | private class Attention: Module {
  9 | 
 10 |     let args: Phi3Configuration
 11 |     let scale: Float
 12 | 
 13 |     let heads: Int
 14 |     let kvHeads: Int
 15 |     let headDim: Int
 16 |     let ropeDim: Int
 17 | 
 18 |     @ModuleInfo(key: "qkv_proj") var wqkv: Linear
 19 |     @ModuleInfo(key: "o_proj") var wo: Linear
 20 | 
 21 |     enum PositionalEncoding {
 22 |         case rope(RoPE)
 23 |         case suScaledRotaryEmbedding(SuScaledRotaryEmbedding)
 24 | 
 25 |         func applyEncoding(_ x: MLXArray, offset: Int = 0) -> MLXArray {
 26 |             switch self {
 27 |             case .rope(let rope):
 28 |                 return rope.callAsFunction(x, offset: offset)
 29 |             case .suScaledRotaryEmbedding(let suScaledRotaryEmbedding):
 30 |                 return suScaledRotaryEmbedding.callAsFunction(x, offset: offset)
 31 |             }
 32 |         }
 33 |     }
 34 | 
 35 |     let rope: PositionalEncoding
 36 | 
 37 |     public init(_ args: Phi3Configuration) {
 38 |         self.args = args
 39 | 
 40 |         let dim = args.hiddenSize
 41 |         self.heads = args.attentionHeads
 42 |         self.kvHeads = args.kvHeads
 43 | 
 44 |         self.headDim = args.hiddenSize / heads
 45 |         self.ropeDim = Int(Float(headDim) * args.partialRotaryFactor)
 46 |         self.scale = pow(Float(headDim), -0.5)
 47 | 
 48 |         self._wqkv.wrappedValue = Linear(dim, (heads + 2 * kvHeads) * headDim, bias: false)
 49 |         self._wo.wrappedValue = Linear(heads * headDim, dim, bias: false)
 50 | 
 51 |         let ropeScale: Float
 52 | 
 53 |         if let ropeScaling = args.ropeScaling, ropeScaling.type == "linear",
 54 |             let factor = ropeScaling.factor
 55 |         {
 56 |             ropeScale = 1 / factor
 57 |         } else {
 58 |             ropeScale = 1
 59 |         }
 60 | 
 61 |         if let ropeScaling = args.ropeScaling,
 62 |             ropeScaling.type == "su" || ropeScaling.type == "longrope",
 63 |             let shortFactor = ropeScaling.shortFactor, let longFactor = ropeScaling.longFactor
 64 |         {
 65 |             self.rope = .suScaledRotaryEmbedding(
 66 |                 SuScaledRotaryEmbedding(
 67 |                     dimensions: ropeDim, base: args.ropeTheta,
 68 |                     maxPositionEmbeddings: args.maxPositionEmbeddings,
 69 |                     originalMaxPositionEmbeddings: args.originalMaxPositionEmbeddings,
 70 |                     longFactor: longFactor))
 71 | 
 72 |         } else {
 73 |             self.rope = .rope(
 74 |                 RoPE(
 75 |                     dimensions: ropeDim, traditional: args.ropeTraditional, base: args.ropeTheta,
 76 |                     scale: ropeScale))
 77 |         }
 78 |     }
 79 | 
 80 |     public func callAsFunction(
 81 |         _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
 82 |     ) -> MLXArray {
 83 |         let (B, L) = (x.dim(0), x.dim(1))
 84 | 
 85 |         let queryPos = heads * headDim
 86 |         let qkv = split(wqkv(x), indices: [queryPos, queryPos + kvHeads * headDim], axis: -1)
 87 |         var queries = qkv[0]
 88 |         var keys = qkv[1]
 89 |         var values = qkv[2]
 90 | 
 91 |         // prepare the queries, keys and values for the attention computation
 92 |         queries = queries.reshaped(B, L, args.attentionHeads, -1).transposed(0, 2, 1, 3)
 93 |         keys = keys.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)
 94 |         values = values.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)
 95 | 
 96 |         if let cache {
 97 |             queries = rope.applyEncoding(queries, offset: cache.offset)
 98 |             keys = rope.applyEncoding(keys, offset: cache.offset)
 99 |             (keys, values) = cache.update(keys: keys, values: values)
100 |         } else {
101 |             queries = rope.applyEncoding(queries)
102 |             keys = rope.applyEncoding(keys)
103 |         }
104 | 
105 |         let output = MLXFast.scaledDotProductAttention(
106 |             queries: queries, keys: keys, values: values, scale: scale, mask: mask
107 |         )
108 |         .transposed(0, 2, 1, 3)
109 |         .reshaped(B, L, -1)
110 | 
111 |         return wo(output)
112 |     }
113 | }
114 | 
115 | private class MLP: Module, UnaryLayer {
116 | 
117 |     @ModuleInfo(key: "gate_up_proj") var gate_up: Linear
118 |     @ModuleInfo(key: "down_proj") var down: Linear
119 | 
120 |     public init(dimensions: Int, hiddenDimensions: Int) {
121 |         self._gate_up.wrappedValue = Linear(dimensions, 2 * hiddenDimensions, bias: false)
122 |         self._down.wrappedValue = Linear(hiddenDimensions, dimensions, bias: false)
123 |     }
124 | 
125 |     public func callAsFunction(_ x: MLXArray) -> MLXArray {
126 |         let gu = split(gate_up(x), parts: 2, axis: -1)
127 |         return down(silu(gu[0]) * gu[1])
128 |     }
129 | }
130 | 
131 | private class TransformerBlock: Module {
132 | 
133 |     @ModuleInfo(key: "self_attn") var attention: Attention
134 |     let mlp: MLP
135 | 
136 |     @ModuleInfo(key: "input_layernorm") var inputLayerNorm: RMSNorm
137 |     @ModuleInfo(key: "post_attention_layernorm") var postAttentionLayerNorm: RMSNorm
138 | 
139 |     public init(_ args: Phi3Configuration) {
140 |         self._attention.wrappedValue = Attention(args)
141 |         self.mlp = MLP(dimensions: args.hiddenSize, hiddenDimensions: args.intermediateSize)
142 |         self._inputLayerNorm.wrappedValue = RMSNorm(
143 |             dimensions: args.hiddenSize, eps: args.rmsNormEps)
144 |         self._postAttentionLayerNorm.wrappedValue = RMSNorm(
145 |             dimensions: args.hiddenSize, eps: args.rmsNormEps)
146 |     }
147 | 
148 |     public func callAsFunction(
149 |         _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
150 |     ) -> MLXArray {
151 |         var r = attention(inputLayerNorm(x), mask: mask, cache: cache)
152 |         let h = x + r
153 |         r = mlp(postAttentionLayerNorm(h))
154 |         let out = h + r
155 |         return out
156 |     }
157 | }
158 | 
159 | private class Phi3ModelInner: Module {
160 | 
161 |     @ModuleInfo(key: "embed_tokens") var embedTokens: Embedding
162 | 
163 |     fileprivate let layers: [TransformerBlock]
164 |     let norm: RMSNorm
165 |     let args: Phi3Configuration
166 | 
167 |     public init(_ args: Phi3Configuration) {
168 |         precondition(args.vocabularySize > 0)
169 |         self.args = args
170 | 
171 |         self._embedTokens.wrappedValue = Embedding(
172 |             embeddingCount: args.vocabularySize, dimensions: args.hiddenSize)
173 | 
174 |         self.layers = (0 ..< args.hiddenLayers)
175 |             .map { _ in
176 |                 TransformerBlock(args)
177 |             }
178 |         self.norm = RMSNorm(dimensions: args.hiddenSize, eps: args.rmsNormEps)
179 |     }
180 | 
181 |     public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {
182 |         var h = embedTokens(inputs)
183 | 
184 |         let mask = createAttentionMask(h: h, cache: cache)
185 | 
186 |         for (i, layer) in layers.enumerated() {
187 |             h = layer(h, mask: mask, cache: cache?[i])
188 |         }
189 | 
190 |         return norm(h)
191 |     }
192 | }
193 | 
194 | public class Phi3Model: Module, LLMModel, KVCacheDimensionProvider {
195 | 
196 |     public let vocabularySize: Int
197 |     public let kvHeads: [Int]
198 | 
199 |     private let model: Phi3ModelInner
200 |     private let args: Phi3Configuration
201 | 
202 |     @ModuleInfo(key: "lm_head") var lmHead: Linear?
203 | 
204 |     public init(_ args: Phi3Configuration) {
205 |         self.vocabularySize = args.vocabularySize
206 |         self.kvHeads = (0 ..< args.hiddenLayers).map { _ in args.kvHeads }
207 |         self.model = Phi3ModelInner(args)
208 |         self.args = args
209 | 
210 |         if !args.tieWordEmbeddings {
211 |             self._lmHead.wrappedValue = Linear(args.hiddenSize, args.vocabularySize, bias: false)
212 |         }
213 |     }
214 | 
215 |     public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {
216 |         let out = model(inputs, cache: cache)
217 |         if args.tieWordEmbeddings {
218 |             return model.embedTokens.asLinear(out)
219 |         } else if let lmHead {
220 |             return lmHead(out)
221 |         } else {
222 |             fatalError(
223 |                 "Model configuration error: Neither tied embeddings nor lm_head is available")
224 |         }
225 |     }
226 | }
227 | 
228 | struct RopeScalingWithFactorArrays: Codable {
229 |     let longFactor: [Float]?
230 |     let shortFactor: [Float]?
231 |     let factor: Float?
232 |     let type: String?
233 |     let longMScale: Float?
234 |     let shortMScale: Float?
235 | 
236 |     enum CodingKeys: String, CodingKey {
237 |         case type
238 |         case factor
239 |         case longFactor = "long_factor"
240 |         case shortFactor = "short_factor"
241 |         case longMScale = "long_mscale"
242 |         case shortMScale = "short_mscale"
243 |     }
244 | }
245 | 
246 | public struct Phi3Configuration: Codable, Sendable {
247 |     var hiddenSize: Int
248 |     var hiddenLayers: Int
249 |     var intermediateSize: Int
250 |     var attentionHeads: Int
251 |     var rmsNormEps: Float
252 |     var vocabularySize: Int
253 |     var kvHeads: Int
254 |     var ropeTheta: Float = 10_000
255 |     var ropeTraditional: Bool = false
256 |     var ropeScaling: RopeScalingWithFactorArrays?
257 |     var partialRotaryFactor: Float = 1.0
258 |     var maxPositionEmbeddings: Int
259 |     var originalMaxPositionEmbeddings: Int
260 |     var tieWordEmbeddings: Bool = false
261 | 
262 |     enum CodingKeys: String, CodingKey {
263 |         case hiddenSize = "hidden_size"
264 |         case hiddenLayers = "num_hidden_layers"
265 |         case intermediateSize = "intermediate_size"
266 |         case attentionHeads = "num_attention_heads"
267 |         case rmsNormEps = "rms_norm_eps"
268 |         case vocabularySize = "vocab_size"
269 |         case kvHeads = "num_key_value_heads"
270 |         case ropeTheta = "rope_theta"
271 |         case ropeTraditional = "rope_traditional"
272 |         case ropeScaling = "rope_scaling"
273 |         case partialRotaryFactor = "partial_rotary_factor"
274 |         case maxPositionEmbeddings = "max_position_embeddings"
275 |         case originalMaxPositionEmbeddings = "original_max_position_embeddings"
276 |         case tieWordEmbeddings = "tie_word_embeddings"
277 |     }
278 | 
279 |     public init(from decoder: Decoder) throws {
280 |         // custom implementation to handle optional keys with required values
281 |         let container: KeyedDecodingContainer<Phi3Configuration.CodingKeys> = try decoder.container(
282 |             keyedBy: Phi3Configuration.CodingKeys.self)
283 | 
284 |         hiddenSize = try container.decode(Int.self, forKey: Phi3Configuration.CodingKeys.hiddenSize)
285 |         hiddenLayers = try container.decode(
286 |             Int.self, forKey: Phi3Configuration.CodingKeys.hiddenLayers)
287 |         intermediateSize = try container.decode(
288 |             Int.self, forKey: Phi3Configuration.CodingKeys.intermediateSize)
289 |         attentionHeads = try container.decode(
290 |             Int.self, forKey: Phi3Configuration.CodingKeys.attentionHeads)
291 |         rmsNormEps = try container.decode(
292 |             Float.self, forKey: Phi3Configuration.CodingKeys.rmsNormEps)
293 |         vocabularySize = try container.decode(
294 |             Int.self, forKey: Phi3Configuration.CodingKeys.vocabularySize)
295 |         kvHeads = try container.decode(Int.self, forKey: Phi3Configuration.CodingKeys.kvHeads)
296 |         ropeTheta =
297 |             try container.decodeIfPresent(
298 |                 Float.self, forKey: Phi3Configuration.CodingKeys.ropeTheta) ?? 10_000
299 |         ropeTraditional =
300 |             try container.decodeIfPresent(
301 |                 Bool.self, forKey: Phi3Configuration.CodingKeys.ropeTraditional) ?? false
302 |         ropeScaling = try container.decodeIfPresent(
303 |             RopeScalingWithFactorArrays.self, forKey: .ropeScaling)
304 |         partialRotaryFactor =
305 |             try container.decodeIfPresent(
306 |                 Float.self, forKey: .partialRotaryFactor) ?? 1.0
307 |         maxPositionEmbeddings = try container.decode(Int.self, forKey: .maxPositionEmbeddings)
308 |         originalMaxPositionEmbeddings = try container.decode(
309 |             Int.self, forKey: .originalMaxPositionEmbeddings)
310 |         tieWordEmbeddings =
311 |             try container.decodeIfPresent(
312 |                 Bool.self, forKey: .tieWordEmbeddings) ?? false
313 |     }
314 | }
315 | 
316 | // MARK: - LoRA
317 | 
318 | extension Phi3Model: LoRAModel {
319 |     public func loraLinearLayers() -> LoRALinearLayers {
320 |         model.layers.map { ($0.attention, ["qkv_proj"]) }
321 |     }
322 | }
323 | 


--------------------------------------------------------------------------------
/Libraries/MLXLLM/Models/PhiMoE.swift:
--------------------------------------------------------------------------------
  1 | import Foundation
  2 | import MLX
  3 | import MLXLMCommon
  4 | import MLXNN
  5 | 
  6 | // Port of https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/models/phimoe.py
  7 | 
  8 | public struct PhiMoEConfiguration: Codable, Sendable {
  9 |     var modelType: String = "phimoe"
 10 |     var vocabularySize: Int = 32064
 11 |     var hiddenSize: Int = 4096
 12 |     var intermediateSize: Int = 6400
 13 |     var hiddenLayers: Int = 32
 14 |     var attentionHeads: Int = 32
 15 |     var kvHeads: Int = 8
 16 |     var maxPositionEmbeddings: Int = 131072
 17 |     var originalMaxPositionEmbeddings: Int = 4096
 18 |     var rmsNormEps: Float = 1e-6
 19 |     var ropeScaling: RopeScalingWithFactorArrays?
 20 |     var numLocalExperts: Int = 16
 21 |     var numExpertsPerToken: Int = 2
 22 |     var ropeTheta: Float = 10000.0
 23 | 
 24 |     enum CodingKeys: String, CodingKey {
 25 |         case modelType = "model_type"
 26 |         case vocabularySize = "vocab_size"
 27 |         case hiddenSize = "hidden_size"
 28 |         case intermediateSize = "intermediate_size"
 29 |         case hiddenLayers = "num_hidden_layers"
 30 |         case attentionHeads = "num_attention_heads"
 31 |         case kvHeads = "num_key_value_heads"
 32 |         case maxPositionEmbeddings = "max_position_embeddings"
 33 |         case originalMaxPositionEmbeddings = "original_max_position_embeddings"
 34 |         case rmsNormEps = "rms_norm_eps"
 35 |         case ropeScaling = "rope_scaling"
 36 |         case numLocalExperts = "num_local_experts"
 37 |         case numExpertsPerToken = "num_experts_per_tok"
 38 |         case ropeTheta = "rope_theta"
 39 |     }
 40 | }
 41 | 
 42 | private class Attention: Module {
 43 |     let args: PhiMoEConfiguration
 44 |     let scale: Float
 45 | 
 46 |     @ModuleInfo(key: "q_proj") var wq: Linear
 47 |     @ModuleInfo(key: "k_proj") var wk: Linear
 48 |     @ModuleInfo(key: "v_proj") var wv: Linear
 49 |     @ModuleInfo(key: "o_proj") var wo: Linear
 50 | 
 51 |     let rope: SuScaledRotaryEmbedding
 52 | 
 53 |     init(_ args: PhiMoEConfiguration) {
 54 |         self.args = args
 55 | 
 56 |         let dim = args.hiddenSize
 57 |         let heads = args.attentionHeads
 58 |         let kvHeads = args.kvHeads
 59 | 
 60 |         let headDim = args.hiddenSize / heads
 61 |         self.scale = pow(Float(headDim), -0.5)
 62 | 
 63 |         self._wq.wrappedValue = Linear(dim, heads * headDim, bias: true)
 64 |         self._wk.wrappedValue = Linear(dim, kvHeads * headDim, bias: true)
 65 |         self._wv.wrappedValue = Linear(dim, kvHeads * headDim, bias: true)
 66 |         self._wo.wrappedValue = Linear(heads * headDim, dim, bias: true)
 67 | 
 68 |         self.rope = SuScaledRotaryEmbedding(
 69 |             dimensions: headDim,
 70 |             base: args.ropeTheta,
 71 |             maxPositionEmbeddings: args.maxPositionEmbeddings,
 72 |             originalMaxPositionEmbeddings: args.originalMaxPositionEmbeddings,
 73 |             longFactor: args.ropeScaling?.longFactor as? [Float] ?? [1.0],
 74 |             longMScale: args.ropeScaling?.longMScale as? Float
 75 |         )
 76 |     }
 77 | 
 78 |     func callAsFunction(
 79 |         _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
 80 |     ) -> MLXArray {
 81 |         let (B, L, _) = (x.dim(0), x.dim(1), x.dim(2))
 82 | 
 83 |         let queries = wq(x)
 84 |         let keys = wk(x)
 85 |         let values = wv(x)
 86 | 
 87 |         // Prepare the queries, keys and values for the attention computation
 88 |         var q = queries.reshaped(B, L, args.attentionHeads, -1).transposed(0, 2, 1, 3)
 89 |         var k = keys.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)
 90 |         var v = values.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)
 91 | 
 92 |         if let cache {
 93 |             q = rope(q, offset: cache.offset)
 94 |             k = rope(k, offset: cache.offset)
 95 |             (k, v) = cache.update(keys: k, values: v)
 96 |         } else {
 97 |             q = rope(q)
 98 |             k = rope(k)
 99 |         }
100 | 
101 |         let output = MLXFast.scaledDotProductAttention(
102 |             queries: q, keys: k, values: v, scale: scale, mask: mask
103 |         )
104 |         .transposed(0, 2, 1, 3)
105 |         .reshaped(B, L, -1)
106 | 
107 |         return wo(output)
108 |     }
109 | }
110 | 
111 | private class PhiMoESparseMoeBlock: Module {
112 |     let hiddenDim: Int
113 |     let ffnDim: Int
114 |     let numExperts: Int
115 |     let topK: Int
116 | 
117 |     @ModuleInfo(key: "gate") var gate: Linear
118 |     @ModuleInfo(key: "switch_mlp") var switchMLP: SwitchGLU
119 | 
120 |     init(_ args: PhiMoEConfiguration) {
121 |         self.hiddenDim = args.hiddenSize
122 |         self.ffnDim = args.intermediateSize
123 |         self.numExperts = args.numLocalExperts
124 |         self.topK = args.numExpertsPerToken
125 | 
126 |         self._gate.wrappedValue = Linear(hiddenDim, numExperts, bias: false)
127 |         self._switchMLP.wrappedValue = SwitchGLU(
128 |             inputDims: hiddenDim, hiddenDims: ffnDim, numExperts: numExperts)
129 |     }
130 | 
131 |     func callAsFunction(_ x: MLXArray) -> MLXArray {
132 |         let gates = gate(x)
133 | 
134 |         let k = self.topK
135 |         let inds = MLX.stopGradient(
136 |             MLX.argPartition(
137 |                 -gates,
138 |                 kth: k - 1,
139 |                 axis: -1
140 |             )[.ellipsis, ..<k])
141 |         let scores = MLX.softmax(MLX.takeAlong(gates, inds, axis: -1), axis: -1, precise: true)
142 | 
143 |         let y = switchMLP(x, inds)
144 |         return (y * scores[.ellipsis, .newAxis]).sum(axis: -2)
145 |     }
146 | }
147 | 
148 | private class PhiMoEDecoderLayer: Module {
149 |     let hiddenSize: Int
150 | 
151 |     @ModuleInfo(key: "self_attn") var selfAttn: Attention
152 |     @ModuleInfo(key: "block_sparse_moe") var blockSparseMoe: PhiMoESparseMoeBlock
153 |     @ModuleInfo(key: "input_layernorm") var inputLayerNorm: LayerNorm
154 |     @ModuleInfo(key: "post_attention_layernorm") var postAttentionLayerNorm: LayerNorm
155 | 
156 |     init(_ args: PhiMoEConfiguration) {
157 |         self.hiddenSize = args.hiddenSize
158 | 
159 |         self._selfAttn.wrappedValue = Attention(args)
160 |         self._blockSparseMoe.wrappedValue = PhiMoESparseMoeBlock(args)
161 |         self._inputLayerNorm.wrappedValue = LayerNorm(
162 |             dimensions: args.hiddenSize, eps: args.rmsNormEps)
163 |         self._postAttentionLayerNorm.wrappedValue = LayerNorm(
164 |             dimensions: args.hiddenSize, eps: args.rmsNormEps)
165 |     }
166 | 
167 |     func callAsFunction(
168 |         _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
169 |     ) -> MLXArray {
170 |         var residual = x
171 |         var hiddenStates = inputLayerNorm(x)
172 |         hiddenStates = selfAttn(hiddenStates, mask: mask, cache: cache)
173 |         hiddenStates = residual + hiddenStates
174 | 
175 |         residual = hiddenStates
176 |         hiddenStates = postAttentionLayerNorm(hiddenStates)
177 |         hiddenStates = blockSparseMoe(hiddenStates)
178 |         hiddenStates = residual + hiddenStates
179 | 
180 |         return hiddenStates
181 |     }
182 | }
183 | 
184 | private class PhiMoEModelInner: Module {
185 |     let args: PhiMoEConfiguration
186 | 
187 |     @ModuleInfo(key: "embed_tokens") var embedTokens: Embedding
188 |     let layers: [PhiMoEDecoderLayer]
189 |     @ModuleInfo(key: "norm") var norm: LayerNorm
190 | 
191 |     init(_ args: PhiMoEConfiguration) {
192 |         self.args = args
193 | 
194 |         self._embedTokens.wrappedValue = Embedding(
195 |             embeddingCount: args.vocabularySize, dimensions: args.hiddenSize)
196 |         self.layers = (0 ..< args.hiddenLayers).map { _ in PhiMoEDecoderLayer(args) }
197 |         self._norm.wrappedValue = LayerNorm(dimensions: args.hiddenSize, eps: args.rmsNormEps)
198 |     }
199 | 
200 |     func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {
201 |         var h = embedTokens(inputs)
202 | 
203 |         let mask = createAttentionMask(h: h, cache: cache)
204 | 
205 |         for (i, layer) in layers.enumerated() {
206 |             h = layer(h, mask: mask, cache: cache?[i])
207 |         }
208 | 
209 |         return norm(h)
210 |     }
211 | }
212 | 
213 | public class PhiMoEModel: Module, LLMModel, KVCacheDimensionProvider {
214 |     public let vocabularySize: Int
215 |     public let kvHeads: [Int]
216 | 
217 |     fileprivate let model: PhiMoEModelInner
218 |     @ModuleInfo(key: "lm_head") var lmHead: Linear
219 | 
220 |     public init(_ args: PhiMoEConfiguration) {
221 |         self.vocabularySize = args.vocabularySize
222 |         self.kvHeads = Array(repeating: args.kvHeads, count: args.hiddenLayers)
223 |         self.model = PhiMoEModelInner(args)
224 |         self._lmHead.wrappedValue = Linear(args.hiddenSize, args.vocabularySize, bias: true)
225 |     }
226 | 
227 |     public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {
228 |         let out = model(inputs, cache: cache)
229 |         return lmHead(out)
230 |     }
231 | 
232 |     public func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {
233 |         var sanitizedWeights = weights
234 |         if sanitizedWeights["model.layers.0.block_sparse_moe.experts.0.w1.weight"] == nil {
235 |             return sanitizedWeights
236 |         }
237 | 
238 |         for l in 0 ..< model.args.hiddenLayers {
239 |             let prefix = "model.layers.\(l)"
240 |             for (n, m) in [("w1", "gate_proj"), ("w2", "down_proj"), ("w3", "up_proj")] {
241 |                 for k in ["weight", "scales", "biases"] {
242 |                     if sanitizedWeights["\(prefix).block_sparse_moe.experts.0.\(n).\(k)"] != nil {
243 |                         let toJoin = (0 ..< model.args.numLocalExperts).map { e in
244 |                             sanitizedWeights.removeValue(
245 |                                 forKey: "\(prefix).block_sparse_moe.experts.\(e).\(n).\(k)")!
246 |                         }
247 |                         sanitizedWeights["\(prefix).block_sparse_moe.switch_mlp.\(m).\(k)"] =
248 |                             MLX.stacked(toJoin)
249 |                     }
250 |                 }
251 |             }
252 |         }
253 | 
254 |         return sanitizedWeights
255 |     }
256 | }
257 | 
258 | // MARK: - LoRA
259 | 
260 | extension PhiMoEModel: LoRAModel {
261 |     public func loraLinearLayers() -> LoRALinearLayers {
262 |         model.layers.map { ($0.selfAttn, ["q_proj", "v_proj"]) }
263 |     }
264 | }
265 | 


--------------------------------------------------------------------------------
/Libraries/MLXLLM/Models/Qwen2.swift:
--------------------------------------------------------------------------------
  1 | //
  2 | //  Qwen2.swift
  3 | //  LLM
  4 | //
  5 | //  Created by John Mai on 2024/3/3.
  6 | //
  7 | 
  8 | import Foundation
  9 | import MLX
 10 | import MLXLMCommon
 11 | import MLXNN
 12 | 
 13 | // port of https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/models/qwen2.py
 14 | 
 15 | private class Attention: Module {
 16 |     let args: Qwen2Configuration
 17 |     let scale: Float
 18 | 
 19 |     @ModuleInfo(key: "q_proj") var wq: Linear
 20 |     @ModuleInfo(key: "k_proj") var wk: Linear
 21 |     @ModuleInfo(key: "v_proj") var wv: Linear
 22 |     @ModuleInfo(key: "o_proj") var wo: Linear
 23 | 
 24 |     let rope: RoPE
 25 | 
 26 |     public init(_ args: Qwen2Configuration) {
 27 |         self.args = args
 28 | 
 29 |         let dim = args.hiddenSize
 30 |         let heads = args.attentionHeads
 31 |         let kvHeads = args.kvHeads
 32 | 
 33 |         let headDim = args.hiddenSize / heads
 34 |         self.scale = pow(Float(headDim), -0.5)
 35 | 
 36 |         _wq.wrappedValue = Linear(dim, heads * headDim, bias: true)
 37 |         _wk.wrappedValue = Linear(dim, kvHeads * headDim, bias: true)
 38 |         _wv.wrappedValue = Linear(dim, kvHeads * headDim, bias: true)
 39 |         _wo.wrappedValue = Linear(heads * headDim, dim, bias: false)
 40 | 
 41 |         let ropeScale: Float
 42 |         if let ropeScaling = args.ropeScaling, ropeScaling["type"] == .string("linear"),
 43 |             let factor = ropeScaling["factor"]
 44 |         {
 45 |             if let v = factor.asFloat() {
 46 |                 ropeScale = 1 / v
 47 |             } else {
 48 |                 fatalError("ropeScaling.factor must be a float")
 49 |             }
 50 |         } else {
 51 |             ropeScale = 1
 52 |         }
 53 | 
 54 |         self.rope = RoPE(
 55 |             dimensions: headDim, traditional: args.ropeTraditional, base: args.ropeTheta,
 56 |             scale: ropeScale)
 57 |     }
 58 | 
 59 |     public func callAsFunction(
 60 |         _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
 61 |     ) -> MLXArray {
 62 |         let (B, L) = (x.dim(0), x.dim(1))
 63 | 
 64 |         var queries = wq(x)
 65 |         var keys = wk(x)
 66 |         var values = wv(x)
 67 | 
 68 |         // prepare the queries, keys and values for the attention computation
 69 |         queries = queries.reshaped(B, L, args.attentionHeads, -1).transposed(0, 2, 1, 3)
 70 |         keys = keys.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)
 71 |         values = values.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)
 72 | 
 73 |         if let cache {
 74 |             queries = rope(queries, offset: cache.offset)
 75 |             keys = rope(keys, offset: cache.offset)
 76 |             (keys, values) = cache.update(keys: keys, values: values)
 77 |         } else {
 78 |             queries = rope(queries)
 79 |             keys = rope(keys)
 80 |         }
 81 | 
 82 |         let output = MLXFast.scaledDotProductAttention(
 83 |             queries: queries, keys: keys, values: values, scale: scale, mask: mask
 84 |         )
 85 |         .transposed(0, 2, 1, 3)
 86 |         .reshaped(B, L, -1)
 87 | 
 88 |         return wo(output)
 89 |     }
 90 | }
 91 | 
 92 | private class MLP: Module, UnaryLayer {
 93 |     @ModuleInfo(key: "gate_proj") var gate: Linear
 94 |     @ModuleInfo(key: "down_proj") var down: Linear
 95 |     @ModuleInfo(key: "up_proj") var up: Linear
 96 | 
 97 |     public init(dimensions: Int, hiddenDimensions: Int) {
 98 |         _gate.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)
 99 |         _down.wrappedValue = Linear(hiddenDimensions, dimensions, bias: false)
100 |         _up.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)
101 |     }
102 | 
103 |     public func callAsFunction(_ x: MLXArray) -> MLXArray {
104 |         down(silu(gate(x)) * up(x))
105 |     }
106 | }
107 | 
108 | private class TransformerBlock: Module {
109 |     @ModuleInfo(key: "self_attn") var attention: Attention
110 |     let mlp: MLP
111 | 
112 |     @ModuleInfo(key: "input_layernorm") var inputLayerNorm: RMSNorm
113 |     @ModuleInfo(key: "post_attention_layernorm") var postAttentionLayerNorm: RMSNorm
114 | 
115 |     public init(_ args: Qwen2Configuration) {
116 |         _attention.wrappedValue = Attention(args)
117 |         self.mlp = MLP(dimensions: args.hiddenSize, hiddenDimensions: args.intermediateSize)
118 |         _inputLayerNorm.wrappedValue = RMSNorm(
119 |             dimensions: args.hiddenSize, eps: args.rmsNormEps)
120 |         _postAttentionLayerNorm.wrappedValue = RMSNorm(
121 |             dimensions: args.hiddenSize, eps: args.rmsNormEps)
122 |     }
123 | 
124 |     public func callAsFunction(
125 |         _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
126 |     ) -> MLXArray {
127 |         var r = attention(inputLayerNorm(x), mask: mask, cache: cache)
128 |         let h = x + r
129 |         r = mlp(postAttentionLayerNorm(h))
130 |         let out = h + r
131 |         return out
132 |     }
133 | }
134 | 
135 | private class Qwen2ModelInner: Module {
136 |     @ModuleInfo(key: "embed_tokens") var embedTokens: Embedding
137 | 
138 |     fileprivate let layers: [TransformerBlock]
139 |     let norm: RMSNorm
140 | 
141 |     public init(_ args: Qwen2Configuration) {
142 |         precondition(args.vocabularySize > 0)
143 | 
144 |         _embedTokens.wrappedValue = Embedding(
145 |             embeddingCount: args.vocabularySize, dimensions: args.hiddenSize)
146 | 
147 |         self.layers = (0 ..< args.hiddenLayers)
148 |             .map { _ in
149 |                 TransformerBlock(args)
150 |             }
151 |         self.norm = RMSNorm(dimensions: args.hiddenSize, eps: args.rmsNormEps)
152 |     }
153 | 
154 |     public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]? = nil) -> MLXArray {
155 |         var h = embedTokens(inputs)
156 | 
157 |         let mask = createAttentionMask(h: h, cache: cache)
158 | 
159 |         for (i, layer) in layers.enumerated() {
160 |             h = layer(h, mask: mask, cache: cache?[i])
161 |         }
162 | 
163 |         return norm(h)
164 |     }
165 | }
166 | 
167 | public class Qwen2Model: Module, LLMModel, KVCacheDimensionProvider {
168 |     public let vocabularySize: Int
169 |     public let kvHeads: [Int]
170 | 
171 |     private let model: Qwen2ModelInner
172 |     let configuration: Qwen2Configuration
173 | 
174 |     @ModuleInfo(key: "lm_head") var lmHead: Linear?
175 | 
176 |     public init(_ args: Qwen2Configuration) {
177 |         self.configuration = args
178 |         self.vocabularySize = args.vocabularySize
179 |         self.kvHeads = (0 ..< args.hiddenLayers).map { _ in args.kvHeads }
180 |         self.model = Qwen2ModelInner(args)
181 | 
182 |         if !args.tieWordEmbeddings {
183 |             _lmHead.wrappedValue = Linear(args.hiddenSize, args.vocabularySize, bias: false)
184 |         }
185 |     }
186 | 
187 |     public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {
188 |         var out = model(inputs, cache: cache)
189 |         if let lmHead {
190 |             out = lmHead(out)
191 |         } else {
192 |             out = model.embedTokens.asLinear(out)
193 |         }
194 |         return out
195 |     }
196 | 
197 |     public func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {
198 |         var weights = weights
199 | 
200 |         if configuration.tieWordEmbeddings {
201 |             weights["lm_head.weight"] = nil
202 |         }
203 | 
204 |         // Remove unused precomputed rotary freqs
205 |         return weights.filter {
206 |             !$0.key.contains("self_attn.rotary_emb.inv_freq")
207 |         }
208 |     }
209 | }
210 | 
211 | public struct Qwen2Configuration: Codable, Sendable {
212 |     var hiddenSize: Int
213 |     var hiddenLayers: Int
214 |     var intermediateSize: Int
215 |     var attentionHeads: Int
216 |     var rmsNormEps: Float
217 |     var vocabularySize: Int
218 |     var kvHeads: Int
219 |     var ropeTheta: Float = 1_000_000
220 |     var ropeTraditional: Bool = false
221 |     var ropeScaling: [String: StringOrNumber]? = nil
222 |     var tieWordEmbeddings = false
223 | 
224 |     enum CodingKeys: String, CodingKey {
225 |         case hiddenSize = "hidden_size"
226 |         case hiddenLayers = "num_hidden_layers"
227 |         case intermediateSize = "intermediate_size"
228 |         case attentionHeads = "num_attention_heads"
229 |         case rmsNormEps = "rms_norm_eps"
230 |         case vocabularySize = "vocab_size"
231 |         case kvHeads = "num_key_value_heads"
232 |         case ropeTheta = "rope_theta"
233 |         case ropeTraditional = "rope_traditional"
234 |         case ropeScaling = "rope_scaling"
235 |         case tieWordEmbeddings = "tie_word_embeddings"
236 |     }
237 | 
238 |     public init(from decoder: Decoder) throws {
239 |         // custom implementation to handle optional keys with required values
240 |         let container: KeyedDecodingContainer<Qwen2Configuration.CodingKeys> =
241 |             try decoder.container(
242 |                 keyedBy: Qwen2Configuration.CodingKeys.self)
243 | 
244 |         self.hiddenSize = try container.decode(
245 |             Int.self, forKey: Qwen2Configuration.CodingKeys.hiddenSize)
246 |         self.hiddenLayers = try container.decode(
247 |             Int.self, forKey: Qwen2Configuration.CodingKeys.hiddenLayers)
248 |         self.intermediateSize = try container.decode(
249 |             Int.self, forKey: Qwen2Configuration.CodingKeys.intermediateSize)
250 |         self.attentionHeads = try container.decode(
251 |             Int.self, forKey: Qwen2Configuration.CodingKeys.attentionHeads)
252 |         self.rmsNormEps = try container.decode(
253 |             Float.self, forKey: Qwen2Configuration.CodingKeys.rmsNormEps)
254 |         self.vocabularySize = try container.decode(
255 |             Int.self, forKey: Qwen2Configuration.CodingKeys.vocabularySize)
256 |         self.kvHeads = try container.decode(Int.self, forKey: Qwen2Configuration.CodingKeys.kvHeads)
257 |         self.ropeTheta =
258 |             try container.decodeIfPresent(
259 |                 Float.self, forKey: Qwen2Configuration.CodingKeys.ropeTheta)
260 |             ?? 1_000_000
261 |         self.ropeTraditional =
262 |             try container.decodeIfPresent(
263 |                 Bool.self, forKey: Qwen2Configuration.CodingKeys.ropeTraditional) ?? false
264 |         self.ropeScaling = try container.decodeIfPresent(
265 |             [String: StringOrNumber].self, forKey: Qwen2Configuration.CodingKeys.ropeScaling)
266 |         self.tieWordEmbeddings =
267 |             try container.decodeIfPresent(Bool.self, forKey: .tieWordEmbeddings) ?? false
268 |     }
269 | }
270 | 
271 | // MARK: - LoRA
272 | 
273 | extension Qwen2Model: LoRAModel {
274 |     public func loraLinearLayers() -> LoRALinearLayers {
275 |         model.layers.map { ($0.attention, ["q_proj", "v_proj"]) }
276 |     }
277 | }
278 | 


--------------------------------------------------------------------------------
/Libraries/MLXLLM/Models/Qwen3.swift:
--------------------------------------------------------------------------------
  1 | //
  2 | //  Qwen3.swift
  3 | //  LLM
  4 | //
  5 | //  Created by John Mai on 2025/4/28.
  6 | //
  7 | 
  8 | import Foundation
  9 | import MLX
 10 | import MLXLMCommon
 11 | import MLXNN
 12 | 
 13 | // port of https://github.com/ml-explore/mlx-lm/blob/main/mlx_lm/models/qwen3.py
 14 | 
 15 | private class Attention: Module {
 16 |     let args: Qwen3Configuration
 17 |     let scale: Float
 18 | 
 19 |     @ModuleInfo(key: "q_proj") var wq: Linear
 20 |     @ModuleInfo(key: "k_proj") var wk: Linear
 21 |     @ModuleInfo(key: "v_proj") var wv: Linear
 22 |     @ModuleInfo(key: "o_proj") var wo: Linear
 23 | 
 24 |     @ModuleInfo(key: "q_norm") var qNorm: RMSNorm
 25 |     @ModuleInfo(key: "k_norm") var kNorm: RMSNorm
 26 | 
 27 |     let rope: RoPE
 28 | 
 29 |     public init(_ args: Qwen3Configuration) {
 30 |         self.args = args
 31 | 
 32 |         let dim = args.hiddenSize
 33 |         let heads = args.attentionHeads
 34 |         let kvHeads = args.kvHeads
 35 | 
 36 |         let headDim = args.headDim
 37 |         self.scale = pow(Float(headDim), -0.5)
 38 | 
 39 |         _wq.wrappedValue = Linear(dim, heads * headDim, bias: false)
 40 |         _wk.wrappedValue = Linear(dim, kvHeads * headDim, bias: false)
 41 |         _wv.wrappedValue = Linear(dim, kvHeads * headDim, bias: false)
 42 |         _wo.wrappedValue = Linear(heads * headDim, dim, bias: false)
 43 | 
 44 |         _qNorm.wrappedValue = RMSNorm(dimensions: headDim, eps: args.rmsNormEps)
 45 |         _kNorm.wrappedValue = RMSNorm(dimensions: headDim, eps: args.rmsNormEps)
 46 | 
 47 |         let ropeScale: Float
 48 |         if let ropeScaling = args.ropeScaling, ropeScaling["type"] == .string("linear"),
 49 |             let factor = ropeScaling["factor"]
 50 |         {
 51 |             if let v = factor.asFloat() {
 52 |                 ropeScale = 1 / v
 53 |             } else {
 54 |                 fatalError("ropeScaling.factor must be a float")
 55 |             }
 56 |         } else {
 57 |             ropeScale = 1
 58 |         }
 59 | 
 60 |         self.rope = RoPE(
 61 |             dimensions: headDim, traditional: false, base: args.ropeTheta,
 62 |             scale: ropeScale)
 63 |     }
 64 | 
 65 |     public func callAsFunction(
 66 |         _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
 67 |     ) -> MLXArray {
 68 |         let (B, L) = (x.dim(0), x.dim(1))
 69 | 
 70 |         var queries = wq(x)
 71 |         var keys = wk(x)
 72 |         var values = wv(x)
 73 | 
 74 |         // prepare the queries, keys and values for the attention computation
 75 |         queries = qNorm(queries.reshaped(B, L, args.attentionHeads, -1)).transposed(0, 2, 1, 3)
 76 |         keys = kNorm(keys.reshaped(B, L, args.kvHeads, -1)).transposed(0, 2, 1, 3)
 77 |         values = values.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)
 78 | 
 79 |         // Apply RoPE positioning
 80 |         if let cache {
 81 |             queries = rope(queries, offset: cache.offset)
 82 |             keys = rope(keys, offset: cache.offset)
 83 |         } else {
 84 |             queries = rope(queries)
 85 |             keys = rope(keys)
 86 |         }
 87 | 
 88 |         // Use the automatic attention router that handles both quantized and regular caches
 89 |         let output = attentionWithCacheUpdate(
 90 |             queries: queries,
 91 |             keys: keys,
 92 |             values: values,
 93 |             cache: cache,
 94 |             scale: scale,
 95 |             mask: mask
 96 |         )
 97 |         .transposed(0, 2, 1, 3)
 98 |         .reshaped(B, L, -1)
 99 | 
100 |         return wo(output)
101 |     }
102 | }
103 | 
104 | private class MLP: Module, UnaryLayer {
105 |     @ModuleInfo(key: "gate_proj") var gate: Linear
106 |     @ModuleInfo(key: "down_proj") var down: Linear
107 |     @ModuleInfo(key: "up_proj") var up: Linear
108 | 
109 |     public init(dimensions: Int, hiddenDimensions: Int) {
110 |         _gate.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)
111 |         _down.wrappedValue = Linear(hiddenDimensions, dimensions, bias: false)
112 |         _up.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)
113 |     }
114 | 
115 |     public func callAsFunction(_ x: MLXArray) -> MLXArray {
116 |         down(silu(gate(x)) * up(x))
117 |     }
118 | }
119 | 
120 | private class TransformerBlock: Module {
121 |     @ModuleInfo(key: "self_attn") var attention: Attention
122 |     let mlp: MLP
123 | 
124 |     @ModuleInfo(key: "input_layernorm") var inputLayerNorm: RMSNorm
125 |     @ModuleInfo(key: "post_attention_layernorm") var postAttentionLayerNorm: RMSNorm
126 | 
127 |     public init(_ args: Qwen3Configuration) {
128 |         _attention.wrappedValue = Attention(args)
129 |         self.mlp = MLP(dimensions: args.hiddenSize, hiddenDimensions: args.intermediateSize)
130 |         _inputLayerNorm.wrappedValue = RMSNorm(
131 |             dimensions: args.hiddenSize, eps: args.rmsNormEps)
132 |         _postAttentionLayerNorm.wrappedValue = RMSNorm(
133 |             dimensions: args.hiddenSize, eps: args.rmsNormEps)
134 |     }
135 | 
136 |     public func callAsFunction(
137 |         _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
138 |     ) -> MLXArray {
139 |         var r = attention(inputLayerNorm(x), mask: mask, cache: cache)
140 |         let h = x + r
141 |         r = mlp(postAttentionLayerNorm(h))
142 |         let out = h + r
143 |         return out
144 |     }
145 | }
146 | 
147 | private class Qwen3ModelInner: Module {
148 |     @ModuleInfo(key: "embed_tokens") var embedTokens: Embedding
149 | 
150 |     fileprivate let layers: [TransformerBlock]
151 |     let norm: RMSNorm
152 | 
153 |     public init(_ args: Qwen3Configuration) {
154 |         precondition(args.vocabularySize > 0)
155 | 
156 |         _embedTokens.wrappedValue = Embedding(
157 |             embeddingCount: args.vocabularySize, dimensions: args.hiddenSize)
158 | 
159 |         self.layers = (0 ..< args.hiddenLayers)
160 |             .map { _ in
161 |                 TransformerBlock(args)
162 |             }
163 |         self.norm = RMSNorm(dimensions: args.hiddenSize, eps: args.rmsNormEps)
164 |     }
165 | 
166 |     public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]? = nil) -> MLXArray {
167 |         var h = embedTokens(inputs)
168 | 
169 |         let mask = createAttentionMask(h: h, cache: cache)
170 | 
171 |         for (i, layer) in layers.enumerated() {
172 |             h = layer(h, mask: mask, cache: cache?[i])
173 |         }
174 | 
175 |         return norm(h)
176 |     }
177 | }
178 | 
179 | public class Qwen3Model: Module, LLMModel, KVCacheDimensionProvider {
180 |     public let vocabularySize: Int
181 |     public let kvHeads: [Int]
182 | 
183 |     private let model: Qwen3ModelInner
184 |     let configuration: Qwen3Configuration
185 | 
186 |     @ModuleInfo(key: "lm_head") var lmHead: Linear?
187 | 
188 |     public init(_ args: Qwen3Configuration) {
189 |         self.configuration = args
190 |         self.vocabularySize = args.vocabularySize
191 |         self.kvHeads = (0 ..< args.hiddenLayers).map { _ in args.kvHeads }
192 |         self.model = Qwen3ModelInner(args)
193 | 
194 |         if !args.tieWordEmbeddings {
195 |             _lmHead.wrappedValue = Linear(args.hiddenSize, args.vocabularySize, bias: false)
196 |         }
197 |     }
198 | 
199 |     public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {
200 |         var out = model(inputs, cache: cache)
201 |         if let lmHead {
202 |             out = lmHead(out)
203 |         } else {
204 |             out = model.embedTokens.asLinear(out)
205 |         }
206 |         return out
207 |     }
208 | 
209 |     public func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {
210 |         var weights = weights
211 | 
212 |         if configuration.tieWordEmbeddings {
213 |             weights["lm_head.weight"] = nil
214 |         }
215 | 
216 |         return weights
217 |     }
218 | }
219 | 
220 | public struct Qwen3Configuration: Codable, Sendable {
221 |     var hiddenSize: Int
222 |     var hiddenLayers: Int
223 |     var intermediateSize: Int
224 |     var attentionHeads: Int
225 |     var rmsNormEps: Float
226 |     var vocabularySize: Int
227 |     var kvHeads: Int
228 |     var ropeTheta: Float = 1_000_000
229 |     var headDim: Int
230 |     var ropeScaling: [String: StringOrNumber]? = nil
231 |     var tieWordEmbeddings = false
232 |     var maxPositionEmbeddings: Int = 32768
233 | 
234 |     enum CodingKeys: String, CodingKey {
235 |         case hiddenSize = "hidden_size"
236 |         case hiddenLayers = "num_hidden_layers"
237 |         case intermediateSize = "intermediate_size"
238 |         case attentionHeads = "num_attention_heads"
239 |         case rmsNormEps = "rms_norm_eps"
240 |         case vocabularySize = "vocab_size"
241 |         case kvHeads = "num_key_value_heads"
242 |         case ropeTheta = "rope_theta"
243 |         case headDim = "head_dim"
244 |         case ropeScaling = "rope_scaling"
245 |         case tieWordEmbeddings = "tie_word_embeddings"
246 |         case maxPositionEmbeddings = "max_position_embeddings"
247 |     }
248 | 
249 |     public init(from decoder: Decoder) throws {
250 |         // custom implementation to handle optional keys with required values
251 |         let container: KeyedDecodingContainer<Qwen3Configuration.CodingKeys> =
252 |             try decoder.container(
253 |                 keyedBy: Qwen3Configuration.CodingKeys.self)
254 | 
255 |         self.hiddenSize = try container.decode(
256 |             Int.self, forKey: Qwen3Configuration.CodingKeys.hiddenSize)
257 |         self.hiddenLayers = try container.decode(
258 |             Int.self, forKey: Qwen3Configuration.CodingKeys.hiddenLayers)
259 |         self.intermediateSize = try container.decode(
260 |             Int.self, forKey: Qwen3Configuration.CodingKeys.intermediateSize)
261 |         self.attentionHeads = try container.decode(
262 |             Int.self, forKey: Qwen3Configuration.CodingKeys.attentionHeads)
263 |         self.rmsNormEps = try container.decode(
264 |             Float.self, forKey: Qwen3Configuration.CodingKeys.rmsNormEps)
265 |         self.vocabularySize = try container.decode(
266 |             Int.self, forKey: Qwen3Configuration.CodingKeys.vocabularySize)
267 |         self.kvHeads = try container.decode(Int.self, forKey: Qwen3Configuration.CodingKeys.kvHeads)
268 |         self.ropeTheta =
269 |             try container.decodeIfPresent(
270 |                 Float.self, forKey: Qwen3Configuration.CodingKeys.ropeTheta)
271 |             ?? 1_000_000
272 |         self.headDim = try container.decode(
273 |             Int.self, forKey: Qwen3Configuration.CodingKeys.headDim)
274 |         self.ropeScaling = try container.decodeIfPresent(
275 |             [String: StringOrNumber].self, forKey: Qwen3Configuration.CodingKeys.ropeScaling)
276 |         self.tieWordEmbeddings =
277 |             try container.decodeIfPresent(Bool.self, forKey: .tieWordEmbeddings) ?? false
278 |         self.maxPositionEmbeddings =
279 |             try container.decodeIfPresent(Int.self, forKey: .maxPositionEmbeddings) ?? 32768
280 |     }
281 | }
282 | 
283 | // MARK: - LoRA
284 | 
285 | extension Qwen3Model: LoRAModel {
286 |     public func loraLinearLayers() -> LoRALinearLayers {
287 |         model.layers.map { ($0.attention, ["q_proj", "v_proj"]) }
288 |     }
289 | }
290 | 


--------------------------------------------------------------------------------
/Libraries/MLXLLM/Models/Qwen3MoE.swift:
--------------------------------------------------------------------------------
  1 | //
  2 | //  Qwen3MoE.swift
  3 | //  LLM
  4 | //
  5 | //  Created by John Mai on 2025/4/30.
  6 | //
  7 | 
  8 | import Foundation
  9 | import MLX
 10 | import MLXLMCommon
 11 | import MLXNN
 12 | 
 13 | // port of https://github.com/ml-explore/mlx-lm/blob/main/mlx_lm/models/qwen3_moe.py
 14 | 
 15 | private class Attention: Module {
 16 |     let args: Qwen3MoEConfiguration
 17 |     let scale: Float
 18 | 
 19 |     @ModuleInfo(key: "q_proj") var wq: Linear
 20 |     @ModuleInfo(key: "k_proj") var wk: Linear
 21 |     @ModuleInfo(key: "v_proj") var wv: Linear
 22 |     @ModuleInfo(key: "o_proj") var wo: Linear
 23 | 
 24 |     @ModuleInfo(key: "q_norm") var qNorm: RMSNorm
 25 |     @ModuleInfo(key: "k_norm") var kNorm: RMSNorm
 26 | 
 27 |     let rope: RoPE
 28 | 
 29 |     public init(_ args: Qwen3MoEConfiguration, layerIdx: Int) {
 30 |         self.args = args
 31 | 
 32 |         let dim = args.hiddenSize
 33 |         let heads = args.attentionHeads
 34 |         let kvHeads = args.kvHeads
 35 | 
 36 |         let headDim = args.headDim
 37 |         self.scale = pow(Float(headDim), -0.5)
 38 | 
 39 |         _wq.wrappedValue = Linear(dim, heads * headDim, bias: false)
 40 |         _wk.wrappedValue = Linear(dim, kvHeads * headDim, bias: false)
 41 |         _wv.wrappedValue = Linear(dim, kvHeads * headDim, bias: false)
 42 |         _wo.wrappedValue = Linear(heads * headDim, dim, bias: false)
 43 | 
 44 |         _qNorm.wrappedValue = RMSNorm(dimensions: headDim, eps: args.rmsNormEps)
 45 |         _kNorm.wrappedValue = RMSNorm(dimensions: headDim, eps: args.rmsNormEps)
 46 | 
 47 |         let ropeScale: Float
 48 |         if let ropeScaling = args.ropeScaling, ropeScaling["type"] == .string("linear"),
 49 |             let factor = ropeScaling["factor"]
 50 |         {
 51 |             if let v = factor.asFloat() {
 52 |                 ropeScale = 1 / v
 53 |             } else {
 54 |                 fatalError("ropeScaling.factor must be a float")
 55 |             }
 56 |         } else {
 57 |             ropeScale = 1
 58 |         }
 59 | 
 60 |         self.rope = RoPE(
 61 |             dimensions: headDim, traditional: false, base: args.ropeTheta,
 62 |             scale: ropeScale)
 63 |     }
 64 | 
 65 |     public func callAsFunction(
 66 |         _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
 67 |     ) -> MLXArray {
 68 |         let (B, L) = (x.dim(0), x.dim(1))
 69 | 
 70 |         var queries = wq(x)
 71 |         var keys = wk(x)
 72 |         var values = wv(x)
 73 | 
 74 |         // prepare the queries, keys and values for the attention computation
 75 |         queries = qNorm(queries.reshaped(B, L, args.attentionHeads, -1)).transposed(0, 2, 1, 3)
 76 |         keys = kNorm(keys.reshaped(B, L, args.kvHeads, -1)).transposed(0, 2, 1, 3)
 77 |         values = values.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)
 78 | 
 79 |         if let cache {
 80 |             queries = rope(queries, offset: cache.offset)
 81 |             keys = rope(keys, offset: cache.offset)
 82 |             (keys, values) = cache.update(keys: keys, values: values)
 83 |         } else {
 84 |             queries = rope(queries)
 85 |             keys = rope(keys)
 86 |         }
 87 | 
 88 |         let output = MLXFast.scaledDotProductAttention(
 89 |             queries: queries, keys: keys, values: values, scale: scale, mask: mask
 90 |         )
 91 |         .transposed(0, 2, 1, 3)
 92 |         .reshaped(B, L, -1)
 93 | 
 94 |         return wo(output)
 95 |     }
 96 | }
 97 | 
 98 | private class MLP: Module, UnaryLayer {
 99 |     @ModuleInfo(key: "gate_proj") var gate: Linear
100 |     @ModuleInfo(key: "down_proj") var down: Linear
101 |     @ModuleInfo(key: "up_proj") var up: Linear
102 | 
103 |     public init(dimensions: Int, hiddenDimensions: Int) {
104 |         _gate.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)
105 |         _down.wrappedValue = Linear(hiddenDimensions, dimensions, bias: false)
106 |         _up.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)
107 |     }
108 | 
109 |     public func callAsFunction(_ x: MLXArray) -> MLXArray {
110 |         down(silu(gate(x)) * up(x))
111 |     }
112 | }
113 | 
114 | private class Qwen3MoESparseMoeBlock: Module, UnaryLayer {
115 |     let numExperts: Int
116 |     let topK: Int
117 |     let normTopkProb: Bool
118 | 
119 |     @ModuleInfo(key: "gate") var gate: Linear
120 |     @ModuleInfo(key: "switch_mlp") var switchMLP: SwitchGLU
121 | 
122 |     init(_ args: Qwen3MoEConfiguration) {
123 |         self.numExperts = args.numExperts
124 |         self.topK = args.numExpertsPerToken
125 |         self.normTopkProb = args.normTopkProb
126 | 
127 |         _gate.wrappedValue = Linear(args.hiddenSize, numExperts, bias: false)
128 |         _switchMLP.wrappedValue = SwitchGLU(
129 |             inputDims: args.hiddenSize, hiddenDims: args.moeIntermediateSize, numExperts: numExperts
130 |         )
131 |     }
132 | 
133 |     func callAsFunction(_ x: MLXArray) -> MLXArray {
134 |         let gates = gate(x)
135 |         let softGates = MLX.softmax(gates, axis: -1, precise: true)
136 | 
137 |         let k = topK
138 |         let inds = MLX.argPartition(-gates, kth: k - 1, axis: -1)[.ellipsis, ..<k]
139 |         var scores = MLX.takeAlong(softGates, inds, axis: -1)
140 | 
141 |         if normTopkProb {
142 |             scores = scores / MLX.sum(scores, axis: -1, keepDims: true)
143 |         }
144 | 
145 |         let y = switchMLP(x, inds)
146 |         return (y * scores[.ellipsis, .newAxis]).sum(axis: -2)
147 |     }
148 | }
149 | 
150 | private class Qwen3MoeDecoderLayer: Module {
151 |     let args: Qwen3MoEConfiguration
152 |     let layerIdx: Int
153 | 
154 |     @ModuleInfo(key: "self_attn") var selfAttn: Attention
155 |     @ModuleInfo(key: "input_layernorm") var inputLayerNorm: RMSNorm
156 |     @ModuleInfo(key: "post_attention_layernorm") var postAttentionLayerNorm: RMSNorm
157 | 
158 |     fileprivate let mlp: UnaryLayer
159 | 
160 |     init(_ args: Qwen3MoEConfiguration, layerIdx: Int) {
161 |         self.args = args
162 |         self.layerIdx = layerIdx
163 | 
164 |         _selfAttn.wrappedValue = Attention(args, layerIdx: layerIdx)
165 |         _inputLayerNorm.wrappedValue = RMSNorm(dimensions: args.hiddenSize, eps: args.rmsNormEps)
166 |         _postAttentionLayerNorm.wrappedValue = RMSNorm(
167 |             dimensions: args.hiddenSize, eps: args.rmsNormEps)
168 | 
169 |         if !args.mlpOnlyLayers.contains(layerIdx),
170 |             args.numExperts > 0, (layerIdx + 1) % args.decoderSparseStep == 0
171 |         {
172 |             self.mlp = Qwen3MoESparseMoeBlock(args)
173 |         } else {
174 |             self.mlp = MLP(dimensions: args.hiddenSize, hiddenDimensions: args.intermediateSize)
175 |         }
176 |     }
177 | 
178 |     func callAsFunction(
179 |         _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
180 |     ) -> MLXArray {
181 |         var r = selfAttn(inputLayerNorm(x), mask: mask, cache: cache)
182 |         let h = x + r
183 |         r = mlp(postAttentionLayerNorm(h))
184 |         let out = h + r
185 |         return out
186 |     }
187 | }
188 | 
189 | private class Qwen3MoEModelInner: Module {
190 |     @ModuleInfo(key: "embed_tokens") var embedTokens: Embedding
191 | 
192 |     fileprivate let layers: [Qwen3MoeDecoderLayer]
193 |     let norm: RMSNorm
194 |     let args: Qwen3MoEConfiguration
195 | 
196 |     init(_ args: Qwen3MoEConfiguration) {
197 |         self.args = args
198 |         precondition(args.vocabularySize > 0)
199 | 
200 |         _embedTokens.wrappedValue = Embedding(
201 |             embeddingCount: args.vocabularySize, dimensions: args.hiddenSize)
202 | 
203 |         self.layers = (0 ..< args.hiddenLayers)
204 |             .map { i in
205 |                 Qwen3MoeDecoderLayer(args, layerIdx: i)
206 |             }
207 |         self.norm = RMSNorm(dimensions: args.hiddenSize, eps: args.rmsNormEps)
208 |     }
209 | 
210 |     func callAsFunction(_ inputs: MLXArray, cache: [KVCache]? = nil) -> MLXArray {
211 |         var h = embedTokens(inputs)
212 | 
213 |         let mask = createAttentionMask(h: h, cache: cache)
214 | 
215 |         for (i, layer) in layers.enumerated() {
216 |             h = layer(h, mask: mask, cache: cache?[i])
217 |         }
218 | 
219 |         return norm(h)
220 |     }
221 | }
222 | 
223 | public class Qwen3MoEModel: Module, LLMModel, KVCacheDimensionProvider {
224 |     public let vocabularySize: Int
225 |     public let kvHeads: [Int]
226 | 
227 |     fileprivate let model: Qwen3MoEModelInner
228 |     let configuration: Qwen3MoEConfiguration
229 | 
230 |     @ModuleInfo(key: "lm_head") var lmHead: Linear?
231 | 
232 |     public init(_ args: Qwen3MoEConfiguration) {
233 |         self.configuration = args
234 |         self.vocabularySize = args.vocabularySize
235 |         self.kvHeads = (0 ..< args.hiddenLayers).map { _ in args.kvHeads }
236 |         self.model = Qwen3MoEModelInner(args)
237 | 
238 |         if !args.tieWordEmbeddings {
239 |             _lmHead.wrappedValue = Linear(args.hiddenSize, args.vocabularySize, bias: false)
240 |         }
241 |     }
242 | 
243 |     public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {
244 |         var out = model(inputs, cache: cache)
245 |         if let lmHead {
246 |             out = lmHead(out)
247 |         } else {
248 |             out = model.embedTokens.asLinear(out)
249 |         }
250 |         return out
251 |     }
252 | 
253 |     public func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {
254 |         var sanitizedWeights = weights
255 | 
256 |         if configuration.tieWordEmbeddings {
257 |             sanitizedWeights["lm_head.weight"] = nil
258 |         }
259 | 
260 |         if sanitizedWeights["model.layers.0.mlp.experts.0.up_proj.weight"] == nil {
261 |             return sanitizedWeights
262 |         }
263 | 
264 |         for l in 0 ..< configuration.hiddenLayers {
265 |             let prefix = "model.layers.\(l)"
266 |             for n in ["up_proj", "down_proj", "gate_proj"] {
267 |                 if sanitizedWeights["\(prefix).mlp.experts.0.\(n).weight"] != nil {
268 |                     let toJoin = (0 ..< configuration.numExperts).map { e in
269 |                         sanitizedWeights.removeValue(
270 |                             forKey: "\(prefix).mlp.experts.\(e).\(n).weight")!
271 |                     }
272 |                     sanitizedWeights["\(prefix).mlp.switch_mlp.\(n).weight"] = MLX.stacked(toJoin)
273 |                 }
274 |             }
275 |         }
276 | 
277 |         return sanitizedWeights
278 |     }
279 | }
280 | 
281 | public struct Qwen3MoEConfiguration: Codable, Sendable {
282 |     var modelType: String = "qwen3_moe"
283 |     var hiddenSize: Int
284 |     var hiddenLayers: Int
285 |     var intermediateSize: Int
286 |     var attentionHeads: Int
287 |     var numExperts: Int
288 |     var numExpertsPerToken: Int
289 |     var decoderSparseStep: Int
290 |     var mlpOnlyLayers: [Int]
291 |     var moeIntermediateSize: Int
292 |     var rmsNormEps: Float
293 |     var vocabularySize: Int
294 |     var kvHeads: Int
295 |     var headDim: Int
296 |     var ropeTheta: Float = 1_000_000
297 |     var tieWordEmbeddings: Bool = false
298 |     var maxPositionEmbeddings: Int = 32768
299 |     var normTopkProb: Bool = false
300 |     var ropeScaling: [String: StringOrNumber]? = nil
301 | 
302 |     enum CodingKeys: String, CodingKey {
303 |         case modelType = "model_type"
304 |         case hiddenSize = "hidden_size"
305 |         case hiddenLayers = "num_hidden_layers"
306 |         case intermediateSize = "intermediate_size"
307 |         case attentionHeads = "num_attention_heads"
308 |         case numExperts = "num_experts"
309 |         case numExpertsPerToken = "num_experts_per_tok"
310 |         case decoderSparseStep = "decoder_sparse_step"
311 |         case mlpOnlyLayers = "mlp_only_layers"
312 |         case moeIntermediateSize = "moe_intermediate_size"
313 |         case rmsNormEps = "rms_norm_eps"
314 |         case vocabularySize = "vocab_size"
315 |         case kvHeads = "num_key_value_heads"
316 |         case headDim = "head_dim"
317 |         case ropeTheta = "rope_theta"
318 |         case tieWordEmbeddings = "tie_word_embeddings"
319 |         case maxPositionEmbeddings = "max_position_embeddings"
320 |         case normTopkProb = "norm_topk_prob"
321 |         case ropeScaling = "rope_scaling"
322 |     }
323 | 
324 |     public init(from decoder: Decoder) throws {
325 |         let container = try decoder.container(keyedBy: CodingKeys.self)
326 | 
327 |         self.modelType =
328 |             try container.decodeIfPresent(String.self, forKey: .modelType) ?? "qwen3_moe"
329 |         self.hiddenSize = try container.decode(Int.self, forKey: .hiddenSize)
330 |         self.hiddenLayers = try container.decode(Int.self, forKey: .hiddenLayers)
331 |         self.intermediateSize = try container.decode(Int.self, forKey: .intermediateSize)
332 |         self.attentionHeads = try container.decode(Int.self, forKey: .attentionHeads)
333 |         self.numExperts = try container.decode(Int.self, forKey: .numExperts)
334 |         self.numExpertsPerToken = try container.decode(Int.self, forKey: .numExpertsPerToken)
335 |         self.decoderSparseStep = try container.decode(Int.self, forKey: .decoderSparseStep)
336 |         self.mlpOnlyLayers = try container.decode([Int].self, forKey: .mlpOnlyLayers)
337 |         self.moeIntermediateSize = try container.decode(Int.self, forKey: .moeIntermediateSize)
338 |         self.rmsNormEps = try container.decode(Float.self, forKey: .rmsNormEps)
339 |         self.vocabularySize = try container.decode(Int.self, forKey: .vocabularySize)
340 |         self.kvHeads = try container.decode(Int.self, forKey: .kvHeads)
341 |         self.headDim = try container.decode(Int.self, forKey: .headDim)
342 |         self.ropeTheta = try container.decodeIfPresent(Float.self, forKey: .ropeTheta) ?? 1_000_000
343 |         self.tieWordEmbeddings =
344 |             try container.decodeIfPresent(Bool.self, forKey: .tieWordEmbeddings) ?? false
345 |         self.maxPositionEmbeddings =
346 |             try container.decodeIfPresent(Int.self, forKey: .maxPositionEmbeddings) ?? 32768
347 |         self.normTopkProb = try container.decodeIfPresent(Bool.self, forKey: .normTopkProb) ?? false
348 |         self.ropeScaling = try container.decodeIfPresent(
349 |             [String: StringOrNumber].self, forKey: .ropeScaling)
350 |     }
351 | }
352 | 
353 | // MARK: - LoRA
354 | 
355 | extension Qwen3MoEModel: LoRAModel {
356 |     public func loraLinearLayers() -> LoRALinearLayers {
357 |         model.layers.map { ($0.selfAttn, ["q_proj", "v_proj"]) }
358 |     }
359 | }
360 | 


--------------------------------------------------------------------------------
/Libraries/MLXLLM/Models/Starcoder2.swift:
--------------------------------------------------------------------------------
  1 | //
  2 | //  Starcoder2.swift
  3 | //  LLM
  4 | //
  5 | //  Created by John Mai on 2024/3/7.
  6 | //
  7 | 
  8 | import Foundation
  9 | import MLX
 10 | import MLXLMCommon
 11 | import MLXNN
 12 | 
 13 | // port of https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/models/starcoder2.py
 14 | 
 15 | private class Attention: Module {
 16 |     let args: Starcoder2Configuration
 17 |     let scale: Float
 18 | 
 19 |     @ModuleInfo(key: "q_proj") var wq: Linear
 20 |     @ModuleInfo(key: "k_proj") var wk: Linear
 21 |     @ModuleInfo(key: "v_proj") var wv: Linear
 22 |     @ModuleInfo(key: "o_proj") var wo: Linear
 23 | 
 24 |     let rope: RoPE
 25 | 
 26 |     public init(_ args: Starcoder2Configuration) {
 27 |         self.args = args
 28 | 
 29 |         let dim = args.hiddenSize
 30 |         let heads = args.attentionHeads
 31 |         let kvHeads = args.kvHeads
 32 | 
 33 |         let headDim = args.hiddenSize / heads
 34 |         self.scale = pow(Float(headDim), -0.5)
 35 | 
 36 |         _wq.wrappedValue = Linear(dim, heads * headDim, bias: true)
 37 |         _wk.wrappedValue = Linear(dim, kvHeads * headDim, bias: true)
 38 |         _wv.wrappedValue = Linear(dim, kvHeads * headDim, bias: true)
 39 |         _wo.wrappedValue = Linear(heads * headDim, dim, bias: true)
 40 | 
 41 |         self.rope = RoPE(dimensions: headDim, traditional: false, base: args.ropeTheta)
 42 |     }
 43 | 
 44 |     public func callAsFunction(
 45 |         _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
 46 |     ) -> MLXArray {
 47 |         let (B, L) = (x.dim(0), x.dim(1))
 48 | 
 49 |         var queries = wq(x)
 50 |         var keys = wk(x)
 51 |         var values = wv(x)
 52 | 
 53 |         // prepare the queries, keys and values for the attention computation
 54 |         queries = queries.reshaped(B, L, args.attentionHeads, -1).transposed(0, 2, 1, 3)
 55 |         keys = keys.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)
 56 |         values = values.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)
 57 | 
 58 |         if let cache {
 59 |             queries = rope(queries, offset: cache.offset)
 60 |             keys = rope(keys, offset: cache.offset)
 61 |             (keys, values) = cache.update(keys: keys, values: values)
 62 |         } else {
 63 |             queries = rope(queries)
 64 |             keys = rope(keys)
 65 |         }
 66 | 
 67 |         let output = MLXFast.scaledDotProductAttention(
 68 |             queries: queries, keys: keys, values: values, scale: scale, mask: mask
 69 |         )
 70 |         .transposed(0, 2, 1, 3)
 71 |         .reshaped(B, L, -1)
 72 | 
 73 |         return wo(output)
 74 |     }
 75 | }
 76 | 
 77 | private class MLP: Module, UnaryLayer {
 78 |     @ModuleInfo(key: "c_fc") var cFc: Linear
 79 |     @ModuleInfo(key: "c_proj") var cProj: Linear
 80 | 
 81 |     public init(dimensions: Int, hiddenDimensions: Int) {
 82 |         _cFc.wrappedValue = Linear(dimensions, hiddenDimensions, bias: true)
 83 |         _cProj.wrappedValue = Linear(hiddenDimensions, dimensions, bias: true)
 84 |     }
 85 | 
 86 |     public func callAsFunction(_ x: MLXArray) -> MLXArray {
 87 |         cProj(gelu(cFc(x)))
 88 |     }
 89 | }
 90 | 
 91 | private class TransformerBlock: Module {
 92 |     @ModuleInfo(key: "self_attn") var attention: Attention
 93 |     let mlp: MLP
 94 | 
 95 |     @ModuleInfo(key: "input_layernorm") var inputLayerNorm: LayerNorm
 96 |     @ModuleInfo(key: "post_attention_layernorm") var postAttentionLayerNorm: LayerNorm
 97 | 
 98 |     public init(_ args: Starcoder2Configuration) {
 99 |         _attention.wrappedValue = Attention(args)
100 |         self.mlp = MLP(dimensions: args.hiddenSize, hiddenDimensions: args.intermediateSize)
101 |         _inputLayerNorm.wrappedValue = LayerNorm(
102 |             dimensions: args.hiddenSize, eps: args.normEpsilon)
103 |         _postAttentionLayerNorm.wrappedValue = LayerNorm(
104 |             dimensions: args.hiddenSize, eps: args.normEpsilon)
105 |     }
106 | 
107 |     public func callAsFunction(
108 |         _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
109 |     ) -> MLXArray {
110 |         var r = attention(inputLayerNorm(x), mask: mask, cache: cache)
111 |         let h = x + r
112 |         r = mlp(postAttentionLayerNorm(h))
113 |         let out = h + r
114 |         return out
115 |     }
116 | }
117 | 
118 | private class Starcoder2ModelInner: Module {
119 |     @ModuleInfo(key: "embed_tokens") var embedTokens: Embedding
120 | 
121 |     fileprivate let layers: [TransformerBlock]
122 |     let norm: LayerNorm
123 | 
124 |     public init(_ args: Starcoder2Configuration) {
125 |         precondition(args.vocabularySize > 0)
126 | 
127 |         _embedTokens.wrappedValue = Embedding(
128 |             embeddingCount: args.vocabularySize, dimensions: args.hiddenSize)
129 | 
130 |         self.layers = (0 ..< args.hiddenLayers)
131 |             .map { _ in
132 |                 TransformerBlock(args)
133 |             }
134 |         self.norm = LayerNorm(dimensions: args.hiddenSize, eps: args.normEpsilon)
135 |     }
136 | 
137 |     public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]? = nil) -> MLXArray {
138 |         var h = embedTokens(inputs)
139 | 
140 |         let mask = createAttentionMask(h: h, cache: cache)
141 | 
142 |         for (i, layer) in layers.enumerated() {
143 |             h = layer(h, mask: mask, cache: cache?[i])
144 |         }
145 | 
146 |         return norm(h)
147 |     }
148 | }
149 | 
150 | public class Starcoder2Model: Module, LLMModel, KVCacheDimensionProvider {
151 |     public let vocabularySize: Int
152 |     public let kvHeads: [Int]
153 | 
154 |     public let tieWordEmbeddings: Bool
155 |     private let model: Starcoder2ModelInner
156 | 
157 |     @ModuleInfo(key: "lm_head") var lmHead: Linear
158 | 
159 |     public init(_ args: Starcoder2Configuration) {
160 |         self.vocabularySize = args.vocabularySize
161 |         self.kvHeads = (0 ..< args.hiddenLayers).map { _ in args.kvHeads }
162 |         self.model = Starcoder2ModelInner(args)
163 |         self.tieWordEmbeddings = args.tieWordEmbeddings
164 |         if !self.tieWordEmbeddings {
165 |             _lmHead.wrappedValue = Linear(args.hiddenSize, args.vocabularySize, bias: false)
166 |         }
167 |     }
168 | 
169 |     public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {
170 |         var out = model(inputs, cache: cache)
171 | 
172 |         if !tieWordEmbeddings {
173 |             return lmHead(out)
174 |         } else {
175 |             out = model.embedTokens.asLinear(out)
176 |             return out
177 |         }
178 |     }
179 | }
180 | 
181 | public struct Starcoder2Configuration: Codable, Sendable {
182 |     var hiddenSize: Int
183 |     var hiddenLayers: Int
184 |     var intermediateSize: Int
185 |     var attentionHeads: Int
186 |     var kvHeads: Int
187 |     var maxPositionEmbeddings: Int = 16384
188 |     var normEpsilon: Float = 1e-5
189 |     var normType: String = "layer_norm"
190 |     var vocabularySize: Int = 49152
191 |     var ropeTheta: Float = 100000
192 |     var tieWordEmbeddings: Bool = true
193 | 
194 |     enum CodingKeys: String, CodingKey {
195 |         case hiddenSize = "hidden_size"
196 |         case hiddenLayers = "num_hidden_layers"
197 |         case intermediateSize = "intermediate_size"
198 |         case attentionHeads = "num_attention_heads"
199 |         case kvHeads = "num_key_value_heads"
200 |         case maxPositionEmbeddings = "max_position_embeddings"
201 |         case normEpsilon = "norm_epsilon"
202 |         case normType = "norm_type"
203 |         case vocabularySize = "vocab_size"
204 |         case ropeTheta = "rope_theta"
205 |         case tieWordEmbeddings = "tie_word_embeddings"
206 |     }
207 | 
208 |     public init(from decoder: Decoder) throws {
209 |         // custom implementation to handle optional keys with required values
210 |         let container: KeyedDecodingContainer<Starcoder2Configuration.CodingKeys> =
211 |             try decoder.container(
212 |                 keyedBy: Starcoder2Configuration.CodingKeys.self)
213 | 
214 |         self.hiddenSize = try container.decode(
215 |             Int.self, forKey: Starcoder2Configuration.CodingKeys.hiddenSize)
216 |         self.hiddenLayers = try container.decode(
217 |             Int.self, forKey: Starcoder2Configuration.CodingKeys.hiddenLayers)
218 |         self.intermediateSize = try container.decode(
219 |             Int.self, forKey: Starcoder2Configuration.CodingKeys.intermediateSize)
220 |         self.attentionHeads = try container.decode(
221 |             Int.self, forKey: Starcoder2Configuration.CodingKeys.attentionHeads)
222 |         self.kvHeads = try container.decode(
223 |             Int.self, forKey: Starcoder2Configuration.CodingKeys.kvHeads)
224 |         self.maxPositionEmbeddings =
225 |             try container.decodeIfPresent(
226 |                 Int.self, forKey: Starcoder2Configuration.CodingKeys.maxPositionEmbeddings) ?? 16384
227 |         self.normEpsilon =
228 |             try container.decodeIfPresent(
229 |                 Float.self, forKey: Starcoder2Configuration.CodingKeys.normEpsilon) ?? 1e-5
230 |         self.normType =
231 |             try container.decodeIfPresent(
232 |                 String.self, forKey: Starcoder2Configuration.CodingKeys.normType) ?? "layer_norm"
233 |         self.vocabularySize =
234 |             try container.decodeIfPresent(
235 |                 Int.self, forKey: Starcoder2Configuration.CodingKeys.vocabularySize) ?? 49152
236 |         self.ropeTheta =
237 |             try container.decodeIfPresent(
238 |                 Float.self, forKey: Starcoder2Configuration.CodingKeys.ropeTheta)
239 |             ?? 100000
240 |         self.tieWordEmbeddings =
241 |             try container.decodeIfPresent(
242 |                 Bool.self, forKey: Starcoder2Configuration.CodingKeys.tieWordEmbeddings)
243 |             ?? true
244 |     }
245 | }
246 | 
247 | // MARK: - LoRA
248 | 
249 | extension Starcoder2Model: LoRAModel {
250 |     public func loraLinearLayers() -> LoRALinearLayers {
251 |         model.layers.map { ($0.attention, ["q_proj", "v_proj"]) }
252 |     }
253 | }
254 | 


--------------------------------------------------------------------------------
/Libraries/MLXLLM/README.md:
--------------------------------------------------------------------------------
  1 | # MLXLLM
  2 | 
  3 | # Documentation
  4 | 
  5 | - [Porting and implementing models](https://swiftpackageindex.com/ml-explore/mlx-swift-examples/main/documentation/mlxlmcommon/porting)
  6 | - [MLXLLMCommon](https://swiftpackageindex.com/ml-explore/mlx-swift-examples/main/documentation/mlxlmcommon) -- common API for LLM and VLM
  7 | - [MLXLLM](https://swiftpackageindex.com/ml-explore/mlx-swift-examples/main/documentation/mlxllm) -- large language model example implementations
  8 | - [MLXVLM](https://swiftpackageindex.com/ml-explore/mlx-swift-examples/main/documentation/mlxvlm) -- vision language model example implementations
  9 | 
 10 | # Contents
 11 | 
 12 | This is a port of several models from:
 13 | 
 14 | - https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/models/
 15 | 
 16 | using the Hugging Face swift transformers package to provide tokenization:
 17 | 
 18 | - https://github.com/huggingface/swift-transformers
 19 | 
 20 | The [LLMModelFactory.swift](LLMModelFactory.swift) provides minor overrides and customization --
 21 | if you require overrides for the tokenizer or prompt customizations they can be
 22 | added there.
 23 | 
 24 | This is set up to load models from Hugging Face, e.g. https://huggingface.co/mlx-community
 25 | 
 26 | The following models have been tried:
 27 | 
 28 | - mlx-community/CodeLlama-13b-Instruct-hf-4bit-MLX
 29 | - mlx-community/Llama-3.2-1B-Instruct-4bit
 30 | - mlx-community/Llama-3.2-3B-Instruct-4bit
 31 | - mlx-community/Meta-Llama-3-8B-Instruct-4bit
 32 | - mlx-community/Meta-Llama-3.1-8B-Instruct-4bit
 33 | - mlx-community/Mistral-7B-Instruct-v0.3-4bit
 34 | - mlx-community/Mistral-Nemo-Instruct-2407-4bit
 35 | - mlx-community/OpenELM-270M-Instruct
 36 | - mlx-community/Phi-3.5-MoE-instruct-4bit
 37 | - mlx-community/Phi-3.5-mini-instruct-4bit
 38 | - mlx-community/Qwen1.5-0.5B-Chat-4bit
 39 | - mlx-community/SmolLM-135M-Instruct-4bit
 40 | - mlx-community/gemma-2-2b-it-4bit
 41 | - mlx-community/gemma-2-9b-it-4bit
 42 | - mlx-community/phi-2-hf-4bit-mlx
 43 | - mlx-community/quantized-gemma-2b-it
 44 | 
 45 | Currently supported model types are:
 46 | 
 47 | - Cohere
 48 | - Gemma
 49 | - Gemma2
 50 | - InternLM2
 51 | - Llama / Mistral
 52 | - OpenELM
 53 | - Phi
 54 | - Phi3
 55 | - PhiMoE
 56 | - Qwen2
 57 | - Qwen3
 58 | - Starcoder2
 59 | - MiMo
 60 | - GLM4
 61 | - AceReason
 62 | 
 63 | See [llm-tool](../../Tools/llm-tool)
 64 | 
 65 | # Quick Start
 66 | 
 67 | Using LLMs and VLMs from MLXLMCommon is as easy as:
 68 | 
 69 | ```swift
 70 | let model = try await loadModel(id: "mlx-community/Qwen3-4B-4bit")
 71 | let session = ChatSession(model)
 72 | print(try await session.respond(to: "What are two things to see in San Francisco?")
 73 | print(try await session.respond(to: "How about a great place to eat?")
 74 | ```
 75 | 
 76 | For more information see 
 77 | [Evaluation](https://swiftpackageindex.com/ml-explore/mlx-swift-examples/main/documentation/mlxlmcommon/evaluation)
 78 | or [Using Models](https://swiftpackageindex.com/ml-explore/mlx-swift-examples/main/documentation/mlxlmcommon/using-model)
 79 | for more advanced API.
 80 | 
 81 | # Adding a Model
 82 | 
 83 | If the model follows the typical LLM pattern:
 84 | 
 85 | - `config.json`, `tokenizer.json`, and `tokenizer_config.json`
 86 | - `*.safetensors`
 87 | 
 88 | You can follow the pattern of the models in the [Models](Models) directory
 89 | and create a `.swift` file for your new model:
 90 | 
 91 | ## Create a Configuration
 92 | 
 93 | Create a configuration struct to match the `config.json` (any parameters needed).
 94 | 
 95 | ```swift
 96 | public struct YourModelConfiguration: Codable, Sendable {
 97 |     public let hiddenSize: Int
 98 |     
 99 |     // use this pattern for values that need defaults
100 |     public let _layerNormEps: Float?
101 |     public var layerNormEps: Float { _layerNormEps ?? 1e-6 }
102 |     
103 |     enum CodingKeys: String, CodingKey {
104 |         case hiddenSize = "hidden_size"
105 |         case _layerNormEps = "layer_norm_eps"
106 |     }
107 | }
108 | ```
109 | 
110 | ## Create the Model Class
111 | 
112 | Create the model class. The top-level public class should have a
113 | structure something like this:
114 | 
115 | ```swift
116 | public class YourModel: Module, LLMModel, KVCacheDimensionProvider, LoRAModel {
117 | 
118 |     public let kvHeads: [Int]
119 | 
120 |     @ModuleInfo var model: YourModelInner
121 | 
122 |     public func loraLinearLayers() -> LoRALinearLayers {
123 |         // TODO: modify as needed
124 |         model.layers.map { ($0.attention, ["q_proj", "v_proj"]) }
125 |     }
126 | 
127 |     public init(_ args: YourModelConfiguration) {
128 |         self.kvHeads = Array(repeating: args.kvHeads, count: args.hiddenLayers)
129 |         self.model = YourModelInner(args)
130 |     }
131 | 
132 |     public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {
133 |         // TODO: modify as needed
134 |         let out = model(inputs, cache: cache)
135 |         return model.embedTokens.asLinear(out)
136 |     }
137 | }
138 | ```
139 | 
140 | ## Register the Model
141 | 
142 | In [LLMModelFactory.swift](LLMModelFactory.swift) register the model type itself
143 | (this is independent of the model id):
144 | 
145 | ```swift
146 | public class LLMTypeRegistry: @unchecked Sendable {
147 | ...
148 |     private var creators: [String: @Sendable (URL) throws -> any LanguageModel] = [
149 |         "yourModel": create(YourModelConfiguration.self, YourModel.init),
150 | ```
151 | 
152 | Add a constant for the model in the `LLMRegistry` (not strictly required but useful
153 | for callers to refer to it in code):
154 | 
155 | ```swift
156 | public class LLMRegistry: @unchecked Sendable {
157 | ...
158 |     static public let yourModel_4bit = ModelConfiguration(
159 |         id: "mlx-community/YourModel-4bit",
160 |         defaultPrompt: "What is the gravity on Mars and the moon?"
161 |     )
162 | ```
163 | 
164 | and finally add it to the all list -- this will let users find the model
165 | configuration by id:
166 | 
167 | ```swift
168 |     private static func all() -> [ModelConfiguration] {
169 |         [
170 |             codeLlama13b4bit,
171 | ...
172 |             yourModel_4bit,
173 | ```
174 | 
175 | # Using a Model
176 | 
177 | See [MLXLMCommon/README.md](../MLXLMCommon/README.md#using-a-model).
178 | 
179 | # LoRA
180 | 
181 | [Lora.swift](Lora.swift) contains an implementation of LoRA based on this example:
182 | 
183 | - https://github.com/ml-explore/mlx-examples/tree/main/lora
184 | 
185 | See [llm-tool/LoraCommands.swift](../../Tools/llm-tool/LoraCommands.swift) for an example of a driver and
186 | [llm-tool](../../Tools/llm-tool) for examples of how to run it.
187 | 


--------------------------------------------------------------------------------
/Libraries/MLXLLM/SuScaledRotaryEmbedding.swift:
--------------------------------------------------------------------------------
 1 | import Foundation
 2 | import MLX
 3 | import MLXNN
 4 | 
 5 | public class SuScaledRotaryEmbedding: Module {
 6 |     let dimensions: Int
 7 |     let maxPositionEmbeddings: Int
 8 |     let originalMaxPositionEmbeddings: Int
 9 |     let scale: Float
10 |     let _freqs: MLXArray
11 | 
12 |     public init(
13 |         dimensions: Int,
14 |         base: Float = 10000.0,
15 |         maxPositionEmbeddings: Int = 131072,
16 |         originalMaxPositionEmbeddings: Int = 4096,
17 |         longFactor: [Float] = [1.0],
18 |         // shortMScale: Float? = nil,
19 |         longMScale: Float? = nil
20 |     ) {
21 |         precondition(dimensions % 2 == 0, "Dimensions must be even")
22 | 
23 |         self.dimensions = dimensions
24 |         self.maxPositionEmbeddings = maxPositionEmbeddings
25 |         self.originalMaxPositionEmbeddings = originalMaxPositionEmbeddings
26 | 
27 |         let exponent =
28 |             MLXArray(stride(from: 0, to: dimensions, by: 2)).asType(.float32) / Float(dimensions)
29 |         let freqs = MLX.pow(MLXArray(base), exponent)
30 |         self._freqs = MLXArray(longFactor).asType(.float32) * freqs
31 | 
32 |         self.scale =
33 |             longMScale
34 |             ?? sqrt(
35 |                 1 + log(Float(maxPositionEmbeddings) / Float(originalMaxPositionEmbeddings))
36 |                     / log(Float(originalMaxPositionEmbeddings))
37 |             )
38 |     }
39 | 
40 |     public func callAsFunction(_ x: MLXArray, offset: Int = 0) -> MLXArray {
41 |         // Apply scaling only to the dimensions that will be rotated
42 |         var scaledX = x
43 |         let sliceToScale = scaledX[.ellipsis, 0 ..< dimensions]
44 |         scaledX[.ellipsis, 0 ..< dimensions] = scale * sliceToScale
45 | 
46 |         return MLXFast.RoPE(
47 |             scaledX,
48 |             dimensions: dimensions,
49 |             traditional: false,
50 |             base: nil,
51 |             scale: 1.0,
52 |             offset: offset,
53 |             freqs: self._freqs
54 |         )
55 |     }
56 | }
57 | 


--------------------------------------------------------------------------------
/Libraries/MLXLLM/SwitchLayers.swift:
--------------------------------------------------------------------------------
  1 | import Foundation
  2 | import MLX
  3 | import MLXNN
  4 | 
  5 | // Port of https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/models/switch_layers.py
  6 | 
  7 | private func gatherSort(x: MLXArray, indices: MLXArray) -> (MLXArray, MLXArray, MLXArray) {
  8 |     let m = indices.dim(-1)
  9 |     let indices = indices.flattened()
 10 |     let order = argSort(indices)
 11 |     let inverseOrder = argSort(order)
 12 | 
 13 |     return (
 14 |         x.flattened(start: 0, end: -3)[order.floorDivide(m)],
 15 |         indices[order],
 16 |         inverseOrder
 17 |     )
 18 | }
 19 | 
 20 | private func scatterUnsort(x: MLXArray, invOrder: MLXArray, shape: [Int]? = nil) -> MLXArray {
 21 |     var x = x[invOrder]
 22 |     if let shape {
 23 |         x = unflatten(x, axis: 0, shape: shape)
 24 |     }
 25 |     return x
 26 | }
 27 | 
 28 | // MARK: - SwitchGLU
 29 | 
 30 | class SwitchGLU: Module {
 31 |     @ModuleInfo(key: "gate_proj") var gateProj: SwitchLinear
 32 |     @ModuleInfo(key: "up_proj") var upProj: SwitchLinear
 33 |     @ModuleInfo(key: "down_proj") var downProj: SwitchLinear
 34 | 
 35 |     let inputDims: Int
 36 |     let hiddenDims: Int
 37 |     let numExperts: Int
 38 |     let activation: (MLXArray) -> MLXArray
 39 | 
 40 |     init(
 41 |         inputDims: Int,
 42 |         hiddenDims: Int,
 43 |         numExperts: Int,
 44 |         activation: @escaping (MLXArray) -> MLXArray = MLXNN.silu,
 45 |         bias: Bool = false
 46 |     ) {
 47 |         self.inputDims = inputDims
 48 |         self.hiddenDims = hiddenDims
 49 |         self.numExperts = numExperts
 50 |         self.activation = activation
 51 | 
 52 |         self._gateProj.wrappedValue = SwitchLinear(
 53 |             inputDims: inputDims, outputDims: hiddenDims, numExperts: numExperts, bias: bias)
 54 |         self._upProj.wrappedValue = SwitchLinear(
 55 |             inputDims: inputDims, outputDims: hiddenDims, numExperts: numExperts, bias: bias)
 56 |         self._downProj.wrappedValue = SwitchLinear(
 57 |             inputDims: hiddenDims, outputDims: inputDims, numExperts: numExperts, bias: bias)
 58 | 
 59 |         super.init()
 60 |     }
 61 | 
 62 |     func callAsFunction(_ x: MLXArray, _ indices: MLXArray) -> MLXArray {
 63 |         var x = MLX.expandedDimensions(x, axes: [-2, -3])
 64 | 
 65 |         let doSort = indices.size > 64
 66 | 
 67 |         var idx = indices
 68 |         var inverseOrder = MLXArray()
 69 | 
 70 |         if doSort {
 71 |             (x, idx, inverseOrder) = gatherSort(x: x, indices: indices)
 72 |         }
 73 | 
 74 |         let xUp = upProj(x, idx, sortedIndices: doSort)
 75 |         let xGate = gateProj(x, idx, sortedIndices: doSort)
 76 |         x = downProj(
 77 |             activation(xGate) * xUp,
 78 |             idx,
 79 |             sortedIndices: doSort)
 80 | 
 81 |         if doSort {
 82 |             x = scatterUnsort(x: x, invOrder: inverseOrder, shape: indices.shape)
 83 |         }
 84 | 
 85 |         return MLX.squeezed(x, axis: -2)
 86 |     }
 87 | }
 88 | 
 89 | class SwitchLinear: Module, Quantizable {
 90 |     @ModuleInfo(key: "weight") var weight: MLXArray
 91 |     @ModuleInfo(key: "bias") var bias: MLXArray?
 92 | 
 93 |     let inputDims: Int
 94 |     let outputDims: Int
 95 |     let numExperts: Int
 96 | 
 97 |     init(inputDims: Int, outputDims: Int, numExperts: Int, bias: Bool = true) {
 98 |         self.inputDims = inputDims
 99 |         self.outputDims = outputDims
100 |         self.numExperts = numExperts
101 | 
102 |         let scale = sqrt(1.0 / Float(inputDims))
103 |         self._weight.wrappedValue = MLXRandom.uniform(
104 |             low: -scale,
105 |             high: scale,
106 |             [numExperts, outputDims, inputDims]
107 |         )
108 | 
109 |         if bias {
110 |             self._bias.wrappedValue = MLXArray.zeros([numExperts, outputDims])
111 |         }
112 | 
113 |         super.init()
114 |     }
115 | 
116 |     /// Initializer meant for subclasses to provide weight and bias arrays directly.
117 |     ///
118 |     /// This is used e.g. by ``QuantizedSwitchLinear`` to provide quantized weights and biases
119 |     /// rather than have ``SwitchLinear`` compute them.
120 |     init(
121 |         inputDims: Int, outputDims: Int, numExperts: Int,
122 |         weight: MLXArray, bias: MLXArray? = nil
123 |     ) {
124 |         self.inputDims = inputDims
125 |         self.outputDims = outputDims
126 |         self.numExperts = numExperts
127 | 
128 |         self._weight.wrappedValue = weight
129 |         self._bias.wrappedValue = bias
130 |     }
131 | 
132 |     func callAsFunction(
133 |         _ x: MLXArray, _ indices: MLXArray, sortedIndices: Bool = false
134 |     ) -> MLXArray {
135 |         let weightT = self.weight.swappedAxes(-1, -2)
136 |         var result = MLX.gatherMatmul(x, weightT, rhsIndices: indices, sortedIndices: sortedIndices)
137 | 
138 |         if let bias = self.bias {
139 |             result = result + MLX.expandedDimensions(bias[indices], axis: -2)
140 |         }
141 | 
142 |         return result
143 |     }
144 | 
145 |     func toQuantized(groupSize: Int = 64, bits: Int = 4) -> Module {
146 |         QuantizedSwitchLinear(self, groupSize: groupSize, bits: bits)
147 |     }
148 | }
149 | 
150 | class QuantizedSwitchLinear: SwitchLinear, Quantized {
151 |     @ModuleInfo(key: "scales") var scales: MLXArray
152 |     @ModuleInfo(key: "biases") var biases: MLXArray
153 | 
154 |     let groupSize: Int
155 |     let bits: Int
156 | 
157 |     init(_ other: SwitchLinear, groupSize: Int = 64, bits: Int = 4) {
158 |         self.groupSize = groupSize
159 |         self.bits = bits
160 | 
161 |         let (quantizedWeight, scales, biases) = MLX.quantized(
162 |             other.weight, groupSize: groupSize, bits: bits)
163 | 
164 |         self._scales.wrappedValue = scales
165 |         self._biases.wrappedValue = biases
166 | 
167 |         super.init(
168 |             inputDims: other.inputDims, outputDims: other.outputDims, numExperts: other.numExperts,
169 |             weight: quantizedWeight, bias: other.bias)
170 | 
171 |         self.freeze()
172 |     }
173 | 
174 |     override func callAsFunction(
175 |         _ x: MLXArray, _ indices: MLXArray, sortedIndices: Bool = false
176 |     ) -> MLXArray {
177 |         var result = MLX.gatherQuantizedMatmul(
178 |             x,
179 |             self.weight,
180 |             scales: self.scales,
181 |             biases: self.biases,
182 |             rhsIndices: indices,
183 |             transpose: true,
184 |             groupSize: self.groupSize,
185 |             bits: self.bits,
186 |             sortedIndices: sortedIndices
187 |         )
188 | 
189 |         if let bias = self.bias {
190 |             result = result + MLX.expandedDimensions(bias[indices], axis: -2)
191 |         }
192 | 
193 |         return result
194 |     }
195 | }
196 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/Adapters/LoRA/DoRA+Layers.swift:
--------------------------------------------------------------------------------
  1 | //
  2 | //  DoRA+Layers.swift
  3 | //  mlx-libraries
  4 | //
  5 | //  Created by Ivan Petrukha on 02.06.2025.
  6 | //
  7 | 
  8 | import Foundation
  9 | import MLX
 10 | import MLXLinalg
 11 | import MLXNN
 12 | import MLXRandom
 13 | 
 14 | /// Performs the forward pass for a DoRA linear layer.
 15 | private func forward(
 16 |     x: MLXArray, y: MLXArray,
 17 |     weight: MLXArray, bias: MLXArray?,
 18 |     loraA: MLXArray, loraB: MLXArray,
 19 |     scale: Float, magnitude: MLXArray
 20 | ) -> MLXArray {
 21 |     let z = matmul(matmul(x, loraA), loraB)
 22 |     var out = y + (scale * z).asType(x.dtype)
 23 | 
 24 |     let adapted = weight + matmul(scale * loraB.T, loraA.T)
 25 |     let denom = norm(adapted, axis: 1)
 26 |     out *= (magnitude / denom).asType(x.dtype)
 27 | 
 28 |     return if let bias {
 29 |         out + bias
 30 |     } else {
 31 |         out
 32 |     }
 33 | }
 34 | 
 35 | /// Fuses the base weights with the DoRA parameters.
 36 | private func fuse(
 37 |     weight: MLXArray,
 38 |     loraA: MLXArray, loraB: MLXArray,
 39 |     scale: Float, magnitude: MLXArray
 40 | ) -> MLXArray {
 41 |     let loraA = loraA.T.asType(weight.dtype)
 42 |     let loraB = (scale * loraB.T).asType(weight.dtype)
 43 | 
 44 |     var adapted = weight + matmul(loraB, loraA)
 45 |     let denom = norm(adapted, axis: 1)
 46 |     adapted *= (magnitude / denom).reshaped([-1, 1])
 47 | 
 48 |     return adapted
 49 | }
 50 | 
 51 | /// Filters out DoRA-specific parameters from a list of module keys.
 52 | private func filterFreezeKeys(from module: Module, keys: [String]?) -> [String] {
 53 |     return
 54 |         (keys
 55 |         ?? module.filterMap(filter: type(of: module).filterLocalParameters)
 56 |         .flattened()
 57 |         .map { $0.0 })
 58 |         .filter { !["lora_a", "lora_b", "m"].contains($0) }
 59 | }
 60 | 
 61 | /// Implementation of DoRA `Linear` replacement layer.
 62 | ///
 63 | /// This layer implements DoRA (Weight-Decomposed Low-Rank Adaptation) for `Linear` layers.
 64 | ///
 65 | /// ``QDoRALinear`` is the equivalent class for `QuantizedLinear`.
 66 | public class DoRALinear: Linear, LoRALayer {
 67 | 
 68 |     let scale: Float
 69 | 
 70 |     @ParameterInfo(key: "lora_a") var loraA: MLXArray
 71 |     @ParameterInfo(key: "lora_b") var loraB: MLXArray
 72 |     @ParameterInfo(key: "m") var magnitude: MLXArray
 73 | 
 74 |     required public init(linear: Linear, rank: Int = 8, scale: Float = 20.0) {
 75 |         let (outputDimensions, inputDimensions) = linear.shape
 76 |         let loraScale = 1 / sqrt(Float(inputDimensions))
 77 | 
 78 |         self.scale = scale
 79 |         self._loraA.wrappedValue = MLXRandom.uniform(
 80 |             low: -loraScale, high: loraScale, [inputDimensions, rank])
 81 |         self._loraB.wrappedValue = MLXArray.zeros([rank, outputDimensions])
 82 |         self._magnitude.wrappedValue = MLXLinalg.norm(linear.weight, axis: 1)
 83 | 
 84 |         super.init(weight: linear.weight, bias: linear.bias)
 85 | 
 86 |         freeze()
 87 |     }
 88 | 
 89 |     public static func from(linear: Linear, rank: Int = 8, scale: Float = 20.0) -> LoRALayer {
 90 |         if let linear = linear as? QuantizedLinear {
 91 |             QDoRALinear(linear: linear, rank: rank, scale: scale)
 92 |         } else {
 93 |             DoRALinear(linear: linear, rank: rank, scale: scale)
 94 |         }
 95 |     }
 96 | 
 97 |     public override func freeze(recursive: Bool = true, keys: [String]? = nil, strict: Bool = false)
 98 |         throws
 99 |     {
100 |         let keys = filterFreezeKeys(from: self, keys: keys)
101 |         try super.freeze(recursive: recursive, keys: keys, strict: strict)
102 |     }
103 | 
104 |     public func fused() -> Module {
105 |         Linear(
106 |             weight: fuse(
107 |                 weight: weight, loraA: loraA, loraB: loraB, scale: scale, magnitude: magnitude),
108 |             bias: bias
109 |         )
110 |     }
111 | 
112 |     public override func callAsFunction(_ x: MLXArray) -> MLXArray {
113 |         let y = matmul(x, weight.T)
114 |         return forward(
115 |             x: x, y: y,
116 |             weight: weight, bias: bias,
117 |             loraA: loraA, loraB: loraB,
118 |             scale: scale, magnitude: magnitude
119 |         )
120 |     }
121 | }
122 | 
123 | /// Implementation of DoRA `QuantizedLinear` replacement layer.
124 | ///
125 | /// See ``DoRALinear`` (equivalent class for `Linear` layers) for more information.
126 | ///
127 | /// ### See Also
128 | /// - ``DoRALinear``
129 | public class QDoRALinear: QuantizedLinear, LoRALayer {
130 | 
131 |     let scale: Float
132 | 
133 |     @ParameterInfo(key: "lora_a") var loraA: MLXArray
134 |     @ParameterInfo(key: "lora_b") var loraB: MLXArray
135 |     @ParameterInfo(key: "m") var magnitude: MLXArray
136 | 
137 |     required public init(linear: QuantizedLinear, rank: Int = 8, scale: Float = 20.0) {
138 |         let (outputDimensions, inputDimensions) = linear.shape
139 |         let loraScale = 1 / sqrt(Float(inputDimensions))
140 | 
141 |         self.scale = scale
142 |         self._loraA.wrappedValue = MLXRandom.uniform(
143 |             low: -loraScale, high: loraScale, [inputDimensions, rank])
144 |         self._loraB.wrappedValue = MLXArray.zeros([rank, outputDimensions])
145 |         self._magnitude.wrappedValue = MLXLinalg.norm(linear.dequantizedWeight, axis: 1)
146 | 
147 |         super.init(
148 |             weight: linear.weight, bias: linear.bias,
149 |             scales: linear.scales, biases: linear.biases,
150 |             groupSize: linear.groupSize, bits: linear.bits
151 |         )
152 | 
153 |         freeze()
154 |     }
155 | 
156 |     public override func freeze(recursive: Bool = true, keys: [String]? = nil, strict: Bool = false)
157 |         throws
158 |     {
159 |         let keys = filterFreezeKeys(from: self, keys: keys)
160 |         try super.freeze(recursive: recursive, keys: keys, strict: strict)
161 |     }
162 | 
163 |     public func fused() -> Module {
164 |         QuantizedLinear(
165 |             weight: fuse(
166 |                 weight: dequantizedWeight, loraA: loraA, loraB: loraB, scale: scale,
167 |                 magnitude: magnitude),
168 |             bias: bias, groupSize: groupSize, bits: bits
169 |         )
170 |     }
171 | 
172 |     public override func callAsFunction(_ x: MLXArray) -> MLXArray {
173 |         let y = quantizedMatmul(
174 |             x, weight, scales: scales, biases: biases, groupSize: groupSize, bits: bits)
175 |         return forward(
176 |             x: x, y: y,
177 |             weight: dequantizedWeight, bias: bias,
178 |             loraA: loraA, loraB: loraB,
179 |             scale: scale, magnitude: magnitude
180 |         )
181 |     }
182 | }
183 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/Adapters/LoRA/LoRA+Layers.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import Foundation
  4 | import MLX
  5 | import MLXNN
  6 | import MLXOptimizers
  7 | import MLXRandom
  8 | 
  9 | /// Implementation of LoRA `Linear` replacement layer.
 10 | ///
 11 | /// This layer implements the LoRA capabilities for `Linear` layers, specifically:
 12 | ///
 13 | /// - converting `Linear` or `QuantizedLinear` layers to ``LoRALinear`` / ``QLoRALinear``
 14 | /// - converting ``LoRALinear`` back to `Linear` or `QuantizedLinear` (``LoRAConvertToLinear``)
 15 | /// - implementing the LoRA evaluation
 16 | ///
 17 | /// ``QLoRALinear`` is the equivalent class for `QuantizedLinear`.
 18 | ///
 19 | /// This is not typically used directly -- ``LoRATrain/convert(model:layers:)`` is used to
 20 | /// add the adapter layers to a given model.
 21 | ///
 22 | /// ### See Also
 23 | /// - [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
 24 | /// - [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
 25 | /// - ``QLoRALinear``
 26 | /// - ``LoRATrain/convert(model:layers:)``
 27 | /// - ``LoRATrain/fuse(model:layers:deQuantize:)``
 28 | public class LoRALinear: Linear, LoRALayer {
 29 | 
 30 |     let scale: Float
 31 | 
 32 |     @ParameterInfo(key: "lora_a") var loraA: MLXArray
 33 |     @ParameterInfo(key: "lora_b") var loraB: MLXArray
 34 | 
 35 |     required public init(
 36 |         _ inputDimensions: Int, _ outputDimensions: Int, rank: Int = 8, bias: Bool = false,
 37 |         scale: Float = 20.0, linear: Linear
 38 |     ) {
 39 |         // Scale for low-rank update
 40 |         self.scale = scale
 41 | 
 42 |         // Low rank lora weights
 43 |         let loraScale = 1 / sqrt(Float(inputDimensions))
 44 |         self._loraA.wrappedValue = MLXRandom.uniform(
 45 |             low: -loraScale, high: loraScale, [inputDimensions, rank])
 46 |         self._loraB.wrappedValue = MLXArray.zeros([rank, outputDimensions])
 47 | 
 48 |         super.init(weight: linear.weight, bias: linear.bias)
 49 | 
 50 |         freeze()
 51 |     }
 52 | 
 53 |     /// Freeze all parameters except the lora parameters
 54 |     public override func freeze(recursive: Bool = true, keys: [String]? = nil, strict: Bool = false)
 55 |         throws
 56 |     {
 57 |         // realize the keys and omit the lora parameters
 58 |         let keys =
 59 |             (keys ?? self.filterMap(filter: Self.filterLocalParameters).flattened().map { $0.0 })
 60 |             .filter {
 61 |                 $0 != "lora_a" && $0 != "lora_b"
 62 |             }
 63 |         try super.freeze(recursive: recursive, keys: keys, strict: strict)
 64 |     }
 65 | 
 66 |     /// Convert a `Linear` or `QuantizedLinear` layer into a new `Linear` layer
 67 |     /// that implements the `LoRA` adapter.
 68 |     ///
 69 |     /// This is typically called via ``LoRATrain/convert(model:layers:)``.
 70 |     ///
 71 |     /// ### See Also
 72 |     /// - ``LoRATrain/convert(model:layers:)``
 73 |     /// - ``QLoRALinear/from(linear:rank:)``
 74 |     public static func from(linear: Linear, rank: Int = 8, scale: Float = 20.0) -> LoRALayer {
 75 |         if let linear = linear as? QuantizedLinear {
 76 |             return QLoRALinear.from(linear: linear, rank: rank, scale: scale)
 77 |         }
 78 |         let (outputDimensions, inputDimensions) = linear.shape
 79 |         return LoRALinear(
 80 |             inputDimensions, outputDimensions, rank: rank, scale: scale, linear: linear)
 81 |     }
 82 | 
 83 |     /// Convert back into a fused `Linear` layer.
 84 |     ///
 85 |     /// This is typically called via ``LoRATrain/fuse(model:layers:deQuantize:)``.
 86 |     ///
 87 |     /// ### See Also
 88 |     /// - ``LoRATrain/fuse(model:layers:deQuantize:)``
 89 |     /// - ``LoRAConvertToLinear``
 90 |     /// - ``QLoRALinear/toLinear(deQuantize:)``
 91 |     public func fused() -> Module {
 92 |         let dtype = weight.dtype
 93 |         let loraB = (scale * loraB.T).asType(dtype)
 94 |         let loraA = loraA.T.asType(dtype)
 95 |         return Linear(weight: weight + matmul(loraB, loraA), bias: bias)
 96 |     }
 97 | 
 98 |     public override func callAsFunction(_ x: MLXArray) -> MLXArray {
 99 |         let y = super.callAsFunction(x.asType(weight.dtype))
100 |         let z = matmul(matmul(x, self.loraA), self.loraB)
101 |         return y + scale * z
102 |     }
103 | }
104 | 
105 | /// Implementation of LoRA `QuantizedLinear` replacement layer.
106 | ///
107 | /// See ``LoRALinear`` (equivalent class for `Linear` layers) for more information.
108 | public class QLoRALinear: QuantizedLinear, LoRALayer {
109 | 
110 |     let scale: Float
111 | 
112 |     @ParameterInfo(key: "lora_a") var loraA: MLXArray
113 |     @ParameterInfo(key: "lora_b") var loraB: MLXArray
114 | 
115 |     required public init(
116 |         _ inputDimensions: Int, _ outputDimensions: Int, rank: Int = 8, bias: Bool = false,
117 |         scale: Float = 20.0, linear: QuantizedLinear
118 |     ) {
119 | 
120 |         // Scale for low-rank update
121 |         self.scale = scale
122 | 
123 |         // Low rank lora weights
124 |         let loraScale = 1 / sqrt(Float(inputDimensions))
125 |         self._loraA.wrappedValue = MLXRandom.uniform(
126 |             low: -loraScale, high: loraScale, [inputDimensions, rank])
127 |         self._loraB.wrappedValue = MLXArray.zeros([rank, outputDimensions])
128 | 
129 |         super.init(
130 |             weight: linear.weight, bias: linear.bias, scales: linear.scales, biases: linear.biases,
131 |             groupSize: linear.groupSize, bits: linear.bits)
132 | 
133 |         // start frozen except for the lora keys
134 |         freeze()
135 |     }
136 | 
137 |     /// Freeze all parameters except the lora parameters
138 |     public override func freeze(recursive: Bool = true, keys: [String]? = nil, strict: Bool = false)
139 |         throws
140 |     {
141 |         // realize the keys and omit the lora parameters
142 |         let keys =
143 |             (keys ?? self.filterMap(filter: Self.filterLocalParameters).flattened().map { $0.0 })
144 |             .filter {
145 |                 $0 != "lora_a" && $0 != "lora_b"
146 |             }
147 |         try super.freeze(recursive: recursive, keys: keys, strict: strict)
148 |     }
149 | 
150 |     /// Convert a `QuantizedLinear` layer into a new `Linear` layer
151 |     /// that implements the `LoRA` adapter.
152 |     ///
153 |     /// This is typically called via ``LoRATrain/convert(model:layers:)``.
154 |     ///
155 |     /// ### See Also
156 |     /// - ``LoRATrain/convert(model:layers:)``
157 |     /// - ``LoRALinear/from(linear:rank:)``
158 |     public static func from(linear: QuantizedLinear, rank: Int = 8, scale: Float = 20.0)
159 |         -> LoRALayer
160 |     {
161 |         let (outputDimensions, inputDimensions) = linear.shape
162 |         return QLoRALinear(
163 |             inputDimensions, outputDimensions, rank: rank, scale: scale, linear: linear)
164 |     }
165 | 
166 |     /// Convert back into a fused `QuantizedLinear` layer.
167 |     ///
168 |     /// This is typically called via ``LoRATrain/fuse(model:layers:deQuantize:)``.
169 |     ///
170 |     /// ### See Also
171 |     /// - ``LoRATrain/fuse(model:layers:deQuantize:)``
172 |     public func fused() -> Module {
173 |         let weight = dequantizedWeight
174 |         let loraB = (scale * loraB.T).asType(.float16)
175 |         let loraA = loraA.T.asType(.float16)
176 |         return QuantizedLinear(
177 |             weight: weight + matmul(loraB, loraA),
178 |             bias: bias,
179 |             groupSize: groupSize,
180 |             bits: bits
181 |         )
182 |     }
183 | 
184 |     public override func callAsFunction(_ x: MLXArray) -> MLXArray {
185 |         let y = super.callAsFunction(x.asType(scales.dtype))
186 |         let z = matmul(matmul(x, self.loraA), self.loraB)
187 |         return y + scale * z
188 |     }
189 | }
190 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/Adapters/LoRA/LoRAContainer.swift:
--------------------------------------------------------------------------------
  1 | //
  2 | //  LoRA+Container.swift
  3 | //  mlx-libraries
  4 | //
  5 | //  Created by Ivan Petrukha on 02.06.2025.
  6 | //
  7 | 
  8 | import Foundation
  9 | import MLX
 10 | import MLXNN
 11 | 
 12 | /// Configuration for how LoRA or DoRA should be applied.
 13 | ///
 14 | /// Note: It's compatible with `adapter_config.json` file created during training using MLX libraries.
 15 | ///
 16 | /// Example:
 17 | /// ```json
 18 | /// {
 19 | ///   "fine_tune_type": "lora",
 20 | ///   "num_layers": 28,
 21 | ///   "lora_parameters": {
 22 | ///     "rank": 16,
 23 | ///     "scale": 20.0
 24 | ///   }
 25 | /// }
 26 | /// ```
 27 | public struct LoRAConfiguration: Codable {
 28 | 
 29 |     public enum FineTuneType: String, Codable {
 30 |         case lora
 31 |         case dora
 32 |     }
 33 | 
 34 |     public struct LoRAParameters: Codable {
 35 | 
 36 |         public let rank: Int
 37 |         public let scale: Float
 38 | 
 39 |         public init(rank: Int = 8, scale: Float = 10.0) {
 40 |             self.rank = rank
 41 |             self.scale = scale
 42 |         }
 43 |     }
 44 | 
 45 |     public let numLayers: Int
 46 |     public let fineTuneType: FineTuneType
 47 |     public let loraParameters: LoRAParameters
 48 | 
 49 |     public init(
 50 |         numLayers: Int = 16,
 51 |         fineTuneType: FineTuneType = .lora,
 52 |         loraParameters: LoRAParameters = .init()
 53 |     ) {
 54 |         self.numLayers = numLayers
 55 |         self.fineTuneType = fineTuneType
 56 |         self.loraParameters = loraParameters
 57 |     }
 58 | 
 59 |     enum CodingKeys: String, CodingKey {
 60 |         case numLayers = "num_layers"
 61 |         case fineTuneType = "fine_tune_type"
 62 |         case loraParameters = "lora_parameters"
 63 |     }
 64 | }
 65 | 
 66 | /// A container for managing LoRA or DoRA adapters and applying them to a language model.
 67 | ///
 68 | /// This struct conforms to `ModelAdapter` and can dynamically inject, remove, or fuse adapters into a model at runtime.
 69 | public struct LoRAContainer: ModelAdapter {
 70 | 
 71 |     /// The configuration used to construct this adapter container.
 72 |     public let configuration: LoRAConfiguration
 73 |     /// The parameter values for the adapter modules.
 74 |     public let parameters: ModuleParameters
 75 | 
 76 |     public init(
 77 |         configuration: LoRAConfiguration,
 78 |         parameters: ModuleParameters
 79 |     ) {
 80 |         self.configuration = configuration
 81 |         self.parameters = parameters
 82 |     }
 83 | 
 84 |     /// Creates a `LoRAContainer` by applying the configuration to a compatible `LanguageModel`.
 85 |     ///
 86 |     /// Note:  This function freezes the model base weights and applies LoRA layers to it.
 87 |     public static func from(
 88 |         model: LanguageModel,
 89 |         configuration: LoRAConfiguration = .init()
 90 |     ) throws -> LoRAContainer {
 91 |         guard let lora = model as? LoRAModel else {
 92 |             throw ModelAdapterError.incompatibleModelType
 93 |         }
 94 | 
 95 |         try model.freeze()
 96 |         let layers = lora.loraLinearLayers(configuration.numLayers)
 97 |         replaceLayers(layers: layers) { (layer: Module) in
 98 |             createReplacementLayer(target: layer, configuration: configuration)
 99 |         }
100 | 
101 |         return LoRAContainer(
102 |             configuration: configuration,
103 |             parameters: model.trainableParameters()
104 |         )
105 |     }
106 | 
107 |     /// Loads a `LoRAContainer` from a directory containing adapter weights and configuration.
108 |     public static func from(directory: URL) throws -> LoRAContainer {
109 |         let configurationURL = directory.appending(component: "adapter_config.json")
110 |         let configurationData = try Data(contentsOf: configurationURL)
111 |         let configuration = try JSONDecoder()
112 |             .decode(LoRAConfiguration.self, from: configurationData)
113 | 
114 |         let weightsURL = directory.appending(component: "adapters.safetensors")
115 |         let weights = try MLX.loadArrays(url: weightsURL)
116 |         let parameters = ModuleParameters.unflattened(weights)
117 | 
118 |         return LoRAContainer(
119 |             configuration: configuration,
120 |             parameters: parameters
121 |         )
122 |     }
123 | 
124 |     /// Applies adapter modules (LoRA or DoRA) to the given model.
125 |     ///
126 |     /// This method replaces target layers in the model with corresponding
127 |     /// adapter layers based on the configuration. It also loads adapter-specific
128 |     /// weights into the model.
129 |     public func load(into model: LanguageModel) throws {
130 |         guard let lora = model as? LoRAModel else {
131 |             throw ModelAdapterError.incompatibleModelType
132 |         }
133 | 
134 |         let layers = lora.loraLinearLayers(configuration.numLayers)
135 |         replaceLayers(layers: layers) { (layer: Module) in
136 |             createReplacementLayer(target: layer, configuration: configuration)
137 |         }
138 | 
139 |         try model.update(
140 |             parameters: parameters,
141 |             verify: .noUnusedKeys
142 |         )
143 |     }
144 | 
145 |     /// Permanently fuses the adapter weights into the model's base layers.
146 |     ///
147 |     /// After fusion, adapter weights become part of the model’s original parameters,
148 |     /// and adapter layers are no longer needed.
149 |     public func fuse(with model: LanguageModel) throws {
150 |         guard let lora = model as? LoRAModel else {
151 |             throw ModelAdapterError.incompatibleModelType
152 |         }
153 | 
154 |         let layers = lora.loraLinearLayers(configuration.numLayers)
155 |         replaceLayers(layers: layers) { (layer: Module) in
156 |             if let lora = layer as? LoRALayer {
157 |                 lora.fused()
158 |             } else {
159 |                 createReplacementLayer(target: layer, configuration: configuration)?.fused()
160 |             }
161 |         }
162 |     }
163 | 
164 |     /// Removes adapter layers (LoRA or DoRA) and restores the model to its original form.
165 |     ///
166 |     /// This method reverts each adapted layer to its original linear layer, if possible.
167 |     public func unload(from model: LanguageModel) {
168 |         guard let lora = model as? LoRAModel else {
169 |             return  // Don't throw an error because nothing was likely applied before
170 |         }
171 | 
172 |         let layers = lora.loraLinearLayers(configuration.numLayers)
173 |         replaceLayers(layers: layers) { (lora: LoRALayer) in
174 |             lora.reverted()
175 |         }
176 |     }
177 | }
178 | 
179 | /// Creates an adapter replacement layer for a given linear layer based on the configuration.
180 | private func createReplacementLayer(
181 |     target: Module,
182 |     configuration: LoRAConfiguration
183 | ) -> LoRALayer? {
184 |     switch (target, configuration.fineTuneType) {
185 |     case (let linear as Linear, .lora):
186 |         return LoRALinear.from(
187 |             linear: linear,
188 |             rank: configuration.loraParameters.rank,
189 |             scale: configuration.loraParameters.scale
190 |         )
191 |     case (let linear as Linear, .dora):
192 |         return DoRALinear.from(
193 |             linear: linear,
194 |             rank: configuration.loraParameters.rank,
195 |             scale: configuration.loraParameters.scale
196 |         )
197 |     default:
198 |         return nil
199 |     }
200 | }
201 | 
202 | /// Traverses the model and replaces its layers using a transformation closure.
203 | private func replaceLayers<T>(
204 |     layers: LoRALinearLayers,
205 |     transforming transform: (T) -> Module?
206 | ) {
207 |     for (layer, keys) in layers {
208 |         var update = ModuleChildren()
209 |         let children = layer.children()
210 | 
211 |         for key in keys {
212 |             if let item = children[key], case .value(let child) = item {
213 |                 if let child = child as? T, let transformed = transform(child) {
214 |                     update[key] = .value(transformed)
215 |                 }
216 |             }
217 |         }
218 | 
219 |         if !update.isEmpty {
220 |             layer.update(modules: update)
221 |         }
222 |     }
223 | }
224 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/Adapters/LoRA/LoRAModel.swift:
--------------------------------------------------------------------------------
 1 | //
 2 | //  LoRAModel.swift
 3 | //  mlx-libraries
 4 | //
 5 | //  Created by Ivan Petrukha on 03.06.2025.
 6 | //
 7 | 
 8 | import Foundation
 9 | import MLX
10 | import MLXNN
11 | 
12 | /// Layers to apply LoRA adapters to.
13 | ///
14 | /// This is the value returned by ``LoRAModel/loraLinearLayers()``.
15 | public typealias LoRALinearLayers = [(Module, [String])]
16 | 
17 | public protocol LoRAModel {
18 |     /// Return the layers and keys to apply LoRA adapters to.
19 |     ///
20 |     /// For example this might apply the adapters to the `q` an `v` projections in the
21 |     /// Attention layers:
22 |     ///
23 |     /// ```swift
24 |     /// model.layers.map { ($0.attention, ["q_proj", "v_proj"]) }
25 |     /// ```
26 |     ///
27 |     /// It is not required that a model implement this protocol to have LoRA adapters applied, but
28 |     /// the command line driver example uses this to produce the ``LoRALinearLayers``.
29 |     ///
30 |     /// ### See Also
31 |     /// - ``LoRATrain/convert(model:layers:)``
32 |     func loraLinearLayers() -> LoRALinearLayers
33 | 
34 |     /// Return a suffix of the layers and keys to apply LoRA adapters to.
35 |     ///
36 |     /// See ``loraLinearLayers()``
37 |     func loraLinearLayers(_ count: Int) -> LoRALinearLayers
38 | }
39 | 
40 | extension LoRAModel {
41 |     public func loraLinearLayers(_ count: Int) -> LoRALinearLayers {
42 |         loraLinearLayers().suffix(count)
43 |     }
44 | }
45 | 
46 | /// A protocol representing a module that includes a LoRA adapter and can be converted
47 | /// back to its original, unadapted form.
48 | public protocol LoRALayer: Module {
49 | 
50 |     /// Returns a version of the module with the LoRA adapter permanently fused in.
51 |     func fused() -> Module
52 | 
53 |     /// Returns the original module, without the LoRA adapter applied.
54 |     func reverted() -> Module
55 | }
56 | 
57 | /// Default implementation of `reverted()` for `Linear` layers, including support for quantized layers.
58 | extension LoRALayer where Self: Linear {
59 |     public func reverted() -> Module {
60 |         if let quantized = self as? QuantizedLinear {
61 |             return QuantizedLinear(
62 |                 weight: quantized.weight, bias: quantized.bias,
63 |                 scales: quantized.scales, biases: quantized.biases,
64 |                 groupSize: quantized.groupSize, bits: quantized.bits
65 |             )
66 |         } else {
67 |             return Linear(weight: weight, bias: bias)
68 |         }
69 |     }
70 | }
71 | 
72 | /// Extension for `QuantizedLinear` to provide helper properties.
73 | extension QuantizedLinear {
74 | 
75 |     /// Computes the dequantized weight matrix using the stored quantization parameters.
76 |     var dequantizedWeight: MLXArray {
77 |         dequantized(
78 |             weight,
79 |             scales: scales,
80 |             biases: biases,
81 |             groupSize: groupSize,
82 |             bits: bits
83 |         )
84 |     }
85 | }
86 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/Adapters/ModelAdapter.swift:
--------------------------------------------------------------------------------
  1 | //
  2 | //  ModelAdapters.swift
  3 | //  mlx-libraries
  4 | //
  5 | //  Created by Ivan Petrukha on 02.06.2025.
  6 | //
  7 | 
  8 | import Foundation
  9 | import Hub
 10 | import MLX
 11 | import MLXNN
 12 | 
 13 | /// Errors that can occur when working with a `ModelAdapter`.
 14 | public enum ModelAdapterError: Error {
 15 |     case unsupportedAdapterType(String)
 16 |     case incompatibleModelType
 17 | }
 18 | 
 19 | /// Protocol defining an adapter that can modify a `LanguageModel`.
 20 | public protocol ModelAdapter: Sendable {
 21 | 
 22 |     /// Loads the adapter into the specified model.
 23 |     func load(into model: LanguageModel) throws
 24 | 
 25 |     /// Permanently fuses the adapter into the specified model.
 26 |     func fuse(with model: LanguageModel) throws
 27 | 
 28 |     /// Unloads the adapter from the specified model.
 29 |     func unload(from model: LanguageModel)
 30 | }
 31 | 
 32 | /// Extension to `LanguageModel` providing convenience methods for adapter usage.
 33 | extension LanguageModel {
 34 | 
 35 |     /// Loads an adapter into the model.
 36 |     ///
 37 |     /// Example:
 38 |     /// ```swift
 39 |     /// let model: any LanguageModel = ...
 40 |     /// let adapter: any ModelAdapter = ...
 41 |     /// try model.load(adapter: adapter)
 42 |     /// ```
 43 |     public func load(adapter: ModelAdapter) throws {
 44 |         try adapter.load(into: self)
 45 |     }
 46 | 
 47 |     /// Fuses an adapter permanently into the model.
 48 |     ///
 49 |     /// Example:
 50 |     /// ```swift
 51 |     /// let model: any LanguageModel = ...
 52 |     /// let adapter: any ModelAdapter = ...
 53 |     /// try model.fuse(with: adapter)
 54 |     /// ```
 55 |     public func fuse(with adapter: ModelAdapter) throws {
 56 |         try adapter.fuse(with: self)
 57 |     }
 58 | 
 59 |     /// Unloads an adapter from the model.
 60 |     ///
 61 |     /// Example:
 62 |     /// ```swift
 63 |     /// let model: any LanguageModel = ...
 64 |     /// let adapter: any ModelAdapter = ...
 65 |     /// model.unload(adapter: adapter)
 66 |     /// ```
 67 |     public func unload(adapter: ModelAdapter) {
 68 |         adapter.unload(from: self)
 69 |     }
 70 | 
 71 |     /// Temporarily loads an adapter, performs a synchronous action, then unloads the adapter.
 72 |     ///
 73 |     /// Example:
 74 |     /// ```swift
 75 |     /// let model: any LanguageModel = ...
 76 |     /// let adapter: any ModelAdapter = ...
 77 |     /// try model.perform(with: adapter) {
 78 |     ///     generate(inputs: ...)
 79 |     /// }
 80 |     /// // Adapter is automatically unloaded after execution
 81 |     /// ```
 82 |     public func perform<R>(
 83 |         with adapter: ModelAdapter, perform: () throws -> R
 84 |     ) throws -> R {
 85 |         defer {
 86 |             adapter.unload(from: self)
 87 |         }
 88 |         try adapter.load(into: self)
 89 |         let result = try perform()
 90 |         return result
 91 |     }
 92 | 
 93 |     /// Temporarily loads an adapter, performs an asynchronous action, then unloads the adapter.
 94 |     ///
 95 |     /// Example:
 96 |     /// ```swift
 97 |     /// let model: any LanguageModel = ...
 98 |     /// let adapter: any ModelAdapter = ...
 99 |     /// try await model.perform(with: adapter) {
100 |     ///     await generate(inputs: ...)
101 |     /// }
102 |     /// // Adapter is automatically unloaded after execution
103 |     /// ```
104 |     public func perform<R>(
105 |         with adapter: ModelAdapter, perform: () async throws -> R
106 |     ) async throws -> R {
107 |         defer {
108 |             adapter.unload(from: self)
109 |         }
110 |         try adapter.load(into: self)
111 |         let result = try await perform()
112 |         return result
113 |     }
114 | }
115 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/Adapters/ModelAdapterFactory.swift:
--------------------------------------------------------------------------------
 1 | //
 2 | //  ModelAdapterFactory.swift
 3 | //  mlx-libraries
 4 | //
 5 | //  Created by Ivan Petrukha on 03.06.2025.
 6 | //
 7 | 
 8 | import Foundation
 9 | import Hub
10 | import MLX
11 | import MLXNN
12 | 
13 | /// Base configuration for any adapter.
14 | ///
15 | /// This struct is parsed by `ModelAdapterFactory` to determine which adapter creator
16 | /// to invoke from the registry. It expects an `adapter_config.json` file containing
17 | /// a `fine_tune_type` field that specifies the adapter type as a string (e.g., "lora", "dora").
18 | ///
19 | /// Note: This configuration does not consider adapter-specific parameters.
20 | ///
21 | /// Example:
22 | /// ```json
23 | /// {
24 | ///   "fine_tune_type": "lora",     // Required
25 | ///   "additional_field": true      // Ignored here
26 | /// }
27 | /// ```
28 | private struct ModelAdapterBaseConfiguration: Decodable {
29 | 
30 |     let fineTuneType: String
31 | 
32 |     enum CodingKeys: String, CodingKey {
33 |         case fineTuneType = "fine_tune_type"
34 |     }
35 | }
36 | 
37 | /// A factory responsible for loading and creating model adapters from hub configurations.
38 | public final class ModelAdapterFactory {
39 | 
40 |     /// Shared instance of the adapter factory.
41 |     public static let shared = ModelAdapterFactory(
42 |         registry: ModelAdapterTypeRegistry(creators: [
43 |             "lora": LoRAContainer.from(directory:),
44 |             "dora": LoRAContainer.from(directory:),
45 |         ])
46 |     )
47 | 
48 |     /// Registry of adapter type creators.
49 |     ///
50 |     /// You can register custom adapter types as follows:
51 |     /// ```swift
52 |     /// let registry = ModelAdapterFactory.shared.registry
53 |     /// let adapterType = "my-adapter"
54 |     /// let adapterCreator = MyAdapterContainer.from(directory:)
55 |     /// registry.registerAdapterType(adapterType, creator: adapterCreator)
56 |     /// ```
57 |     ///
58 |     /// This allows the factory to load your custom adapter from automatically
59 |     /// when the matching type is found in a configuration file.
60 |     public let registry: ModelAdapterTypeRegistry
61 | 
62 |     public init(registry: ModelAdapterTypeRegistry) {
63 |         self.registry = registry
64 |     }
65 | 
66 |     /// Loads a model adapter from the hub using the provided model configuration.
67 |     ///
68 |     /// This method fetches the adapter configuration and weights, decodes the appropriate
69 |     /// fine-tuning format, and initializes a `ModelAdapter` accordingly.
70 |     public func load(
71 |         hub: HubApi = HubApi(),
72 |         configuration: ModelConfiguration,
73 |         progressHandler: @Sendable @escaping (Progress) -> Void = { _ in }
74 |     ) async throws -> ModelAdapter {
75 |         let adapterDirectory = try await downloadModel(
76 |             hub: hub, configuration: configuration, progressHandler: progressHandler
77 |         )
78 | 
79 |         let configurationURL = adapterDirectory.appending(component: "adapter_config.json")
80 |         let configurationData = try Data(contentsOf: configurationURL)
81 |         let configuration = try JSONDecoder()
82 |             .decode(ModelAdapterBaseConfiguration.self, from: configurationData)
83 | 
84 |         return try registry.createAdapter(
85 |             directory: adapterDirectory,
86 |             adapterType: configuration.fineTuneType
87 |         )
88 |     }
89 | }
90 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/Adapters/ModelAdapterTypeRegistry.swift:
--------------------------------------------------------------------------------
 1 | //
 2 | //  ModelAdapterTypeRegistry.swift
 3 | //  mlx-libraries
 4 | //
 5 | //  Created by Ivan Petrukha on 06.06.2025.
 6 | //
 7 | 
 8 | import Foundation
 9 | 
10 | public class ModelAdapterTypeRegistry: @unchecked Sendable {
11 | 
12 |     /// Creates an empty registry.
13 |     public init() {
14 |         self.creators = [:]
15 |     }
16 | 
17 |     /// Creates a registry with given creators.
18 |     public init(creators: [String: @Sendable (URL) throws -> any ModelAdapter]) {
19 |         self.creators = creators
20 |     }
21 | 
22 |     // Note: using NSLock as we have very small (just dictionary get/set)
23 |     // critical sections and expect no contention.  this allows the methods
24 |     // to remain synchronous.
25 |     private let lock = NSLock()
26 |     private var creators: [String: @Sendable (URL) throws -> any ModelAdapter]
27 | 
28 |     /// Add a new model adapter to the type registry.
29 |     public func registerAdapterType(
30 |         _ type: String, creator: @Sendable @escaping (URL) throws -> any ModelAdapter
31 |     ) {
32 |         lock.withLock {
33 |             creators[type] = creator
34 |         }
35 |     }
36 | 
37 |     public func createAdapter(directory: URL, adapterType: String) throws -> ModelAdapter {
38 |         let creator = lock.withLock {
39 |             creators[adapterType]
40 |         }
41 |         guard let creator else {
42 |             throw ModelAdapterError.unsupportedAdapterType(adapterType)
43 |         }
44 |         return try creator(directory)
45 |     }
46 | }
47 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/AttentionUtils.swift:
--------------------------------------------------------------------------------
 1 | import Foundation
 2 | import MLX
 3 | import MLXFast
 4 | 
 5 | /// Attention utilities that match Python mlx-lm's interface
 6 | ///
 7 | /// This provides a single function that automatically routes to quantized or regular
 8 | /// attention based on cache type, matching Python's `scaled_dot_product_attention`
 9 | 
10 | /// Automatic attention with cache update
11 | ///
12 | /// This function matches Python's `scaled_dot_product_attention` in base.py:
13 | /// - Detects if cache is `QuantizedKVCache` using `isinstance` pattern
14 | /// - Routes to `quantizedScaledDotProductAttention` or `MLXFast.scaledDotProductAttention`
15 | /// - Handles cache updating automatically
16 | /// - Transparent to models - they just call this function
17 | ///
18 | /// **Usage in models:**
19 | /// ```swift
20 | /// let output = attentionWithCacheUpdate(
21 | ///     queries: queries,
22 | ///     keys: keys,
23 | ///     values: values,
24 | ///     cache: cache,
25 | ///     scale: scale,
26 | ///     mask: mask
27 | /// )
28 | /// ```
29 | ///
30 | /// - Parameters:
31 | ///   - queries: Query tensor [B, nHeads, L, D]
32 | ///   - keys: Raw key tensor to be cached [B, nKVHeads, L, D]
33 | ///   - values: Raw value tensor to be cached [B, nKVHeads, L, D]
34 | ///   - cache: Cache instance (any type)
35 | ///   - scale: Attention scale factor
36 | ///   - mask: Attention mask
37 | /// - Returns: Attention output [B, nHeads, L, D]
38 | public func attentionWithCacheUpdate(
39 |     queries: MLXArray,
40 |     keys: MLXArray,
41 |     values: MLXArray,
42 |     cache: KVCache?,
43 |     scale: Float,
44 |     mask: MLXFast.ScaledDotProductAttentionMaskMode = .none
45 | ) -> MLXArray {
46 |     guard let cache else {
47 |         return MLXFast.scaledDotProductAttention(
48 |             queries: queries,
49 |             keys: keys,
50 |             values: values,
51 |             scale: scale,
52 |             mask: mask
53 |         )
54 |     }
55 |     if let quantizedKVCache = cache as? QuantizedKVCache {
56 |         let (quantizedKeys, quantizedValues) = quantizedKVCache.updateQuantized(
57 |             keys: keys, values: values)
58 |         return quantizedScaledDotProductAttention(
59 |             queries: queries,
60 |             quantizedKeys: quantizedKeys,
61 |             quantizedValues: quantizedValues,
62 |             scale: scale,
63 |             mask: mask,
64 |             groupSize: quantizedKVCache.groupSize,
65 |             bits: quantizedKVCache.bits
66 |         )
67 |     } else {
68 |         let (cachedKeys, cachedValues) = cache.update(keys: keys, values: values)
69 |         return MLXFast.scaledDotProductAttention(
70 |             queries: queries,
71 |             keys: cachedKeys,
72 |             values: cachedValues,
73 |             scale: scale,
74 |             mask: mask
75 |         )
76 |     }
77 | }
78 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/BaseConfiguration.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2025 Apple Inc.
  2 | 
  3 | import Foundation
  4 | 
  5 | /// Base ``LanguageModel`` configuration -- provides `modelType`
  6 | /// and `quantization` (used in loading the model).
  7 | ///
  8 | /// This is used by ``ModelFactory/load(hub:configuration:progressHandler:)``
  9 | /// to determine the type of model to load.
 10 | public struct BaseConfiguration: Codable, Sendable {
 11 |     public let modelType: String
 12 | 
 13 |     public struct Quantization: Codable, Sendable, Equatable {
 14 |         public init(groupSize: Int, bits: Int) {
 15 |             self.groupSize = groupSize
 16 |             self.bits = bits
 17 |         }
 18 | 
 19 |         public let groupSize: Int
 20 |         public let bits: Int
 21 | 
 22 |         public var asTuple: (Int, Int) { (groupSize, bits) }
 23 | 
 24 |         enum CodingKeys: String, CodingKey {
 25 |             case groupSize = "group_size"
 26 |             case bits = "bits"
 27 |         }
 28 |     }
 29 | 
 30 |     /// handling instructions for ``PerLayerQuantization``
 31 |     public enum QuantizationOption: Sendable {
 32 |         case skip
 33 |         case quantize(Quantization)
 34 |     }
 35 | 
 36 |     /// Per-layer ``Quantization`` values with optional default.
 37 |     public struct PerLayerQuantization: Sendable {
 38 |         public var quantization: Quantization? = nil
 39 |         public var perLayerQuantization: [String: QuantizationOption]
 40 | 
 41 |         public init(
 42 |             quantization: BaseConfiguration.Quantization? = nil,
 43 |             perLayerQuantization: [String: BaseConfiguration.QuantizationOption]
 44 |         ) {
 45 |             self.quantization = quantization
 46 |             self.perLayerQuantization = perLayerQuantization
 47 |         }
 48 | 
 49 |         /// The quantization to apply for the given layer name or nil for no quantization.
 50 |         public func quantization(layer: String) -> Quantization? {
 51 |             if let perLayer = perLayerQuantization[layer] {
 52 |                 switch perLayer {
 53 |                 case .skip:
 54 |                     return nil
 55 |                 case .quantize(let quantization):
 56 |                     return quantization
 57 |                 }
 58 |             } else {
 59 |                 return quantization
 60 |             }
 61 |         }
 62 |     }
 63 | 
 64 |     /// Special codable to support a mixed key: Int / key: Quantization
 65 |     /// structure for hereogenous quantization, e.g.
 66 |     ///
 67 |     /// ```
 68 |     /// "quantization": {
 69 |     ///     "group_size": 64,
 70 |     ///     "bits": 4,
 71 |     ///     "model.embed_tokens": {
 72 |     ///         "group_size": 32,
 73 |     ///         "bits": 4
 74 |     ///     },
 75 |     ///     "model.layers.0.self_attn.q_norm": false,
 76 |     /// ```
 77 |     ///
 78 |     /// This mixed type structure requires manual decoding.
 79 |     struct QuantizationContainer: Codable, Sendable {
 80 |         var quantization: Quantization
 81 |         var perLayerQuantization: PerLayerQuantization
 82 | 
 83 |         // based on Dictionary's coding key
 84 |         internal struct _DictionaryCodingKey: CodingKey {
 85 |             internal let stringValue: String
 86 |             internal let intValue: Int?
 87 | 
 88 |             internal init(stringValue: String) {
 89 |                 self.stringValue = stringValue
 90 |                 self.intValue = Int(stringValue)
 91 |             }
 92 | 
 93 |             internal init(intValue: Int) {
 94 |                 self.stringValue = "\(intValue)"
 95 |                 self.intValue = intValue
 96 |             }
 97 |         }
 98 | 
 99 |         init(from decoder: any Decoder) throws {
100 |             // handle the embedded Quantization
101 |             self.quantization = try Quantization(from: decoder)
102 | 
103 |             // and the interleaved per-layer values
104 |             var perLayerQuantization = [String: QuantizationOption]()
105 |             let container = try decoder.container(keyedBy: _DictionaryCodingKey.self)
106 |             for key in container.allKeys {
107 |                 switch key.stringValue {
108 |                 case Quantization.CodingKeys.groupSize.rawValue: continue
109 |                 case Quantization.CodingKeys.bits.rawValue: continue
110 | 
111 |                 default:
112 |                     if let f = try? container.decode(Bool.self, forKey: key) {
113 |                         if !f {
114 |                             perLayerQuantization[key.stringValue] = .skip
115 |                         }
116 |                     } else {
117 |                         perLayerQuantization[key.stringValue] = .quantize(
118 |                             try container.decode(Quantization.self, forKey: key))
119 |                     }
120 |                 }
121 |             }
122 |             self.perLayerQuantization = PerLayerQuantization(
123 |                 quantization: quantization, perLayerQuantization: perLayerQuantization)
124 |         }
125 | 
126 |         func encode(to encoder: any Encoder) throws {
127 |             try quantization.encode(to: encoder)
128 | 
129 |             var container = encoder.container(keyedBy: _DictionaryCodingKey.self)
130 |             for (key, value) in perLayerQuantization.perLayerQuantization {
131 |                 switch value {
132 |                 case .skip:
133 |                     try container.encode(false, forKey: .init(stringValue: key))
134 |                 case .quantize(let q):
135 |                     try container.encode(q, forKey: .init(stringValue: key))
136 |                 }
137 |             }
138 |         }
139 |     }
140 | 
141 |     var quantizationContainer: QuantizationContainer?
142 | 
143 |     @available(*, deprecated, message: "Please use perLayerQuantization instead")
144 |     public var quantization: Quantization? {
145 |         quantizationContainer?.quantization
146 |     }
147 | 
148 |     public var perLayerQuantization: PerLayerQuantization? {
149 |         quantizationContainer?.perLayerQuantization
150 |     }
151 | 
152 |     enum CodingKeys: String, CodingKey {
153 |         case modelType = "model_type"
154 |         case quantizationContainer = "quantization"
155 |     }
156 | }
157 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/Chat.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2025 Apple Inc.
  2 | 
  3 | public enum Chat {
  4 |     public struct Message {
  5 |         /// The role of the message sender.
  6 |         public var role: Role
  7 | 
  8 |         /// The content of the message.
  9 |         public var content: String
 10 | 
 11 |         /// Array of image data associated with the message.
 12 |         public var images: [UserInput.Image]
 13 | 
 14 |         /// Array of video data associated with the message.
 15 |         public var videos: [UserInput.Video]
 16 | 
 17 |         public init(
 18 |             role: Role, content: String, images: [UserInput.Image] = [],
 19 |             videos: [UserInput.Video] = []
 20 |         ) {
 21 |             self.role = role
 22 |             self.content = content
 23 |             self.images = images
 24 |             self.videos = videos
 25 |         }
 26 | 
 27 |         public static func system(
 28 |             _ content: String, images: [UserInput.Image] = [], videos: [UserInput.Video] = []
 29 |         ) -> Self {
 30 |             Self(role: .system, content: content, images: images, videos: videos)
 31 |         }
 32 | 
 33 |         public static func assistant(
 34 |             _ content: String, images: [UserInput.Image] = [], videos: [UserInput.Video] = []
 35 |         ) -> Self {
 36 |             Self(role: .assistant, content: content, images: images, videos: videos)
 37 |         }
 38 | 
 39 |         public static func user(
 40 |             _ content: String, images: [UserInput.Image] = [], videos: [UserInput.Video] = []
 41 |         ) -> Self {
 42 |             Self(role: .user, content: content, images: images, videos: videos)
 43 |         }
 44 | 
 45 |         public static func tool(_ content: String) -> Self {
 46 |             Self(role: .tool, content: content)
 47 |         }
 48 | 
 49 |         public enum Role: String {
 50 |             case user
 51 |             case assistant
 52 |             case system
 53 |             case tool
 54 |         }
 55 |     }
 56 | }
 57 | 
 58 | /// Protocol for something that can convert structured
 59 | /// ``Chat.Message`` into model specific ``Message``
 60 | /// (raw dictionary) format.
 61 | ///
 62 | /// Typically this is owned and used by a ``UserInputProcessor``:
 63 | ///
 64 | /// ```swift
 65 | /// public func prepare(input: UserInput) async throws -> LMInput {
 66 | ///     let messages = Qwen2VLMessageGenerator().generate(from: input)
 67 | ///     ...
 68 | /// ```
 69 | public protocol MessageGenerator {
 70 | 
 71 |     /// Generates messages from the input.
 72 |     func generate(from input: UserInput) -> [Message]
 73 | 
 74 |     /// Returns array of `[String: Any]` aka ``Message``
 75 |     func generate(messages: [Chat.Message]) -> [Message]
 76 | 
 77 |     /// Returns `[String: Any]` aka ``Message``.
 78 |     func generate(message: Chat.Message) -> Message
 79 | }
 80 | 
 81 | extension MessageGenerator {
 82 | 
 83 |     public func generate(message: Chat.Message) -> Message {
 84 |         [
 85 |             "role": message.role.rawValue,
 86 |             "content": message.content,
 87 |         ]
 88 |     }
 89 | 
 90 |     public func generate(messages: [Chat.Message]) -> [Message] {
 91 |         var rawMessages: [Message] = []
 92 | 
 93 |         for message in messages {
 94 |             let raw = generate(message: message)
 95 |             rawMessages.append(raw)
 96 |         }
 97 | 
 98 |         return rawMessages
 99 |     }
100 | 
101 |     public func generate(from input: UserInput) -> [Message] {
102 |         switch input.prompt {
103 |         case .text(let text):
104 |             generate(messages: [.user(text)])
105 |         case .messages(let messages):
106 |             messages
107 |         case .chat(let messages):
108 |             generate(messages: messages)
109 |         }
110 |     }
111 | }
112 | 
113 | /// Default implementation of ``MessageGenerator`` that produces a
114 | /// `role` and `content`.
115 | ///
116 | /// ```swift
117 | /// [
118 | ///     "role": message.role.rawValue,
119 | ///     "content": message.content,
120 | /// ]
121 | /// ```
122 | public struct DefaultMessageGenerator: MessageGenerator {
123 |     public init() {}
124 | 
125 |     public func generate(message: Chat.Message) -> Message {
126 |         [
127 |             "role": message.role.rawValue,
128 |             "content": message.content,
129 |         ]
130 |     }
131 | }
132 | 
133 | /// Implementation of ``MessageGenerator`` that produces a
134 | /// `role` and `content` but omits `system` roles.
135 | ///
136 | /// ```swift
137 | /// [
138 | ///     "role": message.role.rawValue,
139 | ///     "content": message.content,
140 | /// ]
141 | /// ```
142 | public struct NoSystemMessageGenerator: MessageGenerator {
143 |     public init() {}
144 | 
145 |     public func generate(messages: [Chat.Message]) -> [Message] {
146 |         messages
147 |             .filter { $0.role != .system }
148 |             .map { generate(message: $0) }
149 |     }
150 | }
151 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/Documentation.docc/Documentation.md:
--------------------------------------------------------------------------------
 1 | # ``MLXLMCommon``
 2 | 
 3 | Common language model code.
 4 | 
 5 | ## Other MLX Libraries Packages
 6 | 
 7 | - [MLXEmbedders](MLXEmbedders)
 8 | - [MLXLLM](MLXLLM)
 9 | - [MLXLMCommon](MLXLMCommon)
10 | - [MLXMNIST](MLXMNIST)
11 | - [MLXVLM](MLXVLM)
12 | - [StableDiffusion](StableDiffusion)
13 | 
14 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/Documentation.docc/porting.md:
--------------------------------------------------------------------------------
  1 | # Porting Models
  2 | 
  3 | How to make new models using `mlx-swift`.
  4 | 
  5 | There are a number of ways to implement new models in `MLX` (Swift):
  6 | 
  7 | - built from scratch
  8 | - ported from other ML frameworks
  9 |     - [MLX Documentation](https://ml-explore.github.io/mlx/build/html/examples/llama-inference.html)
 10 | - ported from Python, [e.g. `mlx-lm`](https://github.com/ml-explore/mlx-lm/tree/main/mlx_lm/models)
 11 | 
 12 | This document talks primarily about the latter.
 13 | 
 14 | ## Porting Models from MLX (Python)
 15 | 
 16 | Let's consider a concrete example,
 17 | [gemma.py](https://github.com/ml-explore/mlx-lm/blob/main/mlx_lm/models/gemma.py). For
 18 | reference, here is the current port
 19 | [Gemma.swift](https://github.com/ml-explore/mlx-swift-examples/blob/main/Libraries/MLXLLM/Models/Gemma.swift).
 20 | 
 21 | ### Imports
 22 | 
 23 | When creating the new model you need to import the right modules -- typically these are sufficient:
 24 | 
 25 | ```swift
 26 | import Foundation
 27 | import MLX
 28 | import MLXLMCommon
 29 | import MLXNN
 30 | ```
 31 | 
 32 | - Foundation
 33 |     - used for standard Swift features like Codable -- this let's us easily read the JSON configuration
 34 | - MLX
 35 |     - the base [MLXArray](https://swiftpackageindex.com/ml-explore/mlx-swift/main/documentation/mlx) framework
 36 | - MLXLMCommon
 37 |     - language model support code (this library)
 38 |     - provides weight loading, token generation, etc.
 39 | - MLXNN
 40 |     - the base [Module and NN](https://swiftpackageindex.com/ml-explore/mlx-swift/main/documentation/mlxnn) package for MLX
 41 | 
 42 | ### Configuration
 43 | 
 44 | Next port the configuration type from python -- you can see it at the top of the `gemma.py` file:
 45 | 
 46 | ```python
 47 | @dataclass
 48 | class ModelArgs(BaseModelArgs):
 49 |     model_type: str
 50 |     hidden_size: int
 51 |     num_hidden_layers: int
 52 |     intermediate_size: int
 53 |     num_attention_heads: int
 54 |     head_dim: int
 55 |     rms_norm_eps: float
 56 |     vocab_size: int
 57 |     num_key_value_heads: int
 58 |     rope_theta: float = 10000
 59 |     rope_traditional: bool = False
 60 | ```
 61 | 
 62 | This will be loaded from a JSON file and used to configure the model -- both layer parameters
 63 | like `rms_norm_eps` and structure like `num_hidden_layers`.
 64 | 
 65 | This translates naturally into a ``Codable`` struct in Swift with a few details:
 66 | 
 67 | - the keys in the JSON file will be `snake_case` -- the simplest way to accomodate that is to specify `CodingKeys` to name them explicitly
 68 | - some of the parameters have default values
 69 | 
 70 | ```swift
 71 | public struct GemmaConfiguration: Codable, Sendable {
 72 |     var modelType: String
 73 |     var hiddenSize: Int
 74 |     var hiddenLayers: Int
 75 |     var intermediateSize: Int
 76 |     var attentionHeads: Int
 77 |     var headDimensions: Int
 78 |     var rmsNormEps: Float
 79 |     var vocabularySize: Int
 80 |     var kvHeads: Int
 81 |     private let _ropeTheta: Float?
 82 |     public var ropeTheta: Float { _ropeTheta ?? 10_000 }
 83 |     private let _ropeTraditional: Bool?
 84 |     public var ropeTraditional: Bool { _ropeTraditional ?? false }
 85 | 
 86 |     enum CodingKeys: String, CodingKey {
 87 |         case modelType = "model_type"
 88 |         case hiddenSize = "hidden_size"
 89 |         case hiddenLayers = "num_hidden_layers"
 90 |         case intermediateSize = "intermediate_size"
 91 |         case attentionHeads = "num_attention_heads"
 92 |         case headDimensions = "head_dim"
 93 |         case rmsNormEps = "rms_norm_eps"
 94 |         case vocabularySize = "vocab_size"
 95 |         case kvHeads = "num_key_value_heads"
 96 |         case _ropeTheta = "rope_theta"
 97 |         case _ropeTraditional = "rope_traditional"
 98 |     }
 99 | }
100 | ```
101 | 
102 | * Note: the type is called `ModelArgs` in Python and
103 | this is scoped to the file because of the way Python importing
104 | works. In Swift we need this type to be public so we give it
105 | a unique name, `GemmaConfiguration`.
106 | 
107 | The default values are implemented with this pattern:
108 | 
109 | ```swift
110 | private let _ropeTheta: Float?
111 | public var ropeTheta: Float { _ropeTheta ?? 10_000 }
112 | ```
113 | 
114 | and then the `CodingKeys` case is for `_ropeTheta`:
115 | 
116 | ```swift
117 | enum CodingKeys: String, CodingKey {
118 | ...
119 |     case _ropeTheta = "rope_theta"
120 | ```
121 | 
122 | This will read `rope_theta` from the JSON file but apply a default value of `10_000` if
123 | no value is given.
124 | 
125 | ### Porting Layers -- No Children
126 | 
127 | Now we can begin porting the layers (Modules). Here is an example layer with
128 | no child layers (e.g. `Linear`) but it does have parameters (e.g. `MLXArray`).
129 | 
130 | ```python
131 | class RMSNorm(nn.Module):
132 |     def __init__(self, dims: int, eps: float = 1e-5):
133 |         super().__init__()
134 |         self.weight = mx.ones((dims,))
135 |         self.eps = eps
136 | 
137 |     def __call__(self, x):
138 |         return mx.fast.rms_norm(x, 1.0 + self.weight, self.eps)
139 | ```
140 | 
141 | and the equivalent Swift code:
142 | 
143 | ```swift
144 | private class RMSNorm: Module, UnaryLayer {
145 |     let weight: MLXArray
146 |     let eps: Float
147 | 
148 |     public init(dimensions: Int, eps: Float = 1e-5) {
149 |         self.weight = MLXArray.ones([dimensions])
150 |         self.eps = eps
151 |     }
152 | 
153 |     public func callAsFunction(_ x: MLXArray) -> MLXArray {
154 |         return MLXFast.rmsNorm(x, weight: 1.0 + self.weight, eps: self.eps)
155 |     }
156 | }
157 | ```
158 | 
159 | * Note: the Modules that make up the layers in the model are typically declared as `private` -- many models will use similarly named layers and this prevents the names from leaking between models.
160 | 
161 | Here is a detailed breakdown of the conversion. Consider the python initializer
162 | for the class:
163 | 
164 | ```python
165 | def __init__(self, dims: int, eps: float = 1e-5):
166 |     super().__init__()
167 |     self.weight = mx.ones((dims,))
168 |     self.eps = eps
169 | ```
170 | 
171 | In Python, storing a value into a property is how instance variables (properties) are
172 | created -- it is a dictionary behind the scenes. Swift requires that properties be
173 | declared and given types:
174 | 
175 | ```swift
176 | let weight: MLXArray
177 | let eps: Float
178 | 
179 | public init(dimensions: Int, eps: Float = 1e-5) {
180 |     self.weight = MLXArray.ones([dimensions])
181 |     self.eps = eps
182 | }
183 | ```
184 | 
185 | Note that the weight is given an initial value and shape based on the
186 | parameters to the initializer. In typical inference use these values
187 | will be replaced when the weights are loaded 
188 | (``loadWeights(modelDirectory:model:quantization:)``).
189 | 
190 | * Note:
191 | If the property names in Python don't make good Swift names you can use the `@ParameterInfo` property wrapper to specify the key:\
192 | \
193 | `@ParameterInfo(key: "some_weight") var weight: MLXArray`
194 | 
195 | If using the `@ParameterInfo` to override the parameter
196 | key, be aware that the syntax for initializing the value
197 | changes:
198 | 
199 | ```swift
200 | // was: self.weight = MLXArray.ones([dimensions])
201 | self._weight.wrappedValue = MLXArray.ones([dimensions])
202 | ```
203 | 
204 | Next, the `__call__` method in python is a direct conversion to Swift:
205 | 
206 | ```python
207 | def __call__(self, x):
208 |     return mx.fast.rms_norm(x, 1.0 + self.weight, self.eps)
209 | ```
210 | 
211 | becomes:
212 | 
213 | ```swift
214 | public func callAsFunction(_ x: MLXArray) -> MLXArray {
215 |     return MLXFast.rmsNorm(x, weight: 1.0 + self.weight, eps: self.eps)
216 | }
217 | ```
218 | 
219 | * Note:
220 | [This reference](https://swiftpackageindex.com/ml-explore/mlx-swift/main/documentation/mlx/converting-python) shows the mapping between Python method and function names and Swift.
221 | 
222 | ### Porting Layers -- Children
223 | 
224 | Consider this module from Python that uses the previously defined `RMSNorm` module:
225 | 
226 | ```python
227 | class TransformerBlock(nn.Module):
228 |     def __init__(self, args: ModelArgs):
229 |         super().__init__()
230 |         self.num_attention_heads = args.num_attention_heads
231 |         self.hidden_size = args.hidden_size
232 |         self.self_attn = Attention(args)
233 |         self.mlp = MLP(args.hidden_size, args.intermediate_size)
234 |         self.input_layernorm = RMSNorm(args.hidden_size, eps=args.rms_norm_eps)
235 |         self.post_attention_layernorm = RMSNorm(args.hidden_size, eps=args.rms_norm_eps)
236 |         self.args = args
237 | 
238 |     def __call__(
239 |         self,
240 |         x: mx.array,
241 |         mask: Optional[mx.array] = None,
242 |         cache: Optional[Any] = None,
243 |     ) -> mx.array:
244 |         r = self.self_attn(self.input_layernorm(x), mask, cache)
245 |         h = x + r
246 |         r = self.mlp(self.post_attention_layernorm(h))
247 |         out = h + r
248 |         return out
249 | ```
250 | 
251 | and the full conversion to Swift:
252 | 
253 | ```swift
254 | private class TransformerBlock: Module {
255 |     @ModuleInfo(key: "self_attn") var attention: Attention
256 |     let mlp: MLP
257 | 
258 |     @ModuleInfo(key: "input_layernorm") var inputLayerNorm: RMSNorm
259 |     @ModuleInfo(key: "post_attention_layernorm") var postAttentionLayerNorm: RMSNorm
260 | 
261 |     public init(_ args: GemmaConfiguration) {
262 |         self._attention.wrappedValue = Attention(args)
263 |         self.mlp = MLP(dimensions: args.hiddenSize, hiddenDimensions: args.intermediateSize)
264 |         self._inputLayerNorm.wrappedValue = RMSNorm(
265 |             dimensions: args.hiddenSize, eps: args.rmsNormEps)
266 |         self._postAttentionLayerNorm.wrappedValue = RMSNorm(
267 |             dimensions: args.hiddenSize, eps: args.rmsNormEps)
268 |     }
269 | 
270 |     public func callAsFunction(
271 |         _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
272 |     ) -> MLXArray {
273 |         var r = attention(inputLayerNorm(x), mask: mask, cache: cache)
274 |         let h = x + r
275 |         r = mlp(postAttentionLayerNorm(h))
276 |         return h + r
277 |     }
278 | }
279 | ```
280 | 
281 | As seen previously the `__init__` method converts to an
282 | initializer for the class:
283 | 
284 | ```python
285 | def __init__(self, args: ModelArgs):
286 |     super().__init__()
287 |     self.num_attention_heads = args.num_attention_heads
288 |     self.hidden_size = args.hidden_size
289 |     self.self_attn = Attention(args)
290 |     self.mlp = MLP(args.hidden_size, args.intermediate_size)
291 |     ...
292 | ```
293 | 
294 | This initializer takes `ModelArgs` in Python -- this is the name of the configuration type, which we call `GemmaConfiguration`.
295 | 
296 | ```swift
297 | @ModuleInfo(key: "self_attn") var attention: Attention
298 | let mlp: MLP
299 | 
300 | ...
301 | 
302 | public init(_ args: GemmaConfiguration) {
303 |     self._attention.wrappedValue = Attention(args)
304 |     self.mlp = MLP(dimensions: args.hiddenSize, hiddenDimensions: args.intermediateSize)
305 |     ...
306 | ```
307 | 
308 | Note that some of the lines from Python are omitted:
309 | 
310 | ```python
311 | self.num_attention_heads = args.num_attention_heads
312 | self.hidden_size = args.hidden_size
313 | ```
314 | 
315 | These properties are not used in the code and can be discarded.
316 | If there are many properties that _are_ needed, it may be more
317 | convenient in Swift to simply store the `GemmaConfiguration` --
318 | that provides access to the typed properties inside.
319 | 
320 | * Note:
321 | Much like parameters with non-Swift names, you can (and typically do) use `@ModuleInfo` to give the same naming hint:\
322 | \
323 | `@ModuleInfo(key: "self_attn") var attention: Attention`
324 | 
325 | Finally we convert the `__call__` method from Python:
326 | 
327 | ```python
328 | def __call__(
329 |     self,
330 |     x: mx.array,
331 |     mask: Optional[mx.array] = None,
332 |     cache: Optional[Any] = None,
333 | ) -> mx.array:
334 |     r = self.self_attn(self.input_layernorm(x), mask, cache)
335 |     h = x + r
336 |     r = self.mlp(self.post_attention_layernorm(h))
337 |     out = h + r
338 |     return out
339 | ```
340 | 
341 | Note that the `r` variable is assigned twice so we make this a `var` in Swift:
342 | 
343 | ```swift
344 | public func callAsFunction(
345 |     _ x: MLXArray, mask: MLXArray? = nil, cache: KVCache?
346 | ) -> MLXArray {
347 |     var r = attention(inputLayerNorm(x), mask: mask, cache: cache)
348 |     let h = x + r
349 |     r = mlp(postAttentionLayerNorm(h))
350 |     return h + r
351 | }
352 | ```
353 | 
354 | Sometimes the input parameter is assigned in this function -- to keep the
355 | code structure as close as possible to the Python:
356 | 
357 | ```python
358 | def __call__(self, x) -> mx.array:
359 |     x = x + 1
360 |     x = mx.sqrt(x)
361 |     return x
362 | ```
363 | 
364 | I suggest porting this code as:
365 | 
366 | ```swift
367 | public func callAsFunction(_ x: MLXArray) -> MLXArray {
368 |     var x = x
369 |     x = x + 1
370 |     x = sqrt(x)
371 |     return x
372 | }
373 | ```
374 | 
375 | ### Porting Layers -- Configuration and Structure
376 | 
377 | Sometimes the configuration drives the structure of the model:
378 | 
379 | ```python
380 | class GemmaModel(nn.Module):
381 |     def __init__(self, args: ModelArgs):
382 |         super().__init__()
383 |         ...
384 |         self.layers = [
385 |             TransformerBlock(args=args) for _ in range(args.num_hidden_layers)
386 |         ]
387 | 
388 |     def __call__(
389 |         self,
390 |         inputs: mx.array,
391 |         mask: mx.array = None,
392 |         cache=None,
393 |     ):
394 |         ...
395 |         for layer, c in zip(self.layers, cache):
396 |             h = layer(h, mask, c)
397 |         ...
398 | ```
399 | 
400 | In this case the number of `TransformerBlock` layers depends on
401 | the configuration value `num_hidden_layers` -- it builds an
402 | array of children. In the `__call__` it iterates through these
403 | children, chaining the calls together sequentially.
404 | 
405 | In Swift you do the same thing:
406 | 
407 | ```swift
408 | private class GemmaModelInner: Module {
409 |     ...
410 |     fileprivate let layers: [TransformerBlock]
411 | 
412 |     public init(_ args: GemmaConfiguration) {
413 |         ...
414 |         self.layers = (0 ..< args.hiddenLayers)
415 |             .map { _ in
416 |                 TransformerBlock(args)
417 |             }
418 |     }
419 | 
420 |     public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]? = nil) -> MLXArray {
421 |         ...
422 |         for (i, layer) in layers.enumerated() {
423 |             h = layer(h, mask: mask, cache: cache?[i])
424 |         }
425 |         ...
426 |     }
427 | ```
428 | 
429 | You have an array `layers` that matches the same property in Python.
430 | When calling the Module you simply iterate the `layers` property,
431 | chaining the calls together.
432 | 
433 | ### Model Class
434 | 
435 | Finally we reach the top level of of the model. In Python there is
436 | typically a class named `Model` -- this is the public entrypoint into
437 | the model. There is typically very little code in this module,
438 | though it may prepare the inputs and outputs.
439 | 
440 | There is also usually a class named e.g. `GemmaModel`
441 | which is the implementation of the model itself:
442 | 
443 | ```python
444 | class GemmaModel(nn.Module):
445 |     def __init__(self, args: ModelArgs):
446 |         super().__init__()
447 |     ...
448 | 
449 | class Model(nn.Module):
450 |     def __init__(self, args: ModelArgs):
451 |         super().__init__()
452 |         self.model_type = args.model_type
453 |         self.model = GemmaModel(args)
454 |         self.args = args
455 |     ...
456 | ```
457 | 
458 | In Swift we need to expose the `Model` class as a public
459 | type and leave the `GemmaModel` as a private implementation
460 | detail. Typically these are named like this in Swift:
461 | 
462 | ```swift
463 | private class GemmaModelInner: Module {
464 |     let args: GemmaConfiguration
465 |     let vocabularySize: Int
466 |     let numHiddenLayers: Int
467 | 
468 |     @ModuleInfo(key: "embed_tokens") var embedTokens: Embedding
469 |     fileprivate let layers: [TransformerBlock]
470 |     fileprivate let norm: RMSNorm
471 | 
472 |     public init(_ args: GemmaConfiguration) {
473 |     ...
474 | }
475 | 
476 | public class GemmaModel: Module, LLMModel, KVCacheDimensionProvider {
477 |     public let vocabularySize: Int
478 |     public let kvHeads: [Int]
479 | 
480 |     let modelType: String
481 |     private let model: GemmaModelInner
482 | 
483 |     public init(_ args: GemmaConfiguration) {
484 |         self.modelType = args.modelType
485 |         self.vocabularySize = args.vocabularySize
486 |         self.kvHeads = Array(repeating: args.kvHeads, count: args.hiddenLayers)
487 |         self.model = GemmaModelInner(args)
488 |     }
489 |     ...
490 | }
491 | ```
492 | 
493 | ### Registration
494 | 
495 | The last step before we can use the model is to register the types
496 | so that everything can be found from the configuration file.
497 | 
498 | Since this is an LLM (as opposed to a VLM) we register the type that will
499 | show in the configuration file in the `LLMTypeRegistry`:
500 | 
501 | ```swift
502 | public class LLMTypeRegistry: ModelTypeRegistry, @unchecked Sendable {
503 | 
504 |     private static func all() -> [String: @Sendable (URL) throws -> any LanguageModel] {
505 |     [
506 |         ...
507 |         "gemma": create(GemmaConfiguration.self, GemmaModel.init),
508 | ```
509 | 
510 | Now we can load the model using `llm-tool` or the `LLMEval` example application.
511 | 
512 | If we wanted to do it in code:
513 | 
514 | ```swift
515 | let modelConfiguration = ModelConfiguration(id: "mlx-community/quantized-gemma-2b-it")
516 | 
517 | // this will download the weights from HuggingFace Hub and load the model
518 | let container = try await MLXModelFactory.shared.loadContainer(configuration: modelConfiguration)
519 | 
520 | // prepare the prompt and parameters used to generate the response
521 | let generateParameters = GenerateParameters()
522 | let input = UserInput(prompt: "Are cherries sweet?")
523 | 
524 | // run inference
525 | let result = try await modelContainer.perform { [input] context in
526 |     // convert the UserInput into LMInput
527 |     let input = try context.processor.prepare(input: input)
528 | 
529 |     return generate(input: input, parameters: generateParameters, context: context) { tokens in
530 |         // this could potentially use NaiveStreamingDetokenizer and print
531 |         // text as it was generated
532 |         if tokens.count >= 20 {
533 |             return .stop
534 |         } else {
535 |             return .more
536 |         }
537 |     }
538 | }
539 | 
540 | print(result.output)
541 | ```
542 | 
543 | ## Notes
544 | 
545 | ### Porting a Model Similar to an Existing Model
546 | 
547 | If you are porting a model and it is similar to an existing (already ported) model, you can often take a shortcut and look at the diffs.
548 | 
549 | For example `gemma.py` and `gemma2.py` are related -- if you look at the diff between them it is roughly a dozen changes. If you already have `Gemma.swift` you can copy that to `Gemma2.swift` (and make the appropriate naming changes) and then examine the diffs on the Python side and make the same changes.
550 | 
551 | Many models are related to each other so this can be a very effective way to create new models.
552 | 
553 | ### Debugging a Ported Model
554 | 
555 | What do you do when your ported model spews random text or crashes with a broadcast error?
556 | 
557 | Let's start with the latter:  a broadcast error means that the shapes of the `MLXArray`s are incorrect. For example, if the broadcast error shows up in `Attention` (and it seems like it is usually in Attention) then you can make a helper function in Python:
558 | 
559 | ```python
560 | def trace(name, x):
561 |     print(f"{name}: {x.shape}")
562 | ```
563 | 
564 | and one in Swift:
565 | 
566 | ```swift
567 | func trace(_ name: String, _ x: MLXArray) {
568 |     print("\(name): \(x.shape)")
569 | }
570 | ```
571 | 
572 | In the Python code you can print the known working shapes:
573 | 
574 | ```python
575 | trace("queries", queries)
576 | trace("keys", keys)
577 | # etc. as needed
578 | output = scaled_dot_product_attention(
579 |     queries, keys, values, cache=cache, scale=self.scale, mask=mask
580 | )
581 | ```
582 | 
583 | and the same in Swift:
584 | 
585 | ```swift
586 | trace("queries", queries)
587 | trace("keys", keys)
588 | // etc. as needed
589 | let output = MLXFast.scaledDotProductAttention(
590 |     queries: queries, keys: keys, values: values, scale: scale, mask: mask
591 | )
592 | ```
593 | 
594 | Often it will be a shape like `[1, 128, 256]` vs `[128, 256]` -- there is
595 | a missing `[.newAxis]` somewhere in the code. It may be something more
596 | complicated but either way you know which value is incorrect and you can
597 | track it down.
598 | 
599 | Incorrect output can be investigated in a similar fashion but I usually start with making sure the inputs are correct -- compare the integer tokens from the Python side to what the Swift side generates. The implementations of `transformers` and `swift-transformers` are similar but not identical. If needed, the token array from the Python program can be used directly.
600 | 
601 | After making sure the inputs are identical, make sure the generation parameters (temperature, seed, etc.) are the same.
602 | 
603 | If the output still differs, then you must look at the contents of the arrays during inference, not just the shapes. You can modify the `trace` functions like this:
604 | 
605 | ```python
606 | def trace(name, x):
607 |     print(f"{name}: {x.shape} {x.sum().item()}")
608 | ```
609 | 
610 | and:
611 | 
612 | ```swift
613 | func trace(_ name: String, _ x: MLXArray) {
614 |     print("\(name): \(x.shape) \(x.sum().item(Float.self))")
615 | }
616 | ```
617 | 
618 | This uses `sum()` to give an aggregate value -- if these produce the same (or close to) values then the contents of the array are _probably_ the same. Certainly if they are wildly different then contents of the array are _certainly_ different. If the arrays contain larger numbers you might try different aggregation functions or look at slices of the array, e.g. the first row.
619 | 
620 | You can use these calls on the values passed in to the `__call__`/`callAsFunction` methods and you can also use them on the parameters of the layers themselves.
621 | 
622 | The nice thing about this technique is that it doesn't require understanding how the model works -- you have a reference implementation on the Python side and you only need to identify when it is different. Once you determine the point where it is different you can track backward and figure out why (again, it is just calling functions and doing math).
623 | 
624 | ### Optional Modules and Parameters
625 | 
626 | Models sometimes have optional modules or parameters based on their configuration.
627 | For example [qwen2.py](https://github.com/ml-explore/mlx-lm/blob/main/mlx_lm/models/qwen2.py#L161C1-L163C1)
628 | only creates the `lm_head` module if the `tie_word_embeddings` is `False`:
629 | 
630 | ```python
631 | if not args.tie_word_embeddings:
632 |     self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=False)
633 | ```
634 | 
635 | In Swift it is important to do the same thing using `Optional`:
636 | 
637 | ```swift
638 | public class Qwen2Model: Module, LLMModel, KVCacheDimensionProvider {
639 |     ...
640 | 
641 |     @ModuleInfo(key: "lm_head") var lmHead: Linear?
642 | 
643 |     public init(_ args: Qwen2Configuration) {
644 |         ...
645 |         if !args.tieWordEmbeddings {
646 |             _lmHead.wrappedValue = Linear(args.hiddenSize, args.vocabularySize, bias: false)
647 |         }
648 |     }
649 | 
650 |     public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {
651 |         var out = ...
652 |         if let lmHead {
653 |             out = lmHead(out)
654 |         } else {
655 |             out = model.embedTokens.asLinear(out)
656 |         }
657 |         ...
658 |     }
659 | ```
660 | 
661 | If the `lmHead` module is created but not used, the parameter load will fail validation because the `lm_head` keys will be missing.
662 | 
663 | ### Pre-computed MLXArrays
664 | 
665 | In some cases it is convenient to pre-compute some `MLXArray` but not treat
666 | it as a loadable parameter -- in particular we do not want loading of 
667 | parameters to fail because this MLXArray is "missing".
668 | 
669 | For example in PaliGemma there is a constant
670 | `positionIds` based on the imageSize and patchSize configuration.
671 | If we name the property with a leading underscore (`_`) it will
672 | not be considered as a valid parameter and will be ignored
673 | when loading parameters:
674 | 
675 | ```swift
676 | fileprivate class VisionEmbeddings: Module, UnaryLayer {
677 | 
678 |     ...
679 |     let positions: Int
680 |     let _positionIds: MLXArray
681 | 
682 |     public init(_ config: PaliGemmaConfiguration.VisionConfiguration) {
683 |         ...
684 |         let d = config.imageSize / config.patchSize
685 |         self.positions = d * d
686 |         self._positionIds = MLXArray(0 ..< positions)[.newAxis, 0...]
687 |     }
688 | 
689 |     public func callAsFunction(_ x: MLXArray) -> MLXArray {
690 |         ...
691 |         let embeddings = patchEmbeddings + self.positionEmbedding(self._positionIds)
692 |         ...
693 |     }
694 | }
695 | ```
696 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/Evaluate.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import Foundation
  4 | import MLX
  5 | import Tokenizers
  6 | 
  7 | /// A `LogitSampler` is responsible for sampling `logits` produced by
  8 | /// a ``LanguageModel`` to produce a token.
  9 | ///
 10 | /// See also: ``LogitProcessor``
 11 | public protocol LogitSampler: Sendable {
 12 | 
 13 |     /// Given `logits` produce a new `MLXArray` with the token.
 14 |     func sample(logits: MLXArray) -> MLXArray
 15 | }
 16 | 
 17 | /// A `LogitProcessor` is an optional visitor of `logits`.
 18 | ///
 19 | /// The ``LogitProcessor`` is called with the input (prompt) before generating tokens:
 20 | ///
 21 | /// ```swift
 22 | /// processor?.prompt(input.text.tokens)
 23 | /// ```
 24 | ///
 25 | /// Then for each token generated it has a chance to adjust the logits:
 26 | ///
 27 | /// ```swift
 28 | /// logits = processor?.process(logits: logits) ?? logits
 29 | /// let y = sampler.sample(logits: logits)
 30 | /// processor?.didSample(token: y)
 31 | /// ```
 32 | ///
 33 | /// See also: ``LogitSampler``
 34 | public protocol LogitProcessor: Sendable {
 35 | 
 36 |     /// called before token generation starts with the text tokens of the prompt
 37 |     mutating func prompt(_ prompt: MLXArray)
 38 | 
 39 |     /// called to visit ad possibly modify the logits
 40 |     func process(logits: MLXArray) -> MLXArray
 41 | 
 42 |     /// called to provide the sampled token
 43 |     mutating func didSample(token: MLXArray)
 44 | }
 45 | 
 46 | /// Parameters for text generation, see ``TokenIterator``.
 47 | ///
 48 | /// This produces:
 49 | ///
 50 | /// - ``LogitSampler``
 51 | /// - ``LogitProcessor``
 52 | ///
 53 | /// for the `TokenIterator`.
 54 | public struct GenerateParameters: Sendable {
 55 | 
 56 |     /// Step size for processing the prompt
 57 |     public var prefillStepSize = 512
 58 | 
 59 |     /// Maximum tokens to generate
 60 |     public var maxTokens: Int?
 61 | 
 62 |     /// Maximum size of the key-value cache. Old entries (except the first 4 tokens) will be overwritten.
 63 |     /// When set, uses ``RotatingKVCache`` instead of ``KVCacheSimple``
 64 |     public var maxKVSize: Int?
 65 | 
 66 |     /// Number of bits to use for KV cache quantization. nil implies no cache quantization.
 67 |     public var kvBits: Int?
 68 | 
 69 |     /// Group size for KV cache quantization (default: 64)
 70 |     public var kvGroupSize: Int = 64
 71 | 
 72 |     /// Step to begin using a quantized KV cache when kvBits is non-nil (default: 0)
 73 |     public var quantizedKVStart: Int = 0
 74 | 
 75 |     /// sampling temperature
 76 |     public var temperature: Float = 0.6
 77 | 
 78 |     /// top p sampling
 79 |     public var topP: Float = 1.0
 80 | 
 81 |     /// penalty factor for repeating tokens
 82 |     public var repetitionPenalty: Float?
 83 | 
 84 |     /// number of tokens to consider for repetition penalty
 85 |     public var repetitionContextSize: Int = 20
 86 | 
 87 |     public init(
 88 |         maxTokens: Int? = nil,
 89 |         maxKVSize: Int? = nil,
 90 |         kvBits: Int? = nil,
 91 |         kvGroupSize: Int = 64,
 92 |         quantizedKVStart: Int = 0,
 93 |         temperature: Float = 0.6, topP: Float = 1.0, repetitionPenalty: Float? = nil,
 94 |         repetitionContextSize: Int = 20
 95 |     ) {
 96 |         self.maxTokens = maxTokens
 97 |         self.maxKVSize = maxKVSize
 98 |         self.kvBits = kvBits
 99 |         self.kvGroupSize = kvGroupSize
100 |         self.quantizedKVStart = quantizedKVStart
101 |         self.temperature = temperature
102 |         self.topP = topP
103 |         self.repetitionPenalty = repetitionPenalty
104 |         self.repetitionContextSize = repetitionContextSize
105 |     }
106 | 
107 |     public func sampler() -> LogitSampler {
108 |         if temperature == 0 {
109 |             return ArgMaxSampler()
110 |         } else if topP > 0 && topP < 1 {
111 |             return TopPSampler(temperature: temperature, topP: topP)
112 |         } else {
113 |             return CategoricalSampler(temperature: temperature)
114 |         }
115 |     }
116 | 
117 |     public func processor() -> LogitProcessor? {
118 |         if let repetitionPenalty, repetitionContextSize > 0 {
119 |             return RepetitionContext(
120 |                 repetitionPenalty: repetitionPenalty, repetitionContextSize: repetitionContextSize)
121 |         } else {
122 |             return nil
123 |         }
124 |     }
125 | }
126 | 
127 | /// Sampler that uses `argMax` (most likely) to sample the logits.
128 | public struct ArgMaxSampler: LogitSampler {
129 |     public func sample(logits: MLX.MLXArray) -> MLX.MLXArray {
130 |         argMax(logits, axis: -1)
131 |     }
132 | }
133 | 
134 | /// Sampler that uses `topP` and `temperature` to sample the logits.
135 | public struct TopPSampler: LogitSampler {
136 |     let temp: MLXArray
137 |     let topP: MLXArray
138 | 
139 |     public init(temperature: Float, topP: Float) {
140 |         self.temp = MLXArray(temperature)
141 |         self.topP = MLXArray(topP)
142 |     }
143 | 
144 |     private let compiledTopPSampling: (MLXArray, MLXArray, MLXArray) -> MLXArray = {
145 |         compile(inputs: [MLXRandom.globalState], outputs: [MLXRandom.globalState]) {
146 |             logits, topP, temp in
147 |             let probs = softmax(logits / temp, axis: -1)
148 |             let sortedIndices = argSort(probs, axis: -1)
149 | 
150 |             // probs shape is [B,V] and after take it will be [1, B, V], so we squeeze it back to [B, V]
151 |             let sortedProbs = take(probs, sortedIndices, axis: -1).squeezed(axis: 0)
152 | 
153 |             let cumulativeProbs = cumsum(sortedProbs, axis: -1)
154 | 
155 |             let topProbs = MLX.where(
156 |                 cumulativeProbs .> (1 - topP), sortedProbs, zeros(like: sortedProbs))
157 | 
158 |             let sortedToken = categorical(log(topProbs))
159 |             return sortedIndices.squeezed(axis: 0)[sortedToken]
160 |         }
161 |     }()
162 | 
163 |     public func sample(logits: MLXArray) -> MLXArray {
164 |         var logits = logits
165 |         if logits.dtype == .bfloat16 {
166 |             logits = logits.asType(.float32)
167 |         }
168 | 
169 |         return compiledTopPSampling(logits, topP, temp)
170 |     }
171 | }
172 | 
173 | /// Processor that uses `temperature` to sample the logits
174 | public struct CategoricalSampler: LogitSampler {
175 |     let temp: MLXArray
176 | 
177 |     public init(temperature: Float) {
178 |         self.temp = MLXArray(temperature)
179 |     }
180 | 
181 |     private let compiledCategorical: (MLXArray, MLXArray) -> MLXArray = {
182 |         compile(inputs: [MLXRandom.globalState], outputs: [MLXRandom.globalState]) { logits, temp in
183 |             categorical(logits * (1 / temp))
184 |         }
185 |     }()
186 | 
187 |     public func sample(logits: MLXArray) -> MLXArray {
188 |         compiledCategorical(logits, temp)
189 |     }
190 | }
191 | 
192 | /// Processor that implements a `repetitionPenalty`
193 | public struct RepetitionContext: LogitProcessor {
194 |     /// tokens in the repetition context sliding window
195 |     var tokens = [Int]()
196 | 
197 |     /// current write into into the tokens circular array
198 |     var index = 0
199 | 
200 |     /// penalty factor for repeating tokens
201 |     let repetitionPenalty: Float
202 | 
203 |     /// number of tokens to consider for repetition penalty
204 |     let repetitionContextSize: Int
205 | 
206 |     public init(repetitionPenalty: Float, repetitionContextSize: Int) {
207 |         precondition(repetitionContextSize > 0)
208 |         self.repetitionPenalty = repetitionPenalty
209 |         self.repetitionContextSize = repetitionContextSize
210 |     }
211 | 
212 |     mutating public func prompt(_ prompt: MLXArray) {
213 |         if prompt.shape[0] <= repetitionContextSize {
214 |             self.tokens = prompt.asArray(Int.self)
215 |         } else {
216 |             self.tokens = prompt[(-repetitionContextSize)...].asArray(Int.self)
217 |         }
218 |     }
219 | 
220 |     public func process(logits: MLXArray) -> MLXArray {
221 |         if tokens.count > 0 {
222 |             let indices = MLXArray(tokens.map { UInt32($0) })
223 |             var selectedLogits = logits[0..., indices]
224 | 
225 |             selectedLogits = MLX.where(
226 |                 selectedLogits .< 0, selectedLogits * repetitionPenalty,
227 |                 selectedLogits / repetitionPenalty)
228 | 
229 |             logits[0..., indices] = selectedLogits
230 |             return logits
231 |         }
232 | 
233 |         return logits
234 |     }
235 | 
236 |     mutating public func didSample(token: MLXArray) {
237 |         if tokens.count >= repetitionContextSize {
238 |             tokens[index] = token.item(Int.self)
239 |             index = (index + 1) % repetitionContextSize
240 |         } else {
241 |             tokens.append(token.item(Int.self))
242 |         }
243 |     }
244 | }
245 | 
246 | /// Generator of tokens.
247 | ///
248 | /// This is typically used via a call to ``generate(input:parameters:context:didGenerate:)``.
249 | ///
250 | /// To use it directly:
251 | ///
252 | /// ```swift
253 | /// let generateParameters: GenerateParameters
254 | /// let input: LMInput
255 | /// let model: LanguageModel
256 | ///
257 | /// let iterator = try TokenIterator(input: input, model: model, parameters: parameters)
258 | ///
259 | /// for token in iterator {
260 | ///     ...
261 | /// }
262 | /// ```
263 | ///
264 | /// Tokens are integers that can be passed through a `Tokenizer` or ``StreamingDetokenizer`` to produce Strings.
265 | ///
266 | /// Port of `generate_step()` from https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/utils.py
267 | ///
268 | /// Note: this uses `asyncEval()` and there may be an async evaluation running after a call to `next()`.
269 | public struct TokenIterator: Sequence, IteratorProtocol {
270 |     let model: any LanguageModel
271 |     var state: LMOutput.State?
272 | 
273 |     var y: LMInput.Text
274 |     var cache: [KVCache]
275 |     var processor: LogitProcessor?
276 |     let sampler: LogitSampler
277 | 
278 |     var tokenCount = 0
279 |     let maxTokens: Int?
280 | 
281 |     // Cache quantization parameters
282 |     let kvBits: Int?
283 |     let kvGroupSize: Int
284 |     let quantizedKVStart: Int
285 | 
286 |     /// Initialize a `TokenIterator` with the given tokens. Note: this has been
287 |     /// replaced with ``init(input:model:cache:parameters:)``.
288 |     ///
289 |     /// - Parameters:
290 |     ///   - prompt: the prompt tokens
291 |     ///   - model: the ``LanguageModel``
292 |     ///   - cache: optional ``KVCache``
293 |     ///   - parameters: the generation parameters
294 |     @available(*, deprecated, message: "please use init(input:model:cache:parameters:)")
295 |     public init(
296 |         prompt: MLXArray, model: any LanguageModel, cache: [KVCache]? = nil,
297 |         parameters: GenerateParameters
298 |     ) throws {
299 |         self.model = model
300 |         self.y = .init(tokens: prompt)
301 |         self.cache = cache ?? model.newCache(parameters: parameters)
302 | 
303 |         self.processor = parameters.processor()
304 |         self.sampler = parameters.sampler()
305 |         self.maxTokens = parameters.maxTokens
306 | 
307 |         self.kvBits = parameters.kvBits
308 |         self.kvGroupSize = parameters.kvGroupSize
309 |         self.quantizedKVStart = parameters.quantizedKVStart
310 | 
311 |         try prepare(input: .init(text: y), windowSize: parameters.prefillStepSize)
312 |     }
313 | 
314 |     /// Initialize a `TokenIterator` with the given input.
315 |     ///
316 |     /// If more control is needed over the generation,
317 |     /// ``init(input:model:cache:processor:sampler:prefillStepSize:)``
318 |     /// allows a caller to specify ``LogitProcessor`` and ``LogitSampler``
319 |     /// directly.
320 |     ///
321 |     /// - Parameters:
322 |     ///   - input: language model input
323 |     ///   - model: the ``LanguageModel``
324 |     ///   - cache: optional ``KVCache``
325 |     ///   - parameters: the generation parameters
326 |     public init(
327 |         input: LMInput, model: any LanguageModel, cache: [KVCache]? = nil,
328 |         parameters: GenerateParameters
329 |     ) throws {
330 |         self.model = model
331 |         self.y = input.text
332 |         self.cache = cache ?? model.newCache(parameters: parameters)
333 | 
334 |         self.processor = parameters.processor()
335 |         self.sampler = parameters.sampler()
336 |         self.maxTokens = parameters.maxTokens
337 | 
338 |         self.kvBits = parameters.kvBits
339 |         self.kvGroupSize = parameters.kvGroupSize
340 |         self.quantizedKVStart = parameters.quantizedKVStart
341 | 
342 |         try prepare(input: input, windowSize: parameters.prefillStepSize)
343 |     }
344 | 
345 |     /// Initialize a `TokenIterator` with the given input and logit handling.
346 |     ///
347 |     /// - Parameters:
348 |     ///   - input: language model input
349 |     ///   - model: the ``LanguageModel``
350 |     ///   - cache: optional ``KVCache``
351 |     ///   - processor: the logit processor
352 |     ///   - sampler: the logit sampler
353 |     ///   - prefillStepSize: optional prefill step size
354 |     ///   - maxTokens: maximum number of tokens to generate
355 |     public init(
356 |         input: LMInput, model: any LanguageModel, cache: [KVCache]? = nil,
357 |         processor: LogitProcessor?, sampler: LogitSampler, prefillStepSize: Int = 512,
358 |         maxTokens: Int? = nil
359 |     ) throws {
360 |         self.model = model
361 |         self.y = input.text
362 |         self.cache = cache ?? model.newCache(parameters: nil)
363 | 
364 |         self.processor = processor
365 |         self.sampler = sampler
366 |         self.maxTokens = maxTokens
367 | 
368 |         // No cache quantization for this direct initialization
369 |         self.kvBits = nil
370 |         self.kvGroupSize = 64
371 |         self.quantizedKVStart = 0
372 | 
373 |         try prepare(input: input, windowSize: prefillStepSize)
374 |     }
375 | 
376 |     mutating func prepare(input: LMInput, windowSize: Int? = nil) throws {
377 |         processor?.prompt(input.text.tokens)
378 | 
379 |         switch try model.prepare(input, cache: cache, windowSize: windowSize) {
380 |         case .tokens(let tokens):
381 |             y = tokens
382 | 
383 |             // evaluate the remainder of the prompt -- this primes the pump
384 |             let token = step(previous: y)
385 |             y = .init(tokens: token)
386 |             asyncEval(y.tokens)
387 | 
388 |         case .logits(let result):
389 |             y = .init(tokens: convertToToken(logits: result.logits))
390 |             asyncEval(y.tokens)
391 | 
392 |             break
393 |         }
394 |     }
395 | 
396 |     mutating func convertToToken(logits: MLXArray) -> MLXArray {
397 |         // process the logits (one hot array of possible tokens)
398 |         var logits = logits[0..., -1, 0...]
399 |         logits = processor?.process(logits: logits) ?? logits
400 | 
401 |         // transform logits back to a token
402 |         let y = sampler.sample(logits: logits)
403 | 
404 |         processor?.didSample(token: y)
405 | 
406 |         return y
407 |     }
408 | 
409 |     /// Evaluate the next token and return the new token (y), updating cache state
410 |     mutating func step(previous: LMInput.Text) -> MLXArray {
411 |         let result = model(
412 |             previous[text: .newAxis], cache: cache.isEmpty ? nil : cache, state: state)
413 |         self.state = result.state
414 | 
415 |         // Apply dynamic cache quantization after each step
416 |         maybeQuantizeKVCache(
417 |             cache: &cache,
418 |             kvBits: kvBits,
419 |             kvGroupSize: kvGroupSize,
420 |             quantizedKVStart: quantizedKVStart
421 |         )
422 | 
423 |         return convertToToken(logits: result.logits)
424 |     }
425 | 
426 |     mutating public func next() -> Int? {
427 |         if let maxTokens, tokenCount >= maxTokens {
428 |             return nil
429 |         }
430 | 
431 |         // save current value -- this will be returned
432 |         let previousY = y
433 | 
434 |         // compute the next state and async eval the next token
435 |         let token = step(previous: previousY)
436 |         y = .init(tokens: token)
437 |         asyncEval(token)
438 | 
439 |         tokenCount += 1
440 | 
441 |         return previousY.tokens.item(Int.self)
442 |     }
443 | }
444 | 
445 | /// Result of a call to ``generate(input:parameters:context:didGenerate:)``.
446 | public struct GenerateResult: Sendable {
447 | 
448 |     /// Initializes a new `GenerateResult` instance.
449 |     ///
450 |     /// - Parameters:
451 |     ///   - inputText: The input text used for generation.
452 |     ///   - tokens: The array of tokens generated.
453 |     ///   - output: The generated output string.
454 |     ///   - promptTime: The time taken to prompt the input.
455 |     ///   - generateTime: The time taken to generate the output.
456 |     public init(
457 |         inputText: LMInput.Text, tokens: [Int], output: String, promptTime: TimeInterval,
458 |         generateTime: TimeInterval
459 |     ) {
460 |         self.inputText = inputText
461 |         self.tokens = tokens
462 |         self.output = output
463 |         self.promptTime = promptTime
464 |         self.generateTime = generateTime
465 |     }
466 | 
467 |     /// input (prompt, images, etc.)
468 |     public let inputText: LMInput.Text
469 | 
470 |     @available(*, deprecated, message: "use inputText")
471 |     public var promptTokens: [Int] {
472 |         inputText.tokens.asArray(Int.self)
473 |     }
474 | 
475 |     /// output tokens
476 |     public let tokens: [Int]
477 | 
478 |     /// output text
479 |     public let output: String
480 | 
481 |     /// The number of tokens included in the input prompt.
482 |     public var promptTokenCount: Int { inputText.tokens.size }
483 | 
484 |     /// The number of tokens generated by the language model.
485 |     public var generationTokenCount: Int { tokens.count }
486 | 
487 |     /// time to process the prompt / generate the first token
488 |     public let promptTime: TimeInterval
489 | 
490 |     /// time to generate the remaining tokens
491 |     public let generateTime: TimeInterval
492 | 
493 |     /// The number of tokens processed per second during the prompt phase.
494 |     public var promptTokensPerSecond: Double {
495 |         Double(inputText.tokens.size) / promptTime
496 |     }
497 | 
498 |     /// The number of tokens generated per second during the generation phase.
499 |     public var tokensPerSecond: Double {
500 |         Double(tokens.count) / generateTime
501 |     }
502 | 
503 |     public func summary() -> String {
504 |         """
505 |         Prompt:     \(promptTokenCount) tokens, \(promptTokensPerSecond.formatted()) tokens/s
506 |         Generation: \(generationTokenCount) tokens, \(tokensPerSecond.formatted()) tokens/s, \(generateTime.formatted())s
507 |         """
508 |     }
509 | }
510 | 
511 | /// Action from token visitor callback in ``generate(input:parameters:context:didGenerate:)``.
512 | public enum GenerateDisposition: Sendable {
513 |     /// keep producing tokens until an EOS token is produced
514 |     case more
515 | 
516 |     /// stop producing tokens, e.g. a token limit has been hit
517 |     case stop
518 | }
519 | 
520 | /// Given prompt tokens generate text using the given model and parameters.
521 | ///
522 | /// ``generate(input:parameters:context:didGenerate:)`` is the preferred call.
523 | ///
524 | /// - Parameters:
525 | ///   - promptTokens: tokenized prompt
526 | ///   - parameters: generation parameters
527 | ///   - model: model to evaluate
528 | ///   - tokenizer: tokenizer to convert tokens back into strings and recognizer special tokens
529 | ///   - extraEOSTokens: any additional stop tokens
530 | ///   - didGenerate: visitor for the tokens as they are generated
531 | @available(*, deprecated, message: "please use generate(input:parameters:context:didGenerate:)")
532 | public func generate(
533 |     promptTokens: [Int], parameters: GenerateParameters, model: any LanguageModel,
534 |     tokenizer: Tokenizer,
535 |     extraEOSTokens: Set<String>? = nil,
536 |     didGenerate: ([Int]) -> GenerateDisposition
537 | ) throws -> GenerateResult {
538 |     let tokens = MLXArray(promptTokens)
539 |     let iterator = try TokenIterator(
540 |         prompt: tokens, model: model, parameters: parameters)
541 | 
542 |     // this is a compatibility cover -- create the required values
543 |     // for the iteration
544 |     let input = LMInput(tokens: tokens)
545 |     let configuration = ModelConfiguration(id: "stand-in", extraEOSTokens: extraEOSTokens ?? [])
546 |     let context = ModelContext(
547 |         configuration: configuration, model: model, processor: StandInUserInputProcessor(),
548 |         tokenizer: tokenizer)
549 | 
550 |     return generate(
551 |         input: input, context: context, iterator: iterator, didGenerate: didGenerate)
552 | }
553 | 
554 | /// Generate tokens from an ``LMInput`` and a ``ModelContext``.
555 | ///
556 | /// For example:
557 | ///
558 | /// ```swift
559 | /// let generateParameters: GenerateParameters
560 | /// let input: UserInput
561 | /// let context: ModelContext
562 | ///
563 | /// let lmInput = try context.processor.prepare(input: input)
564 | /// let result = generate(input: lmInput,
565 | ///     parameters: generateParameters,
566 | ///     context: context) { tokens in
567 | ///     .more
568 | /// }
569 | /// ```
570 | ///
571 | /// Internally this constructs a ``TokenIterator`` and calls
572 | /// ``generate(input:context:iterator:didGenerate:)``
573 | ///
574 | /// - Parameters:
575 | ///   - input: prepared language model input
576 | ///   - parameters: parameters controlling the token generation
577 | ///   - context: model context (model and tokenizer)
578 | ///   - didGenerate: token visitor that can output tokens as they are generated and indicate early stop
579 | /// - Returns: the generated output
580 | public func generate(
581 |     input: LMInput, parameters: GenerateParameters, context: ModelContext,
582 |     didGenerate: ([Int]) -> GenerateDisposition
583 | ) throws -> GenerateResult {
584 |     let iterator = try TokenIterator(
585 |         input: input, model: context.model, parameters: parameters)
586 |     return generate(
587 |         input: input, context: context, iterator: iterator, didGenerate: didGenerate)
588 | }
589 | 
590 | /// Low level token generation using a ``TokenIterator``.
591 | ///
592 | /// ``generate(input:parameters:context:didGenerate:)`` is the preferred call.
593 | ///
594 | /// - Parameters:
595 | ///   - input: prepared language model input
596 | ///   - context: model context (model and tokenizer)
597 | ///   - iterator: token iterator
598 | ///   - didGenerate: token visitor that can output tokens as they are generated and indicate early stop
599 | /// - Returns: the generated output
600 | public func generate(
601 |     input: LMInput, context: ModelContext,
602 |     iterator: TokenIterator,
603 |     didGenerate: ([Int]) -> GenerateDisposition
604 | ) -> GenerateResult {
605 |     var start = Date.timeIntervalSinceReferenceDate
606 |     var promptTime: TimeInterval = 0
607 | 
608 |     let additionalEOSTokenIds = Set(
609 |         (context.configuration.extraEOSTokens ?? [])
610 |             .compactMap {
611 |                 context.tokenizer.convertTokenToId($0)
612 |             })
613 | 
614 |     var tokens = [Int]()
615 | 
616 |     for token in iterator {
617 |         // compute the timing for the prompt
618 |         if tokens.isEmpty {
619 |             let now = Date.timeIntervalSinceReferenceDate
620 |             promptTime = now - start
621 |             start = now
622 |         }
623 | 
624 |         if token == context.tokenizer.unknownTokenId || token == context.tokenizer.eosTokenId
625 |             || additionalEOSTokenIds.contains(token)
626 |         {
627 |             break
628 |         }
629 |         tokens.append(token)
630 | 
631 |         if didGenerate(tokens) == .stop {
632 |             break
633 |         }
634 |     }
635 | 
636 |     let now = Date.timeIntervalSinceReferenceDate
637 |     let generateTime = now - start
638 | 
639 |     // TokenIterator uses `asyncEval()` to keep the pipeline full. If the caller
640 |     // exits the program right away, those tasks will still be executing and will
641 |     // hit assertions as the mlx scheduler is torn down. Synchronize with the stream
642 |     // to make sure it is complete.
643 |     Stream().synchronize()
644 | 
645 |     return GenerateResult(
646 |         inputText: input.text, tokens: tokens,
647 |         output: context.tokenizer.decode(tokens: tokens),
648 |         promptTime: promptTime, generateTime: generateTime)
649 | }
650 | 
651 | /// Generate tokens from an ``LMInput`` and a ``ModelContext``.
652 | ///
653 | /// For example:
654 | ///
655 | /// ```swift
656 | /// let generateParameters: GenerateParameters
657 | /// let input: UserInput
658 | /// let context: ModelContext
659 | ///
660 | /// let lmInput = try context.processor.prepare(input: input)
661 | /// let result = generate(input: lmInput,
662 | ///     parameters: generateParameters,
663 | ///     context: context) { token in
664 | ///     .more
665 | /// }
666 | /// ```
667 | ///
668 | /// Internally this constructs a ``TokenIterator`` and calls
669 | /// ``generate(input:context:iterator:didGenerate:)``
670 | ///
671 | /// - Parameters:
672 | ///   - input: prepared language model input
673 | ///   - parameters: parameters controlling the token generation
674 | ///   - context: model context (model and tokenizer)
675 | ///   - didGenerate: token visitor that can output tokens as they are generated and indicate early stop
676 | /// - Returns: Information about the generation
677 | public func generate(
678 |     input: LMInput, parameters: GenerateParameters, context: ModelContext,
679 |     didGenerate: (Int) -> GenerateDisposition
680 | ) throws -> GenerateCompletionInfo {
681 |     let iterator = try TokenIterator(
682 |         input: input, model: context.model, parameters: parameters)
683 |     return generate(
684 |         input: input, context: context, iterator: iterator, didGenerate: didGenerate)
685 | }
686 | 
687 | public func generate(
688 |     input: LMInput, context: ModelContext,
689 |     iterator: TokenIterator,
690 |     didGenerate: (Int) -> GenerateDisposition
691 | ) -> GenerateCompletionInfo {
692 |     var start = Date.timeIntervalSinceReferenceDate
693 |     var promptTime: TimeInterval = 0
694 | 
695 |     let additionalEOSTokenIds = Set(
696 |         (context.configuration.extraEOSTokens ?? [])
697 |             .compactMap {
698 |                 context.tokenizer.convertTokenToId($0)
699 |             })
700 | 
701 |     var tokenCount = 0
702 | 
703 |     for token in iterator {
704 |         // Compute the timing for the prompt
705 |         if promptTime == 0 {
706 |             let now = Date.timeIntervalSinceReferenceDate
707 |             promptTime = now - start
708 |             start = now
709 |         }
710 | 
711 |         // Check for end-of-sequence tokens
712 |         if token == context.tokenizer.unknownTokenId || token == context.tokenizer.eosTokenId
713 |             || additionalEOSTokenIds.contains(token)
714 |         {
715 |             break
716 |         }
717 | 
718 |         tokenCount += 1
719 | 
720 |         // Invoke the callback with the current token
721 |         if didGenerate(token) == .stop {
722 |             break
723 |         }
724 |     }
725 | 
726 |     let now = Date.timeIntervalSinceReferenceDate
727 |     let generateTime = now - start
728 | 
729 |     // Synchronize with the stream to ensure tasks are completed
730 |     Stream().synchronize()
731 | 
732 |     return GenerateCompletionInfo(
733 |         promptTokenCount: input.text.tokens.size,
734 |         generationTokenCount: tokenCount,
735 |         promptTime: promptTime,
736 |         generationTime: generateTime
737 |     )
738 | }
739 | 
740 | /// Generates tokens asynchronously using the provided language model input, parameters, and context.
741 | ///
742 | /// This function initializes a `TokenIterator` with the given input, model, and generation parameters,
743 | /// and then streams the token generation process via an `AsyncStream`. The resulting stream yields
744 | /// instances of the `Generation` enum, which can represent either individual tokens or summary
745 | /// completion information.
746 | ///
747 | /// - Parameters:
748 | ///   - input: The input for the language model.
749 | ///   - cache: optional ``KVCache``
750 | ///   - parameters: The configuration options for token generation.
751 | ///   - context: The model context, including the model itself and associated tokenizer.
752 | /// - Returns: An `AsyncStream` that emits `Generation` values, including generated tokens (`.token`)
753 | ///   and completion information (`.info`).
754 | /// - Throws: An error if the `TokenIterator` initialization fails due to invalid input or model configuration.
755 | ///
756 | /// ### Example Usage:
757 | /// ```swift
758 | /// // Define the input, parameters, and context for token generation.
759 | /// let generateParameters: GenerateParameters
760 | /// let input: UserInput
761 | /// let context: ModelContext
762 | ///
763 | /// let lmInput = try context.processor.prepare(input: input)
764 | ///
765 | /// // Call the generate function to get an AsyncStream.
766 | /// let stream = try generate(input: lmInput, parameters: parameters, context: context)
767 | ///
768 | /// // Process the stream asynchronously to handle generated tokens and completion info.
769 | /// for await generation in stream {
770 | ///     switch generation {
771 | ///     case .token(let token):
772 | ///         print("Generated token: \(context.tokenizer.decode(tokens: [token])")
773 | ///     case .info(let info):
774 | ///         print("Finished: \(info.tokensPerSecond) tokens/s.")
775 | ///     }
776 | /// }
777 | /// ```
778 | public func generate(
779 |     input: LMInput, cache: [KVCache]? = nil, parameters: GenerateParameters, context: ModelContext
780 | ) throws -> AsyncStream<Generation> {
781 |     let iterator = try TokenIterator(
782 |         input: input, model: context.model, cache: cache, parameters: parameters)
783 |     return generate(
784 |         input: input, context: context, iterator: iterator)
785 | }
786 | 
787 | public func generate(
788 |     input: LMInput, context: ModelContext,
789 |     iterator: TokenIterator
790 | ) -> AsyncStream<Generation> {
791 | 
792 |     AsyncStream { continuation in
793 | 
794 |         // Launch a Task to perform iteration asynchronously.
795 |         let task = Task {
796 |             var start = Date.timeIntervalSinceReferenceDate
797 |             var promptTime: TimeInterval = 0
798 | 
799 |             let additionalEOSTokenIds = Set(
800 |                 context.configuration.extraEOSTokens
801 |                     .compactMap {
802 |                         context.tokenizer.convertTokenToId($0)
803 |                     })
804 | 
805 |             var tokenCount = 0
806 |             var detokenizer = NaiveStreamingDetokenizer(tokenizer: context.tokenizer)
807 |             let toolCallProcessor = ToolCallProcessor()
808 | 
809 |             for token in iterator {
810 | 
811 |                 // Check for cancellation on every loop iteration.
812 |                 if Task.isCancelled { break }
813 | 
814 |                 if promptTime == 0 {
815 |                     let now = Date.timeIntervalSinceReferenceDate
816 |                     promptTime = now - start
817 |                     start = now
818 |                 }
819 | 
820 |                 if token == context.tokenizer.unknownTokenId
821 |                     || token == context.tokenizer.eosTokenId
822 |                     || additionalEOSTokenIds.contains(token)
823 |                 {
824 |                     break
825 |                 }
826 | 
827 |                 detokenizer.append(token: token)
828 |                 if let chunk = detokenizer.next() {
829 |                     tokenCount += 1
830 | 
831 |                     // Process chunk through the tool call processor
832 |                     if let textToYield = toolCallProcessor.processChunk(chunk) {
833 |                         continuation.yield(.chunk(textToYield))
834 |                     }
835 | 
836 |                     // Check if we have a complete tool call
837 |                     if let toolCall = toolCallProcessor.toolCalls.popLast() {
838 |                         continuation.yield(.toolCall(toolCall))
839 |                     }
840 |                 }
841 |             }
842 | 
843 |             let now = Date.timeIntervalSinceReferenceDate
844 |             let generateTime = now - start
845 | 
846 |             let info = GenerateCompletionInfo(
847 |                 promptTokenCount: input.text.tokens.size,
848 |                 generationTokenCount: tokenCount,
849 |                 promptTime: promptTime,
850 |                 generationTime: generateTime
851 |             )
852 |             continuation.yield(.info(info))
853 | 
854 |             // Synchronize with the stream to ensure tasks are completed
855 |             Stream().synchronize()
856 | 
857 |             // Finalize the stream
858 |             continuation.finish()
859 |         }
860 |         // When the consumer cancels (or ends) the stream,
861 |         // cancel our underlying task.
862 |         continuation.onTermination = { _ in
863 |             task.cancel()
864 |         }
865 |     }
866 | }
867 | 
868 | /// Represents metadata and statistics related to token generation.
869 | ///
870 | /// Provides information about the number of tokens processed during both the prompt and generation phases, as well as the time taken for each phase.
871 | public struct GenerateCompletionInfo: Sendable {
872 |     /// The number of tokens included in the input prompt.
873 |     public let promptTokenCount: Int
874 | 
875 |     /// The number of tokens generated by the language model.
876 |     public let generationTokenCount: Int
877 | 
878 |     /// The time interval (in seconds) taken to process the input prompt.
879 |     public let promptTime: TimeInterval
880 | 
881 |     /// The time interval (in seconds) taken to generate the output tokens.
882 |     public let generateTime: TimeInterval
883 | 
884 |     /// The number of tokens processed per second during the prompt phase.
885 |     public var promptTokensPerSecond: Double {
886 |         Double(promptTokenCount) / promptTime
887 |     }
888 | 
889 |     /// The number of tokens generated per second during the generation phase.
890 |     public var tokensPerSecond: Double {
891 |         Double(generationTokenCount) / generateTime
892 |     }
893 | 
894 |     public init(
895 |         promptTokenCount: Int,
896 |         generationTokenCount: Int,
897 |         promptTime: TimeInterval,
898 |         generationTime: TimeInterval
899 |     ) {
900 |         self.promptTokenCount = promptTokenCount
901 |         self.generationTokenCount = generationTokenCount
902 |         self.promptTime = promptTime
903 |         self.generateTime = generationTime
904 |     }
905 | 
906 |     public func summary() -> String {
907 |         """
908 |         Prompt:     \(promptTokenCount) tokens, \(promptTokensPerSecond.formatted()) tokens/s
909 |         Generation: \(generationTokenCount) tokens, \(tokensPerSecond.formatted()) tokens/s, \(generateTime.formatted())s
910 |         """
911 |     }
912 | }
913 | 
914 | /// Represents the different stages or outputs of the token generation process.
915 | ///
916 | /// This enum distinguishes between the following:
917 | /// - `.chunk`: A decoded string from one or more tokens generated by the language model.
918 | /// - `.info`: Metadata and performance statistics about the generation process.
919 | public enum Generation: Sendable {
920 |     /// A generated token represented as a String
921 |     case chunk(String)
922 | 
923 |     /// Completion information summarizing token counts and performance metrics.
924 |     case info(GenerateCompletionInfo)
925 | 
926 |     /// A tool call from the language model.
927 |     case toolCall(ToolCall)
928 | 
929 |     /// Generated text or nil
930 |     public var chunk: String? {
931 |         switch self {
932 |         case .chunk(let string): string
933 |         case .info: nil
934 |         case .toolCall: nil
935 |         }
936 |     }
937 | 
938 |     /// Completion info or nil
939 |     public var info: GenerateCompletionInfo? {
940 |         switch self {
941 |         case .chunk: nil
942 |         case .info(let info): info
943 |         case .toolCall: nil
944 |         }
945 |     }
946 | 
947 |     /// Tool call or nil
948 |     public var toolCall: ToolCall? {
949 |         switch self {
950 |         case .chunk: nil
951 |         case .info: nil
952 |         case .toolCall(let toolCall): toolCall
953 |         }
954 |     }
955 | 
956 |     /// Reducer that can be used with `throttle()` to gather elements into a batch
957 |     @Sendable
958 |     public static func collect(_ batch: [Generation]?, _ element: Generation) -> [Generation] {
959 |         (batch ?? []) + [element]
960 |     }
961 | }
962 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/Extensions/Encodable+toolResult.swift:
--------------------------------------------------------------------------------
 1 | // Copyright © 2025 Apple Inc.
 2 | 
 3 | import Foundation
 4 | 
 5 | // Extension on Codable to handle JSON encoding with snake case
 6 | extension Encodable {
 7 |     public var toolResult: String {
 8 |         let encoder = JSONEncoder()
 9 |         encoder.keyEncodingStrategy = .convertToSnakeCase
10 | 
11 |         guard let data = try? encoder.encode(self) else { return "{}" }
12 |         return String(data: data, encoding: .utf8) ?? "{}"
13 |     }
14 | }
15 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/KVCache.swift:
--------------------------------------------------------------------------------
   1 | // Copyright © 2024 Apple Inc.
   2 | 
   3 | import Foundation
   4 | import MLX
   5 | import MLXNN
   6 | 
   7 | /// Implementation of KV cache functionality for MLX Swift
   8 | ///
   9 | ///
  10 | /// ## Quantized Cache Usage
  11 | ///
  12 | /// **Standard caches:**
  13 | /// ```swift
  14 | /// let cache = KVCacheSimple()
  15 | /// let (keys, values) = cache.update(keys: keys, values: values)
  16 | /// let output = MLXFast.scaledDotProductAttention(queries: q, keys: keys, values: values, ...)
  17 | /// ```
  18 | ///
  19 | /// **Quantized cache:**
  20 | /// ```swift
  21 | /// let quantizedCache = QuantizedKVCache(groupSize: 64, bits: 4)
  22 | /// let (qKeys, qValues) = quantizedCache.updateQuantized(keys: keys, values: values)
  23 | ///
  24 | /// let output = quantizedScaledDotProductAttention(
  25 | ///     queries: queries,
  26 | ///     quantizedKeys: qKeys,
  27 | ///     quantizedValues: qValues,
  28 | ///     scale: scale,
  29 | ///     mask: mask,
  30 | ///     groupSize: quantizedCache.groupSize,
  31 | ///     bits: quantizedCache.bits
  32 | /// )
  33 | /// ```
  34 | ///
  35 | /// Interface for Key/Value cache for LLMs.
  36 | ///
  37 | /// See ``LanguageModel/newCache(parameters:)``
  38 | public protocol KVCache: Evaluatable {
  39 |     /// get the current offset
  40 |     var offset: Int { get }
  41 | 
  42 |     /// get the maximum size (if any)
  43 |     var maxSize: Int? { get }
  44 | 
  45 |     /// update the cache with new keys and values and return all keys/values
  46 |     func update(keys: MLXArray, values: MLXArray) -> (MLXArray, MLXArray)
  47 | 
  48 |     /// get the current state for serialization
  49 |     var state: [MLXArray] { get set }
  50 | 
  51 |     /// get/set metadata state as string array for serialization
  52 |     var metaState: [String] { get set }
  53 | 
  54 |     /// whether this cache can be trimmed
  55 |     var isTrimmable: Bool { get }
  56 | 
  57 |     /// trim n tokens from the cache, returning actual number trimmed
  58 |     @discardableResult
  59 |     func trim(_ n: Int) -> Int
  60 | }
  61 | 
  62 | /// Protocol for caches that support efficient quantized operations
  63 | ///
  64 | /// **Usage Example:**
  65 | /// ```swift
  66 | /// // Efficient quantized path
  67 | /// if let quantizedCache = cache as? QuantizedKVCacheProtocol {
  68 | ///     let (qKeys, qValues) = quantizedCache.updateQuantized(keys: k, values: v)
  69 | ///     // Use native quantized operations
  70 | ///     let scores = quantizedMatmul(queries, w: qKeys.0, scales: qKeys.1, biases: qKeys.2, ...)
  71 | /// } else {
  72 | ///     // Regular path
  73 | ///     let (k, v) = cache.update(keys: k, values: v)
  74 | ///     let output = MLXFast.scaledDotProductAttention(queries: q, keys: k, values: v, ...)
  75 | /// }
  76 | /// ```
  77 | public protocol QuantizedKVCacheProtocol: KVCache {
  78 |     /// The quantization group size used
  79 |     var groupSize: Int { get }
  80 | 
  81 |     /// The number of quantization bits used
  82 |     var bits: Int { get }
  83 | 
  84 |     /// Update cache and return quantized tuples for maximum efficiency
  85 |     ///
  86 |     /// - Parameters:
  87 |     ///   - keys: New key data to add to cache
  88 |     ///   - values: New value data to add to cache
  89 |     /// - Returns: Quantized tuples (keys, values) as ((weight, scales, biases), (weight, scales, biases))
  90 |     func updateQuantized(keys: MLXArray, values: MLXArray) -> (
  91 |         (MLXArray, MLXArray, MLXArray), (MLXArray, MLXArray, MLXArray)
  92 |     )
  93 | 
  94 |     /// Get current quantized state without updating
  95 |     ///
  96 |     /// Useful for accessing cached data without adding new tokens.
  97 |     /// - Returns: Current quantized state, or nil if cache is empty
  98 |     func getQuantizedState() -> ((MLXArray, MLXArray, MLXArray), (MLXArray, MLXArray, MLXArray))?
  99 | }
 100 | 
 101 | /// Base cache implementation providing default behaviors
 102 | open class BaseKVCache: KVCache {
 103 |     public var offset: Int = 0
 104 |     public var maxSize: Int? { nil }
 105 | 
 106 |     public func innerState() -> [MLXArray] { [] }
 107 | 
 108 |     open func update(keys: MLXArray, values: MLXArray) -> (MLXArray, MLXArray) {
 109 |         fatalError("update(keys:values:) must be implemented by subclass")
 110 |     }
 111 | 
 112 |     open var state: [MLXArray] {
 113 |         get { [] }
 114 |         set {
 115 |             if !newValue.isEmpty {
 116 |                 fatalError("This cache has no state but a state was set.")
 117 |             }
 118 |         }
 119 |     }
 120 | 
 121 |     open var metaState: [String] {
 122 |         get {
 123 |             // Python base class returns empty string, but we return empty array for Swift compatibility
 124 |             // This is handled in the save/load functions
 125 |             []
 126 |         }
 127 |         set {
 128 |             if !newValue.isEmpty {
 129 |                 fatalError("This cache has no meta_state but a meta_state was set.")
 130 |             }
 131 |         }
 132 |     }
 133 | 
 134 |     open var isTrimmable: Bool { false }
 135 | 
 136 |     @discardableResult
 137 |     open func trim(_ n: Int) -> Int { 0 }
 138 | }
 139 | 
 140 | func createCausalMask(n: Int, offset: Int, windowSize: Int? = nil) -> MLXArray {
 141 |     var rinds = MLXArray(Int32(0) ..< Int32(offset + n))
 142 |     var linds = offset != 0 ? MLXArray(Int32(offset) ..< Int32(offset + n)) : rinds
 143 |     linds = linds[0..., .newAxis]
 144 |     rinds = rinds[.newAxis]
 145 |     var mask = linds .>= rinds
 146 | 
 147 |     if let windowSize {
 148 |         mask = mask & (linds .< rinds + windowSize)
 149 |     }
 150 | 
 151 |     return mask
 152 | }
 153 | 
 154 | /// Create an attention mask using the parameters from the KVCache.
 155 | ///
 156 | /// See also ``MultiHeadAttention/createAdditiveCausalMask(_:dtype:)`` -- same idea
 157 | /// but doesn't honor the cache offset.
 158 | @_disfavoredOverload
 159 | public func createAttentionMask(h: MLXArray, cache: [KVCache]?) -> MLXArray? {
 160 |     let t = h.dim(1)
 161 |     if t > 1 {
 162 |         var offset = 0
 163 |         if let c = cache?.first {
 164 |             offset = c.offset
 165 |         }
 166 |         return createCausalMask(n: t, offset: offset)
 167 |     }
 168 |     return nil
 169 | }
 170 | 
 171 | public func createAttentionMask(h: MLXArray, cache: [KVCache]?, returnArray: Bool = false)
 172 |     -> MLXFast.ScaledDotProductAttentionMaskMode
 173 | {
 174 |     let t = h.dim(1)
 175 |     if t > 1 {
 176 |         var returnArray = returnArray
 177 |         var offset = 0
 178 |         var windowSize: Int? = nil
 179 |         if let c = cache?.first {
 180 |             offset = c.offset
 181 |             if let maxSize = c.maxSize {
 182 |                 windowSize = maxSize
 183 |                 offset = min(maxSize, offset)
 184 |                 if !returnArray {
 185 |                     returnArray = offset + t > maxSize
 186 |                 }
 187 |             }
 188 |         }
 189 | 
 190 |         if returnArray {
 191 |             return .array(createCausalMask(n: t, offset: offset, windowSize: windowSize))
 192 |         } else {
 193 |             return .causal
 194 |         }
 195 |     }
 196 |     return .none
 197 | }
 198 | 
 199 | /// Standard KV cache implementation based on Python's KVCache
 200 | /// See https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/models/base.py#L11
 201 | public class KVCacheSimple: BaseKVCache, CustomDebugStringConvertible {
 202 |     internal var keys: MLXArray?
 203 |     internal var values: MLXArray?
 204 |     public var step = 256
 205 | 
 206 |     public override init() {
 207 |         super.init()
 208 |     }
 209 | 
 210 |     public override func innerState() -> [MLXArray] {
 211 |         [self.keys, self.values].compactMap { $0 }
 212 |     }
 213 | 
 214 |     public override func update(keys: MLXArray, values: MLXArray) -> (MLXArray, MLXArray) {
 215 |         let previous = self.offset
 216 | 
 217 |         let reset =
 218 |             if let currentKeys = self.keys, (previous + keys.dim(2)) > currentKeys.dim(2) {
 219 |                 true
 220 |             } else {
 221 |                 self.keys == nil
 222 |             }
 223 |         if reset {
 224 |             let B = keys.dim(0)
 225 |             let kvHeads = keys.dim(1)
 226 |             let kHeadDim = keys.dim(3)
 227 |             let vHeadDim = values.dim(3)
 228 | 
 229 |             let nSteps = (step + keys.dim(2) - 1) / step
 230 |             let kShape = [B, kvHeads, nSteps * step, kHeadDim]
 231 |             let vShape = [B, kvHeads, nSteps * step, vHeadDim]
 232 |             let newK = MLXArray.zeros(kShape, dtype: keys.dtype)
 233 |             let newV = MLXArray.zeros(vShape, dtype: values.dtype)
 234 | 
 235 |             if var currentKeys = self.keys, var currentValues = self.values {
 236 |                 if previous % step != 0 {
 237 |                     currentKeys = currentKeys[.ellipsis, ..<previous, 0...]
 238 |                     currentValues = currentValues[.ellipsis, ..<previous, 0...]
 239 |                 }
 240 |                 self.keys = concatenated([currentKeys, newK], axis: 2)
 241 |                 self.values = concatenated([currentValues, newV], axis: 2)
 242 |             } else {
 243 |                 self.keys = newK
 244 |                 self.values = newV
 245 |             }
 246 |         }
 247 | 
 248 |         self.offset += keys.dim(2)
 249 | 
 250 |         self.keys?[.ellipsis, previous ..< self.offset, 0...] = keys
 251 |         self.values?[.ellipsis, previous ..< self.offset, 0...] = values
 252 | 
 253 |         let returnedKeys = self.keys![.ellipsis, ..<self.offset, 0...]
 254 |         let returnedValues = self.values![.ellipsis, ..<self.offset, 0...]
 255 | 
 256 |         return (returnedKeys, returnedValues)
 257 |     }
 258 | 
 259 |     public override var state: [MLXArray] {
 260 |         get {
 261 |             guard let keys = self.keys, let values = self.values else { return [] }
 262 |             if offset == keys.dim(2) {
 263 |                 return [keys, values]
 264 |             } else {
 265 |                 return [
 266 |                     keys[.ellipsis, ..<offset, 0...],
 267 |                     values[.ellipsis, ..<offset, 0...],
 268 |                 ]
 269 |             }
 270 |         }
 271 |         set {
 272 |             guard newValue.count == 2 else {
 273 |                 fatalError("KVCacheSimple state must have exactly 2 arrays (keys, values)")
 274 |             }
 275 |             self.keys = newValue[0]
 276 |             self.values = newValue[1]
 277 |             self.offset = self.keys!.dim(2)
 278 |         }
 279 |     }
 280 | 
 281 |     public override var metaState: [String] {
 282 |         get { [] }
 283 |         set {
 284 |             if !newValue.isEmpty {
 285 |                 fatalError("KVCacheSimple should not have metaState.")
 286 |             }
 287 |         }
 288 |     }
 289 | 
 290 |     public override var isTrimmable: Bool { true }
 291 | 
 292 |     @discardableResult
 293 |     public override func trim(_ n: Int) -> Int {
 294 |         let trimmed = min(offset, n)
 295 |         offset -= trimmed
 296 |         return trimmed
 297 |     }
 298 | 
 299 |     /// Convert to quantized cache for maximum efficiency
 300 |     ///
 301 |     /// Use `updateQuantized()` and `quantizedScaledDotProductAttention()` for zero-overhead operation.
 302 |     public func toQuantized(groupSize: Int = 64, bits: Int = 4) -> QuantizedKVCache {
 303 |         let quantizedCache = QuantizedKVCache(groupSize: groupSize, bits: bits)
 304 |         quantizedCache.offset = self.offset
 305 | 
 306 |         if let keys = self.keys, let values = self.values {
 307 |             // Quantize the current keys and values
 308 |             let currentKeys = keys[.ellipsis, ..<offset, 0...]
 309 |             let currentValues = values[.ellipsis, ..<offset, 0...]
 310 | 
 311 |             let quantizedKeys = quantized(currentKeys, groupSize: groupSize, bits: bits)
 312 |             let quantizedValues = quantized(currentValues, groupSize: groupSize, bits: bits)
 313 | 
 314 |             // Set the quantized state
 315 |             quantizedCache.state = [
 316 |                 quantizedKeys.wq, quantizedKeys.scales, quantizedKeys.biases,
 317 |                 quantizedValues.wq, quantizedValues.scales, quantizedValues.biases,
 318 |             ]
 319 |         }
 320 | 
 321 |         return quantizedCache
 322 |     }
 323 | 
 324 |     public var debugDescription: String {
 325 |         "\(String(describing: Self.self)) \(Unmanaged.passUnretained(self).toOpaque()), offset: \(offset), step: \(step), keys: \(keys?.shape.description ?? "-"), values: \(values?.shape.description ?? "-")"
 326 |     }
 327 | }
 328 | 
 329 | /// Rotating KV cache for sliding window attention
 330 | public class RotatingKVCache: BaseKVCache, CustomDebugStringConvertible {
 331 |     private var keep: Int
 332 |     private var keys: MLXArray?
 333 |     private var values: MLXArray?
 334 |     private var maxCacheSize: Int
 335 |     private var step: Int
 336 |     private var idx: Int = 0
 337 | 
 338 |     public override var maxSize: Int? { maxCacheSize }
 339 | 
 340 |     public init(maxSize: Int, keep: Int = 0, step: Int = 256) {
 341 |         self.maxCacheSize = maxSize
 342 |         self.keep = keep
 343 |         self.step = step
 344 |         super.init()
 345 |     }
 346 | 
 347 |     public override func innerState() -> [MLXArray] {
 348 |         [self.keys, self.values].compactMap { $0 }
 349 |     }
 350 | 
 351 |     private func trim(trimSize: Int, _ array: MLXArray, append: MLXArray? = nil) -> MLXArray {
 352 |         var toCat: [MLXArray] = []
 353 |         if trimSize > 0 {
 354 |             toCat = [
 355 |                 array[.ellipsis, ..<keep, 0...],
 356 |                 array[.ellipsis, (trimSize + keep)..., 0...],
 357 |             ]
 358 |         } else {
 359 |             toCat = [array]
 360 |         }
 361 |         if let append {
 362 |             toCat.append(append)
 363 |         }
 364 |         return concatenated(toCat, axis: 2)
 365 |     }
 366 | 
 367 |     private func temporalOrder(_ array: MLXArray) -> MLXArray {
 368 |         // Rearrange the cache into temporal order, slicing off the end if unused
 369 |         if idx == array.dim(2) {
 370 |             return array
 371 |         } else if idx < offset {
 372 |             return concatenated(
 373 |                 [
 374 |                     array[.ellipsis, ..<keep, 0...],
 375 |                     array[.ellipsis, idx..., 0...],
 376 |                     array[.ellipsis, keep ..< idx, 0...],
 377 |                 ], axis: 2)
 378 |         } else {
 379 |             return array[.ellipsis, ..<idx, 0...]
 380 |         }
 381 |     }
 382 | 
 383 |     private func updateConcat(keys: MLXArray, values: MLXArray) -> (MLXArray, MLXArray) {
 384 |         if self.keys == nil {
 385 |             self.keys = keys
 386 |             self.values = values
 387 |         } else {
 388 |             // Put the keys/values in temporal order to preserve context
 389 |             self.keys = temporalOrder(self.keys!)
 390 |             self.values = temporalOrder(self.values!)
 391 |             let trimSize = idx - maxCacheSize
 392 |             self.keys = trim(trimSize: trimSize, self.keys!, append: keys)
 393 |             self.values = trim(trimSize: trimSize, self.values!, append: values)
 394 |         }
 395 | 
 396 |         offset += keys.dim(2)
 397 |         idx = self.keys!.dim(2)
 398 | 
 399 |         return (self.keys!, self.values!)
 400 |     }
 401 | 
 402 |     private func updateInPlace(keys: MLXArray, values: MLXArray) -> (MLXArray, MLXArray) {
 403 |         let B = keys.dim(0)
 404 |         let nKVHeads = keys.dim(1)
 405 |         let S = keys.dim(2)
 406 |         let kHeadDim = keys.dim(3)
 407 |         let vHeadDim = values.dim(3)
 408 |         let prev = offset
 409 | 
 410 |         // May not have hit the max size yet, so potentially keep growing the cache
 411 |         if self.keys == nil
 412 |             || (prev >= self.keys!.dim(2) && self.keys!.dim(2) < maxCacheSize)
 413 |         {
 414 |             let newSize = min(step, maxCacheSize - prev)
 415 | 
 416 |             let kShape = [B, nKVHeads, newSize, kHeadDim]
 417 |             let vShape = [B, nKVHeads, newSize, vHeadDim]
 418 |             let newK = MLXArray.zeros(kShape, dtype: keys.dtype)
 419 |             let newV = MLXArray.zeros(vShape, dtype: values.dtype)
 420 | 
 421 |             if let currentKeys = self.keys, let currentValues = self.values {
 422 |                 self.keys = concatenated([currentKeys, newK], axis: 2)
 423 |                 self.values = concatenated([currentValues, newV], axis: 2)
 424 |             } else {
 425 |                 self.keys = newK
 426 |                 self.values = newV
 427 |             }
 428 |             idx = prev
 429 |         }
 430 | 
 431 |         // Trim if needed
 432 |         let trimSize = self.keys!.dim(2) - maxCacheSize
 433 |         if trimSize > 0 {
 434 |             self.keys = trim(trimSize: trimSize, self.keys!)
 435 |             self.values = trim(trimSize: trimSize, self.values!)
 436 |             idx = maxCacheSize
 437 |         }
 438 | 
 439 |         // Rotate if we've hit the end
 440 |         if idx == maxCacheSize {
 441 |             idx = keep
 442 |         }
 443 | 
 444 |         // Assign
 445 |         self.keys![.ellipsis, idx ..< (idx + S), 0...] = keys
 446 |         self.values![.ellipsis, idx ..< (idx + S), 0...] = values
 447 |         offset += S
 448 |         idx += S
 449 | 
 450 |         // Return the appropriate cache slice
 451 |         if offset < maxCacheSize {
 452 |             return (
 453 |                 self.keys![.ellipsis, ..<offset, 0...],
 454 |                 self.values![.ellipsis, ..<offset, 0...]
 455 |             )
 456 |         }
 457 |         return (self.keys!, self.values!)
 458 |     }
 459 | 
 460 |     public override func update(keys: MLXArray, values: MLXArray) -> (MLXArray, MLXArray) {
 461 |         let result =
 462 |             if keys.dim(2) == 1 {
 463 |                 updateInPlace(keys: keys, values: values)
 464 |             } else {
 465 |                 updateConcat(keys: keys, values: values)
 466 |             }
 467 |         return result
 468 |     }
 469 | 
 470 |     public override var state: [MLXArray] {
 471 |         get {
 472 |             guard let keys = self.keys, let values = self.values else { return [] }
 473 |             if offset < keys.dim(2) {
 474 |                 return [
 475 |                     keys[.ellipsis, ..<offset, 0...],
 476 |                     values[.ellipsis, ..<offset, 0...],
 477 |                 ]
 478 |             } else {
 479 |                 return [keys, values]
 480 |             }
 481 |         }
 482 |         set {
 483 |             guard newValue.count == 2 else {
 484 |                 fatalError("RotatingKVCache state must have exactly 2 arrays")
 485 |             }
 486 |             self.keys = newValue[0]
 487 |             self.values = newValue[1]
 488 |             // Note: RotatingKVCache doesn't set offset from keys like KVCache does
 489 |             // The offset is managed through meta_state
 490 |         }
 491 |     }
 492 | 
 493 |     public override var metaState: [String] {
 494 |         get {
 495 |             return [String(keep), String(maxCacheSize), String(step), String(offset), String(idx)]
 496 |         }
 497 |         set {
 498 |             guard newValue.count == 5 else {
 499 |                 fatalError("RotatingKVCache metaState must have exactly 5 values")
 500 |             }
 501 |             guard let keepVal = Int(newValue[0]),
 502 |                 let stepVal = Int(newValue[2]),
 503 |                 let offsetVal = Int(newValue[3]),
 504 |                 let idxVal = Int(newValue[4])
 505 |             else {
 506 |                 fatalError("Failed to convert metaState values to integers")
 507 |             }
 508 |             if newValue[1] == "None" {
 509 |                 fatalError(
 510 |                     "RotatingKVCache requires a non-nil maxSize. Cannot load cache with maxSize=None."
 511 |                 )
 512 |             }
 513 |             guard let maxSizeVal = Int(newValue[1]) else {
 514 |                 fatalError("Failed to convert maxCacheSize '\(newValue[1])' to integer")
 515 |             }
 516 |             self.keep = keepVal
 517 |             self.maxCacheSize = maxSizeVal
 518 |             self.step = stepVal
 519 |             self.offset = offsetVal
 520 |             self.idx = idxVal
 521 |         }
 522 |     }
 523 | 
 524 |     public override var isTrimmable: Bool {
 525 |         return offset < maxCacheSize
 526 |     }
 527 | 
 528 |     @discardableResult
 529 |     public override func trim(_ n: Int) -> Int {
 530 |         let trimmed = min(offset, n)
 531 |         offset -= trimmed
 532 |         idx -= trimmed
 533 |         return trimmed
 534 |     }
 535 | 
 536 |     public var debugDescription: String {
 537 |         "\(String(describing: Self.self)) offset: \(offset), maxSize: \(maxCacheSize.description), keep: \(keep), idx: \(idx)"
 538 |     }
 539 | 
 540 |     /// Convert to quantized cache
 541 |     /// Note: This is complex due to the rotating nature and temporal ordering
 542 |     public func toQuantized(groupSize: Int = 64, bits: Int = 4) -> QuantizedKVCache {
 543 |         // For now, throw an error like the Python version does
 544 |         // A full implementation would need to handle the temporal ordering correctly
 545 |         fatalError(
 546 |             "RotatingKVCache quantization not yet implemented - temporal ordering makes this complex"
 547 |         )
 548 | 
 549 |         // Future implementation would need to:
 550 |         // 1. Put keys/values in temporal order using temporalOrder()
 551 |         // 2. Quantize the temporally ordered arrays
 552 |         // 3. Store metadata about rotation state
 553 |         // 4. Implement corresponding dequantization with rotation restoration
 554 |     }
 555 | }
 556 | 
 557 | /// Quantized KV cache for memory efficiency using MLX quantization
 558 | public class QuantizedKVCache: BaseKVCache, QuantizedKVCacheProtocol {
 559 |     private var keys: (MLXArray, MLXArray, MLXArray)?
 560 |     private var values: (MLXArray, MLXArray, MLXArray)?
 561 |     private let step: Int
 562 |     public let groupSize: Int
 563 |     public let bits: Int
 564 | 
 565 |     public init(groupSize: Int = 64, bits: Int = 8) {
 566 |         self.groupSize = groupSize
 567 |         self.bits = bits
 568 |         self.step = 256
 569 |         super.init()
 570 |     }
 571 | 
 572 |     public override func innerState() -> [MLXArray] {
 573 |         var arrays: [MLXArray] = []
 574 |         if let keys = keys {
 575 |             arrays.append(contentsOf: [keys.0, keys.1, keys.2])
 576 |         }
 577 |         if let values = values {
 578 |             arrays.append(contentsOf: [values.0, values.1, values.2])
 579 |         }
 580 |         return arrays
 581 |     }
 582 | 
 583 |     /// Tree map equivalent for applying function to tuple elements
 584 |     private func treeMap<T>(_ transform: (MLXArray) -> T, _ tuple: (MLXArray, MLXArray, MLXArray))
 585 |         -> (T, T, T)
 586 |     {
 587 |         return (transform(tuple.0), transform(tuple.1), transform(tuple.2))
 588 |     }
 589 | 
 590 |     /// Tree map for two tuples (like Python's tree_map over (keys, values))
 591 |     private func treeMapPair<T>(
 592 |         _ transform: (MLXArray) -> T, _ tuple1: (MLXArray, MLXArray, MLXArray),
 593 |         _ tuple2: (MLXArray, MLXArray, MLXArray)
 594 |     ) -> ((T, T, T), (T, T, T)) {
 595 |         return (treeMap(transform, tuple1), treeMap(transform, tuple2))
 596 |     }
 597 | 
 598 |     /// Create initial quantized tuples (like Python's init_quant)
 599 |     private func initQuant(dim: Int, shape: [Int], dtype: DType) -> (MLXArray, MLXArray, MLXArray) {
 600 |         // Create temporary zero arrays and quantize them using native MLX Swift
 601 |         let tempArray = MLXArray.zeros(shape + [dim], dtype: dtype)
 602 |         let quantized = quantized(tempArray, groupSize: groupSize, bits: bits)
 603 | 
 604 |         return (quantized.wq, quantized.scales, quantized.biases)
 605 |     }
 606 | 
 607 |     /// Expand quantized tuple
 608 |     private func expandQuant(_ quantTuple: (MLXArray, MLXArray, MLXArray), newShape: [Int]) -> (
 609 |         MLXArray, MLXArray, MLXArray
 610 |     ) {
 611 |         return treeMap(
 612 |             { array in
 613 |                 let newArray = MLXArray.zeros(newShape + [array.dim(-1)], dtype: array.dtype)
 614 |                 return concatenated([array, newArray], axis: -2)
 615 |             }, quantTuple)
 616 |     }
 617 | 
 618 |     /// Get current quantized keys and values as tuples (efficient access)
 619 |     /// - Returns: Tuple of ((keyWeight, keyScales, keyBiases), (valueWeight, valueScales, valueBiases))
 620 |     public func getQuantizedState() -> (
 621 |         (MLXArray, MLXArray, MLXArray), (MLXArray, MLXArray, MLXArray)
 622 |     )? {
 623 |         guard let keys = keys, let values = values else { return nil }
 624 | 
 625 |         let trimmedKeys = treeMap({ $0[.ellipsis, ..<offset, 0...] }, keys)
 626 |         let trimmedValues = treeMap({ $0[.ellipsis, ..<offset, 0...] }, values)
 627 | 
 628 |         return (trimmedKeys, trimmedValues)
 629 |     }
 630 | 
 631 |     /// Update cache and return quantized tuples (Python's update_and_fetch)
 632 |     /// This is needed because `update` in Swift must return `(MLXArray, MLXArray)`
 633 |     ///
 634 |     /// - Parameters:
 635 |     ///   - keys: New key data to add to cache
 636 |     ///   - values: New value data to add to cache
 637 |     /// - Returns: Quantized tuples (keys, values) as ((weight, scales, biases), (weight, scales, biases))
 638 |     public func updateQuantized(keys: MLXArray, values: MLXArray) -> (
 639 |         (MLXArray, MLXArray, MLXArray), (MLXArray, MLXArray, MLXArray)
 640 |     ) {
 641 |         let B = keys.dim(0)
 642 |         let nKVHeads = keys.dim(1)
 643 |         let numSteps = keys.dim(2)
 644 |         let kHeadDim = keys.dim(3)
 645 |         let vHeadDim = values.dim(3)
 646 |         let prev = offset
 647 | 
 648 |         // Check if we need to expand the cache
 649 |         if self.keys == nil || (prev + numSteps) > self.keys!.0.dim(-2) {
 650 |             let newSteps = ((step + numSteps - 1) / step) * step
 651 |             let shape = [B, nKVHeads, newSteps]
 652 | 
 653 |             if let existingKeys = self.keys, let existingValues = self.values {
 654 |                 // Trim if needed
 655 |                 if prev % step != 0 {
 656 |                     // Use tree_map equivalent to trim both keys and values
 657 |                     let (trimmedKeys, trimmedValues) = treeMapPair(
 658 |                         { array in
 659 |                             array[.ellipsis, ..<prev, 0...]
 660 |                         }, existingKeys, existingValues)
 661 | 
 662 |                     self.keys = trimmedKeys
 663 |                     self.values = trimmedValues
 664 |                 }
 665 | 
 666 |                 // Expand using tree_map equivalent (Python's tree_map(expand_quant, ...))
 667 |                 self.keys = expandQuant(self.keys!, newShape: shape)
 668 |                 self.values = expandQuant(self.values!, newShape: shape)
 669 |             } else {
 670 |                 // Initialize new quantized cache
 671 |                 self.keys = initQuant(dim: kHeadDim, shape: shape, dtype: keys.dtype)
 672 |                 self.values = initQuant(dim: vHeadDim, shape: shape, dtype: keys.dtype)
 673 |             }
 674 |         }
 675 | 
 676 |         offset += numSteps
 677 | 
 678 |         let quantizedKeys = quantized(keys, groupSize: groupSize, bits: bits)
 679 |         let quantizedValues = quantized(values, groupSize: groupSize, bits: bits)
 680 | 
 681 |         // Convert named tuples to positional tuples
 682 |         let qKeys = (quantizedKeys.wq, quantizedKeys.scales, quantizedKeys.biases)
 683 |         let qValues = (quantizedValues.wq, quantizedValues.scales, quantizedValues.biases)
 684 | 
 685 |         // Assign to storage
 686 |         guard let currentKeys = self.keys, let currentValues = self.values else {
 687 |             fatalError("Quantized cache not properly initialized")
 688 |         }
 689 | 
 690 |         // Update each component of the quantized tuples
 691 |         currentKeys.0[.ellipsis, prev ..< offset, 0...] = qKeys.0
 692 |         currentKeys.1[.ellipsis, prev ..< offset, 0...] = qKeys.1
 693 |         currentKeys.2[.ellipsis, prev ..< offset, 0...] = qKeys.2
 694 | 
 695 |         currentValues.0[.ellipsis, prev ..< offset, 0...] = qValues.0
 696 |         currentValues.1[.ellipsis, prev ..< offset, 0...] = qValues.1
 697 |         currentValues.2[.ellipsis, prev ..< offset, 0...] = qValues.2
 698 | 
 699 |         self.keys = currentKeys
 700 |         self.values = currentValues
 701 | 
 702 |         // Return quantized tuples
 703 |         let trimmedKeys = treeMap({ $0[.ellipsis, ..<offset, 0...] }, currentKeys)
 704 |         let trimmedValues = treeMap({ $0[.ellipsis, ..<offset, 0...] }, currentValues)
 705 | 
 706 |         return (trimmedKeys, trimmedValues)
 707 |     }
 708 | 
 709 |     /// This method is required by the KVCache protocol, but it is not intended to be used with QuantizedKVCache.
 710 |     /// Use `updateQuantized` instead.
 711 |     public override func update(keys: MLXArray, values: MLXArray) -> (MLXArray, MLXArray) {
 712 |         fatalError(
 713 |             "`update` was called on `QuantizedKVCache`. Use `updateQuantized` instead."
 714 |         )
 715 |     }
 716 | 
 717 |     public override var state: [MLXArray] {
 718 |         get {
 719 |             guard let keys = keys, let values = values else { return [] }
 720 | 
 721 |             if offset < keys.0.dim(2) {
 722 |                 // Trim to current offset using tree_map
 723 |                 let trimmedKeys = treeMap({ $0[.ellipsis, ..<offset, 0...] }, keys)
 724 |                 let trimmedValues = treeMap({ $0[.ellipsis, ..<offset, 0...] }, values)
 725 |                 // Flatten tuples to array for serialization
 726 |                 return [
 727 |                     trimmedKeys.0, trimmedKeys.1, trimmedKeys.2, trimmedValues.0, trimmedValues.1,
 728 |                     trimmedValues.2,
 729 |                 ]
 730 |             } else {
 731 |                 // Flatten tuples to array for serialization
 732 |                 return [keys.0, keys.1, keys.2, values.0, values.1, values.2]
 733 |             }
 734 |         }
 735 |         set {
 736 |             guard newValue.count == 6 else {
 737 |                 fatalError(
 738 |                     "QuantizedKVCache state must have exactly 6 arrays (3 for keys, 3 for values)")
 739 |             }
 740 | 
 741 |             // Reconstruct tuples from flat array
 742 |             keys = (newValue[0], newValue[1], newValue[2])
 743 |             values = (newValue[3], newValue[4], newValue[5])
 744 |         }
 745 |     }
 746 | 
 747 |     public override var metaState: [String] {
 748 |         get { [String(step), String(offset), String(groupSize), String(bits)] }
 749 |         set {
 750 |             guard newValue.count == 4 else {
 751 |                 fatalError("QuantizedKVCache metaState must have exactly 4 values")
 752 |             }
 753 | 
 754 |             self.offset = Int(newValue[1]) ?? 0
 755 | 
 756 |             // Validate that step, groupSize, and bits match current instance
 757 |             let expectedStep = Int(newValue[0]) ?? 256
 758 |             let expectedGroupSize = Int(newValue[2]) ?? 64
 759 |             let expectedBits = Int(newValue[3]) ?? 8
 760 |         }
 761 |     }
 762 | 
 763 |     public override var isTrimmable: Bool { true }
 764 | 
 765 |     @discardableResult
 766 |     public override func trim(_ n: Int) -> Int {
 767 |         let trimmed = min(offset, n)
 768 |         offset -= trimmed
 769 |         return trimmed
 770 |     }
 771 | 
 772 |     /// Convert to unquantized cache
 773 |     public func toUnquantized() -> KVCacheSimple {
 774 |         let simpleCache = KVCacheSimple()
 775 |         simpleCache.offset = self.offset
 776 | 
 777 |         if let keys = keys, let values = values {
 778 |             // Dequantize the current state using tree_map approach
 779 |             let currentKeys = treeMap({ $0[.ellipsis, ..<offset, 0...] }, keys)
 780 |             let currentValues = treeMap({ $0[.ellipsis, ..<offset, 0...] }, values)
 781 | 
 782 |             let dequantizedKeys = dequantized(
 783 |                 currentKeys.0, scales: currentKeys.1, biases: currentKeys.2,
 784 |                 groupSize: groupSize, bits: bits)
 785 |             let dequantizedValues = dequantized(
 786 |                 currentValues.0, scales: currentValues.1, biases: currentValues.2,
 787 |                 groupSize: groupSize, bits: bits)
 788 | 
 789 |             // Set the unquantized state
 790 |             simpleCache.state = [dequantizedKeys, dequantizedValues]
 791 |         }
 792 | 
 793 |         return simpleCache
 794 |     }
 795 | }
 796 | 
 797 | /// Chunked KV cache for processing large contexts in chunks
 798 | public class ChunkedKVCache: KVCacheSimple {
 799 |     private var chunkSize: Int?
 800 |     private var startPosition: Int = 0
 801 | 
 802 |     public init(chunkSize: Int? = nil) {
 803 |         self.chunkSize = chunkSize
 804 |         super.init()
 805 |     }
 806 | 
 807 |     public func maybeTrimFront() {
 808 |         guard let keys = self.keys,
 809 |             let chunkSize = chunkSize,
 810 |             keys.dim(2) >= chunkSize
 811 |         else { return }
 812 | 
 813 |         startPosition += keys.dim(2) - chunkSize
 814 |         self.keys = keys[.ellipsis, (-chunkSize)..., 0...]
 815 |         self.values = values?[.ellipsis, (-chunkSize)..., 0...]
 816 |     }
 817 | 
 818 |     public override func update(keys: MLXArray, values: MLXArray) -> (MLXArray, MLXArray) {
 819 |         let prev = offset - startPosition
 820 | 
 821 |         if self.keys == nil || (prev + keys.dim(2)) > self.keys!.dim(2) {
 822 |             let B = keys.dim(0)
 823 |             let kvHeads = keys.dim(1)
 824 |             let kHeadDim = keys.dim(3)
 825 |             let vHeadDim = values.dim(3)
 826 | 
 827 |             let nSteps = (step + keys.dim(2) - 1) / step
 828 |             let kShape = [B, kvHeads, nSteps * step, kHeadDim]
 829 |             let vShape = [B, kvHeads, nSteps * step, vHeadDim]
 830 |             let newK = MLXArray.zeros(kShape, dtype: keys.dtype)
 831 |             let newV = MLXArray.zeros(vShape, dtype: values.dtype)
 832 | 
 833 |             if var currentKeys = self.keys, var currentValues = self.values {
 834 |                 if prev % step != 0 {
 835 |                     currentKeys = currentKeys[.ellipsis, ..<prev, 0...]
 836 |                     currentValues = currentValues[.ellipsis, ..<prev, 0...]
 837 |                 }
 838 |                 self.keys = concatenated([currentKeys, newK], axis: 2)
 839 |                 self.values = concatenated([currentValues, newV], axis: 2)
 840 |             } else {
 841 |                 self.keys = newK
 842 |                 self.values = newV
 843 |             }
 844 |         }
 845 | 
 846 |         offset += keys.dim(2)
 847 |         let end = offset - startPosition
 848 |         self.keys![.ellipsis, prev ..< end, 0...] = keys
 849 |         self.values![.ellipsis, prev ..< end, 0...] = values
 850 | 
 851 |         return (self.keys![.ellipsis, ..<end, 0...], self.values![.ellipsis, ..<end, 0...])
 852 |     }
 853 | 
 854 |     @discardableResult
 855 |     public override func trim(_ n: Int) -> Int {
 856 |         let trimmed = min(offset - startPosition, n)
 857 |         offset -= trimmed
 858 |         return trimmed
 859 |     }
 860 | 
 861 |     public override var metaState: [String] {
 862 |         get {
 863 |             let chunkSizeStr = chunkSize?.description ?? "None"
 864 |             return [chunkSizeStr, String(startPosition)]
 865 |         }
 866 |         set {
 867 |             guard newValue.count == 2 else {
 868 |                 fatalError("ChunkedKVCache metaState must have exactly 2 values")
 869 |             }
 870 |             if newValue[0] == "None" {
 871 |                 self.chunkSize = nil
 872 |             } else {
 873 |                 self.chunkSize = Int(newValue[0])
 874 |             }
 875 |             self.startPosition = Int(newValue[1]) ?? 0
 876 |         }
 877 |     }
 878 | }
 879 | 
 880 | /// Simple cache for Mamba-style state space models
 881 | public class MambaCache: BaseKVCache {
 882 |     private var cache: [MLXArray?] = [nil, nil]
 883 | 
 884 |     public override init() {
 885 |         super.init()
 886 |     }
 887 | 
 888 |     public override func innerState() -> [MLXArray] {
 889 |         cache.compactMap { $0 }
 890 |     }
 891 | 
 892 |     public subscript(index: Int) -> MLXArray? {
 893 |         get { cache[index] }
 894 |         set { cache[index] = newValue }
 895 |     }
 896 | 
 897 |     public override func update(keys: MLXArray, values: MLXArray) -> (MLXArray, MLXArray) {
 898 |         // Mamba doesn't use traditional KV cache update pattern
 899 |         fatalError("MambaCache should not use update(keys:values:) - use subscript access instead")
 900 |     }
 901 | 
 902 |     public override var state: [MLXArray] {
 903 |         get {
 904 |             // Need to preserve the structure including nils, similar to Python version
 905 |             // Use empty arrays as placeholders for nil values
 906 |             var result: [MLXArray] = []
 907 |             for item in cache {
 908 |                 if let array = item {
 909 |                     result.append(array)
 910 |                 } else {
 911 |                     // Use an empty array as placeholder for nil (this shape should never occur naturally)
 912 |                     result.append(MLXArray.zeros([0], dtype: .float32))
 913 |                 }
 914 |             }
 915 |             return result
 916 |         }
 917 |         set {
 918 |             guard newValue.count == cache.count else {
 919 |                 fatalError("MambaCache state must have exactly \(cache.count) elements")
 920 |             }
 921 |             for (i, array) in newValue.enumerated() {
 922 |                 // Check if this is our nil placeholder (empty array with size 0)
 923 |                 if array.size == 0 {
 924 |                     cache[i] = nil
 925 |                 } else {
 926 |                     cache[i] = array
 927 |                 }
 928 |             }
 929 |         }
 930 |     }
 931 | }
 932 | 
 933 | /// Composite cache that manages multiple sub-caches
 934 | public class CacheList: BaseKVCache {
 935 |     private var caches: [KVCache]
 936 | 
 937 |     public init(_ caches: KVCache...) {
 938 |         self.caches = caches
 939 |         super.init()
 940 |     }
 941 | 
 942 |     public override func innerState() -> [MLXArray] {
 943 |         caches.flatMap { $0.innerState() }
 944 |     }
 945 | 
 946 |     public subscript(index: Int) -> KVCache {
 947 |         return caches[index]
 948 |     }
 949 | 
 950 |     public override func update(keys: MLXArray, values: MLXArray) -> (MLXArray, MLXArray) {
 951 |         fatalError("CacheList should not use update(keys:values:) - use subscript access instead")
 952 |     }
 953 | 
 954 |     public override var state: [MLXArray] {
 955 |         get { caches.flatMap { $0.state } }
 956 |         set {
 957 |             let stateLengths = caches.map { $0.state.count }
 958 |             var start = 0
 959 |             for i in 0 ..< caches.count {
 960 |                 let length = stateLengths[i]
 961 |                 caches[i].state = Array(newValue[start ..< (start + length)])
 962 |                 start += length
 963 |             }
 964 |         }
 965 |     }
 966 | 
 967 |     public override var isTrimmable: Bool {
 968 |         caches.allSatisfy { $0.isTrimmable }
 969 |     }
 970 | 
 971 |     @discardableResult
 972 |     public override func trim(_ n: Int) -> Int {
 973 |         return caches.first?.trim(n) ?? 0
 974 |     }
 975 | }
 976 | 
 977 | // MARK: - Error Types
 978 | 
 979 | struct KVCacheError: Error {
 980 |     let message: String
 981 | }
 982 | 
 983 | // MARK: - Utility Functions
 984 | 
 985 | /// Save a pre-computed prompt cache to a file.
 986 | ///
 987 | /// - Parameters:
 988 | ///   - url: The URL to the `.safetensors` file
 989 | ///   - cache: The model cache state
 990 | ///   - metadata: Optional metadata to save along with cache state
 991 | public func savePromptCache(
 992 |     url: URL,
 993 |     cache: [KVCache],
 994 |     metadata: [String: String] = [:]
 995 | ) throws {
 996 |     let cacheData = cache.map { $0.state }
 997 |     let cacheInfo = cache.map { $0.metaState }
 998 |     // Use Python-compatible class names for cross-platform compatibility
 999 |     let cacheClasses = cache.map { cache -> String in
1000 |         switch cache {
1001 |         case is KVCacheSimple:
1002 |             return "KVCache"  // Python uses "KVCache" for the basic cache
1003 |         case is RotatingKVCache:
1004 |             return "RotatingKVCache"
1005 |         case is QuantizedKVCache:
1006 |             return "QuantizedKVCache"
1007 |         case is ChunkedKVCache:
1008 |             return "ChunkedKVCache"
1009 |         case is MambaCache:
1010 |             return "MambaCache"
1011 |         case is CacheList:
1012 |             return "CacheList"
1013 |         default:
1014 |             return "KVCache"  // Default fallback
1015 |         }
1016 |     }
1017 | 
1018 |     // Flatten cache data using tree_flatten compatible structure: "i.j" format
1019 |     var flattenedData: [String: MLXArray] = [:]
1020 |     for (i, arrays) in cacheData.enumerated() {
1021 |         for (j, array) in arrays.enumerated() {
1022 |             flattenedData["\(i).\(j)"] = array
1023 |         }
1024 |     }
1025 | 
1026 |     // Create cache_metadata structure compatible with Python: [cache_info, metadata, cache_classes]
1027 |     var flattenedMetadata: [String: String] = [:]
1028 | 
1029 |     // Flatten cache_info as "0.i.j" (first element of cache_metadata)
1030 |     for (i, info) in cacheInfo.enumerated() {
1031 |         for (j, metaValue) in info.enumerated() {
1032 |             flattenedMetadata["0.\(i).\(j)"] = metaValue
1033 |         }
1034 |     }
1035 | 
1036 |     // Flatten user metadata as "1.key" (second element of cache_metadata)
1037 |     for (key, value) in metadata {
1038 |         flattenedMetadata["1.\(key)"] = value
1039 |     }
1040 | 
1041 |     // Flatten cache_classes as "2.i" (third element of cache_metadata)
1042 |     for (i, className) in cacheClasses.enumerated() {
1043 |         flattenedMetadata["2.\(i)"] = className
1044 |     }
1045 | 
1046 |     try save(arrays: flattenedData, metadata: flattenedMetadata, url: url)
1047 | }
1048 | 
1049 | /// Load a prompt cache from a file.
1050 | ///
1051 | /// - Parameters:
1052 | ///   - url: The URL to the `.safetensors` file
1053 | /// - Returns: The prompt cache and the metadata
1054 | public func loadPromptCache(
1055 |     url: URL
1056 | ) throws -> ([KVCache], [String: String]?) {
1057 |     let (arrays, metadata) = try loadArraysAndMetadata(url: url)
1058 | 
1059 |     // Unflatten arrays using tree_unflatten compatible logic
1060 |     let cacheData = unflattenArrays(arrays)
1061 | 
1062 |     // Unflatten metadata using tree_unflatten compatible logic
1063 |     let unflattenedMetadata = unflattenMetadata(metadata)
1064 | 
1065 |     // Extract cache_info, user_metadata, and cache_classes from unflattened structure
1066 |     // Structure: [cache_info, user_metadata, cache_classes]
1067 |     guard unflattenedMetadata.count >= 3 else {
1068 |         throw KVCacheError(message: "Invalid cache metadata format")
1069 |     }
1070 | 
1071 |     let cacheInfo = unflattenedMetadata[0] as? [[String]] ?? []
1072 |     let userMetadata = unflattenedMetadata[1] as? [String: String] ?? [:]
1073 |     let cacheClasses = unflattenedMetadata[2] as? [String] ?? []
1074 | 
1075 |     guard cacheData.count == cacheInfo.count && cacheData.count == cacheClasses.count else {
1076 |         throw KVCacheError(message: "Mismatch in cache counts")
1077 |     }
1078 | 
1079 |     // Reconstruct cache instances
1080 |     var caches: [KVCache] = []
1081 |     for i in 0 ..< cacheData.count {
1082 |         let className = cacheClasses[i]
1083 | 
1084 |         var cache: KVCache
1085 |         switch className {
1086 |         case "KVCache", "KVCacheSimple":  // Handle both Python and Swift names
1087 |             cache = KVCacheSimple()
1088 |         case "RotatingKVCache":
1089 |             // Parse metaState first to get maxSize, then create cache
1090 |             let info = i < cacheInfo.count ? cacheInfo[i] : []
1091 |             guard info.count >= 5 else {
1092 |                 throw KVCacheError(message: "Invalid RotatingKVCache metaState - expected 5 values")
1093 |             }
1094 |             if info[1] == "None" {
1095 |                 throw KVCacheError(
1096 |                     message:
1097 |                         "RotatingKVCache with maxSize=None is not supported. This cache was created with invalid parameters."
1098 |                 )
1099 |             }
1100 |             guard let maxSize = Int(info[1]) else {
1101 |                 throw KVCacheError(
1102 |                     message: "Failed to parse RotatingKVCache maxSize from: \(info[1])")
1103 |             }
1104 |             cache = RotatingKVCache(maxSize: maxSize)  // Create with parsed maxSize
1105 |         case "QuantizedKVCache":
1106 |             cache = QuantizedKVCache()
1107 |         case "ChunkedKVCache":
1108 |             cache = ChunkedKVCache()
1109 |         case "MambaCache":
1110 |             cache = MambaCache()
1111 |         case "CacheList":
1112 |             // Note: CacheList requires special handling as it contains sub-caches
1113 |             // For now, create an empty CacheList - this may not work correctly
1114 |             // for complex cache hierarchies loaded from Python
1115 |             cache = CacheList()
1116 |             print("Warning: CacheList loading may not preserve sub-cache structure correctly")
1117 |         default:
1118 |             throw KVCacheError(message: "Unknown cache class: \(className)")
1119 |         }
1120 | 
1121 |         cache.state = cacheData[i]
1122 |         if i < cacheInfo.count {
1123 |             cache.metaState = cacheInfo[i]
1124 |         }
1125 |         caches.append(cache)
1126 |     }
1127 | 
1128 |     return (caches, userMetadata)
1129 | }
1130 | 
1131 | /// Unflatten arrays from tree_flatten format (e.g., "0.1", "1.0") to nested structure
1132 | private func unflattenArrays(_ flatArrays: [String: MLXArray]) -> [[MLXArray]] {
1133 |     var arrayMap: [Int: [Int: MLXArray]] = [:]
1134 | 
1135 |     // Parse all keys and organize by indices
1136 |     for (key, array) in flatArrays {
1137 |         let components = key.split(separator: ".")
1138 |         if components.count >= 2,
1139 |             let i = Int(components[0]),
1140 |             let j = Int(components[1])
1141 |         {
1142 |             if arrayMap[i] == nil {
1143 |                 arrayMap[i] = [:]
1144 |             }
1145 |             arrayMap[i]![j] = array
1146 |         }
1147 |     }
1148 | 
1149 |     // Convert to ordered array structure
1150 |     var result: [[MLXArray]] = []
1151 |     let maxI = arrayMap.keys.max() ?? -1
1152 | 
1153 |     for i in 0 ... maxI {
1154 |         if let innerMap = arrayMap[i] {
1155 |             let maxJ = innerMap.keys.max() ?? -1
1156 |             var innerArray: [MLXArray] = []
1157 |             for j in 0 ... maxJ {
1158 |                 if let array = innerMap[j] {
1159 |                     innerArray.append(array)
1160 |                 }
1161 |             }
1162 |             result.append(innerArray)
1163 |         } else {
1164 |             result.append([])
1165 |         }
1166 |     }
1167 | 
1168 |     return result
1169 | }
1170 | 
1171 | /// Unflatten metadata from tree_flatten format to nested structure
1172 | private func unflattenMetadata(_ flatMetadata: [String: String]) -> [Any] {
1173 |     var cacheInfo: [[String]] = []
1174 |     var userMetadata: [String: String] = [:]
1175 |     var cacheClasses: [String] = []
1176 | 
1177 |     for (key, value) in flatMetadata {
1178 |         let components = key.split(separator: ".")
1179 | 
1180 |         if components.count >= 3 && components[0] == "0" {
1181 |             // Cache info: "0.i.j" format
1182 |             if let i = Int(components[1]), let j = Int(components[2]) {
1183 |                 // Ensure cacheInfo is large enough
1184 |                 while cacheInfo.count <= i {
1185 |                     cacheInfo.append([])
1186 |                 }
1187 |                 // Ensure inner array is large enough
1188 |                 while cacheInfo[i].count <= j {
1189 |                     cacheInfo[i].append("")
1190 |                 }
1191 |                 cacheInfo[i][j] = value
1192 |             }
1193 |         } else if components.count >= 2 && components[0] == "1" {
1194 |             // User metadata: "1.key" format
1195 |             let metaKey = components.dropFirst().joined(separator: ".")
1196 |             userMetadata[metaKey] = value
1197 |         } else if components.count >= 2 && components[0] == "2" {
1198 |             // Cache classes: "2.i" format
1199 |             if let i = Int(components[1]) {
1200 |                 // Ensure cacheClasses is large enough
1201 |                 while cacheClasses.count <= i {
1202 |                     cacheClasses.append("")
1203 |                 }
1204 |                 cacheClasses[i] = value
1205 |             }
1206 |         }
1207 |     }
1208 | 
1209 |     return [cacheInfo, userMetadata, cacheClasses]
1210 | }
1211 | 
1212 | /// Construct the model's cache for use when generating.
1213 | ///
1214 | /// This function will defer the cache construction to the model if it has a
1215 | /// `newCache` method, otherwise it will make a default KV cache.
1216 | public func makePromptCache(
1217 |     model: any LanguageModel,
1218 |     parameters: GenerateParameters? = nil
1219 | ) -> [KVCache] {
1220 |     // The model already conforms to LanguageModel which has newCache
1221 |     // If it also conforms to KVCacheDimensionProvider, the extension will provide the implementation
1222 |     return model.newCache(parameters: parameters)
1223 | }
1224 | 
1225 | /// Legacy function for backwards compatibility
1226 | public func makePromptCache(
1227 |     model: any LanguageModel,
1228 |     maxKVSize: Int? = nil
1229 | ) -> [KVCache] {
1230 |     let parameters = maxKVSize.map { GenerateParameters(maxKVSize: $0) }
1231 |     return makePromptCache(model: model, parameters: parameters)
1232 | }
1233 | 
1234 | /// Fallback function to create cache when layer count is known
1235 | ///
1236 | /// This function creates a default cache structure when the number of layers is known.
1237 | /// Use this when `makePromptCache` cannot determine the layer count automatically.
1238 | public func makePromptCacheWithLayerCount(
1239 |     numLayers: Int,
1240 |     maxKVSize: Int? = nil
1241 | ) -> [KVCache] {
1242 |     if let maxKVSize = maxKVSize {
1243 |         return (0 ..< numLayers).map { _ in
1244 |             RotatingKVCache(maxSize: maxKVSize, keep: 4)
1245 |         }
1246 |     } else {
1247 |         return (0 ..< numLayers).map { _ in KVCacheSimple() }
1248 |     }
1249 | }
1250 | 
1251 | /// Check if model's cache can be trimmed.
1252 | public func canTrimPromptCache(_ cache: [KVCache]) -> Bool {
1253 |     return cache.allSatisfy { $0.isTrimmable }
1254 | }
1255 | 
1256 | /// Trim the model's cache by the given number of tokens.
1257 | ///
1258 | /// This function will trim the cache if possible (in-place) and return the
1259 | /// number of tokens that were trimmed.
1260 | @discardableResult
1261 | public func trimPromptCache(_ cache: [KVCache], numTokens: Int) -> Int {
1262 |     guard canTrimPromptCache(cache), !cache.isEmpty else { return 0 }
1263 |     return cache.first?.trim(numTokens) ?? 0
1264 | }
1265 | 
1266 | // MARK: - Type Aliases
1267 | 
1268 | /// Standard KV cache - alias to KVCacheSimple for compatibility
1269 | public typealias StandardKVCache = KVCacheSimple
1270 | 
1271 | // MARK: - Quantized Attention Operations
1272 | 
1273 | public func quantizedScaledDotProductAttention(
1274 |     queries: MLXArray,
1275 |     quantizedKeys: (MLXArray, MLXArray, MLXArray),
1276 |     quantizedValues: (MLXArray, MLXArray, MLXArray),
1277 |     scale: Float,
1278 |     mask: MLXFast.ScaledDotProductAttentionMaskMode = .none,
1279 |     groupSize: Int = 64,
1280 |     bits: Int = 8
1281 | ) -> MLXArray {
1282 | 
1283 |     let (B, nQHeads, L, D) = (queries.dim(0), queries.dim(1), queries.dim(2), queries.dim(3))
1284 |     let nKVHeads = quantizedKeys.0.dim(-3)
1285 |     let nRepeats = nQHeads / nKVHeads
1286 | 
1287 |     // Scale queries
1288 |     var scaledQueries = queries * scale
1289 | 
1290 |     // Handle GQA (Grouped Query Attention)
1291 |     var qKeys = quantizedKeys
1292 |     var qValues = quantizedValues
1293 |     if nRepeats > 1 {
1294 |         scaledQueries = scaledQueries.reshaped([B, nKVHeads, nRepeats, L, D])
1295 |         qKeys = (
1296 |             expandedDimensions(qKeys.0, axis: -3),
1297 |             expandedDimensions(qKeys.1, axis: -3),
1298 |             expandedDimensions(qKeys.2, axis: -3)
1299 |         )
1300 |         qValues = (
1301 |             expandedDimensions(qValues.0, axis: -3),
1302 |             expandedDimensions(qValues.1, axis: -3),
1303 |             expandedDimensions(qValues.2, axis: -3)
1304 |         )
1305 |     }
1306 | 
1307 |     // Compute attention scores using quantized matmul
1308 |     var scores = quantizedMatmul(
1309 |         scaledQueries, qKeys.0, scales: qKeys.1, biases: qKeys.2,
1310 |         transpose: true, groupSize: groupSize, bits: bits
1311 |     )
1312 | 
1313 |     // Apply mask
1314 |     switch mask {
1315 |     case .causal:
1316 |         let (qL, kL) = (scores.dim(-2), scores.dim(-1))
1317 |         let qIndices = MLXArray(0 ..< qL) + MLXArray(kL - qL)
1318 |         let kIndices = MLXArray(0 ..< kL)
1319 |         let causalMask = greaterEqual(
1320 |             expandedDimensions(qIndices, axis: -1), expandedDimensions(kIndices, axis: -2))
1321 |         scores = MLX.where(causalMask, scores, MLXArray(Float.leastNormalMagnitude))
1322 | 
1323 |     case .array(let maskArray):
1324 |         if maskArray.dtype == .bool {
1325 |             scores = MLX.where(maskArray, scores, MLXArray(Float.leastNormalMagnitude))
1326 |         } else {
1327 |             scores = scores + maskArray
1328 |         }
1329 | 
1330 |     case .arrays(let maskArrays):
1331 |         // Handle multiple mask arrays - just use the first one for simplicity
1332 |         if let maskArray = maskArrays.first {
1333 |             if maskArray.dtype == .bool {
1334 |                 scores = MLX.where(maskArray, scores, MLXArray(Float.leastNormalMagnitude))
1335 |             } else {
1336 |                 scores = scores + maskArray
1337 |             }
1338 |         }
1339 | 
1340 |     case .none:
1341 |         break
1342 |     }
1343 | 
1344 |     let attentionWeights = softmax(scores, axis: -1)
1345 | 
1346 |     // Compute output using quantized matmul
1347 |     var output = quantizedMatmul(
1348 |         attentionWeights, qValues.0, scales: qValues.1, biases: qValues.2,
1349 |         transpose: false, groupSize: groupSize, bits: bits
1350 |     )
1351 | 
1352 |     // Reshape output for GQA
1353 |     if nRepeats > 1 {
1354 |         output = output.reshaped([B, nQHeads, L, D])
1355 |     }
1356 | 
1357 |     return output
1358 | }
1359 | 
1360 | // MARK: - Dynamic Cache Quantization
1361 | 
1362 | /// Dynamically quantize KV caches during generation if conditions are met
1363 | ///
1364 | /// Converts regular caches to quantized caches when:
1365 | /// - kvBits is specified
1366 | /// - The cache is not already quantized
1367 | /// - The cache offset is greater than quantizedKVStart
1368 | ///
1369 | /// - Parameters:
1370 | ///   - cache: Array of KV caches to potentially quantize
1371 | ///   - kvBits: Number of bits for quantization (nil = no quantization)
1372 | ///   - kvGroupSize: Group size for quantization
1373 | ///   - quantizedKVStart: Token count threshold to begin quantizing
1374 | public func maybeQuantizeKVCache(
1375 |     cache: inout [KVCache],
1376 |     kvBits: Int?,
1377 |     kvGroupSize: Int = 64,
1378 |     quantizedKVStart: Int = 0
1379 | ) {
1380 |     guard let kvBits = kvBits,
1381 |         !cache.isEmpty,
1382 |         !(cache[0] is QuantizedKVCache),
1383 |         cache[0].offset > quantizedKVStart
1384 |     else {
1385 |         return
1386 |     }
1387 | 
1388 |     for i in 0 ..< cache.count {
1389 |         // Handle cache types that support quantization
1390 |         if let simpleCache = cache[i] as? KVCacheSimple {
1391 |             cache[i] = simpleCache.toQuantized(groupSize: kvGroupSize, bits: kvBits)
1392 |         }
1393 |         // TODO: RotatingKVCache.toQuantized() is not implemented yet, like in Python.
1394 |         // When implemented, add: else if let rotatingCache = cache[i] as? RotatingKVCache { ... }
1395 |         // MambaCache and CacheList don't use traditional KV quantization
1396 |     }
1397 | }
1398 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/LanguageModel.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import Foundation
  4 | import Hub
  5 | import MLX
  6 | import MLXNN
  7 | import Tokenizers
  8 | 
  9 | /// Time/Height/Width struct to represent information about input images.
 10 | public struct THW: Sendable {
 11 | 
 12 |     public let t: Int
 13 |     public let h: Int
 14 |     public let w: Int
 15 | 
 16 |     public init(_ t: Int, _ h: Int, _ w: Int) {
 17 |         self.t = t
 18 |         self.h = h
 19 |         self.w = w
 20 |     }
 21 | 
 22 |     public var values: (Int, Int, Int) {
 23 |         (t, h, w)
 24 |     }
 25 | 
 26 |     public var product: Int { t * h * w }
 27 | }
 28 | 
 29 | /// Representation of ``LanguageModel`` input.
 30 | ///
 31 | /// This can contain text (tokens), prepared images (`MLXArray`), or other media as
 32 | /// needed. ``LMInput`` is produced by ``UserInputProcessor`` in response
 33 | /// to ``UserInput``.
 34 | ///
 35 | /// The ``ModelContext`` holds the ``UserInputProcessor`` associated with a
 36 | /// ``LanguageModel``.
 37 | public struct LMInput {
 38 |     public let text: Text
 39 |     public let image: ProcessedImage?
 40 |     public let video: ProcessedVideo?
 41 | 
 42 |     /// Representation of tokenized input text.
 43 |     public struct Text {
 44 | 
 45 |         /// input token array
 46 |         public let tokens: MLXArray
 47 | 
 48 |         /// optional mask array
 49 |         public let mask: MLXArray?
 50 | 
 51 |         public init(tokens: MLXArray, mask: MLXArray? = nil) {
 52 |             self.tokens = tokens
 53 |             self.mask = mask
 54 |         }
 55 | 
 56 |         public subscript(
 57 |             indices: MLXArrayIndex..., stream stream: StreamOrDevice = .default
 58 |         ) -> Text {
 59 |             Text(tokens: tokens[indices, stream: stream], mask: mask?[indices, stream: stream])
 60 |         }
 61 | 
 62 |         public subscript(
 63 |             text indices: MLXArrayIndex..., stream stream: StreamOrDevice = .default
 64 |         ) -> Text {
 65 |             Text(tokens: tokens[indices, stream: stream], mask: mask)
 66 |         }
 67 |     }
 68 | 
 69 |     /// Representation of prepared input image(s).
 70 |     public struct ProcessedImage {
 71 | 
 72 |         /// Concatenated pixels from one or more images
 73 |         public let pixels: MLXArray
 74 |         /// Time, height, and width of the images
 75 |         public let frames: [THW]?
 76 | 
 77 |         public init(
 78 |             pixels: MLXArray, frames: [THW]? = nil
 79 |         ) {
 80 |             self.pixels = pixels
 81 |             self.frames = frames
 82 |         }
 83 |     }
 84 | 
 85 |     /// Representation of prepared input video(s).
 86 |     /// For now, this is virtually identical to ProcessedImage.
 87 |     public struct ProcessedVideo {
 88 | 
 89 |         public let pixels: MLXArray
 90 |         public let frames: [THW]?
 91 | 
 92 |         public init(
 93 |             pixels: MLXArray, frames: [THW]? = nil
 94 |         ) {
 95 |             self.pixels = pixels
 96 |             self.frames = frames
 97 |         }
 98 |     }
 99 | 
100 |     public init(tokens: MLXArray, mask: MLXArray? = nil) {
101 |         self.init(text: .init(tokens: tokens, mask: mask))
102 |     }
103 | 
104 |     public init(
105 |         text: LMInput.Text, image: LMInput.ProcessedImage? = nil,
106 |         video: LMInput.ProcessedVideo? = nil
107 |     ) {
108 |         self.text = text
109 |         self.image = image
110 |         self.video = video
111 |     }
112 | }
113 | 
114 | /// ``LanguageModel`` step output. This is consumed internally
115 | /// by the ``TokenIterator``.
116 | public struct LMOutput {
117 | 
118 |     /// logits (one hot vector of probabilities for tokens)
119 |     public let logits: MLXArray
120 | 
121 |     /// optional ``State`` to carry forward into the next step
122 |     public let state: State?
123 | 
124 |     public struct State {
125 |         public let crossAttentionStates: MLXArray?
126 | 
127 |         public init(crossAttentionStates: MLXArray? = nil) {
128 |             self.crossAttentionStates = crossAttentionStates
129 |         }
130 |     }
131 | 
132 |     public init(logits: MLXArray, state: LMOutput.State? = nil) {
133 |         self.logits = logits
134 |         self.state = state
135 |     }
136 | }
137 | 
138 | /// The result of the call to ``LanguageModel/prepare(_:cache:windowSize:)``
139 | public enum PrepareResult {
140 |     /// tokens to process by the ``TokenIterator``
141 |     case tokens(LMInput.Text)
142 | 
143 |     /// logits representing the next token
144 |     case logits(LMOutput)
145 | }
146 | 
147 | /// Interface for all Language Models (e.g. LLM, VLM).
148 | ///
149 | /// The language model is typically called by the ``TokenIterator`` and it:
150 | ///
151 | /// - consumes the ``LMInput``
152 | /// - calls ``prepare(_:cache:windowSize:)`` to initialize the KVCache and consume the prompt
153 | /// - calls ``callAsFunction(_:cache:state:)-9kuvf`` for each token, producing an ``LMOutput``
154 | /// - the ``TokenIterator`` accumulates this information into a ``GenerateResult``
155 | public protocol LanguageModel: Module {
156 | 
157 |     /// Prepare the cache state and consume the ``LMInput``.
158 |     ///
159 |     /// This can return:
160 |     /// - ``PrepareResult/tokens(_:)`` if the caller should evaluate the (remaining) tokens normally
161 |     /// - ``PrepareResult/logits(_:)`` to produce the next token from the prompt
162 |     func prepare(_ input: LMInput, cache: [KVCache], windowSize: Int?) throws -> PrepareResult
163 | 
164 |     /// Primary entry point to produce a step (single token) from the model
165 |     func callAsFunction(_ input: LMInput.Text, cache: [KVCache]?, state: LMOutput.State?)
166 |         -> LMOutput
167 | 
168 |     /// Models may implement this simplified interface if they do not produce any ``LMOutput/State``
169 |     func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray
170 | 
171 |     /// create a new array of ``KVCache`` -- automatic implementation if self
172 |     /// implements ``KVCacheDimensionProvider``
173 |     func newCache(parameters: GenerateParameters?) -> [KVCache]
174 | 
175 |     /// Optionally preprocess the weights and modify / remove values as needed.
176 |     func sanitize(weights: [String: MLXArray]) -> [String: MLXArray]
177 | }
178 | 
179 | extension LanguageModel {
180 |     public func callAsFunction(_ input: LMInput.Text, cache: [KVCache]?, state: LMOutput.State?)
181 |         -> LMOutput
182 |     {
183 |         let logits = callAsFunction(input.tokens, cache: cache)
184 |         return .init(logits: logits)
185 |     }
186 | 
187 |     public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {
188 |         fatalError("callAsFunction(inputs:cache:) not implemented for \(Self.self)")
189 |     }
190 | }
191 | 
192 | extension LanguageModel {
193 |     public func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {
194 |         weights
195 |     }
196 | }
197 | 
198 | /// Optional protocol that can be implemented by ``LanguageModel`` and will
199 | /// provide an automatic implementation of ``LanguageModel/newCache(parameters:)``
200 | public protocol KVCacheDimensionProvider {
201 |     var kvHeads: [Int] { get }
202 | }
203 | 
204 | extension LanguageModel where Self: KVCacheDimensionProvider {
205 |     public func newCache(parameters: GenerateParameters?) -> [KVCache] {
206 |         // Create one cache per layer (kvHeads.count = number of layers)
207 |         // The number of heads per layer (kvHeads[i]) is not used for cache creation
208 |         let numLayers = kvHeads.count
209 | 
210 |         // Follow Python logic: use RotatingKVCache if maxKVSize is provided
211 |         if let maxKVSize = parameters?.maxKVSize {
212 |             return (0 ..< numLayers).map { _ in
213 |                 RotatingKVCache(maxSize: maxKVSize, keep: 4)
214 |             }
215 |         } else {
216 |             return (0 ..< numLayers).map { _ in KVCacheSimple() }
217 |         }
218 |     }
219 | }
220 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/Load.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import Foundation
  4 | import Hub
  5 | import MLX
  6 | import MLXNN
  7 | import Tokenizers
  8 | 
  9 | /// Download the model using the `HubApi`.
 10 | ///
 11 | /// This will download `*.safetensors` and `*.json` if the ``ModelConfiguration``
 12 | /// represents a Hub id, e.g. `mlx-community/gemma-2-2b-it-4bit`.
 13 | ///
 14 | /// This is typically called via ``ModelFactory/load(hub:configuration:progressHandler:)``
 15 | ///
 16 | /// - Parameters:
 17 | ///   - hub: HubApi instance
 18 | ///   - configuration: the model identifier
 19 | ///   - progressHandler: callback for progress
 20 | /// - Returns: URL for the directory containing downloaded files
 21 | public func downloadModel(
 22 |     hub: HubApi, configuration: ModelConfiguration,
 23 |     progressHandler: @Sendable @escaping (Progress) -> Void
 24 | ) async throws -> URL {
 25 |     do {
 26 |         switch configuration.id {
 27 |         case .id(let id, let revision):
 28 |             // download the model weights
 29 |             let repo = Hub.Repo(id: id)
 30 |             let modelFiles = ["*.safetensors", "*.json"]
 31 |             return try await hub.snapshot(
 32 |                 from: repo,
 33 |                 revision: revision,
 34 |                 matching: modelFiles,
 35 |                 progressHandler: progressHandler
 36 |             )
 37 |         case .directory(let directory):
 38 |             return directory
 39 |         }
 40 | 
 41 |     } catch Hub.HubClientError.authorizationRequired {
 42 |         // an authorizationRequired means (typically) that the named repo doesn't exist on
 43 |         // on the server so retry with local only configuration
 44 |         return configuration.modelDirectory(hub: hub)
 45 | 
 46 |     } catch {
 47 |         let nserror = error as NSError
 48 |         if nserror.domain == NSURLErrorDomain && nserror.code == NSURLErrorNotConnectedToInternet {
 49 |             // Error Domain=NSURLErrorDomain Code=-1009 "The Internet connection appears to be offline."
 50 |             // fall back to the local directory
 51 |             return configuration.modelDirectory(hub: hub)
 52 |         } else {
 53 |             throw error
 54 |         }
 55 |     }
 56 | }
 57 | 
 58 | /// Load model weights.
 59 | ///
 60 | /// This is typically called via ``ModelFactory/load(hub:configuration:progressHandler:)``.
 61 | /// This function loads all `safetensor` files in the given `modelDirectory`,
 62 | /// calls ``LanguageModel/sanitize(weights:)``, applies optional quantization, and
 63 | /// updates the model with the weights.
 64 | public func loadWeights(
 65 |     modelDirectory: URL, model: LanguageModel,
 66 |     quantization: BaseConfiguration.Quantization? = nil,
 67 |     perLayerQuantization: BaseConfiguration.PerLayerQuantization? = nil
 68 | ) throws {
 69 |     // load the weights
 70 |     var weights = [String: MLXArray]()
 71 |     let enumerator = FileManager.default.enumerator(
 72 |         at: modelDirectory, includingPropertiesForKeys: nil)!
 73 |     for case let url as URL in enumerator {
 74 |         if url.pathExtension == "safetensors" {
 75 |             let w = try loadArrays(url: url)
 76 |             for (key, value) in w {
 77 |                 weights[key] = value
 78 |             }
 79 |         }
 80 |     }
 81 | 
 82 |     // per-model cleanup
 83 |     weights = model.sanitize(weights: weights)
 84 | 
 85 |     // quantize if needed
 86 |     if quantization != nil || perLayerQuantization != nil {
 87 |         quantize(model: model) { path, module in
 88 |             if weights["\(path).scales"] != nil {
 89 |                 if let perLayerQuantization {
 90 |                     return perLayerQuantization.quantization(layer: path)?.asTuple
 91 |                 } else {
 92 |                     return quantization?.asTuple
 93 |                 }
 94 |             } else {
 95 |                 return nil
 96 |             }
 97 |         }
 98 |     }
 99 | 
100 |     // apply the loaded weights
101 |     let parameters = ModuleParameters.unflattened(weights)
102 |     try model.update(parameters: parameters, verify: [.all])
103 | 
104 |     eval(model)
105 | }
106 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/ModelConfiguration.swift:
--------------------------------------------------------------------------------
 1 | // Copyright © 2024 Apple Inc.
 2 | 
 3 | import Foundation
 4 | import Hub
 5 | 
 6 | /// Configuration for a given model name with overrides for prompts and tokens.
 7 | ///
 8 | /// See e.g. `MLXLM.ModelRegistry` for an example of use.
 9 | public struct ModelConfiguration: Sendable {
10 | 
11 |     public enum Identifier: Sendable {
12 |         case id(String, revision: String = "main")
13 |         case directory(URL)
14 |     }
15 | 
16 |     public var id: Identifier
17 | 
18 |     public var name: String {
19 |         switch id {
20 |         case .id(let id, _):
21 |             id
22 |         case .directory(let url):
23 |             url.deletingLastPathComponent().lastPathComponent + "/" + url.lastPathComponent
24 |         }
25 |     }
26 | 
27 |     /// pull the tokenizer from an alternate id
28 |     public let tokenizerId: String?
29 | 
30 |     /// overrides for TokenizerModel/knownTokenizers -- useful before swift-transformers is updated
31 |     public let overrideTokenizer: String?
32 | 
33 |     /// A reasonable default prompt for the model
34 |     public var defaultPrompt: String
35 | 
36 |     /// Additional tokens to use for end of string
37 |     public var extraEOSTokens: Set<String>
38 | 
39 |     public init(
40 |         id: String, revision: String = "main",
41 |         tokenizerId: String? = nil, overrideTokenizer: String? = nil,
42 |         defaultPrompt: String = "hello",
43 |         extraEOSTokens: Set<String> = [],
44 |         preparePrompt: (@Sendable (String) -> String)? = nil
45 |     ) {
46 |         self.id = .id(id, revision: revision)
47 |         self.tokenizerId = tokenizerId
48 |         self.overrideTokenizer = overrideTokenizer
49 |         self.defaultPrompt = defaultPrompt
50 |         self.extraEOSTokens = extraEOSTokens
51 |     }
52 | 
53 |     public init(
54 |         directory: URL,
55 |         tokenizerId: String? = nil, overrideTokenizer: String? = nil,
56 |         defaultPrompt: String = "hello",
57 |         extraEOSTokens: Set<String> = []
58 |     ) {
59 |         self.id = .directory(directory)
60 |         self.tokenizerId = tokenizerId
61 |         self.overrideTokenizer = overrideTokenizer
62 |         self.defaultPrompt = defaultPrompt
63 |         self.extraEOSTokens = extraEOSTokens
64 |     }
65 | 
66 |     public func modelDirectory(hub: HubApi = HubApi()) -> URL {
67 |         switch id {
68 |         case .id(let id, _):
69 |             // download the model weights and config
70 |             let repo = Hub.Repo(id: id)
71 |             return hub.localRepoLocation(repo)
72 | 
73 |         case .directory(let directory):
74 |             return directory
75 |         }
76 |     }
77 | }
78 | 
79 | extension ModelConfiguration: Equatable {
80 | 
81 | }
82 | 
83 | extension ModelConfiguration.Identifier: Equatable {
84 | 
85 |     public static func == (lhs: ModelConfiguration.Identifier, rhs: ModelConfiguration.Identifier)
86 |         -> Bool
87 |     {
88 |         switch (lhs, rhs) {
89 |         case (.id(let lhsID, let lhsRevision), .id(let rhsID, let rhsRevision)):
90 |             lhsID == rhsID && lhsRevision == rhsRevision
91 |         case (.directory(let lhsURL), .directory(let rhsURL)):
92 |             lhsURL == rhsURL
93 |         default:
94 |             false
95 |         }
96 |     }
97 | }
98 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/ModelContainer.swift:
--------------------------------------------------------------------------------
 1 | // Copyright © 2024 Apple Inc.
 2 | 
 3 | import Foundation
 4 | import Hub
 5 | import MLX
 6 | import MLXNN
 7 | import Tokenizers
 8 | 
 9 | /// Container for models that guarantees single threaded access.
10 | ///
11 | /// Wrap models used by e.g. the UI in a ModelContainer. Callers can access
12 | /// the model and/or tokenizer (any values from the ``ModelContext``):
13 | ///
14 | /// ```swift
15 | /// let messages = [["role": "user", "content": prompt]]
16 | /// let promptTokens = try await modelContainer.perform { context in
17 | ///     try context.tokenizer.applyChatTemplate(messages: messages)
18 | /// }
19 | /// ```
20 | ///
21 | /// or:
22 | ///
23 | /// ```swift
24 | /// let userInput: UserInput
25 | /// let result = await modelContainer.perform { context in
26 | ///     let input = try await context.processor.prepare(input: userInput)
27 | ///     return generate(
28 | ///         input: input, parameters: generateParameters, context: context
29 | ///     ) { tokens in
30 | ///     ...
31 | ///     }
32 | /// }
33 | /// ```
34 | public actor ModelContainer {
35 |     var context: ModelContext
36 |     public var configuration: ModelConfiguration { context.configuration }
37 | 
38 |     public init(context: ModelContext) {
39 |         self.context = context
40 |     }
41 | 
42 |     /// Perform an action on the model and/or tokenizer. Callers _must_ eval any `MLXArray` before returning as
43 |     /// `MLXArray` is not `Sendable`.
44 |     @available(*, deprecated, message: "prefer perform(_:) that uses a ModelContext")
45 |     public func perform<R>(_ action: @Sendable (any LanguageModel, Tokenizer) throws -> R) rethrows
46 |         -> R
47 |     {
48 |         try action(context.model, context.tokenizer)
49 |     }
50 | 
51 |     /// Perform an action on the model and/or tokenizer with additional context values.
52 |     /// Callers _must_ eval any `MLXArray` before returning as
53 |     /// `MLXArray` is not `Sendable`.
54 |     @available(*, deprecated, message: "prefer perform(values:_:) that uses a ModelContext")
55 |     public func perform<V, R>(
56 |         values: V, _ action: @Sendable (any LanguageModel, Tokenizer, V) throws -> R
57 |     ) rethrows -> R {
58 |         try action(context.model, context.tokenizer, values)
59 |     }
60 | 
61 |     /// Perform an action on the ``ModelContext``. Callers _must_ eval any `MLXArray` before returning as
62 |     /// `MLXArray` is not `Sendable`.
63 |     public func perform<R>(_ action: @Sendable (ModelContext) async throws -> R) async rethrows -> R
64 |     {
65 |         try await action(context)
66 |     }
67 | 
68 |     /// Perform an action on the ``ModelContext`` with additional context values.
69 |     /// Callers _must_ eval any `MLXArray` before returning as
70 |     /// `MLXArray` is not `Sendable`.
71 |     public func perform<V, R>(
72 |         values: V, _ action: @Sendable (ModelContext, V) async throws -> R
73 |     ) async rethrows -> R {
74 |         try await action(context, values)
75 |     }
76 | 
77 |     /// Update the owned `ModelContext`.
78 |     /// - Parameter action: update action
79 |     public func update(_ action: @Sendable (inout ModelContext) -> Void) {
80 |         action(&context)
81 |     }
82 | }
83 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/ModelFactory.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import Foundation
  4 | import Hub
  5 | import Tokenizers
  6 | 
  7 | public enum ModelFactoryError: LocalizedError {
  8 |     case unsupportedModelType(String)
  9 |     case unsupportedProcessorType(String)
 10 |     case configurationDecodingError(String, String, DecodingError)
 11 |     case noModelFactoryAvailable
 12 | 
 13 |     public var errorDescription: String? {
 14 |         switch self {
 15 |         case .unsupportedModelType(let type):
 16 |             return "Unsupported model type: \(type)"
 17 |         case .unsupportedProcessorType(let type):
 18 |             return "Unsupported processor type: \(type)"
 19 |         case .noModelFactoryAvailable:
 20 |             return "No model factory available via ModelFactoryRegistry"
 21 |         case .configurationDecodingError(let file, let modelName, let decodingError):
 22 |             let errorDetail = extractDecodingErrorDetail(decodingError)
 23 |             return "Failed to parse \(file) for model '\(modelName)': \(errorDetail)"
 24 |         }
 25 |     }
 26 | 
 27 |     private func extractDecodingErrorDetail(_ error: DecodingError) -> String {
 28 |         switch error {
 29 |         case .keyNotFound(let key, let context):
 30 |             let path = (context.codingPath + [key]).map { $0.stringValue }.joined(separator: ".")
 31 |             return "Missing field '\(path)'"
 32 |         case .typeMismatch(_, let context):
 33 |             let path = context.codingPath.map { $0.stringValue }.joined(separator: ".")
 34 |             return "Type mismatch at '\(path)'"
 35 |         case .valueNotFound(_, let context):
 36 |             let path = context.codingPath.map { $0.stringValue }.joined(separator: ".")
 37 |             return "Missing value at '\(path)'"
 38 |         case .dataCorrupted(let context):
 39 |             if context.codingPath.isEmpty {
 40 |                 return "Invalid JSON"
 41 |             } else {
 42 |                 let path = context.codingPath.map { $0.stringValue }.joined(separator: ".")
 43 |                 return "Invalid data at '\(path)'"
 44 |             }
 45 |         @unknown default:
 46 |             return error.localizedDescription
 47 |         }
 48 |     }
 49 | }
 50 | 
 51 | /// Context of types that work together to provide a ``LanguageModel``.
 52 | ///
 53 | /// A ``ModelContext`` is created by ``ModelFactory/load(hub:configuration:progressHandler:)``.
 54 | /// This contains the following:
 55 | ///
 56 | /// - ``ModelConfiguration`` -- identifier for the model
 57 | /// - ``LanguageModel`` -- the model itself, see ``generate(input:parameters:context:didGenerate:)``
 58 | /// - ``UserInputProcessor`` -- can convert ``UserInput`` into ``LMInput``
 59 | /// - `Tokenizer` -- the tokenizer used by ``UserInputProcessor``
 60 | ///
 61 | /// See also ``ModelFactory/loadContainer(hub:configuration:progressHandler:)`` and
 62 | /// ``ModelContainer``.
 63 | public struct ModelContext {
 64 |     public var configuration: ModelConfiguration
 65 |     public var model: any LanguageModel
 66 |     public var processor: any UserInputProcessor
 67 |     public var tokenizer: Tokenizer
 68 | 
 69 |     public init(
 70 |         configuration: ModelConfiguration, model: any LanguageModel,
 71 |         processor: any UserInputProcessor, tokenizer: any Tokenizer
 72 |     ) {
 73 |         self.configuration = configuration
 74 |         self.model = model
 75 |         self.processor = processor
 76 |         self.tokenizer = tokenizer
 77 |     }
 78 | }
 79 | 
 80 | /// Protocol for code that can load models.
 81 | ///
 82 | /// ## See Also
 83 | /// - ``loadModel(hub:id:progressHandler:)``
 84 | /// - ``loadModel(hub:directory:progressHandler:)``
 85 | /// - ``loadModelContainer(hub:id:progressHandler:)``
 86 | /// - ``loadModelContainer(hub:directory:progressHandler:)``
 87 | public protocol ModelFactory: Sendable {
 88 | 
 89 |     var modelRegistry: AbstractModelRegistry { get }
 90 | 
 91 |     func _load(
 92 |         hub: HubApi, configuration: ModelConfiguration,
 93 |         progressHandler: @Sendable @escaping (Progress) -> Void
 94 |     ) async throws -> sending ModelContext
 95 | 
 96 |     func _loadContainer(
 97 |         hub: HubApi, configuration: ModelConfiguration,
 98 |         progressHandler: @Sendable @escaping (Progress) -> Void
 99 |     ) async throws -> ModelContainer
100 | 
101 | }
102 | 
103 | extension ModelFactory {
104 | 
105 |     /// Resolve a model identifier, e.g. "mlx-community/Llama-3.2-3B-Instruct-4bit", into
106 |     /// a ``ModelConfiguration``.
107 |     ///
108 |     /// This will either create a new (mostly unconfigured) ``ModelConfiguration`` or
109 |     /// return a registered instance that matches the id.
110 |     ///
111 |     /// - Note: If the id doesn't exists in the configuration, this will return a new instance of it.
112 |     /// If you want to check if the configuration in model registry, you should use ``contains(id:)``.
113 |     public func configuration(id: String) -> ModelConfiguration {
114 |         modelRegistry.configuration(id: id)
115 |     }
116 | 
117 |     /// Returns true if ``modelRegistry`` contains a model with the id. Otherwise, false.
118 |     public func contains(id: String) -> Bool {
119 |         modelRegistry.contains(id: id)
120 |     }
121 | 
122 | }
123 | 
124 | /// Default instance of HubApi to use.  This is configured to save downloads into the caches directory.
125 | public var defaultHubApi: HubApi = {
126 |     HubApi(downloadBase: FileManager.default.urls(for: .cachesDirectory, in: .userDomainMask).first)
127 | }()
128 | 
129 | extension ModelFactory {
130 | 
131 |     /// Load a model identified by a ``ModelConfiguration`` and produce a ``ModelContext``.
132 |     ///
133 |     /// This method returns a ``ModelContext``. See also
134 |     /// ``loadContainer(hub:configuration:progressHandler:)`` for a method that
135 |     /// returns a ``ModelContainer``.
136 |     ///
137 |     /// ## See Also
138 |     /// - ``loadModel(hub:id:progressHandler:)``
139 |     /// - ``loadModelContainer(hub:id:progressHandler:)``
140 |     public func load(
141 |         hub: HubApi = defaultHubApi, configuration: ModelConfiguration,
142 |         progressHandler: @Sendable @escaping (Progress) -> Void = { _ in }
143 |     ) async throws -> sending ModelContext {
144 |         try await _load(hub: hub, configuration: configuration, progressHandler: progressHandler)
145 |     }
146 | 
147 |     /// Load a model identified by a ``ModelConfiguration`` and produce a ``ModelContainer``.
148 |     public func loadContainer(
149 |         hub: HubApi = defaultHubApi, configuration: ModelConfiguration,
150 |         progressHandler: @Sendable @escaping (Progress) -> Void = { _ in }
151 |     ) async throws -> ModelContainer {
152 |         try await _loadContainer(
153 |             hub: hub, configuration: configuration, progressHandler: progressHandler)
154 |     }
155 | 
156 |     public func _loadContainer(
157 |         hub: HubApi = defaultHubApi, configuration: ModelConfiguration,
158 |         progressHandler: @Sendable @escaping (Progress) -> Void = { _ in }
159 |     ) async throws -> ModelContainer {
160 |         let context = try await _load(
161 |             hub: hub, configuration: configuration, progressHandler: progressHandler)
162 |         return ModelContainer(context: context)
163 |     }
164 | 
165 | }
166 | 
167 | /// Load a model given a ``ModelConfiguration``.
168 | ///
169 | /// This will load and return a ``ModelContext``.  This holds the model and tokenzier without
170 | /// an `actor` providing an isolation context.  Use this call when you control the isolation context
171 | /// and can hold the ``ModelContext`` directly.
172 | ///
173 | /// - Parameters:
174 | ///   - hub: optional HubApi -- by default uses ``defaultHubApi``
175 | ///   - configuration: a ``ModelConfiguration``
176 | ///   - progressHandler: optional callback for progress
177 | /// - Returns: a ``ModelContext``
178 | public func loadModel(
179 |     hub: HubApi = defaultHubApi, configuration: ModelConfiguration,
180 |     progressHandler: @Sendable @escaping (Progress) -> Void = { _ in }
181 | ) async throws -> sending ModelContext {
182 |     try await load {
183 |         try await $0.load(hub: hub, configuration: configuration, progressHandler: progressHandler)
184 |     }
185 | }
186 | 
187 | /// Load a model given a ``ModelConfiguration``.
188 | ///
189 | /// This will load and return a ``ModelContainer``.  This holds a ``ModelContext``
190 | /// inside an actor providing isolation control for the values.
191 | ///
192 | /// - Parameters:
193 | ///   - hub: optional HubApi -- by default uses ``defaultHubApi``
194 | ///   - configuration: a ``ModelConfiguration``
195 | ///   - progressHandler: optional callback for progress
196 | /// - Returns: a ``ModelContainer``
197 | public func loadModelContainer(
198 |     hub: HubApi = defaultHubApi, configuration: ModelConfiguration,
199 |     progressHandler: @Sendable @escaping (Progress) -> Void = { _ in }
200 | ) async throws -> sending ModelContainer {
201 |     try await load {
202 |         try await $0.loadContainer(
203 |             hub: hub, configuration: configuration, progressHandler: progressHandler)
204 |     }
205 | }
206 | 
207 | /// Load a model given a huggingface identifier.
208 | ///
209 | /// This will load and return a ``ModelContext``.  This holds the model and tokenzier without
210 | /// an `actor` providing an isolation context.  Use this call when you control the isolation context
211 | /// and can hold the ``ModelContext`` directly.
212 | ///
213 | /// - Parameters:
214 | ///   - hub: optional HubApi -- by default uses ``defaultHubApi``
215 | ///   - id: huggingface model identifier, e.g "mlx-community/Qwen3-4B-4bit"
216 | ///   - progressHandler: optional callback for progress
217 | /// - Returns: a ``ModelContext``
218 | public func loadModel(
219 |     hub: HubApi = defaultHubApi, id: String, revision: String = "main",
220 |     progressHandler: @Sendable @escaping (Progress) -> Void = { _ in }
221 | ) async throws -> sending ModelContext {
222 |     try await load {
223 |         try await $0.load(
224 |             hub: hub, configuration: .init(id: id, revision: revision),
225 |             progressHandler: progressHandler)
226 |     }
227 | }
228 | 
229 | /// Load a model given a huggingface identifier.
230 | ///
231 | /// This will load and return a ``ModelContainer``.  This holds a ``ModelContext``
232 | /// inside an actor providing isolation control for the values.
233 | ///
234 | /// - Parameters:
235 | ///   - hub: optional HubApi -- by default uses ``defaultHubApi``
236 | ///   - id: huggingface model identifier, e.g "mlx-community/Qwen3-4B-4bit"
237 | ///   - progressHandler: optional callback for progress
238 | /// - Returns: a ``ModelContainer``
239 | public func loadModelContainer(
240 |     hub: HubApi = defaultHubApi, id: String, revision: String = "main",
241 |     progressHandler: @Sendable @escaping (Progress) -> Void = { _ in }
242 | ) async throws -> sending ModelContainer {
243 |     try await load {
244 |         try await $0.loadContainer(
245 |             hub: hub, configuration: .init(id: id, revision: revision),
246 |             progressHandler: progressHandler)
247 |     }
248 | }
249 | 
250 | /// Load a model given a directory of configuration and weights.
251 | ///
252 | /// This will load and return a ``ModelContext``.  This holds the model and tokenzier without
253 | /// an `actor` providing an isolation context.  Use this call when you control the isolation context
254 | /// and can hold the ``ModelContext`` directly.
255 | ///
256 | /// - Parameters:
257 | ///   - hub: optional HubApi -- by default uses ``defaultHubApi``
258 | ///   - directory: directory of configuration and weights
259 | ///   - progressHandler: optional callback for progress
260 | /// - Returns: a ``ModelContext``
261 | public func loadModel(
262 |     hub: HubApi = defaultHubApi, directory: URL,
263 |     progressHandler: @Sendable @escaping (Progress) -> Void = { _ in }
264 | ) async throws -> sending ModelContext {
265 |     try await load {
266 |         try await $0.load(
267 |             hub: hub, configuration: .init(directory: directory), progressHandler: progressHandler)
268 |     }
269 | }
270 | 
271 | /// Load a model given a directory of configuration and weights.
272 | ///
273 | /// This will load and return a ``ModelContainer``.  This holds a ``ModelContext``
274 | /// inside an actor providing isolation control for the values.
275 | ///
276 | /// - Parameters:
277 | ///   - hub: optional HubApi -- by default uses ``defaultHubApi``
278 | ///   - directory: directory of configuration and weights
279 | ///   - progressHandler: optional callback for progress
280 | /// - Returns: a ``ModelContainer``
281 | public func loadModelContainer(
282 |     hub: HubApi = defaultHubApi, directory: URL,
283 |     progressHandler: @Sendable @escaping (Progress) -> Void = { _ in }
284 | ) async throws -> sending ModelContainer {
285 |     try await load {
286 |         try await $0.loadContainer(
287 |             hub: hub, configuration: .init(directory: directory), progressHandler: progressHandler)
288 |     }
289 | }
290 | 
291 | private func load<R>(loader: (ModelFactory) async throws -> sending R) async throws -> sending R {
292 |     let factories = ModelFactoryRegistry.shared.modelFactories()
293 |     var lastError: Error?
294 |     for factory in factories {
295 |         do {
296 |             let model = try await loader(factory)
297 |             return model
298 |         } catch {
299 |             lastError = error
300 |         }
301 |     }
302 | 
303 |     if let lastError {
304 |         throw lastError
305 |     } else {
306 |         throw ModelFactoryError.noModelFactoryAvailable
307 |     }
308 | }
309 | 
310 | /// Protocol for types that can provide ModelFactory instances.
311 | ///
312 | /// Not used directly.
313 | ///
314 | /// This is used internally to provide dynamic lookup of a trampoline -- this lets
315 | /// API in MLXLMCommon use code present in MLXLLM:
316 | ///
317 | /// ```swift
318 | /// public class TrampolineModelFactory: NSObject, ModelFactoryTrampoline {
319 | ///     public static func modelFactory() -> (any MLXLMCommon.ModelFactory)? {
320 | ///         LLMModelFactory.shared
321 | ///     }
322 | /// }
323 | /// ```
324 | ///
325 | /// That is looked up dynamically with:
326 | ///
327 | /// ```swift
328 | /// {
329 | ///     (NSClassFromString("MLXVLM.TrampolineModelFactory") as? ModelFactoryTrampoline.Type)?
330 | ///         .modelFactory()
331 | /// }
332 | /// ```
333 | ///
334 | /// ## See Also
335 | /// - ``ModelFactoryRegistry``
336 | public protocol ModelFactoryTrampoline {
337 |     static func modelFactory() -> ModelFactory?
338 | }
339 | 
340 | /// Registry of ``ModelFactory`` trampolines.
341 | ///
342 | /// This allows ``loadModel(hub:id:progressHandler:)`` to use any ``ModelFactory`` instances
343 | /// available but be defined in the `LLMCommon` layer.  This is not typically used directly -- it is
344 | /// called via ``loadModel(hub:id:progressHandler:)``:
345 | ///
346 | /// ```swift
347 | /// let model = try await loadModel(id: "mlx-community/Qwen3-4B-4bit")
348 | /// ```
349 | ///
350 | /// ## See Also
351 | /// - ``loadModel(hub:id:progressHandler:)``
352 | /// - ``loadModel(hub:directory:progressHandler:)``
353 | /// - ``loadModelContainer(hub:id:progressHandler:)``
354 | /// - ``loadModelContainer(hub:directory:progressHandler:)``
355 | final public class ModelFactoryRegistry: @unchecked Sendable {
356 |     public static let shared = ModelFactoryRegistry()
357 | 
358 |     private let lock = NSLock()
359 |     private var trampolines: [() -> ModelFactory?]
360 | 
361 |     private init() {
362 |         self.trampolines = [
363 |             {
364 |                 (NSClassFromString("MLXVLM.TrampolineModelFactory") as? ModelFactoryTrampoline.Type)?
365 |                     .modelFactory()
366 |             },
367 |             {
368 |                 (NSClassFromString("MLXLLM.TrampolineModelFactory") as? ModelFactoryTrampoline.Type)?
369 |                     .modelFactory()
370 |             },
371 |         ]
372 |     }
373 | 
374 |     public func addTrampoline(_ trampoline: @escaping () -> ModelFactory?) {
375 |         lock.withLock {
376 |             trampolines.append(trampoline)
377 |         }
378 |     }
379 | 
380 |     public func modelFactories() -> [ModelFactory] {
381 |         lock.withLock {
382 |             trampolines.compactMap { $0() }
383 |         }
384 |     }
385 | }
386 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/Models/Gemma.swift:
--------------------------------------------------------------------------------
 1 | //
 2 | //  Gemma.swift
 3 | //  mlx-swift-examples
 4 | //
 5 | //  Created by Anthony DePasquale on 17.03.2025.
 6 | //
 7 | 
 8 | import Foundation
 9 | import MLX
10 | import MLXFast
11 | import MLXNN
12 | 
13 | public enum Gemma {
14 |     /// Specialized norm for gemma
15 |     public class RMSNorm: Module, UnaryLayer {
16 |         let weight: MLXArray
17 |         let eps: Float
18 | 
19 |         public init(dimensions: Int, eps: Float = 1e-5) {
20 |             self.weight = MLXArray.ones([dimensions])
21 |             self.eps = eps
22 |             super.init()
23 |         }
24 | 
25 |         public func callAsFunction(_ x: MLXArray) -> MLXArray {
26 |             return MLXFast.rmsNorm(x, weight: 1.0 + self.weight, eps: self.eps)
27 |         }
28 |     }
29 | 
30 |     /// Clips residual connections to prevent overflow in float16 operations
31 |     static public func clipResidual(_ x: MLXArray, _ y: MLXArray) -> MLXArray {
32 |         if x.dtype != .float16 {
33 |             return x + y
34 |         }
35 |         // IEEE 754 half-precision maximum finite value
36 |         let bound: Float = 65504.0  // Float16 maximum finite value
37 |         let xFloat32 = x.asType(.float32)
38 |         let yFloat32 = y.asType(.float32)
39 |         let result = xFloat32 + yFloat32
40 |         return clip(result, min: MLXArray(-bound), max: MLXArray(bound)).asType(.float16)
41 |     }
42 | }
43 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/Module+Extensions.swift:
--------------------------------------------------------------------------------
 1 | // Copyright © 2024 Apple Inc.
 2 | 
 3 | import MLXNN
 4 | 
 5 | extension Module {
 6 | 
 7 |     /// Compute the number of parameters in a possibly quantized model
 8 |     public func numParameters() -> Int {
 9 |         return leafModules().flattenedValues().map {
10 |             mod -> Int in
11 |             if let qlin = mod as? QuantizedLinear {
12 |                 return qlin.scales.size * qlin.groupSize
13 |             } else if let qemb = mod as? QuantizedEmbedding {
14 |                 return qemb.scales.size * qemb.groupSize
15 |             } else {
16 |                 return mod.parameters().flattenedValues().reduce(
17 |                     0,
18 |                     {
19 |                         $0 + $1.size
20 |                     })
21 |             }
22 |         }.reduce(0, +)
23 |     }
24 | }
25 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/README.md:
--------------------------------------------------------------------------------
  1 | # MLXLMCommon
  2 | 
  3 | # Documentation
  4 | 
  5 | - [Porting and implementing models](https://swiftpackageindex.com/ml-explore/mlx-swift-examples/main/documentation/mlxlmcommon/porting)
  6 | - [MLXLLMCommon](https://swiftpackageindex.com/ml-explore/mlx-swift-examples/main/documentation/mlxlmcommon) -- common API for LLM and VLM
  7 | - [MLXLLM](https://swiftpackageindex.com/ml-explore/mlx-swift-examples/main/documentation/mlxllm) -- large language model example implementations
  8 | - [MLXVLM](https://swiftpackageindex.com/ml-explore/mlx-swift-examples/main/documentation/mlxvlm) -- vision language model example implementations
  9 | 
 10 | # Quick Start
 11 | 
 12 | Using LLMs and VLMs from MLXLMCommon is as easy as:
 13 | 
 14 | ```swift
 15 | let model = try await loadModel(id: "mlx-community/Qwen3-4B-4bit")
 16 | let session = ChatSession(model)
 17 | print(try await session.respond(to: "What are two things to see in San Francisco?")
 18 | print(try await session.respond(to: "How about a great place to eat?")
 19 | ```
 20 | 
 21 | For more information see 
 22 | [Evaluation](https://swiftpackageindex.com/ml-explore/mlx-swift-examples/main/documentation/mlxlmcommon/evaluation)
 23 | or [Using Models](https://swiftpackageindex.com/ml-explore/mlx-swift-examples/main/documentation/mlxlmcommon/using-model)
 24 | for more advanced API.
 25 | 
 26 | # Contents
 27 | 
 28 | MLXLMCommon contains types and code that is generic across many types
 29 | of language models, from LLMs to VLMs:
 30 | 
 31 | - Evaluation
 32 | - KVCache
 33 | - Loading
 34 | - UserInput
 35 | 
 36 | ## Loading a Model
 37 | 
 38 | A model is typically loaded by using a `ModelFactory` and a `ModelConfiguration`:
 39 | 
 40 | ```swift
 41 | // e.g. VLMModelFactory.shared
 42 | let modelFactory: ModelFactory
 43 | 
 44 | // e.g. VLMRegistry.paligemma3bMix4488bit
 45 | let modelConfiguration: ModelConfiguration
 46 | 
 47 | let container = try await modelFactory.loadContainer(configuration: modelConfiguration)
 48 | ```
 49 | 
 50 | The `container` provides an isolation context (an `actor`) to run inference in the model.
 51 | 
 52 | Predefined `ModelConfiguration` instances are provided as static variables
 53 | on the `ModelRegistry` types or they can be created:
 54 | 
 55 | ```swift
 56 | let modelConfiguration = ModelConfiguration(id: "mlx-community/paligemma-3b-mix-448-8bit")
 57 | ```
 58 | 
 59 | The flow inside the `ModelFactory` goes like this:
 60 | 
 61 | ```swift
 62 | public class VLMModelFactory: ModelFactory {
 63 | 
 64 |     public func _load(
 65 |         hub: HubApi, configuration: ModelConfiguration,
 66 |         progressHandler: @Sendable @escaping (Progress) -> Void
 67 |     ) async throws -> ModelContext {
 68 |         // download the weight and config using HubApi
 69 |         // load the base configuration
 70 |         // using the typeRegistry create a model (random weights)
 71 |         // load the weights, apply quantization as needed, update the model
 72 |             // calls model.sanitize() for weight preparation
 73 |         // load the tokenizer
 74 |         // (vlm) load the processor configuration, create the processor
 75 |     }
 76 | }
 77 | ```
 78 | 
 79 | Callers with specialized requirements can use these individual components to manually
 80 | load models, if needed.
 81 | 
 82 | ## Evaluation Flow
 83 | 
 84 | - Load the Model
 85 | - UserInput
 86 | - LMInput
 87 | - generate()
 88 |     - NaiveStreamingDetokenizer
 89 |     - TokenIterator
 90 | 
 91 | ## Using a Model
 92 | 
 93 | Once a model is loaded you can evaluate a prompt or series of
 94 | messages. Minimally you need to prepare the user input:
 95 | 
 96 | ```swift
 97 | let prompt = "Describe the image in English"
 98 | var input = UserInput(prompt: prompt, images: image.map { .url($0) })
 99 | input.processing.resize = .init(width: 256, height: 256)
100 | ```
101 | 
102 | This example shows adding some images and processing instructions -- if
103 | model accepts text only then these parts can be omitted. The inference
104 | calls are the same.
105 | 
106 | Assuming you are using a `ModelContainer` (an actor that holds
107 | a `ModelContext`, which is the bundled set of types that implement a
108 | model), the first step is to convert the `UserInput` into the
109 | `LMInput` (LanguageModel Input):
110 | 
111 | ```swift
112 | let generateParameters: GenerateParameters
113 | let input: UserInput
114 | 
115 | let result = try await modelContainer.perform { [input] context in
116 |     let input = try context.processor.prepare(input: input)
117 | 
118 | ```
119 | 
120 | Given that `input` we can call `generate()` to produce a stream
121 | of tokens. In this example we use a `NaiveStreamingDetokenizer`
122 | to assist in converting a stream of tokens into text and print it.
123 | The stream is stopped after we hit a maximum number of tokens:
124 | 
125 | ```
126 |     var detokenizer = NaiveStreamingDetokenizer(tokenizer: context.tokenizer)
127 | 
128 |     return try MLXLMCommon.generate(
129 |         input: input, parameters: generateParameters, context: context
130 |     ) { tokens in
131 | 
132 |         if let last = tokens.last {
133 |             detokenizer.append(token: last)
134 |         }
135 | 
136 |         if let new = detokenizer.next() {
137 |             print(new, terminator: "")
138 |             fflush(stdout)
139 |         }
140 | 
141 |         if tokens.count >= maxTokens {
142 |             return .stop
143 |         } else {
144 |             return .more
145 |         }
146 |     }
147 | }
148 | ```
149 | 
150 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/Registries/AbstractModelRegistry.swift:
--------------------------------------------------------------------------------
 1 | // Copyright © 2024 Apple Inc.
 2 | 
 3 | import Foundation
 4 | 
 5 | open class AbstractModelRegistry: @unchecked Sendable {
 6 | 
 7 |     /// Creates an empty registry.
 8 |     public init() {
 9 |         self.registry = Dictionary()
10 |     }
11 | 
12 |     /// Creates a new registry with from given model configurations.
13 |     public init(modelConfigurations: [ModelConfiguration]) {
14 |         self.registry = Dictionary(uniqueKeysWithValues: modelConfigurations.map { ($0.name, $0) })
15 |     }
16 | 
17 |     private let lock = NSLock()
18 |     private var registry: [String: ModelConfiguration]
19 | 
20 |     public func register(configurations: [ModelConfiguration]) {
21 |         lock.withLock {
22 |             for c in configurations {
23 |                 registry[c.name] = c
24 |             }
25 |         }
26 |     }
27 | 
28 |     /// Returns configuration from ``modelRegistry``.
29 |     ///
30 |     /// - Note: If the id doesn't exists in the configuration, this will return a new instance of it.
31 |     /// If you want to check if the configuration in model registry, you should use ``contains(id:)``.
32 |     public func configuration(id: String) -> ModelConfiguration {
33 |         lock.withLock {
34 |             if let c = registry[id] {
35 |                 return c
36 |             } else {
37 |                 return ModelConfiguration(id: id)
38 |             }
39 |         }
40 |     }
41 | 
42 |     /// Returns true if the registry contains a model with the id. Otherwise, false.
43 |     public func contains(id: String) -> Bool {
44 |         lock.withLock {
45 |             registry[id] != nil
46 |         }
47 |     }
48 | 
49 |     public var models: some Collection<ModelConfiguration> & Sendable {
50 |         lock.withLock {
51 |             return registry.values
52 |         }
53 |     }
54 | }
55 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/Registries/ModelTypeRegistry.swift:
--------------------------------------------------------------------------------
 1 | // Copyright © 2024 Apple Inc.
 2 | 
 3 | import Foundation
 4 | 
 5 | open class ModelTypeRegistry: @unchecked Sendable {
 6 | 
 7 |     /// Creates an empty registry.
 8 |     public init() {
 9 |         self.creators = [:]
10 |     }
11 | 
12 |     /// Creates a registry with given creators.
13 |     public init(creators: [String: @Sendable (URL) throws -> any LanguageModel]) {
14 |         self.creators = creators
15 |     }
16 | 
17 |     // Note: using NSLock as we have very small (just dictionary get/set)
18 |     // critical sections and expect no contention. this allows the methods
19 |     // to remain synchronous.
20 |     private let lock = NSLock()
21 |     private var creators: [String: @Sendable (URL) throws -> any LanguageModel]
22 | 
23 |     /// Add a new model to the type registry.
24 |     public func registerModelType(
25 |         _ type: String, creator: @Sendable @escaping (URL) throws -> any LanguageModel
26 |     ) {
27 |         lock.withLock {
28 |             creators[type] = creator
29 |         }
30 |     }
31 | 
32 |     /// Given a `modelType` and configuration file instantiate a new `LanguageModel`.
33 |     public func createModel(configuration: URL, modelType: String) throws -> LanguageModel {
34 |         let creator = lock.withLock {
35 |             creators[modelType]
36 |         }
37 |         guard let creator else {
38 |             throw ModelFactoryError.unsupportedModelType(modelType)
39 |         }
40 |         return try creator(configuration)
41 |     }
42 | 
43 | }
44 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/Registries/ProcessorTypeRegistry.swift:
--------------------------------------------------------------------------------
 1 | // Copyright © 2024 Apple Inc.
 2 | 
 3 | import Foundation
 4 | import Tokenizers
 5 | 
 6 | open class ProcessorTypeRegistry: @unchecked Sendable {
 7 | 
 8 |     /// Creates an empty registry.
 9 |     public init() {
10 |         self.creators = [:]
11 |     }
12 | 
13 |     /// Creates a registry with given creators.
14 |     public init(creators: [String: @Sendable (URL, any Tokenizer) throws -> any UserInputProcessor])
15 |     {
16 |         self.creators = creators
17 |     }
18 | 
19 |     // Note: using NSLock as we have very small (just dictionary get/set)
20 |     // critical sections and expect no contention. this allows the methods
21 |     // to remain synchronous.
22 |     private let lock = NSLock()
23 | 
24 |     private var creators: [String: @Sendable (URL, any Tokenizer) throws -> any UserInputProcessor]
25 | 
26 |     /// Add a new model to the type registry.
27 |     public func registerProcessorType(
28 |         _ type: String,
29 |         creator: @Sendable @escaping (
30 |             URL,
31 |             any Tokenizer
32 |         ) throws -> any UserInputProcessor
33 |     ) {
34 |         lock.withLock {
35 |             creators[type] = creator
36 |         }
37 |     }
38 | 
39 |     /// Given a `processorType` and configuration file instantiate a new `UserInputProcessor`.
40 |     public func createModel(configuration: URL, processorType: String, tokenizer: any Tokenizer)
41 |         throws -> any UserInputProcessor
42 |     {
43 |         let creator = lock.withLock {
44 |             creators[processorType]
45 |         }
46 |         guard let creator else {
47 |             throw ModelFactoryError.unsupportedProcessorType(processorType)
48 |         }
49 |         return try creator(configuration, tokenizer)
50 |     }
51 | 
52 | }
53 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/Streamlined.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2025 Apple Inc.
  2 | 
  3 | import Foundation
  4 | import MLX
  5 | 
  6 | /// Implementation of simplified API -- see ``ChatSession``.
  7 | private class Generator {
  8 |     enum Model {
  9 |         case container(ModelContainer)
 10 |         case context(ModelContext)
 11 |     }
 12 | 
 13 |     let model: Model
 14 |     var messages = [Chat.Message]()
 15 |     let processing: UserInput.Processing
 16 |     let generateParameters: GenerateParameters
 17 |     var cache: [KVCache]
 18 | 
 19 |     init(
 20 |         model: Model, instructions: String?, prompt: String, image: UserInput.Image?,
 21 |         video: UserInput.Video?, processing: UserInput.Processing,
 22 |         generateParameters: GenerateParameters
 23 |     ) {
 24 |         self.model = model
 25 |         self.messages = []
 26 |         if let instructions = instructions {
 27 |             messages.append(.system(instructions))
 28 |         }
 29 |         messages.append(
 30 |             .user(
 31 |                 prompt, images: image.flatMap { [$0] } ?? [], videos: video.flatMap { [$0] } ?? []))
 32 |         self.processing = processing
 33 |         self.generateParameters = generateParameters
 34 |         self.cache = []
 35 |     }
 36 | 
 37 |     init(
 38 |         model: Model, instructions: String?, processing: UserInput.Processing,
 39 |         generateParameters: GenerateParameters
 40 |     ) {
 41 |         self.model = model
 42 |         if let instructions {
 43 |             self.messages = [.system(instructions)]
 44 |         } else {
 45 |             self.messages = []
 46 |         }
 47 |         self.processing = processing
 48 |         self.generateParameters = generateParameters
 49 |         self.cache = []
 50 |     }
 51 | 
 52 |     func generate() async throws -> String {
 53 |         func generate(context: ModelContext) async throws -> String {
 54 |             // prepare the input -- first the structured messages,
 55 |             // next the tokens
 56 |             let userInput = UserInput(chat: messages, processing: processing)
 57 |             let input = try await context.processor.prepare(input: userInput)
 58 | 
 59 |             if cache.isEmpty {
 60 |                 cache = context.model.newCache(parameters: generateParameters)
 61 |             }
 62 | 
 63 |             // generate the output
 64 |             let iterator = try TokenIterator(
 65 |                 input: input, model: context.model, cache: cache, parameters: generateParameters)
 66 |             let result: GenerateResult = MLXLMCommon.generate(
 67 |                 input: input, context: context, iterator: iterator
 68 |             ) { _ in .more }
 69 | 
 70 |             Stream.gpu.synchronize()
 71 | 
 72 |             return result.output
 73 |         }
 74 | 
 75 |         switch model {
 76 |         case .container(let container):
 77 |             return try await container.perform { context in
 78 |                 try await generate(context: context)
 79 |             }
 80 |         case .context(let context):
 81 |             return try await generate(context: context)
 82 |         }
 83 |     }
 84 | 
 85 |     func stream() -> AsyncThrowingStream<String, Error> {
 86 |         func stream(
 87 |             context: ModelContext,
 88 |             continuation: AsyncThrowingStream<String, Error>.Continuation
 89 |         ) async {
 90 |             do {
 91 |                 // prepare the input -- first the structured messages,
 92 |                 // next the tokens
 93 |                 let userInput = UserInput(chat: messages, processing: processing)
 94 |                 let input = try await context.processor.prepare(input: userInput)
 95 | 
 96 |                 if cache.isEmpty {
 97 |                     cache = context.model.newCache(parameters: generateParameters)
 98 |                 }
 99 | 
100 |                 // stream the responses back
101 |                 for await item in try MLXLMCommon.generate(
102 |                     input: input, cache: cache, parameters: generateParameters, context: context)
103 |                 {
104 |                     if let chunk = item.chunk {
105 |                         continuation.yield(chunk)
106 |                     }
107 |                 }
108 | 
109 |                 Stream.gpu.synchronize()
110 | 
111 |                 continuation.finish()
112 |             } catch {
113 |                 continuation.finish(throwing: error)
114 |             }
115 |         }
116 | 
117 |         return AsyncThrowingStream { continuation in
118 |             Task { [model, continuation] in
119 |                 switch model {
120 |                 case .container(let container):
121 |                     await container.perform { context in
122 |                         await stream(context: context, continuation: continuation)
123 |                     }
124 |                 case .context(let context):
125 |                     await stream(context: context, continuation: continuation)
126 |                 }
127 |             }
128 |         }
129 |     }
130 | }
131 | 
132 | /// Simplified API for loading models and preparing responses to prompts
133 | /// for both LLMs and VLMs.
134 | ///
135 | /// For example:
136 | ///
137 | /// ```swift
138 | /// let model = try await loadModel(id: "mlx-community/Qwen3-4B-4bit")
139 | /// let session = ChatSession(model)
140 | /// print(try await session.respond(to: "What are two things to see in San Francisco?")
141 | /// print(try await session.respond(to: "How about a great place to eat?")
142 | /// ```
143 | ///
144 | /// This manages the chat context (KVCache) and can produce both single string responses or
145 | /// streaming responses.
146 | public class ChatSession {
147 | 
148 |     private let generator: Generator
149 | 
150 |     /// Initialize the `ChatSession`.
151 |     ///
152 |     /// - Parameters:
153 |     ///   - model: the ``ModelContainer``
154 |     ///   - instructions: optional instructions to the chat session, e.g. describing what type of responses to give
155 |     ///   - generateParameters: parameters that control the generation of output, e.g. token limits and temperature
156 |     ///   - processing: optional media processing instructions
157 |     public init(
158 |         _ model: ModelContainer, instructions: String? = nil,
159 |         generateParameters: GenerateParameters = .init(),
160 |         processing: UserInput.Processing = .init(resize: CGSize(width: 512, height: 512))
161 |     ) {
162 |         self.generator = .init(
163 |             model: .container(model), instructions: instructions, processing: processing,
164 |             generateParameters: generateParameters)
165 |     }
166 | 
167 |     /// Initialize the `ChatSession`.
168 |     ///
169 |     /// - Parameters:
170 |     ///   - model: the ``ModelContext``
171 |     ///   - instructions: optional instructions to the chat session, e.g. describing what type of responses to give
172 |     ///   - generateParameters: parameters that control the generation of output, e.g. token limits and temperature
173 |     ///   - processing: optional media processing instructions
174 |     public init(
175 |         _ model: ModelContext, instructions: String? = nil,
176 |         generateParameters: GenerateParameters = .init(),
177 |         processing: UserInput.Processing = .init(resize: CGSize(width: 512, height: 512))
178 |     ) {
179 |         self.generator = .init(
180 |             model: .context(model), instructions: instructions, processing: processing,
181 |             generateParameters: generateParameters)
182 |     }
183 | 
184 |     /// Produces a response to a prompt.
185 |     ///
186 |     /// - Parameters:
187 |     ///   - prompt: the prompt
188 |     ///   - image: optional image (for use with VLMs)
189 |     ///   - video: optional video (for use with VLMs)
190 |     /// - Returns: response from the model
191 |     public func respond(
192 |         to prompt: String, image: UserInput.Image? = nil, video: UserInput.Video? = nil
193 |     ) async throws -> String {
194 |         generator.messages = [
195 |             .user(
196 |                 prompt,
197 |                 images: image.flatMap { [$0] } ?? [],
198 |                 videos: video.flatMap { [$0] } ?? [])
199 |         ]
200 |         return try await generator.generate()
201 |     }
202 | 
203 |     /// Produces a response to a prompt.
204 |     ///
205 |     /// - Parameters:
206 |     ///   - prompt: the prompt
207 |     ///   - image: optional image (for use with VLMs)
208 |     ///   - video: optional video (for use with VLMs)
209 |     /// - Returns: a stream of tokens (as Strings) from the model
210 |     public func streamResponse(
211 |         to prompt: String, image: UserInput.Image? = nil, video: UserInput.Video? = nil
212 |     ) -> AsyncThrowingStream<String, Error> {
213 |         generator.messages = [
214 |             .user(
215 |                 prompt,
216 |                 images: image.flatMap { [$0] } ?? [],
217 |                 videos: video.flatMap { [$0] } ?? [])
218 |         ]
219 |         return generator.stream()
220 |     }
221 | }
222 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/StringOrNumber.swift:
--------------------------------------------------------------------------------
 1 | // Copyright © 2024 Apple Inc.
 2 | 
 3 | import Foundation
 4 | 
 5 | /// Representation of a heterogenous type in a JSON configuration file.
 6 | ///
 7 | /// This can be: a string, a numeric value or an array of numeric values.
 8 | /// There are methods to do unwrapping, see e.g. ``asFloat()`` and
 9 | /// ``asFloats()`` or callers can switch on the enum.
10 | public enum StringOrNumber: Codable, Equatable, Sendable {
11 |     case string(String)
12 |     case int(Int)
13 |     case float(Float)
14 |     case ints([Int])
15 |     case floats([Float])
16 | 
17 |     public init(from decoder: Decoder) throws {
18 |         let values = try decoder.singleValueContainer()
19 | 
20 |         if let v = try? values.decode(Int.self) {
21 |             self = .int(v)
22 |         } else if let v = try? values.decode(Float.self) {
23 |             self = .float(v)
24 |         } else if let v = try? values.decode([Int].self) {
25 |             self = .ints(v)
26 |         } else if let v = try? values.decode([Float].self) {
27 |             self = .floats(v)
28 |         } else {
29 |             let v = try values.decode(String.self)
30 |             self = .string(v)
31 |         }
32 |     }
33 | 
34 |     public func encode(to encoder: Encoder) throws {
35 |         var container = encoder.singleValueContainer()
36 |         switch self {
37 |         case .string(let v): try container.encode(v)
38 |         case .int(let v): try container.encode(v)
39 |         case .float(let v): try container.encode(v)
40 |         case .ints(let v): try container.encode(v)
41 |         case .floats(let v): try container.encode(v)
42 |         }
43 |     }
44 | 
45 |     /// Return the value as an optional array of integers.
46 |     ///
47 |     /// This will not coerce `Float` or `String` to `Int`.
48 |     public func asInts() -> [Int]? {
49 |         switch self {
50 |         case .string(let string): nil
51 |         case .int(let v): [v]
52 |         case .float(let float): nil
53 |         case .ints(let array): array
54 |         case .floats(let array): nil
55 |         }
56 |     }
57 | 
58 |     /// Return the value as an optional integer.
59 |     ///
60 |     /// This will not coerce `Float` or `String` to `Int`.
61 |     public func asInt() -> Int? {
62 |         switch self {
63 |         case .string(let string): nil
64 |         case .int(let v): v
65 |         case .float(let float): nil
66 |         case .ints(let array): array.count == 1 ? array[0] : nil
67 |         case .floats(let array): nil
68 |         }
69 |     }
70 | 
71 |     /// Return the value as an optional array of floats.
72 |     ///
73 |     /// This will not coerce `Int` or `String` to `Float`.
74 |     public func asFloats() -> [Float]? {
75 |         switch self {
76 |         case .string(let string): nil
77 |         case .int(let v): [Float(v)]
78 |         case .float(let float): [float]
79 |         case .ints(let array): array.map { Float($0) }
80 |         case .floats(let array): array
81 |         }
82 |     }
83 | 
84 |     /// Return the value as an optional float.
85 |     ///
86 |     /// This will not coerce `Int` or `String` to `Float`.
87 |     public func asFloat() -> Float? {
88 |         switch self {
89 |         case .string(let string): nil
90 |         case .int(let v): Float(v)
91 |         case .float(let float): float
92 |         case .ints(let array): array.count == 1 ? Float(array[0]) : nil
93 |         case .floats(let array): array.count == 1 ? array[0] : nil
94 |         }
95 |     }
96 | }
97 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/Tokenizer.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import Foundation
  4 | import Hub
  5 | import Tokenizers
  6 | 
  7 | struct TokenizerError: Error {
  8 |     let message: String
  9 | }
 10 | 
 11 | public func loadTokenizer(configuration: ModelConfiguration, hub: HubApi) async throws -> Tokenizer
 12 | {
 13 |     let (tokenizerConfig, tokenizerData) = try await loadTokenizerConfig(
 14 |         configuration: configuration, hub: hub)
 15 | 
 16 |     return try PreTrainedTokenizer(
 17 |         tokenizerConfig: tokenizerConfig, tokenizerData: tokenizerData)
 18 | }
 19 | 
 20 | public func loadTokenizerConfig(configuration: ModelConfiguration, hub: HubApi) async throws -> (
 21 |     Config, Config
 22 | ) {
 23 |     // from AutoTokenizer.from() -- this lets us override parts of the configuration
 24 |     let config: LanguageModelConfigurationFromHub
 25 | 
 26 |     switch configuration.id {
 27 |     case .id(let id, let revision):
 28 |         do {
 29 |             // the load can fail (async when we try to use it)
 30 |             let loaded = LanguageModelConfigurationFromHub(
 31 |                 modelName: configuration.tokenizerId ?? id, revision: revision, hubApi: hub)
 32 |             _ = try await loaded.tokenizerConfig
 33 |             config = loaded
 34 |         } catch {
 35 |             let nserror = error as NSError
 36 |             if nserror.domain == NSURLErrorDomain
 37 |                 && nserror.code == NSURLErrorNotConnectedToInternet
 38 |             {
 39 |                 // Internet connection appears to be offline -- fall back to loading from
 40 |                 // the local directory
 41 |                 config = LanguageModelConfigurationFromHub(
 42 |                     modelFolder: configuration.modelDirectory(hub: hub), hubApi: hub)
 43 |             } else {
 44 |                 throw error
 45 |             }
 46 |         }
 47 |     case .directory(let directory):
 48 |         config = LanguageModelConfigurationFromHub(modelFolder: directory, hubApi: hub)
 49 |     }
 50 | 
 51 |     guard var tokenizerConfig = try await config.tokenizerConfig else {
 52 |         throw TokenizerError(message: "missing config")
 53 |     }
 54 |     let tokenizerData = try await config.tokenizerData
 55 | 
 56 |     tokenizerConfig = updateTokenizerConfig(tokenizerConfig)
 57 | 
 58 |     return (tokenizerConfig, tokenizerData)
 59 | }
 60 | 
 61 | private func updateTokenizerConfig(_ tokenizerConfig: Config) -> Config {
 62 |     // Workaround: replacement tokenizers for unhandled values in swift-transformers
 63 |     if let tokenizerClass = tokenizerConfig.tokenizerClass?.string(),
 64 |         let replacement = replacementTokenizers[tokenizerClass]
 65 |     {
 66 |         if var dictionary = tokenizerConfig.dictionary() {
 67 |             dictionary["tokenizer_class"] = .init(replacement)
 68 |             return Config(dictionary)
 69 |         }
 70 |     }
 71 |     return tokenizerConfig
 72 | }
 73 | 
 74 | public class TokenizerReplacementRegistry: @unchecked Sendable {
 75 | 
 76 |     // Note: using NSLock as we have very small (just dictionary get/set)
 77 |     // critical sections and expect no contention. this allows the methods
 78 |     // to remain synchronous.
 79 |     private let lock = NSLock()
 80 | 
 81 |     /// overrides for TokenizerModel/knownTokenizers
 82 |     private var replacementTokenizers = [
 83 |         "InternLM2Tokenizer": "PreTrainedTokenizer",
 84 |         "Qwen2Tokenizer": "PreTrainedTokenizer",
 85 |         "Qwen3Tokenizer": "PreTrainedTokenizer",
 86 |         "CohereTokenizer": "PreTrainedTokenizer",
 87 |     ]
 88 | 
 89 |     public subscript(key: String) -> String? {
 90 |         get {
 91 |             lock.withLock {
 92 |                 replacementTokenizers[key]
 93 |             }
 94 |         }
 95 |         set {
 96 |             lock.withLock {
 97 |                 replacementTokenizers[key] = newValue
 98 |             }
 99 |         }
100 |     }
101 | }
102 | 
103 | public let replacementTokenizers = TokenizerReplacementRegistry()
104 | 
105 | public protocol StreamingDetokenizer: IteratorProtocol<String> {
106 | 
107 |     mutating func append(token: Int)
108 | 
109 | }
110 | 
111 | public struct NaiveStreamingDetokenizer: StreamingDetokenizer {
112 |     let tokenizer: Tokenizer
113 | 
114 |     var segmentTokens = [Int]()
115 |     var segment = ""
116 | 
117 |     public init(tokenizer: Tokenizer) {
118 |         self.tokenizer = tokenizer
119 |     }
120 | 
121 |     mutating public func append(token: Int) {
122 |         segmentTokens.append(token)
123 |     }
124 | 
125 |     mutating func startNewSegment() {
126 |         let lastToken = segmentTokens.last
127 |         segmentTokens.removeAll()
128 |         if let lastToken {
129 |             segmentTokens.append(lastToken)
130 |             segment = tokenizer.decode(tokens: segmentTokens)
131 |         } else {
132 |             segment = ""
133 |         }
134 |     }
135 | 
136 |     public mutating func next() -> String? {
137 |         let newSegment = tokenizer.decode(tokens: segmentTokens)
138 |         let new = newSegment.suffix(newSegment.count - segment.count)
139 | 
140 |         // if the new segment ends with REPLACEMENT CHARACTER this means
141 |         // that the token didn't produce a complete unicode character
142 |         if new.last == "\u{fffd}" {
143 |             return nil
144 |         }
145 | 
146 |         if new.hasSuffix("\n") {
147 |             startNewSegment()
148 |         } else {
149 |             self.segment = newSegment
150 |         }
151 | 
152 |         return String(new)
153 |     }
154 | 
155 | }
156 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/Tool/Tool.swift:
--------------------------------------------------------------------------------
 1 | // Copyright © 2025 Apple Inc.
 2 | 
 3 | import Foundation
 4 | import Tokenizers
 5 | 
 6 | /// Protocol defining the requirements for a tool.
 7 | public protocol ToolProtocol: Sendable {
 8 |     /// The JSON Schema describing the tool's interface.
 9 |     var schema: ToolSpec { get }
10 | }
11 | 
12 | public struct Tool<Input: Codable, Output: Codable>: ToolProtocol {
13 |     /// The JSON Schema describing the tool's interface.
14 |     public let schema: ToolSpec
15 | 
16 |     /// The handler for the tool.
17 |     public let handler: (Input) async throws -> Output
18 | 
19 |     /// The name of the tool extracted from the schema
20 |     public var name: String {
21 |         let function = schema["function"] as? [String: Any]
22 |         let name = function?["name"] as? String
23 |         return name ?? ""
24 |     }
25 | 
26 |     public init(
27 |         name: String,
28 |         description: String,
29 |         parameters: [ToolParameter],
30 |         handler: @escaping (Input) async throws -> Output
31 |     ) {
32 |         var properties = [String: Any]()
33 |         var requiredParams = [String]()
34 | 
35 |         for param in parameters {
36 |             properties[param.name] = param.schema
37 |             if param.isRequired {
38 |                 requiredParams.append(param.name)
39 |             }
40 |         }
41 | 
42 |         self.schema = [
43 |             "type": "function",
44 |             "function": [
45 |                 "name": name,
46 |                 "description": description,
47 |                 "parameters": [
48 |                     "type": "object",
49 |                     "properties": properties,
50 |                     "required": requiredParams,
51 |                 ],
52 |             ],
53 |         ]
54 | 
55 |         self.handler = handler
56 |     }
57 | 
58 |     public init(schema: ToolSpec, handler: @escaping (Input) async throws -> Output) {
59 |         self.schema = schema
60 |         self.handler = handler
61 |     }
62 | }
63 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/Tool/ToolCall.swift:
--------------------------------------------------------------------------------
 1 | // Copyright © 2025 Apple Inc.
 2 | 
 3 | import Foundation
 4 | 
 5 | public struct ToolCall: Hashable, Codable, Sendable {
 6 |     /// Represents the function details for a tool call
 7 |     public struct Function: Hashable, Codable, Sendable {
 8 |         /// The name of the function
 9 |         public let name: String
10 | 
11 |         /// The arguments passed to the function
12 |         public let arguments: [String: JSONValue]
13 | 
14 |         public init(name: String, arguments: [String: Any]) {
15 |             self.name = name
16 |             self.arguments = arguments.mapValues { JSONValue.from($0) }
17 |         }
18 |     }
19 | 
20 |     /// The function to be called
21 |     public let function: Function
22 | }
23 | 
24 | extension ToolCall {
25 |     public func execute<Input, Output>(with tool: Tool<Input, Output>) async throws -> Output {
26 |         // Check that the tool name matches the function name
27 |         guard tool.name == function.name else {
28 |             throw ToolError.nameMismatch(toolName: tool.name, functionName: function.name)
29 |         }
30 | 
31 |         // Convert the JSONValue arguments dictionary to a JSON-encoded Data object
32 |         let jsonObject = function.arguments.mapValues { $0.anyValue }
33 |         let jsonData = try JSONSerialization.data(withJSONObject: jsonObject)
34 | 
35 |         // Decode the Input type from the JSON data
36 |         let input = try JSONDecoder().decode(Input.self, from: jsonData)
37 | 
38 |         // Execute the tool's handler with the decoded input
39 |         return try await tool.handler(input)
40 |     }
41 | }
42 | 
43 | // Define Tool-related errors
44 | public enum ToolError: Error, LocalizedError {
45 |     case nameMismatch(toolName: String, functionName: String)
46 | 
47 |     public var errorDescription: String? {
48 |         switch self {
49 |         case .nameMismatch(let toolName, let functionName):
50 |             return "Tool name mismatch: expected '\(toolName)' but got '\(functionName)'"
51 |         }
52 |     }
53 | }
54 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/Tool/ToolCallProcessor.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2025 Apple Inc.
  2 | 
  3 | import Foundation
  4 | 
  5 | /// Used to process generated text to detect tool calls and manage the generation flow
  6 | public class ToolCallProcessor {
  7 | 
  8 |     public init() {}
  9 | 
 10 |     // Track the current state of processing
 11 |     private enum State {
 12 |         case normal
 13 |         case potentialToolCall
 14 |         case collectingToolCall
 15 |     }
 16 | 
 17 |     private var state = State.normal
 18 |     private var toolCallBuffer = ""
 19 | 
 20 |     // Tags to detect
 21 |     private static let toolUseStartTag = "<tool_call>"
 22 |     private static let toolUseEndTag = "</tool_call>"
 23 | 
 24 |     private static let toolCallRegex = #/<tool_call>\s*(\{.*?\})\s*<\/tool_call>/#
 25 | 
 26 |     /// The current parsed tool call, if any
 27 |     public var toolCalls: [ToolCall] = []
 28 | 
 29 |     /// Append a generated text chunk and process for tool call tags
 30 |     /// - Parameter chunk: The text chunk to process
 31 |     /// - Returns: Any regular text that should be yielded (non-tool call content)
 32 |     public func processChunk(_ chunk: String) -> String? {
 33 |         guard (state == .normal && chunk.contains("<")) || state != .normal else {
 34 |             return chunk
 35 |         }
 36 | 
 37 |         toolCallBuffer += chunk
 38 |         var leadingToken: String?
 39 | 
 40 |         switch state {
 41 |         case .normal:
 42 |             // Change state to potential tool call
 43 |             state = .potentialToolCall
 44 | 
 45 |             leadingToken = separateToken(from: &toolCallBuffer, separator: "<", returnLeading: true)
 46 | 
 47 |             fallthrough
 48 |         case .potentialToolCall:
 49 |             if partialMatch(buffer: toolCallBuffer, tag: Self.toolUseStartTag) {
 50 |                 if toolCallBuffer.starts(with: Self.toolUseStartTag) {
 51 |                     state = .collectingToolCall
 52 |                     fallthrough
 53 |                 } else {
 54 |                     return nil
 55 |                 }
 56 |             } else {
 57 |                 // Otherwise, return the collected text and reset the state
 58 |                 state = .normal
 59 |                 let buffer = toolCallBuffer
 60 |                 toolCallBuffer = ""
 61 |                 return (leadingToken ?? "") + buffer
 62 |             }
 63 |         case .collectingToolCall:
 64 |             if toolCallBuffer.contains(Self.toolUseEndTag) {
 65 |                 // Separate the trailing token
 66 |                 let trailingToken = separateToken(
 67 |                     from: &toolCallBuffer, separator: Self.toolUseEndTag, returnLeading: false)
 68 | 
 69 |                 // Parse the tool call
 70 |                 if let toolCall = parseToolCall(toolCallBuffer) {
 71 |                     toolCalls.append(toolCall)
 72 |                 }
 73 | 
 74 |                 state = .normal
 75 |                 toolCallBuffer = ""
 76 | 
 77 |                 // If the token contains a "<", there may be more tool calls to come
 78 |                 if let trailingToken, trailingToken.contains("<") {
 79 |                     return processChunk(trailingToken)
 80 |                 } else {
 81 |                     // Otherwise, return the collected token, or nil if it's empty
 82 |                     return trailingToken?.isEmpty ?? true ? nil : trailingToken
 83 |                 }
 84 |             } else {
 85 |                 return nil
 86 |             }
 87 |         }
 88 |     }
 89 | 
 90 |     /// Separates a token from a string buffer based on a separator
 91 |     /// - Parameters:
 92 |     ///   - buffer: The string buffer to modify
 93 |     ///   - separator: The separator string to search for
 94 |     ///   - returnLeading: If true, returns text before separator; if false, returns text after
 95 |     /// - Returns: The separated token, or nil if separator not found
 96 |     private func separateToken(from buffer: inout String, separator: String, returnLeading: Bool)
 97 |         -> String?
 98 |     {
 99 |         guard let range = buffer.range(of: separator) else { return nil }
100 | 
101 |         let token: String
102 |         if returnLeading {
103 |             token = String(buffer[..<range.lowerBound])
104 |             buffer = String(buffer[range.lowerBound...])
105 |         } else {
106 |             token = String(buffer[range.upperBound...])
107 |             buffer = String(buffer[..<range.upperBound])
108 |         }
109 | 
110 |         return token
111 |     }
112 | 
113 |     private func partialMatch(buffer: String, tag: String) -> Bool {
114 |         for (tagIndex, bufferIndex) in zip(tag.indices, buffer.indices) {
115 |             if buffer[bufferIndex] != tag[tagIndex] {
116 |                 return false
117 |             }
118 |         }
119 | 
120 |         return true
121 |     }
122 | 
123 |     /// Parse a tool call from the content inside <tool_use> tags
124 |     private func parseToolCall(_ content: String) -> ToolCall? {
125 |         guard let match = content.firstMatch(of: Self.toolCallRegex) else { return nil }
126 | 
127 |         let jsonData = String(match.output.1).data(using: .utf8)!
128 | 
129 |         if let json = try? JSONDecoder().decode(ToolCall.Function.self, from: jsonData) {
130 |             return ToolCall(function: json)
131 |         } else {
132 |             return nil
133 |         }
134 |     }
135 | }
136 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/Tool/ToolParameter.swift:
--------------------------------------------------------------------------------
 1 | // Copyright © 2025 Apple Inc.
 2 | 
 3 | public indirect enum ToolParameterType {
 4 |     case string
 5 |     case bool
 6 |     case int
 7 |     case double
 8 |     case array(elementType: ToolParameterType)
 9 |     case object(properties: [ToolParameter])
10 |     case data
11 | 
12 |     var schemaType: [String: Any] {
13 |         switch self {
14 |         case .string: return ["type": "string"]
15 |         case .bool: return ["type": "boolean"]
16 |         case .int: return ["type": "integer"]
17 |         case .double: return ["type": "number"]
18 |         case .data: return ["type": "string", "contentEncoding": "base64"]
19 |         case .array(let elementType):
20 |             return ["type": "array", "items": elementType.schemaType]
21 |         case .object(let properties):
22 |             var props = [String: Any]()
23 |             var required = [String]()
24 | 
25 |             for param in properties {
26 |                 props[param.name] = param.schema
27 |                 if param.isRequired {
28 |                     required.append(param.name)
29 |                 }
30 |             }
31 | 
32 |             return ["type": "object", "properties": props, "required": required]
33 |         }
34 |     }
35 | }
36 | 
37 | public struct ToolParameter {
38 |     public let name: String
39 |     public let type: ToolParameterType
40 |     public let description: String
41 |     public let isRequired: Bool
42 |     public let extraProperties: [String: Any]
43 | 
44 |     public var schema: [String: Any] {
45 |         var schema = type.schemaType
46 |         schema["description"] = description
47 | 
48 |         // Add extra properties
49 |         for (key, value) in extraProperties {
50 |             schema[key] = value
51 |         }
52 | 
53 |         return schema
54 |     }
55 | 
56 |     public static func required(
57 |         _ name: String,
58 |         type: ToolParameterType,
59 |         description: String,
60 |         extraProperties: [String: Any] = [:]
61 |     ) -> ToolParameter {
62 |         ToolParameter(
63 |             name: name,
64 |             type: type,
65 |             description: description,
66 |             isRequired: true,
67 |             extraProperties: extraProperties
68 |         )
69 |     }
70 | 
71 |     public static func optional(
72 |         _ name: String,
73 |         type: ToolParameterType,
74 |         description: String,
75 |         extraProperties: [String: Any] = [:]
76 |     ) -> ToolParameter {
77 |         ToolParameter(
78 |             name: name,
79 |             type: type,
80 |             description: description,
81 |             isRequired: false,
82 |             extraProperties: extraProperties
83 |         )
84 |     }
85 | }
86 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/Tool/Value.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2025 Apple Inc.
  2 | 
  3 | import Foundation
  4 | 
  5 | /// Type-safe representation of JSON values
  6 | public enum JSONValue: Hashable, Codable, Sendable {
  7 |     case null
  8 |     case bool(Bool)
  9 |     case int(Int)
 10 |     case double(Double)
 11 |     case string(String)
 12 |     case array([JSONValue])
 13 |     case object([String: JSONValue])
 14 | 
 15 |     public init(from decoder: Decoder) throws {
 16 |         let container = try decoder.singleValueContainer()
 17 | 
 18 |         if container.decodeNil() {
 19 |             self = .null
 20 |         } else if let bool = try? container.decode(Bool.self) {
 21 |             self = .bool(bool)
 22 |         } else if let int = try? container.decode(Int.self) {
 23 |             self = .int(int)
 24 |         } else if let double = try? container.decode(Double.self) {
 25 |             self = .double(double)
 26 |         } else if let string = try? container.decode(String.self) {
 27 |             self = .string(string)
 28 |         } else if let array = try? container.decode([JSONValue].self) {
 29 |             self = .array(array)
 30 |         } else if let object = try? container.decode([String: JSONValue].self) {
 31 |             self = .object(object)
 32 |         } else {
 33 |             throw DecodingError.dataCorruptedError(
 34 |                 in: container, debugDescription: "Cannot decode JSON value")
 35 |         }
 36 |     }
 37 | 
 38 |     public func encode(to encoder: Encoder) throws {
 39 |         var container = encoder.singleValueContainer()
 40 | 
 41 |         switch self {
 42 |         case .null:
 43 |             try container.encodeNil()
 44 |         case .bool(let value):
 45 |             try container.encode(value)
 46 |         case .int(let value):
 47 |             try container.encode(value)
 48 |         case .double(let value):
 49 |             try container.encode(value)
 50 |         case .string(let value):
 51 |             try container.encode(value)
 52 |         case .array(let value):
 53 |             try container.encode(value)
 54 |         case .object(let value):
 55 |             try container.encode(value)
 56 |         }
 57 |     }
 58 | 
 59 |     public static func from(_ value: Any) -> JSONValue {
 60 |         switch value {
 61 |         case is NSNull:
 62 |             return .null
 63 |         case let bool as Bool:
 64 |             return .bool(bool)
 65 |         case let int as Int:
 66 |             return .int(int)
 67 |         case let double as Double:
 68 |             return .double(double)
 69 |         case let string as String:
 70 |             return .string(string)
 71 |         case let array as [Any]:
 72 |             return .array(array.map { from($0) })
 73 |         case let dict as [String: Any]:
 74 |             var result = [String: JSONValue]()
 75 |             for (key, value) in dict {
 76 |                 result[key] = from(value)
 77 |             }
 78 |             return .object(result)
 79 |         default:
 80 |             return .string(String(describing: value))
 81 |         }
 82 |     }
 83 | 
 84 |     public var anyValue: Any {
 85 |         switch self {
 86 |         case .null:
 87 |             return NSNull()
 88 |         case .bool(let value):
 89 |             return value
 90 |         case .int(let value):
 91 |             return value
 92 |         case .double(let value):
 93 |             return value
 94 |         case .string(let value):
 95 |             return value
 96 |         case .array(let value):
 97 |             return value.map { $0.anyValue }
 98 |         case .object(let value):
 99 |             return value.mapValues { $0.anyValue }
100 |         }
101 |     }
102 | 
103 |     /// Convert to JSON Schema representation
104 |     public var asSchema: [String: Any] {
105 |         switch self {
106 |         case .null:
107 |             return ["type": "null"]
108 |         case .bool:
109 |             return ["type": "boolean"]
110 |         case .int:
111 |             return ["type": "integer"]
112 |         case .double:
113 |             return ["type": "number"]
114 |         case .string:
115 |             return ["type": "string"]
116 |         case .array(let elements):
117 |             if let first = elements.first {
118 |                 return ["type": "array", "items": first.asSchema]
119 |             }
120 |             return ["type": "array"]
121 |         case .object(let properties):
122 |             var props: [String: Any] = [:]
123 | 
124 |             for (key, value) in properties {
125 |                 props[key] = value.asSchema
126 |             }
127 | 
128 |             return ["type": "object", "properties": props]
129 |         }
130 |     }
131 | }
132 | 


--------------------------------------------------------------------------------
/Libraries/MLXLMCommon/UserInput.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import AVFoundation
  4 | import CoreImage
  5 | import Foundation
  6 | import MLX
  7 | import Tokenizers
  8 | 
  9 | public typealias Message = [String: Any]
 10 | 
 11 | /// Container for raw user input.
 12 | ///
 13 | /// A ``UserInputProcessor`` can convert this to ``LMInput``.
 14 | /// See also ``ModelContext``.
 15 | public struct UserInput: Sendable {
 16 | 
 17 |     /// Representation of a prompt or series of messages (conversation).
 18 |     ///
 19 |     /// This may be a single string with a user prompt or a series of back
 20 |     /// and forth responses representing a conversation.
 21 |     public enum Prompt: Sendable, CustomStringConvertible {
 22 |         /// a single string
 23 |         case text(String)
 24 | 
 25 |         /// model specific array of dictionaries
 26 |         case messages([Message])
 27 | 
 28 |         /// model agnostic structured chat (series of messages)
 29 |         case chat([Chat.Message])
 30 | 
 31 |         public var description: String {
 32 |             switch self {
 33 |             case .text(let text):
 34 |                 return text
 35 |             case .messages(let messages):
 36 |                 return messages.map { $0.description }.joined(separator: "\n")
 37 |             case .chat(let messages):
 38 |                 return messages.map(\.content).joined(separator: "\n")
 39 |             }
 40 |         }
 41 |     }
 42 | 
 43 |     /// Representation of a video resource.
 44 |     public enum Video: Sendable {
 45 |         case avAsset(AVAsset)
 46 |         case url(URL)
 47 | 
 48 |         public func asAVAsset() -> AVAsset {
 49 |             switch self {
 50 |             case .avAsset(let asset):
 51 |                 return asset
 52 |             case .url(let url):
 53 |                 return AVAsset(url: url)
 54 |             }
 55 |         }
 56 |     }
 57 | 
 58 |     /// Representation of an image resource.
 59 |     public enum Image: Sendable {
 60 |         case ciImage(CIImage)
 61 |         case url(URL)
 62 |         case array(MLXArray)
 63 | 
 64 |         public func asCIImage() throws -> CIImage {
 65 |             switch self {
 66 |             case .ciImage(let image):
 67 |                 return image
 68 | 
 69 |             case .url(let url):
 70 |                 if let image = CIImage(contentsOf: url) {
 71 |                     return image
 72 |                 }
 73 |                 throw UserInputError.unableToLoad(url)
 74 | 
 75 |             case .array(let array):
 76 |                 guard array.ndim == 3 else {
 77 |                     throw UserInputError.arrayError("array must have 3 dimensions: \(array.ndim)")
 78 |                 }
 79 | 
 80 |                 var array = array
 81 | 
 82 |                 // convert to 0 .. 255
 83 |                 if array.max().item(Float.self) <= 1.0 {
 84 |                     array = array * 255
 85 |                 }
 86 | 
 87 |                 // planar -> pixels
 88 |                 switch array.dim(0) {
 89 |                 case 3, 4:
 90 |                     // channels first (planar)
 91 |                     array = array.transposed(1, 2, 0)
 92 |                 default:
 93 |                     break
 94 |                 }
 95 | 
 96 |                 // 4 components per pixel
 97 |                 switch array.dim(-1) {
 98 |                 case 3:
 99 |                     // pad to 4 bytes per pixel
100 |                     array = padded(array, widths: [0, 0, [0, 1]], value: MLXArray(255))
101 |                 case 4:
102 |                     // good
103 |                     break
104 |                 default:
105 |                     throw UserInputError.arrayError(
106 |                         "channel dimension must be last and 3/4: \(array.shape)")
107 |                     break
108 |                 }
109 | 
110 |                 let arrayData = array.asData()
111 |                 let (H, W, C) = array.shape3
112 |                 let cs = CGColorSpace(name: CGColorSpace.sRGB)!
113 | 
114 |                 return CIImage(
115 |                     bitmapData: arrayData.data, bytesPerRow: W * 4,
116 |                     size: .init(width: W, height: H),
117 |                     format: .RGBA8, colorSpace: cs)
118 |             }
119 |         }
120 |     }
121 | 
122 |     /// Representation of processing to apply to media.
123 |     public struct Processing: Sendable {
124 |         public var resize: CGSize?
125 | 
126 |         public init(resize: CGSize? = nil) {
127 |             self.resize = resize
128 |         }
129 |     }
130 | 
131 |     /// The prompt to evaluate.
132 |     public var prompt: Prompt {
133 |         didSet {
134 |             switch prompt {
135 |             case .text, .messages:
136 |                 // no action
137 |                 break
138 |             case .chat(let messages):
139 |                 // rebuild images & videos
140 |                 self.images = messages.reduce(into: []) { result, message in
141 |                     result.append(contentsOf: message.images)
142 |                 }
143 |                 self.videos = messages.reduce(into: []) { result, message in
144 |                     result.append(contentsOf: message.videos)
145 |                 }
146 |             }
147 |         }
148 |     }
149 | 
150 |     /// The images associated with the `UserInput`.
151 |     ///
152 |     /// If the ``prompt-swift.property`` is a ``Prompt-swift.enum/chat(_:)`` this will
153 |     /// collect the images from the chat messages, otherwise these are the stored images with the ``UserInput``.
154 |     public var images = [Image]()
155 | 
156 |     /// The images associated with the `UserInput`.
157 |     ///
158 |     /// If the ``prompt-swift.property`` is a ``Prompt-swift.enum/chat(_:)`` this will
159 |     /// collect the videos from the chat messages, otherwise these are the stored videos with the ``UserInput``.
160 |     public var videos = [Video]()
161 | 
162 |     public var tools: [ToolSpec]?
163 | 
164 |     /// Additional values provided for the chat template rendering context
165 |     public var additionalContext: [String: Any]?
166 |     public var processing: Processing = .init()
167 | 
168 |     /// Initialize the `UserInput` with a single text prompt.
169 |     ///
170 |     /// - Parameters:
171 |     ///   - prompt: text prompt
172 |     ///   - images: optional images
173 |     ///   - videos: optional videos
174 |     ///   - tools: optional tool specifications
175 |     ///   - additionalContext: optional context (model specific)
176 |     /// ### See Also
177 |     /// - ``Prompt-swift.enum/text(_:)``
178 |     /// - ``init(chat:tools:additionalContext:)``
179 |     public init(
180 |         prompt: String, images: [Image] = [Image](), videos: [Video] = [Video](),
181 |         tools: [ToolSpec]? = nil,
182 |         additionalContext: [String: Any]? = nil
183 |     ) {
184 |         self.prompt = .chat([
185 |             .user(prompt, images: images, videos: videos)
186 |         ])
187 |         self.tools = tools
188 |         self.additionalContext = additionalContext
189 |     }
190 | 
191 |     /// Initialize the `UserInput` with model specific mesage structures.
192 |     ///
193 |     /// For example, the Qwen2VL model wants input in this format:
194 |     ///
195 |     /// ```
196 |     /// [
197 |     ///     [
198 |     ///         "role": "user",
199 |     ///         "content": [
200 |     ///             [
201 |     ///                 "type": "text",
202 |     ///                 "text": "What is this?"
203 |     ///             ],
204 |     ///             [
205 |     ///                 "type": "image",
206 |     ///             ],
207 |     ///         ]
208 |     ///     ]
209 |     /// ]
210 |     /// ```
211 |     ///
212 |     /// Typically the ``init(chat:tools:additionalContext:)`` should be used instead
213 |     /// along with a model specific ``MessageGenerator`` (supplied by the ``UserInputProcessor``).
214 |     ///
215 |     /// - Parameters:
216 |     ///   - messages: array of dictionaries representing the prompt in a model specific format
217 |     ///   - images: optional images
218 |     ///   - videos: optional videos
219 |     ///   - tools: optional tool specifications
220 |     ///   - additionalContext: optional context (model specific)
221 |     /// ### See Also
222 |     /// - ``Prompt-swift.enum/text(_:)``
223 |     /// - ``init(chat:tools:additionalContext:)``
224 |     public init(
225 |         messages: [Message], images: [Image] = [Image](), videos: [Video] = [Video](),
226 |         tools: [ToolSpec]? = nil,
227 |         additionalContext: [String: Any]? = nil
228 |     ) {
229 |         self.prompt = .messages(messages)
230 |         self.images = images
231 |         self.videos = videos
232 |         self.tools = tools
233 |         self.additionalContext = additionalContext
234 |     }
235 | 
236 |     /// Initialize the `UserInput` with a model agnostic structured context.
237 |     ///
238 |     /// For example:
239 |     ///
240 |     /// ```
241 |     /// let chat: [Chat.Message] = [
242 |     ///     .system("You are a helpful photographic assistant."),
243 |     ///     .user("Please describe the photo.", images: [image1]),
244 |     /// ]
245 |     /// let userInput = UserInput(chat: chat)
246 |     /// ```
247 |     ///
248 |     /// A model specific ``MessageGenerator`` (supplied by the ``UserInputProcessor``)
249 |     /// is used to convert this into a model specific format.
250 |     ///
251 |     /// - Parameters:
252 |     ///   - chat: structured content
253 |     ///   - tools: optional tool specifications
254 |     ///   - processing: optional processing to be applied to media
255 |     ///   - additionalContext: optional context (model specific)
256 |     /// ### See Also
257 |     /// - ``Prompt-swift.enum/text(_:)``
258 |     /// - ``init(chat:tools:additionalContext:)``
259 |     public init(
260 |         chat: [Chat.Message],
261 |         processing: Processing = .init(),
262 |         tools: [ToolSpec]? = nil,
263 |         additionalContext: [String: Any]? = nil
264 |     ) {
265 |         self.prompt = .chat(chat)
266 | 
267 |         // note: prompt.didSet is not triggered in init
268 |         self.images = chat.reduce(into: []) { result, message in
269 |             result.append(contentsOf: message.images)
270 |         }
271 |         self.videos = chat.reduce(into: []) { result, message in
272 |             result.append(contentsOf: message.videos)
273 |         }
274 | 
275 |         self.processing = processing
276 |         self.tools = tools
277 |         self.additionalContext = additionalContext
278 |     }
279 | 
280 |     /// Initialize the `UserInput` with a preconfigured ``Prompt-swift.enum``.
281 |     ///
282 |     /// ``init(chat:tools:additionalContext:)`` is the preferred mechanism.
283 |     ///
284 |     /// - Parameters:
285 |     ///   - prompt: the prompt
286 |     ///   - images: optional images
287 |     ///   - videos: optional videos
288 |     ///   - tools: optional tool specifications
289 |     ///   - processing: optional processing to be applied to media
290 |     ///   - additionalContext: optional context (model specific)
291 |     /// ### See Also
292 |     /// - ``Prompt-swift.enum/text(_:)``
293 |     /// - ``init(chat:tools:additionalContext:)``
294 |     public init(
295 |         prompt: Prompt,
296 |         images: [Image] = [Image](),
297 |         videos: [Video] = [Video](),
298 |         processing: Processing = .init(),
299 |         tools: [ToolSpec]? = nil, additionalContext: [String: Any]? = nil
300 |     ) {
301 |         self.prompt = prompt
302 |         switch prompt {
303 |         case .text, .messages:
304 |             self.images = images
305 |             self.videos = videos
306 |         case .chat:
307 |             break
308 |         }
309 |         self.processing = processing
310 |         self.tools = tools
311 |         self.additionalContext = additionalContext
312 |     }
313 | }
314 | 
315 | /// Protocol for a type that can convert ``UserInput`` to ``LMInput``.
316 | ///
317 | /// See also ``ModelContext``.
318 | public protocol UserInputProcessor {
319 |     func prepare(input: UserInput) async throws -> LMInput
320 | }
321 | 
322 | private enum UserInputError: LocalizedError {
323 |     case notImplemented
324 |     case unableToLoad(URL)
325 |     case arrayError(String)
326 | 
327 |     var errorDescription: String? {
328 |         switch self {
329 |         case .notImplemented:
330 |             return String(localized: "This functionality is not implemented.")
331 |         case .unableToLoad(let url):
332 |             return String(localized: "Unable to load image from URL: \(url.path).")
333 |         case .arrayError(let message):
334 |             return String(localized: "Error processing image array: \(message).")
335 |         }
336 |     }
337 | }
338 | 
339 | /// A do-nothing ``UserInputProcessor``.
340 | public struct StandInUserInputProcessor: UserInputProcessor {
341 |     public init() {}
342 | 
343 |     public func prepare(input: UserInput) throws -> LMInput {
344 |         throw UserInputError.notImplemented
345 |     }
346 | }
347 | 


--------------------------------------------------------------------------------
/Libraries/MLXMNIST/Files.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import Foundation
  4 | import Gzip
  5 | import MLX
  6 | 
  7 | // based on https://github.com/ml-explore/mlx-examples/blob/main/mnist/mnist.py
  8 | 
  9 | public enum Use: String, Hashable, Sendable {
 10 |     case test
 11 |     case training
 12 | }
 13 | 
 14 | public enum DataKind: String, Hashable, Sendable {
 15 |     case images
 16 |     case labels
 17 | }
 18 | 
 19 | public struct FileKind: Hashable, CustomStringConvertible, Sendable {
 20 |     let use: Use
 21 |     let data: DataKind
 22 | 
 23 |     public init(_ use: Use, _ data: DataKind) {
 24 |         self.use = use
 25 |         self.data = data
 26 |     }
 27 | 
 28 |     public var description: String {
 29 |         "\(use.rawValue)-\(data.rawValue)"
 30 |     }
 31 | }
 32 | 
 33 | struct LoadInfo: Sendable {
 34 |     let name: String
 35 |     let offset: Int
 36 |     let convert: @Sendable (MLXArray) -> MLXArray
 37 | }
 38 | 
 39 | let baseURL = URL(string: "https://raw.githubusercontent.com/fgnt/mnist/master/")!
 40 | 
 41 | private let files = [
 42 |     FileKind(.training, .images): LoadInfo(
 43 |         name: "train-images-idx3-ubyte.gz",
 44 |         offset: 16,
 45 |         convert: {
 46 |             $0.reshaped([-1, 28, 28, 1]).asType(.float32) / 255.0
 47 |         }),
 48 |     FileKind(.test, .images): LoadInfo(
 49 |         name: "t10k-images-idx3-ubyte.gz",
 50 |         offset: 16,
 51 |         convert: {
 52 |             $0.reshaped([-1, 28, 28, 1]).asType(.float32) / 255.0
 53 |         }),
 54 |     FileKind(.training, .labels): LoadInfo(
 55 |         name: "train-labels-idx1-ubyte.gz",
 56 |         offset: 8,
 57 |         convert: {
 58 |             $0.asType(.uint32)
 59 |         }),
 60 |     FileKind(.test, .labels): LoadInfo(
 61 |         name: "t10k-labels-idx1-ubyte.gz",
 62 |         offset: 8,
 63 |         convert: {
 64 |             $0.asType(.uint32)
 65 |         }),
 66 | ]
 67 | 
 68 | public func download(into: URL) async throws {
 69 |     for (_, info) in files {
 70 |         let fileURL = into.appending(component: info.name)
 71 |         if !FileManager.default.fileExists(atPath: fileURL.path()) {
 72 |             print("Download: \(info.name)")
 73 |             let url = baseURL.appending(component: info.name)
 74 |             let (data, response) = try await URLSession.shared.data(from: url)
 75 | 
 76 |             guard let httpResponse = response as? HTTPURLResponse else {
 77 |                 fatalError("Unable to download \(url), not an http response: \(response)")
 78 |             }
 79 |             guard httpResponse.statusCode == 200 else {
 80 |                 fatalError("Unable to download \(url): \(httpResponse)")
 81 |             }
 82 | 
 83 |             try data.write(to: fileURL)
 84 |         }
 85 |     }
 86 | }
 87 | 
 88 | public func load(from: URL) throws -> [FileKind: MLXArray] {
 89 |     var result = [FileKind: MLXArray]()
 90 | 
 91 |     for (key, info) in files {
 92 |         let fileURL = from.appending(component: info.name)
 93 |         let data = try Data(contentsOf: fileURL).gunzipped()
 94 | 
 95 |         let array = MLXArray(
 96 |             data.dropFirst(info.offset), [data.count - info.offset], type: UInt8.self)
 97 | 
 98 |         result[key] = info.convert(array)
 99 |     }
100 | 
101 |     return result
102 | }
103 | 


--------------------------------------------------------------------------------
/Libraries/MLXMNIST/MNIST.swift:
--------------------------------------------------------------------------------
 1 | // Copyright © 2024 Apple Inc.
 2 | 
 3 | import Foundation
 4 | import MLX
 5 | import MLXNN
 6 | 
 7 | // based on https://github.com/ml-explore/mlx-examples/blob/main/mnist/main.py
 8 | 
 9 | public class LeNet: Module, UnaryLayer {
10 | 
11 |     @ModuleInfo var conv1: Conv2d
12 |     @ModuleInfo var conv2: Conv2d
13 |     @ModuleInfo var pool1: MaxPool2d
14 |     @ModuleInfo var pool2: MaxPool2d
15 |     @ModuleInfo var fc1: Linear
16 |     @ModuleInfo var fc2: Linear
17 |     @ModuleInfo var fc3: Linear
18 | 
19 |     override public init() {
20 |         conv1 = Conv2d(inputChannels: 1, outputChannels: 6, kernelSize: 5, padding: 2)
21 |         conv2 = Conv2d(inputChannels: 6, outputChannels: 16, kernelSize: 5, padding: 0)
22 |         pool1 = MaxPool2d(kernelSize: 2, stride: 2)
23 |         pool2 = MaxPool2d(kernelSize: 2, stride: 2)
24 |         fc1 = Linear(16 * 5 * 5, 120)
25 |         fc2 = Linear(120, 84)
26 |         fc3 = Linear(84, 10)
27 |     }
28 | 
29 |     public func callAsFunction(_ x: MLXArray) -> MLXArray {
30 |         var x = x
31 |         x = pool1(tanh(conv1(x)))
32 |         x = pool2(tanh(conv2(x)))
33 |         x = flattened(x, start: 1)
34 |         x = tanh(fc1(x))
35 |         x = tanh(fc2(x))
36 |         x = fc3(x)
37 |         return x
38 |     }
39 | }
40 | 
41 | public func loss(model: LeNet, x: MLXArray, y: MLXArray) -> MLXArray {
42 |     crossEntropy(logits: model(x), targets: y, reduction: .mean)
43 | }
44 | 
45 | public func eval(model: LeNet, x: MLXArray, y: MLXArray) -> MLXArray {
46 |     mean(argMax(model(x), axis: 1) .== y)
47 | }
48 | 
49 | private struct BatchSequence: Sequence, IteratorProtocol {
50 | 
51 |     let batchSize: Int
52 |     let x: MLXArray
53 |     let y: MLXArray
54 | 
55 |     let indexes: MLXArray
56 |     var index = 0
57 | 
58 |     init(batchSize: Int, x: MLXArray, y: MLXArray, using generator: inout any RandomNumberGenerator)
59 |     {
60 |         self.batchSize = batchSize
61 |         self.x = x
62 |         self.y = y
63 |         self.indexes = MLXArray(Array(0 ..< y.size).shuffled(using: &generator))
64 |     }
65 | 
66 |     mutating func next() -> (MLXArray, MLXArray)? {
67 |         guard index < y.size else { return nil }
68 | 
69 |         let range = index ..< Swift.min(index + batchSize, y.size)
70 |         index += batchSize
71 |         let ids = indexes[range]
72 |         return (x[ids], y[ids])
73 |     }
74 | }
75 | 
76 | public func iterateBatches(
77 |     batchSize: Int, x: MLXArray, y: MLXArray, using generator: inout any RandomNumberGenerator
78 | ) -> some Sequence<(MLXArray, MLXArray)> {
79 |     BatchSequence(batchSize: batchSize, x: x, y: y, using: &generator)
80 | }
81 | 


--------------------------------------------------------------------------------
/Libraries/MLXMNIST/README.md:
--------------------------------------------------------------------------------
 1 | #  MNIST
 2 | 
 3 | This is a port of the MNIST training code from the [Python MLX example](https://github.com/ml-explore/mlx-examples/blob/main/mnist). This example uses a [LeNet](https://en.wikipedia.org/wiki/LeNet) instead of an MLP.
 4 | 
 5 | It provides code to:
 6 | 
 7 | - Download the MNIST test/train data
 8 | - Build the LeNet
 9 | - Some functions to shuffle and batch the data
10 | 
11 | See [mnist-tool](../../Tools/mnist-tool) for an example of how to run this. The training loop also lives there.
12 | 


--------------------------------------------------------------------------------
/Libraries/MLXMNIST/Random.swift:
--------------------------------------------------------------------------------
 1 | // Copyright © 2024 Apple Inc.
 2 | 
 3 | import Foundation
 4 | 
 5 | // From https://github.com/apple/swift/blob/cb0fb1ea051631219c0b944b84c78571448d58c2/benchmark/utils/TestsUtils.swift#L254
 6 | //
 7 | // This is just a seedable RandomNumberGenerator for shuffle()
 8 | 
 9 | // This is a fixed-increment version of Java 8's SplittableRandom generator.
10 | // It is a very fast generator passing BigCrush, with 64 bits of state.
11 | // See http://dx.doi.org/10.1145/2714064.2660195 and
12 | // http://docs.oracle.com/javase/8/docs/api/java/util/SplittableRandom.html
13 | //
14 | // Derived from public domain C implementation by Sebastiano Vigna
15 | // See http://xoshiro.di.unimi.it/splitmix64.c
16 | public struct SplitMix64: RandomNumberGenerator, Sendable {
17 |     private var state: UInt64
18 | 
19 |     public init(seed: UInt64) {
20 |         self.state = seed
21 |     }
22 | 
23 |     public mutating func next() -> UInt64 {
24 |         self.state &+= 0x9e37_79b9_7f4a_7c15
25 |         var z: UInt64 = self.state
26 |         z = (z ^ (z &>> 30)) &* 0xbf58_476d_1ce4_e5b9
27 |         z = (z ^ (z &>> 27)) &* 0x94d0_49bb_1331_11eb
28 |         return z ^ (z &>> 31)
29 |     }
30 | }
31 | 


--------------------------------------------------------------------------------
/Libraries/MLXVLM/MediaProcessing.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import AVFoundation
  4 | import CoreImage.CIFilterBuiltins
  5 | import MLX
  6 | import MLXLMCommon
  7 | 
  8 | public struct VideoFrame {
  9 |     let frame: CIImage
 10 |     let timeStamp: CMTime
 11 | }
 12 | 
 13 | public struct ProcessedFrames {
 14 |     let frames: [MLXArray]
 15 |     let timestamps: [CMTime]
 16 |     let totalDuration: CMTime
 17 | }
 18 | 
 19 | private let context = CIContext()
 20 | 
 21 | /// Collection of methods for processing media (images, video, etc.).
 22 | ///
 23 | /// A typical image preparation pipeline might look like this:
 24 | ///
 25 | /// ```swift
 26 | /// var image: CIImage
 27 | /// image = MediaProcessing.inSRGBToneCurveSpace(image)
 28 | ///
 29 | /// // Apply user instructions
 30 | /// image = MediaProcessing.apply(image, processing: processing)
 31 | ///
 32 | /// image = MediaProcessing.resampleBicubic(image, to: config.size.cgSize)
 33 | /// image = MediaProcessing.normalize(
 34 | ///     image, mean: config.imageMeanTuple, std: config.imageStdTuple)
 35 | ///
 36 | /// return MediaProcessing.asMLXArray(image)
 37 | /// ```
 38 | ///
 39 | /// This is the responsibility of the `UserInputProcessor`.
 40 | public enum MediaProcessing {
 41 | 
 42 |     /// VLM media processing is normally done without regard to the colorspace. Many,
 43 |     /// though not all, images are stored in sRGB and this will be the implicit colorspace
 44 |     /// used. This converts to a colorspace with an sRGB tone curve, though not necessarily
 45 |     /// sRGB primaries, etc.
 46 |     ///
 47 |     /// See ``inLinearToneCurveSpace(_:)``
 48 |     public static func inSRGBToneCurveSpace(_ image: CIImage) -> CIImage {
 49 |         let filter = CIFilter.linearToSRGBToneCurve()
 50 |         filter.inputImage = image
 51 |         return filter.outputImage!
 52 |     }
 53 | 
 54 |     /// Inverse of ``inSRGBToneCurveSpace(_:)`` (for completeness).
 55 |     public static func inLinearToneCurveSpace(_ image: CIImage) -> CIImage {
 56 |         let filter = CIFilter.sRGBToneCurveToLinear()
 57 |         filter.inputImage = image
 58 |         return filter.outputImage!
 59 |     }
 60 | 
 61 |     /// Compute the best fit size of one size in another (respecting aspect ratio).
 62 |     public static func bestFit(_ size: CGSize, in other: CGSize) -> CGSize {
 63 |         let scale = bestFitScale(size, in: other)
 64 |         return CGSize(width: round(size.width * scale), height: round(size.height * scale))
 65 |     }
 66 | 
 67 |     /// Compute the best fit scale of one size in another (respecting aspect ratio).
 68 |     public static func bestFitScale(_ size: CGSize, in other: CGSize) -> CGFloat {
 69 |         min(other.width / size.width, other.height / size.height)
 70 |     }
 71 | 
 72 |     static public func aspectRatioForResample(_ image: CIImage, size: CGSize) -> Float {
 73 |         let inputAspectRatio = image.extent.width / image.extent.height
 74 |         let desiredAspectRatio = size.width / size.height
 75 |         return Float(1 / inputAspectRatio * desiredAspectRatio)
 76 |     }
 77 | 
 78 |     /// Resample the image using Lanczos interpolation.
 79 |     static public func resampleLanczos(_ image: CIImage, to size: CGSize) -> CIImage {
 80 |         // Create a bicubic scale filter
 81 | 
 82 |         let yScale = size.height / image.extent.height
 83 |         let xScale = size.width / image.extent.width
 84 | 
 85 |         let filter = CIFilter.lanczosScaleTransform()
 86 |         filter.inputImage = image
 87 |         filter.scale = Float(yScale)
 88 |         filter.aspectRatio = Float(xScale / yScale)
 89 |         let scaledImage = filter.outputImage!
 90 | 
 91 |         // Create a rect with the exact dimensions we want
 92 |         let exactRect = CGRect(
 93 |             x: 0,
 94 |             y: 0,
 95 |             width: size.width,
 96 |             height: size.height
 97 |         )
 98 | 
 99 |         // Crop to ensure exact dimensions
100 |         return scaledImage.cropped(to: exactRect)
101 |     }
102 | 
103 |     /// Resample the image using bicubic interpolation.
104 |     /// - Parameters:
105 |     ///   - image: The image to resample
106 |     ///   - size: The target size
107 |     /// - Returns: The resampled image
108 |     public static func resampleBicubic(_ image: CIImage, to size: CGSize) -> CIImage {
109 |         // Create a bicubic scale filter
110 | 
111 |         let yScale = size.height / image.extent.height
112 |         let xScale = size.width / image.extent.width
113 | 
114 |         let filter = CIFilter.bicubicScaleTransform()
115 |         filter.inputImage = image
116 |         filter.scale = Float(yScale)
117 |         filter.aspectRatio = Float(xScale / yScale)
118 |         let scaledImage = filter.outputImage!
119 | 
120 |         // Create a rect with the exact dimensions we want
121 |         let exactRect = CGRect(
122 |             x: 0,
123 |             y: 0,
124 |             width: size.width,
125 |             height: size.height
126 |         )
127 | 
128 |         // Crop to ensure exact dimensions
129 |         return scaledImage.cropped(to: exactRect)
130 |     }
131 | 
132 |     /// Normalize the image using the given mean and standard deviation parameters.
133 |     public static func normalize(
134 |         _ image: CIImage, mean: (CGFloat, CGFloat, CGFloat), std: (CGFloat, CGFloat, CGFloat)
135 |     ) -> CIImage {
136 |         let filter = CIFilter.colorMatrix()
137 |         filter.inputImage = image
138 | 
139 |         // This should match
140 |         // https://pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html
141 |         //
142 |         // output[channel] = (input[channel] - mean[channel]) / std[channel]
143 |         //
144 |         // The CI filter computes input * factor + bias so we want to do:
145 |         // input * 1 / std - mean / std
146 | 
147 |         filter.rVector = .init(x: 1 / std.0, y: 0, z: 0, w: 0)
148 |         filter.gVector = .init(x: 0, y: 1 / std.1, z: 0, w: 0)
149 |         filter.bVector = .init(x: 0, y: 0, z: 1 / std.2, w: 0)
150 | 
151 |         filter.aVector = .init(x: 0, y: 0, z: 0, w: 1)
152 |         filter.biasVector = .init(x: -mean.0 / std.0, y: -mean.1 / std.1, z: -mean.2 / std.2, w: 0)
153 | 
154 |         return filter.outputImage!
155 |     }
156 | 
157 |     /// Convert the CIImage into a planar 3 channel MLXArray `[1, C, H, W]`
158 |     /// - Parameters:
159 |     ///   - image: The image to convert
160 |     ///   - colorSpace: Optional color space for rendering
161 |     /// - Returns: The MLXArray representation of the image
162 |     public static func asMLXArray(_ image: CIImage, colorSpace: CGColorSpace? = nil) -> MLXArray {
163 |         let size = image.extent.size
164 |         let w = Int(size.width.rounded())
165 |         let h = Int(size.height.rounded())
166 | 
167 |         // probably not strictly necessary, but this is what happens in
168 |         // e.g. image_processing_siglip in transformers (float32)
169 |         let format = CIFormat.RGBAf
170 |         let componentsPerPixel = 4
171 |         let bytesPerPixel = componentsPerPixel * 4
172 |         let bytesPerRow = w * bytesPerPixel
173 | 
174 |         var data = Data(count: w * h * bytesPerPixel)
175 |         data.withUnsafeMutableBytes { ptr in
176 |             context.render(
177 |                 image, toBitmap: ptr.baseAddress!, rowBytes: bytesPerRow, bounds: image.extent,
178 |                 format: format, colorSpace: colorSpace)
179 |             context.clearCaches()
180 |         }
181 | 
182 |         var array = MLXArray(data, [h, w, 4], type: Float32.self)
183 | 
184 |         // Drop 4th channel
185 |         array = array[0..., 0..., ..<3]
186 | 
187 |         // Convert to 1, C, H, W
188 |         array = array.reshaped(1, h, w, 3).transposed(0, 3, 1, 2)
189 | 
190 |         return array
191 |     }
192 | 
193 |     /// Return `true` if the size is smaller or equal to the size of the `extent`.
194 |     public static func rectSmallerOrEqual(_ extent: CGRect, size: CGSize) -> Bool {
195 |         return extent.width <= size.width && extent.height <= size.height
196 |     }
197 | 
198 |     /// Given an `extent` and a target `size` produce the `CGRect` that will be a center crop.
199 |     public static func centerCrop(_ extent: CGRect, size: CGSize) -> CGRect {
200 |         let targetWidth = min(extent.width, size.width)
201 |         let targetHeight = min(extent.height, size.height)
202 | 
203 |         return CGRect(
204 |             x: (extent.maxX - targetWidth) / 2,
205 |             y: (extent.maxY - targetHeight) / 2,
206 |             width: targetWidth, height: targetHeight
207 |         )
208 |     }
209 | 
210 |     /// Given an `image` and a target `size` produce the `CIImage` that will be a center crop.
211 |     public static func centerCrop(_ image: CIImage, size: CGSize) -> CIImage {
212 |         let extent = image.extent
213 |         if rectSmallerOrEqual(extent, size: size) {
214 |             return image
215 |         }
216 | 
217 |         let crop = centerCrop(extent, size: size)
218 |         return
219 |             image
220 |             .cropped(to: crop)
221 |             .transformed(by: CGAffineTransform(translationX: -crop.minX, y: -crop.minY))
222 |     }
223 | 
224 |     /// Given a `size` and a target `shortestEdge` compute a new size
225 |     /// that respects the aspect ratio of the original `size` and is
226 |     /// constrained by the `shortestEdge`.
227 |     public static func fitIn(_ size: CGSize, shortestEdge: Int) -> CGSize {
228 |         let floatShortestEdge = CGFloat(shortestEdge)
229 | 
230 |         let (short, long) =
231 |             size.width <= size.height ? (size.width, size.height) : (size.height, size.width)
232 |         let newShort = floatShortestEdge
233 |         let newLong = floatShortestEdge * long / short
234 | 
235 |         return size.width <= size.height
236 |             ? CGSize(width: newShort, height: newLong) : CGSize(width: newLong, height: newShort)
237 |     }
238 | 
239 |     /// Given a `size` and a target `longestEdge` compute a new size
240 |     /// that respects the aspect ratio of the original `size` and is
241 |     /// constrained by the `longestEdge`.
242 |     public static func fitIn(_ size: CGSize, longestEdge: Int) -> CGSize {
243 |         let floatLongestEdge = CGFloat(longestEdge)
244 | 
245 |         var (newShort, newLong) =
246 |             size.width <= size.height ? (size.width, size.height) : (size.height, size.width)
247 | 
248 |         if newLong > floatLongestEdge {
249 |             newLong = floatLongestEdge
250 |             newShort = floatLongestEdge * newShort / newLong
251 |         }
252 | 
253 |         return size.width <= size.height
254 |             ? CGSize(width: newShort, height: newLong) : CGSize(width: newLong, height: newShort)
255 |     }
256 | 
257 |     /// Apply `UserInput.Processing`, if needed, to the image.
258 |     public static func apply(_ image: CIImage, processing: UserInput.Processing?) -> CIImage {
259 |         var image = image
260 | 
261 |         if let resize = processing?.resize {
262 |             let scale = bestFitScale(image.extent.size, in: resize)
263 |             image = image.transformed(by: CGAffineTransform(scaleX: scale, y: scale))
264 |         }
265 | 
266 |         return image
267 |     }
268 | 
269 |     public static func asCIImageSequence(_ asset: AVAsset, samplesPerSecond: Int) async throws
270 |         -> [CIImage]
271 |     {
272 |         // Use AVAssetImageGenerator to extract frames
273 |         let generator = AVAssetImageGenerator(asset: asset)
274 |         generator.appliesPreferredTrackTransform = true
275 |         generator.requestedTimeToleranceBefore = .zero
276 |         generator.requestedTimeToleranceAfter = .zero
277 | 
278 |         // Calculate the time values we want to sample
279 |         guard let duration = try? await asset.load(.duration) else {
280 |             throw NSError(
281 |                 domain: "MediaProcessing", code: -1,
282 |                 userInfo: [NSLocalizedDescriptionKey: "Failed to load the asset's duration"])
283 |         }
284 | 
285 |         let durationInSeconds = duration.seconds
286 |         let samplesPerSecond = Double(samplesPerSecond)
287 |         let secondsPerSample = 1.0 / samplesPerSecond
288 |         let totalFramesToSample = durationInSeconds * samplesPerSecond
289 |         let durationTimeValue = duration.value
290 |         let sampledTimeValues = MLXArray.linspace(
291 |             0, durationTimeValue, count: Int(totalFramesToSample)
292 |         ).asArray(Int64.self)
293 | 
294 |         // Construct a CMTime using the sampled CMTimeValue's and the asset's timescale
295 |         let timescale = duration.timescale
296 |         let sampledTimes = sampledTimeValues.map { CMTime(value: $0, timescale: timescale) }
297 | 
298 |         // Collect the frames
299 |         var ciImages: [CIImage] = []
300 |         for await result in await generator.images(for: sampledTimes) {
301 |             switch result {
302 |             case .success(requestedTime: let requested, let image, actualTime: let actual):
303 |                 let ciImage = CIImage(
304 |                     cgImage: image, options: [.colorSpace: CGColorSpace(name: CGColorSpace.sRGB)!])
305 |                 ciImages.append(ciImage)
306 |             case .failure(requestedTime: let requested, let error):
307 |                 break
308 |             }
309 |         }
310 | 
311 |         return ciImages
312 |     }
313 | 
314 |     static public func asProcessedSequence(
315 |         _ asset: AVAsset, samplesPerSecond: Int,
316 |         frameProcessing: (VideoFrame) throws -> VideoFrame = { $0 }
317 |     ) async throws -> ProcessedFrames {
318 |         return try await asProcessedSequence(
319 |             asset, maxFrames: Int.max, targetFPS: { _ in Double(samplesPerSecond) },
320 |             frameProcessing: frameProcessing)
321 |     }
322 | 
323 |     static public func asProcessedSequence(
324 |         _ asset: AVAsset, maxFrames: Int, targetFPS: (CMTime) -> Double,
325 |         frameProcessing: (VideoFrame) throws -> VideoFrame = { $0 }
326 |     ) async throws -> ProcessedFrames {
327 |         // Use AVAssetImageGenerator to extract frames
328 |         let generator = AVAssetImageGenerator(asset: asset)
329 |         generator.appliesPreferredTrackTransform = true
330 |         generator.requestedTimeToleranceBefore = .zero
331 |         generator.requestedTimeToleranceAfter = .zero
332 | 
333 |         guard let duration = try? await asset.load(.duration) else {
334 |             throw NSError(
335 |                 domain: "MediaProcessing", code: -1,
336 |                 userInfo: [NSLocalizedDescriptionKey: "Failed to load the asset's duration"])
337 |         }
338 |         let fps = targetFPS(duration)
339 |         // Note: the round was not present in `asCIImageSequence`, so we may now be passing 1 more frame to Qwen depending on video duration.
340 |         let estimatedFrames = Int(round(fps * duration.seconds))
341 |         var desiredFrames = min(estimatedFrames, maxFrames)
342 |         let finalFrameCount = max(desiredFrames, 1)
343 | 
344 |         let sampledTimeValues = MLXArray.linspace(
345 |             0, duration.value, count: Int(finalFrameCount)
346 |         ).asArray(Int64.self)
347 | 
348 |         // Construct a CMTime using the sampled CMTimeValue's and the asset's timescale
349 |         let timescale = duration.timescale
350 |         let sampledTimes = sampledTimeValues.map { CMTime(value: $0, timescale: timescale) }
351 | 
352 |         // Collect the frames
353 |         var ciImages: [CIImage] = []
354 |         var timestamps: [CMTime] = []
355 | 
356 |         var frames: [VideoFrame] = []
357 | 
358 |         for await result in await generator.images(for: sampledTimes) {
359 |             switch result {
360 |             case .success(requestedTime: let requested, let image, actualTime: let actual):
361 |                 let ciImage = CIImage(
362 |                     cgImage: image, options: [.colorSpace: CGColorSpace(name: CGColorSpace.sRGB)!])
363 |                 let frame = try frameProcessing(.init(frame: ciImage, timeStamp: actual))
364 |                 ciImages.append(frame.frame)
365 |                 timestamps.append(frame.timeStamp)
366 |             case .failure(requestedTime: let requested, let error):
367 |                 break
368 |             }
369 |         }
370 | 
371 |         let framesAsArrays = ciImages.map { $0.asMLXArray() }
372 |         return ProcessedFrames(
373 |             frames: framesAsArrays,
374 |             timestamps: timestamps,
375 |             totalDuration: duration
376 |         )
377 |     }
378 | }
379 | 
380 | // MARK: - Convenience
381 | 
382 | extension CIImage {
383 |     public enum ResamplingMethod {
384 |         case bicubic
385 |         case lanczos
386 |     }
387 | 
388 |     public func resampled(to size: CGSize, method: ResamplingMethod = .bicubic) -> CIImage {
389 |         switch method {
390 |         case .bicubic:
391 |             return MediaProcessing.resampleBicubic(self, to: size)
392 |         case .lanczos:
393 |             return MediaProcessing.resampleLanczos(self, to: size)
394 |         }
395 |     }
396 | 
397 |     public func toSRGB() -> CIImage {
398 |         return MediaProcessing.inSRGBToneCurveSpace(self)
399 |     }
400 | 
401 |     public func toLinear() -> CIImage {
402 |         return MediaProcessing.inLinearToneCurveSpace(self)
403 |     }
404 | 
405 |     public func normalized(mean: (CGFloat, CGFloat, CGFloat), std: (CGFloat, CGFloat, CGFloat))
406 |         -> CIImage
407 |     {
408 |         return MediaProcessing.normalize(self, mean: mean, std: std)
409 |     }
410 | 
411 |     public func asMLXArray(colorSpace: CGColorSpace? = nil) -> MLXArray {
412 |         return MediaProcessing.asMLXArray(self, colorSpace: colorSpace)
413 |     }
414 | }
415 | 


--------------------------------------------------------------------------------
/Libraries/MLXVLM/Models/Gemma3.swift:
--------------------------------------------------------------------------------
   1 | import CoreImage
   2 | import MLX
   3 | import MLXFast
   4 | import MLXLMCommon
   5 | import MLXNN
   6 | import Tokenizers
   7 | 
   8 | // Based on https://github.com/Blaizzy/mlx-vlm/tree/main/mlx_vlm/models/gemma3
   9 | 
  10 | // MARK: - Text Configuration
  11 | 
  12 | public struct Gemma3TextConfiguration: Codable, Sendable {
  13 |     public let modelType: String
  14 |     public let hiddenSize: Int
  15 |     public let hiddenLayers: Int
  16 |     public let intermediateSize: Int
  17 |     public let slidingWindow: Int
  18 |     public let ropeScaling: [String: StringOrNumber]?
  19 |     public let finalLogitSoftcapping: Float?
  20 | 
  21 |     public let vocabularySize: Int = 262208
  22 |     public let rmsNormEps: Float = 1.0e-6
  23 | 
  24 |     // Decoded from JSON when present, with fallback if not
  25 | 
  26 |     private let _attentionHeads: Int?
  27 |     private let _kvHeads: Int?
  28 |     private let _headDim: Int?
  29 |     private let _queryPreAttnScalar: Float?
  30 | 
  31 |     // Not included in 4B model config.json, included for 12B and 27B models
  32 |     public var attentionHeads: Int {
  33 |         _attentionHeads ?? 8
  34 |     }
  35 | 
  36 |     // Not included in 4B model config.json, included for 12B and 27B models
  37 |     public var kvHeads: Int {
  38 |         _kvHeads ?? 4
  39 |     }
  40 | 
  41 |     // Not included in 4B and 12B model config.json, included for 27B model
  42 |     public var headDim: Int {
  43 |         _headDim ?? 256
  44 |     }
  45 | 
  46 |     // Not included in 4B and 12B model config.json, included for 27B model
  47 |     public var queryPreAttnScalar: Float {
  48 |         _queryPreAttnScalar ?? 256
  49 |     }
  50 | 
  51 |     public let ropeGlobalBaseFreq: Float = 1_000_000.0
  52 |     public let ropeLocalBaseFreq: Float = 10_000.0
  53 |     public let ropeTraditional: Bool = false
  54 |     public let mmTokensPerImage: Int = 256
  55 |     public let slidingWindowPattern: Int = 6
  56 |     public let maxPositionEmbeddings: Int = 4096
  57 | 
  58 |     enum CodingKeys: String, CodingKey {
  59 |         case modelType = "model_type"
  60 |         case hiddenSize = "hidden_size"
  61 |         case hiddenLayers = "num_hidden_layers"
  62 |         case intermediateSize = "intermediate_size"
  63 |         case slidingWindow = "sliding_window"
  64 |         case ropeScaling = "rope_scaling"
  65 |         case finalLogitSoftcapping = "final_logit_softcapping"
  66 |         case _attentionHeads = "num_attention_heads"
  67 |         case _kvHeads = "num_key_value_heads"
  68 |         case _headDim = "head_dim"
  69 |         case _queryPreAttnScalar = "query_pre_attn_scalar"
  70 |     }
  71 | }
  72 | 
  73 | // MARK: - Vision Configuration
  74 | 
  75 | public struct Gemma3VisionConfiguration: Codable, Sendable {
  76 |     public let modelType: String
  77 |     public let hiddenLayers: Int
  78 |     public let hiddenSize: Int
  79 |     public let intermediateSize: Int
  80 |     public let attentionHeads: Int
  81 |     public let patchSize: Int
  82 |     public let imageSize: Int
  83 | 
  84 |     public let numChannels: Int = 3
  85 |     public let layerNormEps: Float = 1e-6
  86 | 
  87 |     enum CodingKeys: String, CodingKey {
  88 |         case modelType = "model_type"
  89 |         case hiddenLayers = "num_hidden_layers"
  90 |         case hiddenSize = "hidden_size"
  91 |         case intermediateSize = "intermediate_size"
  92 |         case attentionHeads = "num_attention_heads"
  93 |         case patchSize = "patch_size"
  94 |         case imageSize = "image_size"
  95 |     }
  96 | }
  97 | 
  98 | // MARK: - Quantization Configuration
  99 | 
 100 | public struct QuantizationConfig: Codable, Sendable {
 101 |     public let groupSize: Int
 102 |     public let bits: Int
 103 | 
 104 |     enum CodingKeys: String, CodingKey {
 105 |         case groupSize = "group_size"
 106 |         case bits
 107 |     }
 108 | }
 109 | 
 110 | // MARK: - Model Configuration
 111 | 
 112 | public struct Gemma3Configuration: Codable, Sendable {
 113 |     public let textConfiguration: Gemma3TextConfiguration
 114 |     public let visionConfiguration: Gemma3VisionConfiguration
 115 |     public let modelType: String
 116 |     public let mmTokensPerImage: Int
 117 |     public let quantization: QuantizationConfig?
 118 | 
 119 |     private let _vocabularySize: Int?
 120 |     private let _padTokenId: Int?
 121 | 
 122 |     // Computed properties that use the text configuration or provide defaults
 123 | 
 124 |     public var vocabularySize: Int {
 125 |         _vocabularySize ?? textConfiguration.vocabularySize
 126 |     }
 127 | 
 128 |     public var hiddenSize: Int {
 129 |         textConfiguration.hiddenSize
 130 |     }
 131 | 
 132 |     public var padTokenId: Int {
 133 |         _padTokenId ?? 0
 134 |     }
 135 | 
 136 |     enum CodingKeys: String, CodingKey {
 137 |         case textConfiguration = "text_config"
 138 |         case visionConfiguration = "vision_config"
 139 |         case modelType = "model_type"
 140 |         case mmTokensPerImage = "mm_tokens_per_image"
 141 |         case quantization
 142 | 
 143 |         case _vocabularySize = "vocab_size"
 144 |         case _padTokenId = "pad_token_id"
 145 |     }
 146 | }
 147 | 
 148 | // MARK: - Attention
 149 | 
 150 | private class Attention: Module {
 151 |     let numHeads: Int
 152 |     let numKVHeads: Int
 153 |     let repeats: Int
 154 |     let headDim: Int
 155 |     let layerIdx: Int
 156 |     let scale: Float
 157 |     let isSliding: Bool
 158 | 
 159 |     @ModuleInfo(key: "q_proj") var queryProj: Linear
 160 |     @ModuleInfo(key: "k_proj") var keyProj: Linear
 161 |     @ModuleInfo(key: "v_proj") var valueProj: Linear
 162 |     @ModuleInfo(key: "o_proj") var outputProj: Linear
 163 | 
 164 |     @ModuleInfo(key: "q_norm") var queryNorm: Gemma.RMSNorm
 165 |     @ModuleInfo(key: "k_norm") var keyNorm: Gemma.RMSNorm
 166 | 
 167 |     @ModuleInfo var rope: RoPE
 168 | 
 169 |     init(config: Gemma3TextConfiguration, layerIdx: Int) {
 170 |         let dim = config.hiddenSize
 171 |         self.numHeads = config.attentionHeads
 172 |         self.numKVHeads = config.kvHeads
 173 |         self.repeats = numHeads / numKVHeads
 174 |         self.headDim = config.headDim
 175 |         self.layerIdx = layerIdx
 176 | 
 177 |         self.scale = pow(config.queryPreAttnScalar, -0.5)
 178 | 
 179 |         self._queryProj.wrappedValue = Linear(dim, numHeads * headDim, bias: false)
 180 |         self._keyProj.wrappedValue = Linear(dim, numKVHeads * headDim, bias: false)
 181 |         self._valueProj.wrappedValue = Linear(dim, numKVHeads * headDim, bias: false)
 182 |         self._outputProj.wrappedValue = Linear(numHeads * headDim, dim, bias: false)
 183 | 
 184 |         self._queryNorm.wrappedValue = Gemma.RMSNorm(
 185 |             dimensions: headDim, eps: config.rmsNormEps)
 186 |         self._keyNorm.wrappedValue = Gemma.RMSNorm(dimensions: headDim, eps: config.rmsNormEps)
 187 | 
 188 |         // Gemma3 uses sliding window attention pattern
 189 |         self.isSliding = (layerIdx + 1) % config.slidingWindowPattern != 0
 190 | 
 191 |         let baseFreq = isSliding ? config.ropeLocalBaseFreq : config.ropeGlobalBaseFreq
 192 |         self._rope.wrappedValue = RoPE(
 193 |             dimensions: headDim,
 194 |             traditional: config.ropeTraditional,
 195 |             base: baseFreq
 196 |         )
 197 |     }
 198 | 
 199 |     func callAsFunction(
 200 |         _ x: MLXArray,
 201 |         mask: MLXFast.ScaledDotProductAttentionMaskMode,
 202 |         cache: KVCache? = nil
 203 |     ) -> MLXArray {
 204 |         let (B, L, _) = (x.dim(0), x.dim(1), x.dim(2))
 205 | 
 206 |         var queries = queryProj(x)
 207 |         var keys = keyProj(x)
 208 |         var values = valueProj(x)
 209 | 
 210 |         // Reshape for multi-head attention
 211 |         queries = queries.reshaped(B, L, numHeads, -1).transposed(0, 2, 1, 3)
 212 |         keys = keys.reshaped(B, L, numKVHeads, -1).transposed(0, 2, 1, 3)
 213 |         values = values.reshaped(B, L, numKVHeads, -1).transposed(0, 2, 1, 3)
 214 | 
 215 |         // Apply normalization
 216 |         queries = queryNorm(queries)
 217 |         keys = keyNorm(keys)
 218 | 
 219 |         // Apply rotary position embedding
 220 |         if let cache {
 221 |             queries = rope(queries, offset: cache.offset)
 222 |             keys = rope(keys, offset: cache.offset)
 223 |             (keys, values) = cache.update(keys: keys, values: values)
 224 |         } else {
 225 |             queries = rope(queries)
 226 |             keys = rope(keys)
 227 |         }
 228 | 
 229 |         // Handle sliding window masking
 230 |         var finalMask = mask
 231 |         if case .array(let maskArray) = mask, maskArray.shape.last! != keys.shape[2] {
 232 |             let keyLen = keys.shape[2]
 233 |             let slicedMask = maskArray[.ellipsis, (-keyLen)...]
 234 |             finalMask = .array(slicedMask)
 235 |         }
 236 | 
 237 |         // Scaled dot-product attention with native GQA support
 238 |         let output = MLXFast.scaledDotProductAttention(
 239 |             queries: queries,
 240 |             keys: keys,
 241 |             values: values,
 242 |             scale: scale,
 243 |             mask: finalMask
 244 |         )
 245 |         .transposed(0, 2, 1, 3)
 246 |         .reshaped(B, L, -1)
 247 | 
 248 |         return outputProj(output)
 249 |     }
 250 | }
 251 | 
 252 | // MARK: - MLP
 253 | 
 254 | private class MLP: Module, UnaryLayer {
 255 |     @ModuleInfo(key: "gate_proj") var gateProj: Linear
 256 |     @ModuleInfo(key: "down_proj") var downProj: Linear
 257 |     @ModuleInfo(key: "up_proj") var upProj: Linear
 258 | 
 259 |     init(dimensions: Int, hiddenDimensions: Int) {
 260 |         self._gateProj.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)
 261 |         self._downProj.wrappedValue = Linear(hiddenDimensions, dimensions, bias: false)
 262 |         self._upProj.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)
 263 |     }
 264 | 
 265 |     func callAsFunction(_ x: MLXArray) -> MLXArray {
 266 |         downProj(geluApproximate(gateProj(x)) * upProj(x))
 267 |     }
 268 | }
 269 | 
 270 | // MARK: - TransformerBlock
 271 | 
 272 | private class TransformerBlock: Module {
 273 |     @ModuleInfo(key: "self_attn") var selfAttention: Attention
 274 |     @ModuleInfo var mlp: MLP
 275 |     @ModuleInfo(key: "input_layernorm") var inputLayerNorm: Gemma.RMSNorm
 276 |     @ModuleInfo(key: "post_attention_layernorm") var postAttentionLayerNorm: Gemma.RMSNorm
 277 |     @ModuleInfo(key: "pre_feedforward_layernorm") var preFeedforwardLayerNorm: Gemma.RMSNorm
 278 |     @ModuleInfo(key: "post_feedforward_layernorm") var postFeedforwardLayerNorm: Gemma.RMSNorm
 279 | 
 280 |     let numAttentionHeads: Int
 281 |     let hiddenSize: Int
 282 | 
 283 |     init(config: Gemma3TextConfiguration, layerIdx: Int) {
 284 |         self.numAttentionHeads = config.attentionHeads
 285 |         self.hiddenSize = config.hiddenSize
 286 | 
 287 |         self._selfAttention.wrappedValue = Attention(config: config, layerIdx: layerIdx)
 288 |         self.mlp = MLP(dimensions: config.hiddenSize, hiddenDimensions: config.intermediateSize)
 289 | 
 290 |         self._inputLayerNorm.wrappedValue = Gemma.RMSNorm(
 291 |             dimensions: config.hiddenSize, eps: config.rmsNormEps)
 292 |         self._postAttentionLayerNorm.wrappedValue = Gemma.RMSNorm(
 293 |             dimensions: config.hiddenSize, eps: config.rmsNormEps)
 294 |         self._preFeedforwardLayerNorm.wrappedValue = Gemma.RMSNorm(
 295 |             dimensions: config.hiddenSize, eps: config.rmsNormEps)
 296 |         self._postFeedforwardLayerNorm.wrappedValue = Gemma.RMSNorm(
 297 |             dimensions: config.hiddenSize, eps: config.rmsNormEps)
 298 |     }
 299 | 
 300 |     func callAsFunction(
 301 |         _ x: MLXArray,
 302 |         mask: MLXFast.ScaledDotProductAttentionMaskMode,
 303 |         cache: KVCache? = nil
 304 |     ) -> MLXArray {
 305 |         let r = selfAttention(inputLayerNorm(x), mask: mask, cache: cache)
 306 |         let h = Gemma.clipResidual(x, postAttentionLayerNorm(r))
 307 |         let r2 = mlp(preFeedforwardLayerNorm(h))
 308 |         let out = Gemma.clipResidual(h, postFeedforwardLayerNorm(r2))
 309 |         return out
 310 |     }
 311 | }
 312 | 
 313 | // MARK: - GemmaModel
 314 | 
 315 | private class GemmaModel: Module {
 316 |     @ModuleInfo(key: "embed_tokens") var embedTokens: Embedding
 317 |     @ModuleInfo var layers: [TransformerBlock]
 318 |     @ModuleInfo var norm: Gemma.RMSNorm
 319 | 
 320 |     let config: Gemma3TextConfiguration
 321 | 
 322 |     init(_ config: Gemma3TextConfiguration) {
 323 |         self.config = config
 324 | 
 325 |         self._embedTokens.wrappedValue = Embedding(
 326 |             embeddingCount: config.vocabularySize,
 327 |             dimensions: config.hiddenSize
 328 |         )
 329 | 
 330 |         self._layers.wrappedValue = (0 ..< config.hiddenLayers).map { layerIdx in
 331 |             TransformerBlock(config: config, layerIdx: layerIdx)
 332 |         }
 333 | 
 334 |         self.norm = Gemma.RMSNorm(dimensions: config.hiddenSize, eps: config.rmsNormEps)
 335 |     }
 336 | 
 337 |     func callAsFunction(
 338 |         _ inputs: MLXArray? = nil,
 339 |         inputEmbedding: MLXArray? = nil,
 340 |         mask: MLXFast.ScaledDotProductAttentionMaskMode? = nil,
 341 |         cache: [KVCache?]? = nil
 342 |     ) -> MLXArray {
 343 |         var h: MLXArray
 344 |         if let inputEmbedding = inputEmbedding {
 345 |             h = inputEmbedding
 346 |         } else if let inputs = inputs {
 347 |             h = embedTokens(inputs)
 348 |         } else {
 349 |             fatalError("Either inputs or inputEmbedding must be provided")
 350 |         }
 351 | 
 352 |         // Apply embedding scaling
 353 |         let scale = MLXArray(sqrtf(Float(config.hiddenSize)), dtype: .bfloat16).asType(
 354 |             inputs?.dtype ?? h.dtype)
 355 |         h = h * scale
 356 | 
 357 |         var layerCache = cache
 358 |         if layerCache == nil {
 359 |             layerCache = Array(repeating: nil as KVCache?, count: layers.count)
 360 |         }
 361 | 
 362 |         // Create attention masks for global and sliding window layers
 363 |         var fullMask: MLXFast.ScaledDotProductAttentionMaskMode = .none
 364 |         var slidingWindowMask: MLXFast.ScaledDotProductAttentionMaskMode = .none
 365 | 
 366 |         if mask == nil {
 367 |             let j = config.slidingWindowPattern
 368 |             if j > 0 && j <= layerCache!.count {
 369 |                 let globalCacheSlice = layerCache![(j - 1) ..< j].compactMap { $0 }
 370 |                 fullMask = createAttentionMask(h: h, cache: globalCacheSlice)
 371 |             }
 372 |             slidingWindowMask = createAttentionMask(h: h, cache: layerCache?.compactMap { $0 })
 373 |         }
 374 | 
 375 |         for (i, layer) in layers.enumerated() {
 376 |             let isGlobal = (i % config.slidingWindowPattern == config.slidingWindowPattern - 1)
 377 | 
 378 |             let localMask: MLXFast.ScaledDotProductAttentionMaskMode
 379 |             if let mask = mask {
 380 |                 localMask = mask
 381 |             } else if isGlobal {
 382 |                 localMask = fullMask
 383 |             } else {
 384 |                 localMask = slidingWindowMask
 385 |             }
 386 | 
 387 |             h = layer(h, mask: localMask, cache: layerCache?[i])
 388 |         }
 389 | 
 390 |         return norm(h)
 391 |     }
 392 | }
 393 | 
 394 | // MARK: - LanguageModel
 395 | 
 396 | private class LanguageModel: Module, KVCacheDimensionProvider {
 397 |     @ModuleInfo var model: GemmaModel
 398 |     @ModuleInfo(key: "lm_head") var lmHead: Module  // Can be Linear or QuantizedLinear
 399 | 
 400 |     let config: Gemma3TextConfiguration
 401 |     var kvHeads: [Int]
 402 | 
 403 |     init(_ config: Gemma3TextConfiguration) {
 404 |         self.config = config
 405 |         self.model = GemmaModel(config)
 406 |         self._lmHead.wrappedValue = Linear(config.hiddenSize, config.vocabularySize, bias: false)
 407 |         self.kvHeads = Array(repeating: config.kvHeads, count: config.hiddenLayers)
 408 |     }
 409 | 
 410 |     /// Creates appropriate cache types for each layer
 411 |     public func newCache(parameters: GenerateParameters?) -> [any KVCache] {
 412 |         var caches: [any KVCache] = []
 413 |         let slidingWindow = config.slidingWindow > 0 ? config.slidingWindow : 4096
 414 |         let slidingWindowPattern = config.slidingWindowPattern
 415 |         for i in 0 ..< config.hiddenLayers {
 416 |             let isGlobalLayer = (i % slidingWindowPattern == slidingWindowPattern - 1)
 417 |             if isGlobalLayer {
 418 |                 caches.append(StandardKVCache())
 419 |             } else {
 420 |                 caches.append(RotatingKVCache(maxSize: slidingWindow, keep: 0))
 421 |             }
 422 |         }
 423 |         return caches
 424 |     }
 425 | 
 426 |     func callAsFunction(
 427 |         _ inputs: MLXArray? = nil,
 428 |         cache: [KVCache]? = nil,
 429 |         inputEmbedding: MLXArray? = nil,
 430 |         mask: MLXFast.ScaledDotProductAttentionMaskMode? = nil
 431 |     ) -> LMOutput {
 432 |         let optionalCache = cache?.map { $0 as KVCache? }
 433 |         let out = model(inputs, inputEmbedding: inputEmbedding, mask: mask, cache: optionalCache)
 434 | 
 435 |         // Call the lmHead (works whether it's Linear or QuantizedLinear)
 436 |         var finalLogits: MLXArray
 437 |         if let linear = lmHead as? Linear {
 438 |             finalLogits = linear(out)
 439 |         } else if let quantized = lmHead as? QuantizedLinear {
 440 |             finalLogits = quantized(out)
 441 |         } else {
 442 |             fatalError("lmHead must be Linear or QuantizedLinear")
 443 |         }
 444 | 
 445 |         // Apply final logit softcapping if configured
 446 |         if let softcap = config.finalLogitSoftcapping, softcap > 0 {
 447 |             let scale = MLXArray(softcap)
 448 |             finalLogits = tanh(finalLogits / scale) * scale
 449 |         }
 450 | 
 451 |         return LMOutput(logits: finalLogits)
 452 |     }
 453 | 
 454 |     func sanitize(weights: [String: MLXArray], quantizationConfig: QuantizationConfig? = nil)
 455 |         -> [String: MLXArray]
 456 |     {
 457 |         var processedWeights = weights
 458 | 
 459 |         // Check if we have quantized weights
 460 |         let hasQuantizedLmHead = hasQuantizedWeights(
 461 |             layerPath: "language_model.lm_head", in: weights)
 462 | 
 463 |         if hasQuantizedLmHead {
 464 |             // Use quantization config from model configuration if available
 465 |             let groupSize = quantizationConfig?.groupSize ?? 64
 466 |             let bits = quantizationConfig?.bits ?? 4
 467 | 
 468 |             // Only quantize layers that actually have quantized weights
 469 |             quantize(model: self) { path, module in
 470 |                 // Check each specific layer path for quantized weights
 471 |                 let fullPath = "language_model.\(path)"
 472 |                 if weights["\(fullPath).scales"] != nil && weights["\(fullPath).biases"] != nil
 473 |                     && weights["\(fullPath).weight"]?.dtype == .uint32
 474 |                 {
 475 |                     return (groupSize, bits)
 476 |                 }
 477 |                 return nil
 478 |             }
 479 |         } else {
 480 |             // Handle weight tying for regular (non-quantized) lm_head
 481 |             if processedWeights["language_model.lm_head.weight"] == nil {
 482 |                 if let embedWeight = processedWeights["language_model.model.embed_tokens.weight"] {
 483 |                     processedWeights["language_model.lm_head.weight"] = embedWeight
 484 |                 }
 485 |             }
 486 |         }
 487 | 
 488 |         // Remove unused precomputed rotary freqs
 489 |         return processedWeights.filter { key, _ in
 490 |             !key.contains("self_attn.rotary_emb.inv_freq")
 491 |         }
 492 |     }
 493 | 
 494 |     /// Check if a layer has quantized weights
 495 |     private func hasQuantizedWeights(layerPath: String, in weights: [String: MLXArray]) -> Bool {
 496 |         let scalesKey = "\(layerPath).scales"
 497 |         let biasesKey = "\(layerPath).biases"
 498 |         let weightKey = "\(layerPath).weight"
 499 | 
 500 |         let hasScales = weights[scalesKey] != nil
 501 |         let hasBiases = weights[biasesKey] != nil
 502 |         let hasWeight = weights[weightKey]?.dtype == .uint32
 503 | 
 504 |         return hasScales && hasBiases && hasWeight
 505 |     }
 506 | }
 507 | 
 508 | // MARK: - Vision Model Components
 509 | 
 510 | private class VisionAttention: Module {
 511 |     @ModuleInfo(key: "q_proj") var queryProj: Linear
 512 |     @ModuleInfo(key: "k_proj") var keyProj: Linear
 513 |     @ModuleInfo(key: "v_proj") var valueProj: Linear
 514 |     @ModuleInfo(key: "out_proj") var outputProj: Linear
 515 | 
 516 |     let numHeads: Int
 517 |     let scale: Float
 518 | 
 519 |     init(
 520 |         dimensions: Int,
 521 |         numHeads: Int,
 522 |         queryInputDimensions: Int? = nil,
 523 |         keyInputDimensions: Int? = nil,
 524 |         valueInputDimensions: Int? = nil,
 525 |         valueDimensions: Int? = nil,
 526 |         valueOutputDimensions: Int? = nil,
 527 |         bias: Bool = true
 528 |     ) {
 529 |         if dimensions % numHeads != 0 {
 530 |             fatalError("The input feature dimensions should be divisible by the number of heads")
 531 |         }
 532 | 
 533 |         self.numHeads = numHeads
 534 |         let headDim = dimensions / numHeads
 535 |         self.scale = pow(Float(headDim), -0.5)
 536 | 
 537 |         let queryInputDims = queryInputDimensions ?? dimensions
 538 |         let keyInputDims = keyInputDimensions ?? dimensions
 539 |         let valueInputDims = valueInputDimensions ?? keyInputDims
 540 |         let valueDims = valueDimensions ?? dimensions
 541 |         let valueOutputDims = valueOutputDimensions ?? dimensions
 542 | 
 543 |         self._queryProj.wrappedValue = Linear(queryInputDims, dimensions, bias: bias)
 544 |         self._keyProj.wrappedValue = Linear(keyInputDims, dimensions, bias: bias)
 545 |         self._valueProj.wrappedValue = Linear(valueInputDims, valueDims, bias: bias)
 546 |         self._outputProj.wrappedValue = Linear(valueDims, valueOutputDims, bias: bias)
 547 |     }
 548 | 
 549 |     func callAsFunction(_ x: MLXArray, mask: MLXArray? = nil) -> MLXArray {
 550 |         var queries = queryProj(x)
 551 |         var keys = keyProj(x)
 552 |         var values = valueProj(x)
 553 | 
 554 |         let (B, L, _) = (queries.dim(0), queries.dim(1), queries.dim(2))
 555 |         let S = keys.dim(1)
 556 | 
 557 |         queries = queries.reshaped(B, L, numHeads, -1).transposed(0, 2, 1, 3)
 558 |         keys = keys.reshaped(B, S, numHeads, -1).transposed(0, 2, 1, 3)
 559 |         values = values.reshaped(B, S, numHeads, -1).transposed(0, 2, 1, 3)
 560 | 
 561 |         let output = MLXFast.scaledDotProductAttention(
 562 |             queries: queries,
 563 |             keys: keys,
 564 |             values: values,
 565 |             scale: scale,
 566 |             mask: mask
 567 |         )
 568 |         .transposed(0, 2, 1, 3)
 569 |         .reshaped(B, L, -1)
 570 | 
 571 |         return outputProj(output)
 572 |     }
 573 | }
 574 | 
 575 | private class VisionMLP: Module, UnaryLayer {
 576 |     @ModuleInfo(key: "fc1") var fc1: Linear
 577 |     @ModuleInfo(key: "fc2") var fc2: Linear
 578 |     @ModuleInfo var activationFn: GELU
 579 | 
 580 |     init(config: Gemma3VisionConfiguration) {
 581 |         self.activationFn = GELU(approximation: .precise)
 582 |         self._fc1.wrappedValue = Linear(config.hiddenSize, config.intermediateSize, bias: true)
 583 |         self._fc2.wrappedValue = Linear(config.intermediateSize, config.hiddenSize, bias: true)
 584 |     }
 585 | 
 586 |     func callAsFunction(_ x: MLXArray) -> MLXArray {
 587 |         var x = fc1(x)
 588 |         x = activationFn(x)
 589 |         return fc2(x)
 590 |     }
 591 | }
 592 | 
 593 | private class EncoderLayer: Module {
 594 |     @ModuleInfo(key: "self_attn") var selfAttention: VisionAttention
 595 |     @ModuleInfo(key: "layer_norm1") var layerNorm1: LayerNorm
 596 |     @ModuleInfo var mlp: VisionMLP
 597 |     @ModuleInfo(key: "layer_norm2") var layerNorm2: LayerNorm
 598 | 
 599 |     let embedDim: Int
 600 | 
 601 |     init(config: Gemma3VisionConfiguration) {
 602 |         self.embedDim = config.hiddenSize
 603 | 
 604 |         self._selfAttention.wrappedValue = VisionAttention(
 605 |             dimensions: config.hiddenSize,
 606 |             numHeads: config.attentionHeads,
 607 |             bias: true
 608 |         )
 609 | 
 610 |         self._layerNorm1.wrappedValue = LayerNorm(dimensions: embedDim, eps: config.layerNormEps)
 611 |         self.mlp = VisionMLP(config: config)
 612 |         self._layerNorm2.wrappedValue = LayerNorm(dimensions: embedDim, eps: config.layerNormEps)
 613 |     }
 614 | 
 615 |     func callAsFunction(_ x: MLXArray, mask: MLXArray? = nil) -> MLXArray {
 616 |         let r = selfAttention(layerNorm1(x), mask: mask)
 617 |         let h = x + r
 618 |         let r2 = mlp(layerNorm2(h))
 619 |         return h + r2
 620 |     }
 621 | }
 622 | 
 623 | private class Encoder: Module {
 624 |     @ModuleInfo var layers: [EncoderLayer]
 625 | 
 626 |     init(config: Gemma3VisionConfiguration) {
 627 |         self._layers.wrappedValue = (0 ..< config.hiddenLayers).map { _ in
 628 |             EncoderLayer(config: config)
 629 |         }
 630 |     }
 631 | 
 632 |     func callAsFunction(
 633 |         _ x: MLXArray,
 634 |         outputHiddenStates: Bool = false,
 635 |         mask: MLXArray? = nil
 636 |     ) -> (MLXArray, [MLXArray]?) {
 637 |         var encoderStates: [MLXArray]? = outputHiddenStates ? [x] : nil
 638 |         var h = x
 639 | 
 640 |         for layer in layers {
 641 |             h = layer(h)
 642 |             if outputHiddenStates {
 643 |                 encoderStates?.append(h)
 644 |             }
 645 |         }
 646 | 
 647 |         return (h, encoderStates)
 648 |     }
 649 | }
 650 | 
 651 | private class VisionEmbeddings: Module, UnaryLayer {
 652 |     @ModuleInfo(key: "patch_embedding") var patchEmbedding: Conv2d
 653 |     @ModuleInfo(key: "position_embedding") var positionEmbedding: Embedding
 654 | 
 655 |     let config: Gemma3VisionConfiguration
 656 |     let embedDim: Int
 657 |     let imageSize: Int
 658 |     let patchSize: Int
 659 |     let numPatches: Int
 660 |     let numPositions: Int
 661 | 
 662 |     init(config: Gemma3VisionConfiguration) {
 663 |         self.config = config
 664 |         self.embedDim = config.hiddenSize
 665 |         self.imageSize = config.imageSize
 666 |         self.patchSize = config.patchSize
 667 | 
 668 |         self._patchEmbedding.wrappedValue = Conv2d(
 669 |             inputChannels: config.numChannels,
 670 |             outputChannels: embedDim,
 671 |             kernelSize: IntOrPair(patchSize),
 672 |             stride: IntOrPair(patchSize)
 673 |         )
 674 | 
 675 |         self.numPatches = (imageSize / patchSize) * (imageSize / patchSize)
 676 |         self.numPositions = numPatches
 677 | 
 678 |         self._positionEmbedding.wrappedValue = Embedding(
 679 |             embeddingCount: numPositions,
 680 |             dimensions: embedDim
 681 |         )
 682 |     }
 683 | 
 684 |     func callAsFunction(_ x: MLXArray) -> MLXArray {
 685 |         var patchEmbeddings = patchEmbedding(x)
 686 |         patchEmbeddings = flattened(patchEmbeddings, start: 1, end: 2)
 687 | 
 688 |         // Check if we have the expected number of patches (safety net for config mismatches)
 689 |         let actualNumPatches = patchEmbeddings.dim(1)
 690 |         let useNumPositions = min(actualNumPatches, numPositions)
 691 | 
 692 |         // Use position IDs from 0 to numPositions
 693 |         let positionIds = MLXArray(Array(0 ..< useNumPositions))[.newAxis, 0...]
 694 |         var embeddings = patchEmbeddings
 695 | 
 696 |         // Add position embeddings only to the patches we have positions for
 697 |         if useNumPositions == actualNumPatches {
 698 |             // Normal case: add position embeddings to all patches
 699 |             embeddings = embeddings + positionEmbedding(positionIds)
 700 |         } else {
 701 |             // Safety case: only add to first N patches to avoid broadcast error
 702 |             let positionedPatches =
 703 |                 embeddings[0..., ..<useNumPositions, 0...] + positionEmbedding(positionIds)
 704 |             let remainingPatches = embeddings[0..., useNumPositions..., 0...]
 705 |             embeddings = concatenated([positionedPatches, remainingPatches], axis: 1)
 706 |         }
 707 | 
 708 |         return embeddings
 709 |     }
 710 | }
 711 | 
 712 | private class SigLipVisionModel: Module {
 713 |     @ModuleInfo var embeddings: VisionEmbeddings
 714 |     @ModuleInfo var encoder: Encoder
 715 |     @ModuleInfo(key: "post_layernorm") var postLayerNorm: LayerNorm
 716 | 
 717 |     init(config: Gemma3VisionConfiguration) {
 718 |         self.embeddings = VisionEmbeddings(config: config)
 719 |         self.encoder = Encoder(config: config)
 720 |         self._postLayerNorm.wrappedValue = LayerNorm(dimensions: config.hiddenSize)
 721 |         super.init()
 722 |     }
 723 | 
 724 |     func callAsFunction(
 725 |         _ x: MLXArray,
 726 |         outputHiddenStates: Bool = false
 727 |     ) -> (MLXArray, MLXArray, [MLXArray]?) {
 728 |         let x = embeddings(x)
 729 | 
 730 |         let (encoderOutput, encoderStates) = encoder(
 731 |             x,
 732 |             outputHiddenStates: outputHiddenStates,
 733 |             mask: nil
 734 |         )
 735 | 
 736 |         let poolerOutput = postLayerNorm(encoderOutput)
 737 | 
 738 |         return (poolerOutput, x, encoderStates)
 739 |     }
 740 | }
 741 | 
 742 | private class VisionModel: Module {
 743 |     @ModuleInfo(key: "vision_model") var visionModel: SigLipVisionModel
 744 | 
 745 |     let modelType: String
 746 | 
 747 |     init(config: Gemma3VisionConfiguration) {
 748 |         self.modelType = config.modelType
 749 |         self._visionModel.wrappedValue = SigLipVisionModel(config: config)
 750 |     }
 751 | 
 752 |     func callAsFunction(
 753 |         _ x: MLXArray,
 754 |         outputHiddenStates: Bool = false
 755 |     ) -> (MLXArray, MLXArray, [MLXArray]?) {
 756 |         visionModel(x, outputHiddenStates: outputHiddenStates)
 757 |     }
 758 | 
 759 |     /// Check if array is already in MLX format for conv2d weights
 760 |     private func checkArrayShape(_ arr: MLXArray) -> Bool {
 761 |         let shape = arr.shape
 762 | 
 763 |         // Check if the shape has 4 dimensions
 764 |         guard shape.count == 4 else { return false }
 765 | 
 766 |         let (outChannels, kH, kW, _) = (shape[0], shape[1], shape[2], shape[3])
 767 | 
 768 |         // Check if out_channels is the largest, and kH and kW are the same
 769 |         return (outChannels >= kH) && (outChannels >= kW) && (kH == kW)
 770 |     }
 771 | 
 772 |     func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {
 773 |         var sanitizedWeights = [String: MLXArray]()
 774 | 
 775 |         for (k, v) in weights {
 776 |             // Handle vision model quantized weights if they exist
 777 |             if k.contains("vision_tower") && hasQuantizedWeights(layerPath: k, in: weights) {
 778 |                 // Keep quantized weights as-is - they will be handled by QuantizedLinear at runtime
 779 |                 sanitizedWeights[k] = v
 780 |             } else if k.contains("patch_embedding.weight") {
 781 |                 // PyTorch conv2d weight tensors have shape:
 782 |                 //   [out_channels, in_channels, kH, KW]
 783 |                 // MLX conv2d expects the weight be of shape:
 784 |                 //   [out_channels, kH, KW, in_channels]
 785 |                 if checkArrayShape(v) {
 786 |                     sanitizedWeights[k] = v
 787 |                 } else {
 788 |                     sanitizedWeights[k] = v.transposed(0, 2, 3, 1)
 789 |                 }
 790 |             } else {
 791 |                 sanitizedWeights[k] = v
 792 |             }
 793 |         }
 794 | 
 795 |         return sanitizedWeights
 796 |     }
 797 | 
 798 |     /// Check if a layer has quantized weights (copied from LanguageModel)
 799 |     private func hasQuantizedWeights(layerPath: String, in weights: [String: MLXArray]) -> Bool {
 800 |         let scalesKey = "\(layerPath).scales"
 801 |         let biasesKey = "\(layerPath).biases"
 802 |         let weightKey = "\(layerPath).weight"
 803 | 
 804 |         return weights[scalesKey] != nil && weights[biasesKey] != nil
 805 |             && weights[weightKey]?.dtype == .uint32
 806 |     }
 807 | }
 808 | 
 809 | // MARK: - Multimodal Projector
 810 | 
 811 | class Gemma3MultiModalProjector: Module, UnaryLayer {
 812 |     @ModuleInfo(key: "mm_input_projection_weight") var mmInputProjectionWeight: MLXArray
 813 |     @ModuleInfo(key: "mm_soft_emb_norm") var mmSoftEmbNorm: Gemma.RMSNorm
 814 |     @ModuleInfo var avgPool: AvgPool2d
 815 | 
 816 |     let config: Gemma3Configuration
 817 |     let patchesPerImage: Int
 818 |     let tokensPerSide: Int
 819 |     let kernelSize: Int
 820 | 
 821 |     init(config: Gemma3Configuration) {
 822 |         self.config = config
 823 | 
 824 |         self._mmInputProjectionWeight.wrappedValue = ones([
 825 |             config.visionConfiguration.hiddenSize,
 826 |             config.textConfiguration.hiddenSize,
 827 |         ])
 828 | 
 829 |         self._mmSoftEmbNorm.wrappedValue = Gemma.RMSNorm(
 830 |             dimensions: config.visionConfiguration.hiddenSize,
 831 |             eps: config.visionConfiguration.layerNormEps
 832 |         )
 833 | 
 834 |         self.patchesPerImage =
 835 |             config.visionConfiguration.imageSize / config.visionConfiguration.patchSize
 836 | 
 837 |         self.tokensPerSide = Int(sqrt(Double(config.mmTokensPerImage)))
 838 |         self.kernelSize = patchesPerImage / tokensPerSide
 839 | 
 840 |         self.avgPool = AvgPool2d(
 841 |             kernelSize: IntOrPair(kernelSize),
 842 |             stride: IntOrPair(kernelSize)
 843 |         )
 844 |     }
 845 | 
 846 |     func callAsFunction(_ x: MLXArray) -> MLXArray {
 847 |         let (b, _, l) = (x.dim(0), x.dim(1), x.dim(2))
 848 | 
 849 |         // Use fixed config values
 850 |         var reshapedVisionOutputs = x.transposed(0, 2, 1)
 851 |         reshapedVisionOutputs = reshapedVisionOutputs.reshaped(
 852 |             b, l, patchesPerImage, patchesPerImage
 853 |         )
 854 | 
 855 |         // Transpose to place spatial dimensions in indices 1, 2
 856 |         reshapedVisionOutputs = reshapedVisionOutputs.transposed(0, 2, 3, 1)
 857 |         // Use fixed average pooling
 858 |         var pooledVisionOutputs = avgPool(reshapedVisionOutputs)
 859 |         pooledVisionOutputs = pooledVisionOutputs.transposed(0, 3, 1, 2).flattened(start: 2)
 860 |         pooledVisionOutputs = pooledVisionOutputs.transposed(0, 2, 1)
 861 | 
 862 |         let normedVisionOutputs = mmSoftEmbNorm(pooledVisionOutputs)
 863 | 
 864 |         let projectedVisionOutputs = einsum(
 865 |             "btm,md->btd",
 866 |             normedVisionOutputs,
 867 |             mmInputProjectionWeight
 868 |         )
 869 | 
 870 |         return projectedVisionOutputs.asType(x.dtype)
 871 |     }
 872 | }
 873 | 
 874 | /// Inserts image features into text embeddings at specified token positions
 875 | /// Implements the multimodal fusion approach used in Gemma3 VLM
 876 | private func maskedScatter(
 877 |     finalEmbedding: MLXArray,
 878 |     imageMaskExpanded: MLXArray,
 879 |     scaledImageFeatures: MLXArray
 880 | ) -> MLXArray {
 881 |     // Reshape the tensors to 1D
 882 |     let finalEmbeddingShape = finalEmbedding.shape
 883 |     let scaledImageFeaturesFlattened = scaledImageFeatures.flattened()
 884 |     let finalEmbeddingFlattened = finalEmbedding.flattened()
 885 |     let imageMaskExpandedFlattened = imageMaskExpanded.flattened()
 886 | 
 887 |     let maskValues = imageMaskExpandedFlattened.asArray(Bool.self)
 888 |     let imagePositionIndices = maskValues.enumerated().compactMap { index, value in
 889 |         value ? UInt32(index) : nil
 890 |     }
 891 | 
 892 |     guard !imagePositionIndices.isEmpty else {
 893 |         return finalEmbedding
 894 |     }
 895 | 
 896 |     // Scatter the scaled image features into the special image token positions
 897 |     let imagePositions = MLXArray(imagePositionIndices)
 898 |     guard scaledImageFeaturesFlattened.shape[0] == imagePositions.shape[0] else {
 899 |         fatalError(
 900 |             """
 901 |             Critical error in maskedScatter: Size mismatch between image features and positions.
 902 |             Image features: \(scaledImageFeaturesFlattened.shape[0])
 903 |             Image positions: \(imagePositions.shape[0])
 904 |             """)
 905 |     }
 906 |     finalEmbeddingFlattened[imagePositions] = scaledImageFeaturesFlattened
 907 |     return finalEmbeddingFlattened.reshaped(finalEmbeddingShape)
 908 | }
 909 | 
 910 | // MARK: - Gemma 3 Model
 911 | 
 912 | public class Gemma3: Module, VLMModel, KVCacheDimensionProvider {
 913 |     @ModuleInfo(key: "vision_tower") private var visionTower: VisionModel
 914 |     @ModuleInfo(key: "language_model") private var languageModel: LanguageModel
 915 |     @ModuleInfo(key: "multi_modal_projector") var multiModalProjector: Gemma3MultiModalProjector
 916 | 
 917 |     public let config: Gemma3Configuration
 918 | 
 919 |     public var vocabularySize: Int { config.vocabularySize }
 920 |     public var kvHeads: [Int] { languageModel.kvHeads }
 921 | 
 922 |     /// Create cache with proper types for each layer
 923 |     public func newCache(parameters: GenerateParameters?) -> [any KVCache] {
 924 |         return languageModel.newCache(parameters: parameters)
 925 |     }
 926 | 
 927 |     public init(_ config: Gemma3Configuration) {
 928 |         self.config = config
 929 | 
 930 |         self._visionTower.wrappedValue = VisionModel(config: config.visionConfiguration)
 931 |         self._languageModel.wrappedValue = LanguageModel(config.textConfiguration)
 932 |         self._multiModalProjector.wrappedValue = Gemma3MultiModalProjector(config: config)
 933 |     }
 934 | 
 935 |     private func getInputEmbeddings(
 936 |         inputIds: MLXArray? = nil,
 937 |         pixelValues: MLXArray? = nil,
 938 |         mask: MLXArray? = nil
 939 |     ) -> (MLXArray, MLXArray?) {
 940 |         guard let pixelValues else {
 941 |             return (languageModel.model.embedTokens(inputIds!), nil)
 942 |         }
 943 | 
 944 |         let inputsEmbeds = languageModel.model.embedTokens(inputIds!)
 945 | 
 946 |         // Process image through vision tower
 947 |         let processedPixels = pixelValues.transposed(0, 2, 3, 1).asType(inputsEmbeds.dtype)
 948 | 
 949 |         let (hiddenState, _, _) = visionTower(
 950 |             processedPixels,
 951 |             outputHiddenStates: true
 952 |         )
 953 | 
 954 |         let imageFeatures = multiModalProjector(hiddenState)
 955 | 
 956 |         let (finalEmbedding, finalAttentionMask4d) = prepareInputsForMultimodal(
 957 |             imageFeatures: imageFeatures,
 958 |             inputsEmbeds: inputsEmbeds,
 959 |             inputIds: inputIds!,
 960 |             attentionMask: mask
 961 |         )
 962 | 
 963 |         return (finalEmbedding, finalAttentionMask4d)
 964 |     }
 965 | 
 966 |     private func prepareInputsForMultimodal(
 967 |         imageFeatures: MLXArray,
 968 |         inputsEmbeds: MLXArray,
 969 |         inputIds: MLXArray,
 970 |         attentionMask: MLXArray?
 971 |     ) -> (MLXArray, MLXArray?) {
 972 |         let embedDim = inputsEmbeds.dim(2)
 973 |         let batchSize = inputIds.dim(0)
 974 |         let sequenceLength = inputIds.dim(1)
 975 | 
 976 |         // Scale image features to match text embedding magnitude
 977 |         let scaledImageFeatures = imageFeatures / sqrt(Float(config.textConfiguration.hiddenSize))
 978 | 
 979 |         // Use input embeddings as starting point
 980 |         var finalEmbedding = inputsEmbeds
 981 | 
 982 |         let padTokenId = config.padTokenId
 983 |         let imageTokenId = 262144  // Image token used after expansion
 984 | 
 985 |         // Create masks for different token types
 986 |         let textMask = MLX.logicalAnd(
 987 |             MLX.notEqual(inputIds, MLXArray(imageTokenId)),
 988 |             MLX.notEqual(inputIds, MLXArray(padTokenId))
 989 |         )
 990 |         let imageMask = MLX.equal(inputIds, MLXArray(imageTokenId))
 991 |         let padMask = MLX.equal(inputIds, MLXArray(padTokenId))
 992 | 
 993 |         // Expand masks to match embedding dimension
 994 |         var imageMaskExpanded = expandedDimensions(imageMask, axis: -1)
 995 |         imageMaskExpanded = repeated(imageMaskExpanded, count: embedDim, axis: -1)
 996 | 
 997 |         // Apply pad mask to final embedding
 998 |         var padMaskExpanded = expandedDimensions(padMask, axis: -1)
 999 |         padMaskExpanded = repeated(padMaskExpanded, count: embedDim, axis: -1)
1000 |         finalEmbedding = MLX.where(
1001 |             padMaskExpanded, MLXArray.zeros(like: finalEmbedding), finalEmbedding)
1002 | 
1003 |         // Insert image token embeddings using masked_scatter
1004 |         finalEmbedding = maskedScatter(
1005 |             finalEmbedding: finalEmbedding,
1006 |             imageMaskExpanded: imageMaskExpanded,
1007 |             scaledImageFeatures: scaledImageFeatures
1008 |         )
1009 | 
1010 |         var finalAttentionMask4d: MLXArray? = nil
1011 |         if let attentionMask = attentionMask {
1012 |             let attentionMaskExpanded1 = expandedDimensions(attentionMask, axis: 1)
1013 |             let attentionMaskExpanded2 = expandedDimensions(attentionMask, axis: 2)
1014 |             finalAttentionMask4d = attentionMaskExpanded1 * attentionMaskExpanded2
1015 |             finalAttentionMask4d = expandedDimensions(finalAttentionMask4d!, axis: 1)
1016 |         }
1017 | 
1018 |         return (finalEmbedding.asType(inputsEmbeds.dtype), finalAttentionMask4d)
1019 |     }
1020 | 
1021 |     public func prepare(_ input: LMInput, cache: [any KVCache], windowSize: Int?) throws
1022 |         -> PrepareResult
1023 |     {
1024 |         guard let imagePixels = input.image?.pixels else {
1025 |             // Text-only input
1026 |             let convertedCache = cache.compactMap { $0 as? KVCache }
1027 |             let result = languageModel(
1028 |                 input.text.tokens, cache: convertedCache, inputEmbedding: nil, mask: nil)
1029 |             return .logits(result)
1030 |         }
1031 | 
1032 |         let (inputEmbeddings, _) = getInputEmbeddings(
1033 |             inputIds: input.text.tokens,
1034 |             pixelValues: imagePixels,
1035 |             mask: input.text.mask
1036 |         )
1037 | 
1038 |         let convertedCache = cache.compactMap { $0 as? KVCache }
1039 |         // Use causal masking for text generation
1040 |         let maskMode: MLXFast.ScaledDotProductAttentionMaskMode = .causal
1041 | 
1042 |         let result = languageModel(
1043 |             nil,  // Pass nil for tokens when using embeddings
1044 |             cache: convertedCache,
1045 |             inputEmbedding: inputEmbeddings,
1046 |             mask: maskMode
1047 |         )
1048 | 
1049 |         return .logits(result)
1050 |     }
1051 | 
1052 |     public func callAsFunction(_ inputs: MLXArray, cache: [any KVCache]?) -> MLXArray {
1053 |         return languageModel(inputs, cache: cache).logits
1054 |     }
1055 | 
1056 |     public func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {
1057 |         let lmHeadKeys = weights.keys.filter { $0.contains("lm_head") }
1058 | 
1059 |         // Also check attention layer structures
1060 |         let attnKeys = weights.keys.filter {
1061 |             $0.contains("self_attn")
1062 |                 && ($0.contains("q_proj") || $0.contains("k_proj") || $0.contains("v_proj")
1063 |                     || $0.contains("o_proj"))
1064 |         }
1065 | 
1066 |         // Handle language model sanitization first (quantization, weight tying, etc.)
1067 |         var processedWeights = languageModel.sanitize(
1068 |             weights: weights, quantizationConfig: config.quantization)
1069 | 
1070 |         // Handle vision model sanitization (conv2d weight reshaping, etc.)
1071 |         processedWeights = visionTower.sanitize(weights: processedWeights)
1072 | 
1073 |         return processedWeights
1074 |     }
1075 | }
1076 | 
1077 | public class Gemma3Processor: UserInputProcessor {
1078 |     private let config: Gemma3ProcessorConfiguration
1079 |     private let tokenizer: any Tokenizer
1080 | 
1081 |     public init(_ config: Gemma3ProcessorConfiguration, tokenizer: any Tokenizer) {
1082 |         self.config = config
1083 |         self.tokenizer = tokenizer
1084 |     }
1085 | 
1086 |     public func preprocess(images: [CIImage], processing: UserInput.Processing?) throws -> (
1087 |         MLXArray, THW
1088 |     ) {
1089 |         var userProcessing = processing ?? UserInput.Processing()
1090 |         // Always use the vision configuration's imageSize. Ignore UserInput resize setting.
1091 |         let targetSize = CGSize(width: config.imageSize, height: config.imageSize)
1092 | 
1093 |         // Force the correct size for vision model alignment
1094 |         userProcessing.resize = targetSize
1095 | 
1096 |         let processedImages = try images.map { image in
1097 |             let processedImage = MediaProcessing.apply(image, processing: userProcessing)
1098 |             let srgbImage = MediaProcessing.inSRGBToneCurveSpace(processedImage)
1099 |             let resizedImage = try MediaProcessing.resampleBicubic(srgbImage, to: targetSize)
1100 |             let normalizedImage = MediaProcessing.normalize(
1101 |                 resizedImage, mean: config.imageMeanTuple, std: config.imageStdTuple)
1102 |             return MediaProcessing.asMLXArray(normalizedImage)
1103 |         }
1104 | 
1105 |         let pixelValues = concatenated(processedImages)
1106 | 
1107 |         return (pixelValues, THW(images.count, config.imageSize, config.imageSize))
1108 |     }
1109 | 
1110 |     public func prepare(input: UserInput) async throws -> LMInput {
1111 |         // Use structured content message generator for Gemma3's chat template
1112 |         let messages = Qwen2VLMessageGenerator().generate(from: input)
1113 | 
1114 |         var promptTokens = try tokenizer.applyChatTemplate(messages: messages)
1115 | 
1116 |         // Process images if any
1117 |         var processedImage: LMInput.ProcessedImage?
1118 | 
1119 |         if !input.images.isEmpty {
1120 |             let imagePixelsAndFrames = try input.images.map {
1121 |                 try preprocess(images: [$0.asCIImage()], processing: input.processing)
1122 |             }
1123 |             let imagePixelsConcatenated = concatenated(imagePixelsAndFrames.map { $0.0 })
1124 |             processedImage = LMInput.ProcessedImage(
1125 |                 pixels: imagePixelsConcatenated,
1126 |                 frames: imagePixelsAndFrames.map { $0.1 }
1127 |             )
1128 | 
1129 |             // Expand single <start_of_image> token to multiple image tokens
1130 |             let startOfImageTokenId = 255999
1131 |             let imageTokenId = 262144
1132 |             let numImageTokens = config.imageSeqLength  // 256
1133 | 
1134 |             var expandedTokens: [Int] = []
1135 | 
1136 |             for token in promptTokens {
1137 |                 if token == startOfImageTokenId {
1138 |                     // Replace with 256 image tokens
1139 |                     expandedTokens.append(
1140 |                         contentsOf: Array(repeating: imageTokenId, count: numImageTokens))
1141 |                 } else {
1142 |                     expandedTokens.append(token)
1143 |                 }
1144 |             }
1145 | 
1146 |             promptTokens = expandedTokens
1147 |         }
1148 | 
1149 |         let promptArray = MLXArray(promptTokens).expandedDimensions(axis: 0)
1150 |         let mask = ones(like: promptArray).asType(.int8)
1151 |         return LMInput(
1152 |             text: .init(tokens: promptArray, mask: mask),
1153 |             image: processedImage
1154 |         )
1155 |     }
1156 | }
1157 | 
1158 | public struct Gemma3ProcessorConfiguration: Codable, Sendable {
1159 |     // Fields from the preprocessor_config.json
1160 |     public let processorClass: String
1161 |     public let imageProcessorType: String
1162 |     public let doNormalize: Bool
1163 |     public let doRescale: Bool
1164 |     public let doResize: Bool
1165 |     public let imageMean: [CGFloat]
1166 |     public let imageStd: [CGFloat]
1167 |     public let imageSeqLength: Int
1168 |     public let resample: Int
1169 |     public let rescaleFactor: Float
1170 |     public let size: ImageSize
1171 | 
1172 |     // Optional fields
1173 |     public let doConvertRgb: Bool?
1174 |     public let doPanAndScan: Bool?
1175 |     public let panAndScanMaxNumCrops: Int?
1176 |     public let panAndScanMinCropSize: Int?
1177 |     public let panAndScanMinRatioToActivate: Float?
1178 | 
1179 |     // Image token identifier from model configuration
1180 |     public let imageTokenId: Int = 262144
1181 | 
1182 |     public struct ImageSize: Codable, Sendable {
1183 |         public let height: Int
1184 |         public let width: Int
1185 |     }
1186 | 
1187 |     // Computed properties for convenience
1188 |     public var imageSize: Int { size.height }
1189 | 
1190 |     public var imageMeanTuple: (CGFloat, CGFloat, CGFloat) {
1191 |         (imageMean[0], imageMean[1], imageMean[2])
1192 |     }
1193 | 
1194 |     public var imageStdTuple: (CGFloat, CGFloat, CGFloat) {
1195 |         (imageStd[0], imageStd[1], imageStd[2])
1196 |     }
1197 | 
1198 |     enum CodingKeys: String, CodingKey {
1199 |         case processorClass = "processor_class"
1200 |         case imageProcessorType = "image_processor_type"
1201 |         case doNormalize = "do_normalize"
1202 |         case doRescale = "do_rescale"
1203 |         case doResize = "do_resize"
1204 |         case doConvertRgb = "do_convert_rgb"
1205 |         case doPanAndScan = "do_pan_and_scan"
1206 |         case imageMean = "image_mean"
1207 |         case imageStd = "image_std"
1208 |         case imageSeqLength = "image_seq_length"
1209 |         case resample
1210 |         case rescaleFactor = "rescale_factor"
1211 |         case size
1212 |         case panAndScanMaxNumCrops = "pan_and_scan_max_num_crops"
1213 |         case panAndScanMinCropSize = "pan_and_scan_min_crop_size"
1214 |         case panAndScanMinRatioToActivate = "pan_and_scan_min_ratio_to_activate"
1215 |     }
1216 | }
1217 | 
1218 | extension Gemma3: LoRAModel {
1219 |     public func loraLinearLayers() -> LoRALinearLayers {
1220 |         return languageModel.model.layers.map { ($0.selfAttention, ["q_proj", "v_proj"]) }
1221 |     }
1222 | }
1223 | 


--------------------------------------------------------------------------------
/Libraries/MLXVLM/Models/Idefics3.swift:
--------------------------------------------------------------------------------
  1 | //
  2 | //  Idefics3.swift
  3 | //  mlx-swift-examples
  4 | //
  5 | //  Created by SHUHONG WU on 12/13/24.
  6 | //
  7 | 
  8 | import CoreImage
  9 | import Foundation
 10 | import Hub
 11 | import MLX
 12 | import MLXLMCommon
 13 | import MLXNN
 14 | import Tokenizers
 15 | 
 16 | // MARK: - Configuration
 17 | 
 18 | public struct Idefics3Configuration: Codable, Sendable {
 19 | 
 20 |     public struct TextConfiguration: Codable, Sendable {
 21 |         public let modelType: String
 22 |         public let hiddenSize: Int
 23 |         public var numHiddenLayers: Int { _numHiddenLayers ?? 32 }
 24 |         public let intermediateSize: Int
 25 |         public let numAttentionHeads: Int
 26 |         public let rmsNormEps: Float
 27 |         public let vocabSize: Int
 28 |         public let numKeyValueHeads: Int
 29 |         public let ropeTheta: Float
 30 |         public var ropeTraditional: Bool { _ropeTraditional ?? false }
 31 |         public var tieWordEmbeddings: Bool { _tieWordEmbeddings ?? false }
 32 | 
 33 |         private let _numHiddenLayers: Int?
 34 |         private let _ropeTraditional: Bool?
 35 |         private let _tieWordEmbeddings: Bool?
 36 | 
 37 |         enum CodingKeys: String, CodingKey {
 38 |             case modelType = "model_type"
 39 |             case hiddenSize = "hidden_size"
 40 |             case _numHiddenLayers = "num_hidden_layers"
 41 |             case intermediateSize = "intermediate_size"
 42 |             case numAttentionHeads = "num_attention_heads"
 43 |             case rmsNormEps = "rms_norm_eps"
 44 |             case vocabSize = "vocab_size"
 45 |             case numKeyValueHeads = "num_key_value_heads"
 46 |             case ropeTheta = "rope_theta"
 47 |             case _ropeTraditional = "rope_traditional"
 48 |             case _tieWordEmbeddings = "tie_word_embeddings"
 49 |         }
 50 |     }
 51 | 
 52 |     public struct VisionConfiguration: Codable, Sendable {
 53 |         public let modelType: String
 54 |         public var numHiddenLayers: Int { _numHiddenLayers ?? 12 }
 55 |         public let hiddenSize: Int
 56 |         public var intermediateSize: Int { _intermediateSize ?? 3072 }
 57 |         public let numAttentionHeads: Int
 58 |         public let patchSize: Int
 59 |         public let imageSize: Int
 60 |         public var numChannels: Int { _numChannels ?? 3 }
 61 |         public var layerNormEps: Float { _layerNormEps ?? 1e-6 }
 62 | 
 63 |         private let _numHiddenLayers: Int?
 64 |         private let _intermediateSize: Int?
 65 |         private let _numChannels: Int?
 66 |         private let _layerNormEps: Float?
 67 | 
 68 |         enum CodingKeys: String, CodingKey {
 69 |             case modelType = "model_type"
 70 |             case _numHiddenLayers = "num_hidden_layers"
 71 |             case hiddenSize = "hidden_size"
 72 |             case _intermediateSize = "intermediate_size"
 73 |             case numAttentionHeads = "num_attention_heads"
 74 |             case patchSize = "patch_size"
 75 |             case imageSize = "image_size"
 76 |             case _numChannels = "num_channels"
 77 |             case _layerNormEps = "layer_norm_eps"
 78 |         }
 79 |     }
 80 | 
 81 |     public let textConfig: TextConfiguration
 82 |     public let visionConfig: VisionConfiguration
 83 |     public let modelType: String
 84 |     public let ignoreIndex: Int
 85 |     public let vocabSize: Int
 86 |     public let scaleFactor: Int
 87 |     public let imageTokenId: Int
 88 |     public let imageTokenIndex: Int
 89 | 
 90 |     enum CodingKeys: String, CodingKey {
 91 |         case textConfig = "text_config"
 92 |         case visionConfig = "vision_config"
 93 |         case modelType = "model_type"
 94 |         case ignoreIndex = "ignore_index"
 95 |         case vocabSize = "vocab_size"
 96 |         case scaleFactor = "scale_factor"
 97 |         case imageTokenId = "image_token_id"
 98 |         case imageTokenIndex = "image_token_index"
 99 |     }
100 | 
101 |     public init(from decoder: any Swift.Decoder) throws {
102 |         let container = try decoder.container(keyedBy: CodingKeys.self)
103 | 
104 |         self.textConfig =
105 |             try container
106 |             .decode(TextConfiguration.self, forKey: .textConfig)
107 |         self.visionConfig =
108 |             try container
109 |             .decode(VisionConfiguration.self, forKey: .visionConfig)
110 |         self.modelType = try container.decode(String.self, forKey: .modelType)
111 |         self.ignoreIndex = (try? container.decode(Int.self, forKey: .ignoreIndex)) ?? -100
112 |         self.vocabSize = (try? container.decode(Int.self, forKey: .vocabSize)) ?? 128259
113 |         self.scaleFactor = (try? container.decode(Int.self, forKey: .scaleFactor)) ?? 2
114 |         self.imageTokenId = (try? container.decode(Int.self, forKey: .imageTokenId)) ?? 49153
115 |         self.imageTokenIndex =
116 |             (try? container.decode(Int.self, forKey: .imageTokenIndex)) ?? self.imageTokenId
117 |     }
118 | }
119 | 
120 | // MARK: - Connector
121 | 
122 | private class Idefics3MLP: Module, UnaryLayer {
123 |     @ModuleInfo var proj: Linear
124 |     init(_ config: Idefics3Configuration) {
125 |         let inputSize = config.visionConfig.hiddenSize * (config.scaleFactor * config.scaleFactor)
126 |         let outputSize = config.textConfig.hiddenSize
127 |         self._proj.wrappedValue = Linear(inputSize, outputSize, bias: false)
128 |     }
129 | 
130 |     func callAsFunction(_ x: MLXArray) -> MLXArray {
131 |         let out = proj(x)
132 |         return out
133 |     }
134 | }
135 | 
136 | private class Idefics3Connector: Module {
137 |     let scaleFactor: Int
138 |     @ModuleInfo(key: "modality_projection") var modalityProjection: Idefics3MLP
139 | 
140 |     init(_ config: Idefics3Configuration) {
141 |         self.scaleFactor = config.scaleFactor
142 |         self._modalityProjection.wrappedValue = Idefics3MLP(config)
143 |     }
144 | 
145 |     func pixelShuffle(_ x: MLXArray, scaleFactor: Int) -> MLXArray {
146 |         let B = x.dim(0)
147 |         let seq = x.dim(1)
148 |         let embed_dim = x.dim(2)
149 |         let side = Int(Double(seq).squareRoot())
150 | 
151 |         var reshaped = x.reshaped(B, side, side, embed_dim)
152 |         reshaped =
153 |             reshaped
154 |             .reshaped(B, side, side / scaleFactor, embed_dim * scaleFactor)
155 |         reshaped = reshaped.transposed(0, 2, 1, 3)
156 |         reshaped =
157 |             reshaped
158 |             .reshaped(
159 |                 B,
160 |                 side / scaleFactor,
161 |                 side / scaleFactor,
162 |                 embed_dim * (scaleFactor * scaleFactor)
163 |             )
164 |         reshaped = reshaped.transposed(0, 2, 1, 3)
165 |         reshaped =
166 |             reshaped
167 |             .reshaped(
168 |                 B,
169 |                 seq / (scaleFactor * scaleFactor),
170 |                 embed_dim * (scaleFactor * scaleFactor)
171 |             )
172 |         return reshaped
173 |     }
174 | 
175 |     func callAsFunction(_ imageHiddenStates: MLXArray) -> MLXArray {
176 |         let shuffled = pixelShuffle(imageHiddenStates, scaleFactor: scaleFactor)
177 |         let out = modalityProjection(shuffled)
178 |         return out
179 |     }
180 | }
181 | 
182 | // MARK: - Language
183 | 
184 | private enum Language {
185 |     fileprivate class Attention: Module {
186 |         let nHeads: Int
187 |         let nKVHeads: Int
188 |         let scale: Float
189 |         @ModuleInfo(key: "q_proj") var q_proj: Linear
190 |         @ModuleInfo(key: "k_proj") var k_proj: Linear
191 |         @ModuleInfo(key: "v_proj") var v_proj: Linear
192 |         @ModuleInfo(key: "o_proj") var o_proj: Linear
193 |         @ModuleInfo(key: "rope") var ropeEmbed: RoPE
194 | 
195 |         init(_ config: Idefics3Configuration.TextConfiguration) {
196 |             let dim = config.hiddenSize
197 |             self.nHeads = config.numAttentionHeads
198 |             self.nKVHeads = config.numKeyValueHeads
199 |             let headDim = dim / nHeads
200 |             self.scale = pow(Float(headDim), -0.5)
201 | 
202 |             self._q_proj.wrappedValue = Linear(dim, nHeads * headDim, bias: false)
203 |             self._k_proj.wrappedValue = Linear(
204 |                 dim,
205 |                 nKVHeads * headDim,
206 |                 bias: false
207 |             )
208 |             self._v_proj.wrappedValue = Linear(
209 |                 dim,
210 |                 nKVHeads * headDim,
211 |                 bias: false
212 |             )
213 |             self._o_proj.wrappedValue = Linear(nHeads * headDim, dim, bias: false)
214 | 
215 |             self._ropeEmbed.wrappedValue = RoPE(
216 |                 dimensions: headDim,
217 |                 traditional: config.ropeTraditional,
218 |                 base: config.ropeTheta
219 |             )
220 |         }
221 | 
222 |         func callAsFunction(
223 |             _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache? = nil
224 |         ) -> MLXArray {
225 |             let B = x.dim(0)
226 |             let L = x.dim(1)
227 |             var q = q_proj(x)
228 |             var k = k_proj(x)
229 |             var v = v_proj(x)
230 | 
231 |             q = q.reshaped(B, L, nHeads, -1).transposed(0, 2, 1, 3)
232 |             k = k.reshaped(B, L, nKVHeads, -1).transposed(0, 2, 1, 3)
233 |             v = v.reshaped(B, L, nKVHeads, -1).transposed(0, 2, 1, 3)
234 | 
235 |             let offset = cache?.offset ?? 0
236 |             q = ropeEmbed(q, offset: offset)
237 |             k = ropeEmbed(k, offset: offset)
238 | 
239 |             if let cache {
240 |                 let (nk, nv) = cache.update(keys: k, values: v)
241 |                 k = nk
242 |                 v = nv
243 |             }
244 | 
245 |             let out = MLXFast.scaledDotProductAttention(
246 |                 queries: q,
247 |                 keys: k,
248 |                 values: v,
249 |                 scale: scale,
250 |                 mask: mask
251 |             )
252 |             .transposed(0, 2, 1, 3).reshaped(B, L, -1)
253 |             let final = o_proj(out)
254 |             return final
255 |         }
256 |     }
257 | 
258 |     fileprivate class MLP: Module, UnaryLayer {
259 |         @ModuleInfo(key: "gate_proj") var gate_proj: Linear
260 |         @ModuleInfo(key: "down_proj") var down_proj: Linear
261 |         @ModuleInfo(key: "up_proj") var up_proj: Linear
262 |         init(dim: Int, hiddenDim: Int) {
263 |             self._gate_proj.wrappedValue = Linear(dim, hiddenDim, bias: false)
264 |             self._down_proj.wrappedValue = Linear(hiddenDim, dim, bias: false)
265 |             self._up_proj.wrappedValue = Linear(dim, hiddenDim, bias: false)
266 |         }
267 | 
268 |         func callAsFunction(_ x: MLXArray) -> MLXArray {
269 |             let g = gate_proj(x)
270 |             let r = down_proj(silu(g) * up_proj(x))
271 |             return r
272 |         }
273 |     }
274 | 
275 |     fileprivate class TransformerBlock: Module {
276 |         @ModuleInfo(key: "self_attn") var selfAttn: Attention
277 |         @ModuleInfo(key: "input_layernorm") var inputLayerNorm: RMSNorm
278 |         @ModuleInfo(
279 |             key: "post_attention_layernorm"
280 |         ) var postAttentionLayerNorm: RMSNorm
281 |         let mlp: MLP
282 | 
283 |         init(_ config: Idefics3Configuration.TextConfiguration) {
284 |             self._selfAttn.wrappedValue = Attention(config)
285 |             self._inputLayerNorm.wrappedValue = RMSNorm(
286 |                 dimensions: config.hiddenSize,
287 |                 eps: config.rmsNormEps
288 |             )
289 |             self._postAttentionLayerNorm.wrappedValue = RMSNorm(
290 |                 dimensions: config.hiddenSize,
291 |                 eps: config.rmsNormEps
292 |             )
293 |             self.mlp = MLP(
294 |                 dim: config.hiddenSize,
295 |                 hiddenDim: config.intermediateSize
296 |             )
297 |         }
298 | 
299 |         func callAsFunction(
300 |             _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
301 |         ) -> MLXArray {
302 |             let a = selfAttn(inputLayerNorm(x), mask: mask, cache: cache)
303 |             let h = x + a
304 |             let m = mlp(postAttentionLayerNorm(h))
305 |             let out = h + m
306 |             return out
307 |         }
308 |     }
309 | 
310 |     fileprivate class LanguageModel: Module, KVCacheDimensionProvider {
311 |         @ModuleInfo(key: "embed_tokens") var embedTokens: Embedding
312 |         var layers: [TransformerBlock]
313 |         let norm: RMSNorm
314 |         let config: Idefics3Configuration.TextConfiguration
315 |         @ModuleInfo(key: "lm_head") var lmHead: Linear?
316 | 
317 |         var kvHeads: [Int] {
318 |             (0 ..< config.numHiddenLayers).map { _ in config.numKeyValueHeads }
319 |         }
320 |         var headDim: MLX.IntOrPair {
321 |             .init(config.hiddenSize / config.numAttentionHeads)
322 |         }
323 | 
324 |         init(_ config: Idefics3Configuration.TextConfiguration) {
325 |             self.config = config
326 |             self._embedTokens.wrappedValue = Embedding(
327 |                 embeddingCount: config.vocabSize,
328 |                 dimensions: config.hiddenSize
329 |             )
330 |             self.layers = (0 ..< config.numHiddenLayers)
331 |                 .map { _ in TransformerBlock(config) }
332 |             self.norm = RMSNorm(
333 |                 dimensions: config.hiddenSize,
334 |                 eps: config.rmsNormEps
335 |             )
336 |             let lmHeadNeeded = !config.tieWordEmbeddings
337 |             if lmHeadNeeded {
338 |                 self._lmHead.wrappedValue = Linear(
339 |                     config.hiddenSize,
340 |                     config.vocabSize,
341 |                     bias: false
342 |                 )
343 |             }
344 |         }
345 | 
346 |         func getEmbeddings(for inputIds: MLXArray) -> MLXArray {
347 |             let e = embedTokens(inputIds)
348 |             return e
349 |         }
350 | 
351 |         func callAsFunction(
352 |             _ inputs: MLXArray?, cache: [KVCache]? = nil, inputs_embeds: MLXArray? = nil
353 |         ) -> LMOutput {
354 |             let h: MLXArray
355 |             if let inputs_embeds = inputs_embeds {
356 |                 h = inputs_embeds.asType(norm.weight.dtype)
357 |             } else if let inputs = inputs {
358 |                 h = embedTokens(inputs)
359 |             } else {
360 |                 fatalError(
361 |                     "At least one of inputs or inputs_embeds must be provided."
362 |                 )
363 |             }
364 | 
365 |             let mask = createAttentionMask(h: h, cache: cache)
366 |             var x = h
367 |             for (i, layer) in layers.enumerated() {
368 |                 let c = i < (cache?.count ?? 0) ? cache![i] : nil
369 |                 x = layer(x, mask: mask, cache: c)
370 |             }
371 | 
372 |             x = norm(x)
373 |             let out = lmHead != nil ? lmHead!(x) : embedTokens.asLinear(x)
374 |             return LMOutput(logits: out)
375 |         }
376 | 
377 |         func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {
378 |             // filter out rotary_emb.inv_freq
379 |             return
380 |                 weights
381 |                 .filter { !$0.key.contains("self_attn.rotary_emb.inv_freq") }
382 |         }
383 |     }
384 | }
385 | 
386 | // MARK: - Vision
387 | 
388 | private enum Vision {
389 |     static func checkArrayShape(_ arr: MLXArray) -> Bool {
390 |         if arr.ndim != 4 { return false }
391 |         let (o, h, w, _) = (arr.dim(0), arr.dim(1), arr.dim(2), arr.dim(3))
392 |         return (o >= h && o >= w && h == w)
393 |     }
394 | 
395 |     fileprivate class Attention: Module {
396 |         let numHeads: Int
397 |         let scale: Float
398 |         @ModuleInfo(key: "q_proj") var q_proj: Linear
399 |         @ModuleInfo(key: "k_proj") var k_proj: Linear
400 |         @ModuleInfo(key: "v_proj") var v_proj: Linear
401 |         @ModuleInfo(key: "out_proj") var o_proj: Linear
402 | 
403 |         init(_ config: Idefics3Configuration.VisionConfiguration) {
404 |             self.numHeads = config.numAttentionHeads
405 |             let headDim = config.hiddenSize / config.numAttentionHeads
406 |             self.scale = pow(Float(headDim), -0.5)
407 |             self._q_proj.wrappedValue = Linear(
408 |                 config.hiddenSize,
409 |                 config.hiddenSize,
410 |                 bias: true
411 |             )
412 |             self._k_proj.wrappedValue = Linear(
413 |                 config.hiddenSize,
414 |                 config.hiddenSize,
415 |                 bias: true
416 |             )
417 |             self._v_proj.wrappedValue = Linear(
418 |                 config.hiddenSize,
419 |                 config.hiddenSize,
420 |                 bias: true
421 |             )
422 |             self._o_proj.wrappedValue = Linear(
423 |                 config.hiddenSize,
424 |                 config.hiddenSize,
425 |                 bias: true
426 |             )
427 |         }
428 | 
429 |         func callAsFunction(_ x: MLXArray, mask: MLXArray? = nil) -> MLXArray {
430 |             let (B, L, D) = (x.dim(0), x.dim(1), x.dim(2))
431 |             let q = q_proj(x).reshaped(B, L, numHeads, D / numHeads).transposed(
432 |                 0,
433 |                 2,
434 |                 1,
435 |                 3
436 |             )
437 |             let k = k_proj(x).reshaped(B, L, numHeads, D / numHeads).transposed(
438 |                 0,
439 |                 2,
440 |                 1,
441 |                 3
442 |             )
443 |             let v = v_proj(x).reshaped(B, L, numHeads, D / numHeads).transposed(
444 |                 0,
445 |                 2,
446 |                 1,
447 |                 3
448 |             )
449 | 
450 |             let out = MLXFast.scaledDotProductAttention(
451 |                 queries: q,
452 |                 keys: k,
453 |                 values: v,
454 |                 scale: scale,
455 |                 mask: mask
456 |             )
457 |             .transposed(0, 2, 1, 3).reshaped(B, L, D)
458 |             let final = o_proj(out)
459 |             return final
460 |         }
461 |     }
462 | 
463 |     fileprivate class MLP: Module, UnaryLayer {
464 |         @ModuleInfo var fc1: Linear
465 |         @ModuleInfo var fc2: Linear
466 |         let activation = GELU(approximation: .precise)
467 | 
468 |         init(_ config: Idefics3Configuration.VisionConfiguration) {
469 |             self.fc1 = Linear(
470 |                 config.hiddenSize,
471 |                 config.intermediateSize,
472 |                 bias: true
473 |             )
474 |             self.fc2 = Linear(
475 |                 config.intermediateSize,
476 |                 config.hiddenSize,
477 |                 bias: true
478 |             )
479 |         }
480 | 
481 |         func callAsFunction(_ x: MLXArray) -> MLXArray {
482 |             let out = fc2(activation(fc1(x)))
483 |             return out
484 |         }
485 |     }
486 | 
487 |     fileprivate class EncoderLayer: Module {
488 |         @ModuleInfo(key: "self_attn") var self_attn: Attention
489 |         @ModuleInfo(key: "layer_norm1") var layerNorm1: LayerNorm
490 |         @ModuleInfo var mlp: MLP
491 |         @ModuleInfo(key: "layer_norm2") var layerNorm2: LayerNorm
492 | 
493 |         init(_ config: Idefics3Configuration.VisionConfiguration) {
494 |             self._self_attn.wrappedValue = Attention(config)
495 |             self._layerNorm1.wrappedValue = LayerNorm(
496 |                 dimensions: config.hiddenSize,
497 |                 eps: config.layerNormEps
498 |             )
499 |             self.mlp = MLP(config)
500 |             self._layerNorm2.wrappedValue = LayerNorm(
501 |                 dimensions: config.hiddenSize,
502 |                 eps: config.layerNormEps
503 |             )
504 |         }
505 | 
506 |         func callAsFunction(_ x: MLXArray, mask: MLXArray? = nil) -> MLXArray {
507 |             let h = x + self_attn(layerNorm1(x), mask: mask)
508 |             let out = h + mlp(layerNorm2(h))
509 |             return out
510 |         }
511 |     }
512 | 
513 |     fileprivate class Encoder: Module {
514 |         var layers: [EncoderLayer]
515 |         init(_ config: Idefics3Configuration.VisionConfiguration) {
516 |             self.layers = (0 ..< config.numHiddenLayers)
517 |                 .map { _ in EncoderLayer(config) }
518 |         }
519 | 
520 |         func callAsFunction(_ x: MLXArray, outputHiddenStates: Bool = false, mask: MLXArray? = nil)
521 |             -> (
522 |                 MLXArray,
523 |                 [MLXArray]?
524 |             )
525 |         {
526 |             var encoderStates: [MLXArray]? = outputHiddenStates ? [x] : nil
527 |             var h = x
528 |             for l in layers {
529 |                 h = l(h, mask: mask)
530 |                 if outputHiddenStates {
531 |                     encoderStates?.append(h)
532 |                 }
533 |             }
534 |             return (h, encoderStates)
535 |         }
536 |     }
537 | 
538 |     fileprivate class VisionEmbeddings: Module, UnaryLayer {
539 |         @ModuleInfo(key: "patch_embedding") var patchEmbedding: Conv2d
540 |         @ModuleInfo(key: "position_embedding") var positionEmbedding: Embedding
541 |         let numPositions: Int
542 | 
543 |         init(_ config: Idefics3Configuration.VisionConfiguration) {
544 |             self._patchEmbedding.wrappedValue = Conv2d(
545 |                 inputChannels: config.numChannels,
546 |                 outputChannels: config.hiddenSize,
547 |                 kernelSize: .init(config.patchSize),
548 |                 stride: .init(config.patchSize)
549 |             )
550 |             let numPatches =
551 |                 (config.imageSize / config.patchSize) * (config.imageSize / config.patchSize)
552 |             self.numPositions = numPatches
553 |             self._positionEmbedding.wrappedValue = Embedding(
554 |                 embeddingCount: numPatches,
555 |                 dimensions: config.hiddenSize
556 |             )
557 |         }
558 | 
559 |         func callAsFunction(_ x: MLXArray) -> MLXArray {
560 |             var patchEmbeddings = patchEmbedding(x)
561 |             patchEmbeddings = patchEmbeddings.flattened(start: 1, end: 2)
562 |             let positionIds = MLXArray(0 ..< numPositions)[.newAxis, 0...]
563 |             let posEmbedding = positionEmbedding(positionIds)
564 |             let embeddings = patchEmbeddings + posEmbedding
565 |             return embeddings
566 |         }
567 |     }
568 | 
569 |     fileprivate class VisionModel: Module {
570 |         @ModuleInfo(key: "embeddings") var embeddings: VisionEmbeddings
571 |         @ModuleInfo(key: "encoder") var encoder: Encoder
572 |         @ModuleInfo(key: "post_layernorm") var postLayernorm: LayerNorm
573 |         let config: Idefics3Configuration.VisionConfiguration
574 | 
575 |         init(_ config: Idefics3Configuration.VisionConfiguration) {
576 |             self.config = config
577 |             self._embeddings.wrappedValue = VisionEmbeddings(config)
578 |             self._encoder.wrappedValue = Encoder(config)
579 |             self._postLayernorm.wrappedValue = LayerNorm(
580 |                 dimensions: config.hiddenSize,
581 |                 eps: config.layerNormEps
582 |             )
583 |         }
584 | 
585 |         func callAsFunction(_ x: MLXArray, outputHiddenStates: Bool = true) -> (
586 |             MLXArray,
587 |             MLXArray,
588 |             [MLXArray]?
589 |         ) {
590 |             let e = embeddings(x)
591 |             let (encoded, hiddenStates) = encoder(
592 |                 e,
593 |                 outputHiddenStates: outputHiddenStates
594 |             )
595 |             let pooler_output = postLayernorm(encoded)
596 |             return (pooler_output, e, hiddenStates)
597 |         }
598 | 
599 |         func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {
600 |             var sanitizedWeights = [String: MLXArray]()
601 |             for (k, v) in weights {
602 |                 if k.contains("position_ids") {
603 |                     continue
604 |                 } else if k.contains("patch_embedding.weight") {
605 |                     if Vision.checkArrayShape(v) {
606 |                         sanitizedWeights[k] = v
607 |                     } else {
608 |                         sanitizedWeights[k] = v.transposed(0, 2, 3, 1)
609 |                     }
610 |                 } else {
611 |                     sanitizedWeights[k] = v
612 |                 }
613 |             }
614 |             return sanitizedWeights
615 |         }
616 |     }
617 | }
618 | 
619 | // MARK: - Model
620 | 
621 | public class Idefics3: Module, VLMModel, KVCacheDimensionProvider {
622 |     @ModuleInfo(key: "vision_model") private var visionModel: Vision.VisionModel
623 |     @ModuleInfo(
624 |         key: "language_model"
625 |     ) private var languageModel: Language.LanguageModel
626 |     @ModuleInfo(key: "connector") private var connector: Idefics3Connector
627 |     public let config: Idefics3Configuration
628 | 
629 |     public var vocabularySize: Int { config.vocabSize }
630 |     public var kvHeads: [Int] { languageModel.kvHeads }
631 |     public var headDim: MLX.IntOrPair { languageModel.headDim }
632 | 
633 |     public func loraLinearLayers() -> LoRALinearLayers {
634 |         languageModel.layers.map { ($0.selfAttn, ["q_proj", "v_proj"]) }
635 |     }
636 | 
637 |     public init(_ config: Idefics3Configuration) {
638 |         self.config = config
639 |         self._visionModel.wrappedValue = Vision.VisionModel(config.visionConfig)
640 |         self._languageModel.wrappedValue =
641 |             Language
642 |             .LanguageModel(config.textConfig)
643 |         self._connector.wrappedValue = Idefics3Connector(config)
644 |     }
645 | 
646 |     private func getInputEmbeddings(inputIds: MLXArray?, pixelValues: MLXArray?) -> MLXArray {
647 |         if pixelValues == nil {
648 |             guard let inputIds = inputIds else {
649 |                 fatalError("inputIds required if no pixelValues")
650 |             }
651 |             let inputs_embeds = languageModel.getEmbeddings(for: inputIds)
652 |             return inputs_embeds
653 |         }
654 | 
655 |         guard let inputIds = inputIds, let pixelValues = pixelValues else {
656 |             fatalError("inputIds and pixelValues required")
657 |         }
658 | 
659 |         let inputs_embeds = languageModel.getEmbeddings(for: inputIds)
660 |         let (pooler_output, _, _) = visionModel(
661 |             pixelValues,
662 |             outputHiddenStates: true
663 |         )
664 |         // Match dtype with inputs_embeds
665 |         let image_features = connector(
666 |             pooler_output.asType(inputs_embeds.dtype)
667 |         )
668 | 
669 |         let final = prepareInputsForMultimodal(
670 |             imageFeatures: image_features,
671 |             inputs_embeds: inputs_embeds,
672 |             inputIds: inputIds
673 |         )
674 |         return final
675 |     }
676 | 
677 |     // inputs_merger
678 |     private func prepareInputsForMultimodal(
679 |         imageFeatures: MLXArray, inputs_embeds: MLXArray, inputIds: MLXArray
680 |     ) -> MLXArray {
681 |         // Assumes bs == 1
682 |         // inputIds shape: (1, seq_len)
683 |         // asArray(Int.self) -> [[Int]], take [0] to get [Int]
684 |         let ids: [[Int]] = [inputIds.asArray(Int.self)]
685 | 
686 |         let inputIdArray: [Int] = ids[0]
687 | 
688 |         let imageTokenIndex = config.imageTokenIndex
689 |         let imagePositions = inputIdArray.enumerated().compactMap {
690 |             $1 == imageTokenIndex ? $0 : nil
691 |         }
692 | 
693 |         var segments = [MLXArray]()
694 |         var start_idx = 0
695 | 
696 |         let chunkSize = imageFeatures.shape[1]  // 64
697 |         let chunkCount = imagePositions.count / chunkSize  // Should be imageFeatures.shape[0]
698 |         let chunks = (0 ..< chunkCount).map { startIndex in
699 |             let start = startIndex * chunkSize
700 |             let end = start + chunkSize
701 |             return Array(imagePositions[start ..< end])
702 |         }
703 | 
704 |         for (chunkIndex, chunk) in chunks.enumerated() {
705 |             let currentImage = imageFeatures[chunkIndex]
706 | 
707 |             for (i, pos) in chunk.enumerated() {
708 |                 if pos > start_idx {
709 |                     segments.append(inputs_embeds[0, start_idx ..< pos])
710 |                 }
711 |                 segments.append(currentImage[i ..< i + 1])
712 |                 start_idx = pos + 1
713 |             }
714 |         }
715 | 
716 |         if start_idx < inputs_embeds.dim(1) {
717 |             segments.append(inputs_embeds[0, start_idx...])
718 |         }
719 | 
720 |         let finalEmbeds = concatenated(segments, axis: 0)
721 |         return finalEmbeds.expandedDimensions(axis: 0)
722 |     }
723 | 
724 |     public func prepare(_ input: LMInput, cache: [any KVCache], windowSize: Int?) throws
725 |         -> PrepareResult
726 |     {
727 |         let inputIds = input.text.tokens
728 |         let pixelValues = input.image?.pixels
729 |         let embeddings = getInputEmbeddings(
730 |             inputIds: inputIds,
731 |             pixelValues: pixelValues
732 |         )
733 |         let result = languageModel(nil, cache: cache, inputs_embeds: embeddings)
734 |         return .logits(result)
735 |     }
736 | 
737 |     public func callAsFunction(_ inputs: MLXArray, cache: [any KVCache]?) -> MLXArray {
738 |         let out = languageModel(inputs, cache: cache).logits
739 |         return out
740 |     }
741 | 
742 |     public func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {
743 |         // Rename keys to match Python logic
744 |         var renamed = [String: MLXArray]()
745 |         for (k, v) in weights {
746 |             var newKey = k
747 |             if newKey.hasPrefix("model.") {
748 |                 newKey.removeFirst("model.".count)
749 |             } else if newKey.hasPrefix("lm_head.") {
750 |                 newKey = "language_model." + newKey
751 |             }
752 |             renamed[newKey] = v
753 |         }
754 | 
755 |         var final = [String: MLXArray]()
756 |         for (k, v) in renamed {
757 |             if k.hasPrefix("text_model.") {
758 |                 let suffix = String(k.dropFirst("text_model.".count))
759 |                 final["language_model." + suffix] = v
760 |             } else {
761 |                 final[k] = v
762 |             }
763 |         }
764 | 
765 |         // Remove rotary_emb.inv_freq
766 |         final = final.filter {
767 |             !$0.key.contains("self_attn.rotary_emb.inv_freq")
768 |         }
769 | 
770 |         return final
771 |     }
772 | }
773 | 
774 | // MARK: - Processor Configuration
775 | public struct Idefics3ProcessorConfiguration: Codable, Sendable {
776 |     public struct Size: Codable, Sendable {
777 |         public let longestEdge: Int
778 |         enum CodingKeys: String, CodingKey {
779 |             case longestEdge = "longest_edge"
780 |         }
781 |     }
782 | 
783 |     public let imageMean: [CGFloat]
784 |     public let imageStd: [CGFloat]
785 |     public let size: Size
786 |     public let imageSequenceLength: Int?
787 | 
788 |     public var imageMeanTuple: (CGFloat, CGFloat, CGFloat) {
789 |         (imageMean[0], imageMean[1], imageMean[2])
790 |     }
791 |     public var imageStdTuple: (CGFloat, CGFloat, CGFloat) {
792 |         (imageStd[0], imageStd[1], imageStd[2])
793 |     }
794 | 
795 |     enum CodingKeys: String, CodingKey {
796 |         case imageMean = "image_mean"
797 |         case imageStd = "image_std"
798 |         case size
799 |         case imageSequenceLength = "image_seq_len"
800 |     }
801 | }
802 | 
803 | // MARK: - Processor
804 | 
805 | public class Idefics3Processor: UserInputProcessor {
806 |     private let config: Idefics3ProcessorConfiguration
807 |     private let tokenizer: any Tokenizer
808 |     private let fixedImageSize = 384
809 | 
810 |     // From the Python code and default config, we know image_token_id is usually 49153.
811 |     // Hardcode this since we can't pass it in or rely on it from the processor config.
812 |     private let imageTokenId = 49153
813 | 
814 |     public init(
815 |         _ config: Idefics3ProcessorConfiguration,
816 |         tokenizer: any Tokenizer
817 |     ) {
818 |         self.config = config
819 |         self.tokenizer = tokenizer
820 |     }
821 | 
822 |     private func prompt(from userInput: UserInput) -> String {
823 |         switch userInput.prompt {
824 |         case .text(let text):
825 |             text
826 |         case .messages(let messages):
827 |             messages.last?["content"] as? String ?? ""
828 |         case .chat(let messages):
829 |             messages.last?.content ?? ""
830 |         }
831 |     }
832 | 
833 |     public func prepare(input: UserInput) throws -> LMInput {
834 |         let prompt = prompt(from: input)
835 |         if input.images.isEmpty {
836 |             // No image scenario
837 |             let tokens = try tokenizer.encode(text: prompt)
838 |             let tokensArray = MLXArray(tokens).expandedDimensions(axis: 0)
839 |             let mask = ones(like: tokensArray)
840 |             return LMInput(text: .init(tokens: tokensArray, mask: mask), image: nil)
841 |         } else {
842 |             // Single image scenario
843 |             guard input.images.count == 1 else {
844 |                 throw VLMError.singleImageAllowed
845 |             }
846 | 
847 |             let count = config.imageSequenceLength ?? 1
848 | 
849 |             // Encode only the text part of the prompt, without <image>
850 |             var promptTokens = try tokenizer.encode(text: prompt)
851 | 
852 |             let imageTokenIndex = promptTokens.count / 2
853 |             promptTokens.insert(imageTokenId, at: imageTokenIndex)
854 | 
855 |             let promptArray = MLXArray(promptTokens).expandedDimensions(axis: 0)
856 |             let mask = ones(like: promptArray)
857 | 
858 |             var image = try input.images[0].asCIImage()
859 |             image = MediaProcessing.inSRGBToneCurveSpace(image)
860 |             let targetSize = CGSize(
861 |                 width: fixedImageSize,
862 |                 height: fixedImageSize
863 |             )
864 |             image = MediaProcessing.apply(image, processing: input.processing)
865 |             image = try MediaProcessing.resampleBicubic(image, to: targetSize)
866 |             image = MediaProcessing.normalize(
867 |                 image,
868 |                 mean: config.imageMeanTuple,
869 |                 std: config.imageStdTuple
870 |             )
871 |             var pixels = MediaProcessing.asMLXArray(image)
872 | 
873 |             if pixels.ndim == 2 {
874 |                 pixels = pixels.expandedDimensions(axis: -1)
875 |             }
876 | 
877 |             if pixels.ndim == 3 {
878 |                 pixels = pixels.expandedDimensions(axis: 0)
879 |             }
880 | 
881 |             // If shape is (B,C,H,W), transpose to (B,H,W,C)
882 |             if pixels
883 |                 .dim(1) == 3
884 |                 && pixels
885 |                     .dim(2) == fixedImageSize
886 |                 && pixels
887 |                     .dim(3) == fixedImageSize
888 |             {
889 |                 pixels = pixels.transposed(0, 2, 3, 1)
890 |             }
891 | 
892 |             return LMInput(
893 |                 text: .init(tokens: promptArray, mask: mask),
894 |                 image: .init(pixels: pixels)
895 |             )
896 |         }
897 |     }
898 | }
899 | 


--------------------------------------------------------------------------------
/Libraries/MLXVLM/Models/Paligemma.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | // port of https://github.com/Blaizzy/mlx-vlm/tree/main/mlx_vlm/models/paligemma
  4 | 
  5 | import CoreImage
  6 | import Foundation
  7 | import Hub
  8 | import MLX
  9 | import MLXLMCommon
 10 | import MLXNN
 11 | import Tokenizers
 12 | 
 13 | // MARK: - Language
 14 | 
 15 | private enum Language {
 16 |     fileprivate class Attention: Module {
 17 | 
 18 |         let args: PaliGemmaConfiguration.TextConfiguration
 19 |         let scale: Float
 20 | 
 21 |         @ModuleInfo(key: "q_proj") var wq: Linear
 22 |         @ModuleInfo(key: "k_proj") var wk: Linear
 23 |         @ModuleInfo(key: "v_proj") var wv: Linear
 24 |         @ModuleInfo(key: "o_proj") var wo: Linear
 25 | 
 26 |         let rope: RoPE
 27 | 
 28 |         public init(_ args: PaliGemmaConfiguration.TextConfiguration) {
 29 |             self.args = args
 30 | 
 31 |             let dim = args.hiddenSize
 32 |             let heads = args.attentionHeads
 33 |             let kvHeads = args.kvHeads
 34 | 
 35 |             let headDim = args.hiddenSize / heads
 36 |             self.scale = pow(Float(headDim), -0.5)
 37 | 
 38 |             self._wq.wrappedValue = Linear(dim, heads * headDim, bias: false)
 39 |             self._wk.wrappedValue = Linear(dim, kvHeads * headDim, bias: false)
 40 |             self._wv.wrappedValue = Linear(dim, kvHeads * headDim, bias: false)
 41 |             self._wo.wrappedValue = Linear(heads * headDim, dim, bias: false)
 42 | 
 43 |             self.rope = RoPE(
 44 |                 dimensions: headDim, traditional: args.ropeTraditional, base: args.ropeTheta)
 45 |         }
 46 | 
 47 |         public func callAsFunction(
 48 |             _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
 49 |         ) -> MLXArray {
 50 |             let (B, L) = (x.dim(0), x.dim(1))
 51 | 
 52 |             var queries = wq(x)
 53 |             var keys = wk(x)
 54 |             var values = wv(x)
 55 | 
 56 |             // prepare the queries, keys and values for the attention computation
 57 |             queries = queries.reshaped(B, L, args.attentionHeads, -1).transposed(0, 2, 1, 3)
 58 |             keys = keys.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)
 59 |             values = values.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)
 60 | 
 61 |             if let cache {
 62 |                 queries = rope(queries, offset: cache.offset)
 63 |                 keys = rope(keys, offset: cache.offset)
 64 |                 (keys, values) = cache.update(keys: keys, values: values)
 65 |             } else {
 66 |                 queries = rope(queries)
 67 |                 keys = rope(keys)
 68 |             }
 69 | 
 70 |             let output = MLXFast.scaledDotProductAttention(
 71 |                 queries: queries, keys: keys, values: values, scale: scale, mask: mask
 72 |             )
 73 |             .transposed(0, 2, 1, 3)
 74 |             .reshaped(B, L, -1)
 75 | 
 76 |             return wo(output)
 77 |         }
 78 |     }
 79 | 
 80 |     fileprivate class MLP: Module, UnaryLayer {
 81 | 
 82 |         @ModuleInfo(key: "gate_proj") var gate: Linear
 83 |         @ModuleInfo(key: "down_proj") var down: Linear
 84 |         @ModuleInfo(key: "up_proj") var up: Linear
 85 | 
 86 |         public init(dimensions: Int, hiddenDimensions: Int) {
 87 |             self._gate.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)
 88 |             self._down.wrappedValue = Linear(hiddenDimensions, dimensions, bias: false)
 89 |             self._up.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)
 90 |         }
 91 | 
 92 |         public func callAsFunction(_ x: MLXArray) -> MLXArray {
 93 |             down(gelu(gate(x)) * up(x))
 94 |         }
 95 |     }
 96 | 
 97 |     fileprivate class TransformerBlock: Module {
 98 | 
 99 |         @ModuleInfo(key: "self_attn") var attention: Attention
100 |         let mlp: MLP
101 | 
102 |         @ModuleInfo(key: "input_layernorm") var inputLayerNorm: Gemma.RMSNorm
103 |         @ModuleInfo(key: "post_attention_layernorm") var postAttentionLayerNorm: Gemma.RMSNorm
104 | 
105 |         public init(_ args: PaliGemmaConfiguration.TextConfiguration) {
106 |             self._attention.wrappedValue = Attention(args)
107 |             self.mlp = MLP(dimensions: args.hiddenSize, hiddenDimensions: args.intermediateSize)
108 |             self._inputLayerNorm.wrappedValue = Gemma.RMSNorm(
109 |                 dimensions: args.hiddenSize, eps: args.rmsNormEps)
110 |             self._postAttentionLayerNorm.wrappedValue = Gemma.RMSNorm(
111 |                 dimensions: args.hiddenSize, eps: args.rmsNormEps)
112 |         }
113 | 
114 |         public func callAsFunction(
115 |             _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
116 |         ) -> MLXArray {
117 |             var r = attention(inputLayerNorm(x), mask: mask, cache: cache)
118 |             let h = x + r
119 |             r = mlp(postAttentionLayerNorm(h))
120 |             let out = h + r
121 |             return out
122 |         }
123 |     }
124 | 
125 |     fileprivate class GemmaModel: Module {
126 | 
127 |         @ModuleInfo(key: "embed_tokens") var embedTokens: Embedding
128 | 
129 |         fileprivate let layers: [TransformerBlock]
130 |         fileprivate let norm: Gemma.RMSNorm
131 | 
132 |         let hiddenScale: Float
133 | 
134 |         public init(_ args: PaliGemmaConfiguration.TextConfiguration) {
135 |             precondition(args.vocabularySize > 0)
136 | 
137 |             self._embedTokens.wrappedValue = Embedding(
138 |                 embeddingCount: args.vocabularySize, dimensions: args.hiddenSize)
139 | 
140 |             self.hiddenScale = pow(Float(args.hiddenSize), 0.5)
141 | 
142 |             self.layers = (0 ..< args.hiddenLayers)
143 |                 .map { _ in
144 |                     TransformerBlock(args)
145 |                 }
146 |             self.norm = Gemma.RMSNorm(dimensions: args.hiddenSize, eps: args.rmsNormEps)
147 |         }
148 | 
149 |         public func callAsFunction(
150 |             _ inputs: MLXArray, cache: [KVCache]? = nil, inputEmbedding: MLXArray? = nil,
151 |             mask: MLXArray? = nil
152 |         ) -> MLXArray {
153 |             var h = inputEmbedding ?? embedTokens(inputs)
154 |             h = h * hiddenScale
155 | 
156 |             let mask =
157 |                 if mask == nil || (cache?[0].offset ?? 0) > 0 {
158 |                     createAttentionMask(h: h, cache: cache)
159 |                 } else {
160 |                     MLXFast.ScaledDotProductAttentionMaskMode.none
161 |                 }
162 | 
163 |             for (i, layer) in layers.enumerated() {
164 |                 h = layer(h, mask: mask, cache: cache?[i])
165 |             }
166 | 
167 |             return norm(h)
168 |         }
169 |     }
170 | 
171 |     fileprivate class LanguageModel: Module, KVCacheDimensionProvider {
172 |         @ModuleInfo var model: GemmaModel
173 | 
174 |         var kvHeads: [Int]
175 | 
176 |         public init(_ args: PaliGemmaConfiguration.TextConfiguration) {
177 |             self.model = GemmaModel(args)
178 | 
179 |             self.kvHeads = (0 ..< args.hiddenLayers).map { _ in args.kvHeads }
180 |         }
181 | 
182 |         public func callAsFunction(
183 |             _ inputs: MLXArray, cache: [KVCache]? = nil, inputEmbedding: MLXArray? = nil,
184 |             mask: MLXArray? = nil
185 |         ) -> LMOutput {
186 |             var out = model(inputs, cache: cache, inputEmbedding: inputEmbedding, mask: mask)
187 |             out = model.embedTokens.asLinear(out)
188 |             return LMOutput(logits: out)
189 |         }
190 | 
191 |         func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {
192 |             weights.filter {
193 |                 !$0.key.contains("self_attn.rotary_emb.inv_freq")
194 |             }
195 |         }
196 |     }
197 | }
198 | 
199 | // MARK: - Vision
200 | 
201 | private enum Vision {
202 |     fileprivate class Attention: Module {
203 | 
204 |         let numHeads: Int
205 |         let scale: Float
206 | 
207 |         @ModuleInfo(key: "q_proj") var wq: Linear
208 |         @ModuleInfo(key: "k_proj") var wk: Linear
209 |         @ModuleInfo(key: "v_proj") var wv: Linear
210 |         @ModuleInfo(key: "out_proj") var wo: Linear
211 | 
212 |         public init(dims: Int, numHeads: Int, bias: Bool = true) {
213 |             precondition(dims % numHeads == 0, "Dimensions must be divisible by numHeads")
214 | 
215 |             self.numHeads = numHeads
216 |             let headDim = dims / numHeads
217 |             self.scale = pow(Float(headDim), -0.5)
218 | 
219 |             self._wq.wrappedValue = Linear(dims, dims, bias: bias)
220 |             self._wk.wrappedValue = Linear(dims, dims, bias: bias)
221 |             self._wv.wrappedValue = Linear(dims, dims, bias: bias)
222 |             self._wo.wrappedValue = Linear(dims, dims, bias: bias)
223 |         }
224 | 
225 |         public func callAsFunction(
226 |             _ x: MLXArray, mask: MLXArray? = nil
227 |         ) -> MLXArray {
228 |             var queries = wq(x)
229 |             var keys = wk(x)
230 |             var values = wv(x)
231 | 
232 |             let (B, L) = (queries.dim(0), queries.dim(1))
233 |             let S = keys.dim(1)
234 | 
235 |             queries = queries.reshaped(B, L, numHeads, -1).transposed(0, 2, 1, 3)
236 |             keys = keys.reshaped(B, S, numHeads, -1).transposed(0, 2, 1, 3)
237 |             values = values.reshaped(B, S, numHeads, -1).transposed(0, 2, 1, 3)
238 | 
239 |             let output = MLXFast.scaledDotProductAttention(
240 |                 queries: queries, keys: keys, values: values, scale: scale, mask: mask
241 |             )
242 |             .transposed(0, 2, 1, 3)
243 |             .reshaped(B, L, -1)
244 | 
245 |             return wo(output)
246 |         }
247 |     }
248 | 
249 |     fileprivate class PhiMLP: Module, UnaryLayer {
250 | 
251 |         @ModuleInfo var fc1: Linear
252 |         @ModuleInfo var fc2: Linear
253 | 
254 |         public init(_ config: PaliGemmaConfiguration.VisionConfiguration) {
255 |             self.fc1 = Linear(config.hiddenSize, config.intermediateSize, bias: true)
256 |             self.fc2 = Linear(config.intermediateSize, config.hiddenSize, bias: true)
257 |         }
258 | 
259 |         public func callAsFunction(_ x: MLXArray) -> MLXArray {
260 |             fc2(geluApproximate(fc1(x)))
261 |         }
262 |     }
263 | 
264 |     fileprivate class EncoderLayer: Module {
265 | 
266 |         @ModuleInfo(key: "self_attn") var attention: Attention
267 |         @ModuleInfo(key: "layer_norm1") var layerNorm1: LayerNorm
268 |         @ModuleInfo var mlp: PhiMLP
269 |         @ModuleInfo(key: "layer_norm2") var layerNorm2: LayerNorm
270 | 
271 |         public init(_ config: PaliGemmaConfiguration.VisionConfiguration) {
272 |             self._attention.wrappedValue = Attention(
273 |                 dims: config.hiddenSize, numHeads: config.attentionHeads, bias: true)
274 |             self._layerNorm1.wrappedValue = LayerNorm(
275 |                 dimensions: config.hiddenSize, eps: config.layerNormEps)
276 |             self.mlp = PhiMLP(config)
277 |             self._layerNorm2.wrappedValue = LayerNorm(
278 |                 dimensions: config.hiddenSize, eps: config.layerNormEps)
279 |         }
280 | 
281 |         public func callAsFunction(_ x: MLXArray, mask: MLXArray? = nil) -> MLXArray {
282 |             var r = attention(layerNorm1(x), mask: mask)
283 |             let h = x + r
284 |             r = mlp(layerNorm2(h))
285 |             return h + r
286 |         }
287 |     }
288 | 
289 |     fileprivate class Encoder: Module {
290 |         var layers: [EncoderLayer]
291 | 
292 |         public init(_ config: PaliGemmaConfiguration.VisionConfiguration) {
293 |             self.layers = (0 ..< config.hiddenLayers).map { _ in
294 |                 EncoderLayer(config)
295 |             }
296 |         }
297 | 
298 |         public func callAsFunction(
299 |             _ x: MLXArray, outputHiddenStates: Bool = false, mask: MLXArray? = nil
300 |         ) -> (MLXArray, [MLXArray]?) {
301 |             var encoderStates: [MLXArray]? = outputHiddenStates ? [] : nil
302 |             var h = x
303 |             var x = x
304 |             for l in layers {
305 |                 x = l(x, mask: mask)
306 |                 if outputHiddenStates {
307 |                     encoderStates?.append(x)
308 |                 }
309 |                 h = x[0]
310 |             }
311 |             return (h, encoderStates)
312 |         }
313 |     }
314 | 
315 |     fileprivate class VisionEmbeddings: Module, UnaryLayer {
316 | 
317 |         @ModuleInfo(key: "patch_embedding") var patchEmbedding: Conv2d
318 |         @ModuleInfo(key: "position_embedding") var positionEmbedding: Embedding
319 | 
320 |         let positions: Int
321 |         let _positionIds: MLXArray
322 | 
323 |         public init(_ config: PaliGemmaConfiguration.VisionConfiguration) {
324 |             self._patchEmbedding.wrappedValue = Conv2d(
325 |                 inputChannels: config.channels, outputChannels: config.hiddenSize,
326 |                 kernelSize: .init(config.patchSize), stride: .init(config.patchSize)
327 |             )
328 |             let d = config.imageSize / config.patchSize
329 |             self.positions = d * d
330 |             self._positionEmbedding.wrappedValue = Embedding(
331 |                 embeddingCount: positions, dimensions: config.hiddenSize
332 |             )
333 |             self._positionIds = MLXArray(0 ..< positions)[.newAxis, 0...]
334 |         }
335 | 
336 |         public func callAsFunction(_ x: MLXArray) -> MLXArray {
337 |             var patchEmbeddings = self.patchEmbedding(x)
338 |             patchEmbeddings = patchEmbeddings.flattened(start: 1, end: 2)
339 |             let embeddings = patchEmbeddings + self.positionEmbedding(self._positionIds)
340 |             return embeddings
341 |         }
342 |     }
343 | 
344 |     fileprivate class SigLipVisionModel: Module {
345 | 
346 |         @ModuleInfo var embeddings: VisionEmbeddings
347 |         @ModuleInfo var encoder: Encoder
348 |         @ModuleInfo(key: "post_layernorm") var postLayerNorm: LayerNorm
349 | 
350 |         public init(_ config: PaliGemmaConfiguration.VisionConfiguration) {
351 |             self.embeddings = VisionEmbeddings(config)
352 |             self.encoder = Encoder(config)
353 |             self._postLayerNorm.wrappedValue = LayerNorm(dimensions: config.hiddenSize)
354 |         }
355 | 
356 |         public func callAsFunction(_ x: MLXArray, outputHiddenStates: Bool = false) -> (
357 |             MLXArray, MLXArray, MLXArray?
358 |         ) {
359 |             let x = embeddings(x)
360 | 
361 |             let (encoderOutput, hiddenStates) = encoder(x, outputHiddenStates: outputHiddenStates)
362 |             let poolerOutput = postLayerNorm(encoderOutput)
363 | 
364 |             return (poolerOutput, x, hiddenStates?.last)
365 |         }
366 |     }
367 | 
368 |     fileprivate class VisionModel: Module {
369 | 
370 |         @ModuleInfo(key: "vision_model") var visionModel: SigLipVisionModel
371 | 
372 |         public init(_ config: PaliGemmaConfiguration.VisionConfiguration) {
373 |             precondition(
374 |                 config.modelType == "siglip_vision_model",
375 |                 "Unsupported modelType: \(config.modelType)")
376 |             self._visionModel.wrappedValue = SigLipVisionModel(config)
377 |         }
378 | 
379 |         public func callAsFunction(_ x: MLXArray, outputHiddenStates: Bool = false) -> (
380 |             MLXArray, MLXArray, MLXArray?
381 |         ) {
382 |             visionModel(x, outputHiddenStates: outputHiddenStates)
383 |         }
384 | 
385 |         private func isMLXWeight(_ array: MLXArray) -> Bool {
386 |             if array.ndim != 4 {
387 |                 return false
388 |             }
389 | 
390 |             let (outChannels, kH, kW) = (array.dim(0), array.dim(1), array.dim(2))
391 |             return outChannels >= kH && outChannels >= kW && kH == kW
392 |         }
393 | 
394 |         func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {
395 |             var sanitizedWeights = [String: MLXArray]()
396 | 
397 |             for (k, v) in weights {
398 |                 if k.contains("position_id") {
399 |                     // Remove unused position_ids
400 |                     continue
401 |                 } else if k.contains("patch_embedding.weight") {
402 |                     // PyTorch conv2d weight tensors have shape:
403 |                     //   [out_channels, in_channels, kH, KW]
404 |                     // MLX conv2d expects the weight be of shape:
405 |                     //   [out_channels, kH, KW, in_channels]
406 |                     if isMLXWeight(v) {
407 |                         sanitizedWeights[k] = v
408 |                     } else {
409 |                         sanitizedWeights[k] = v.transposed(0, 2, 3, 1)
410 |                     }
411 |                 } else {
412 |                     sanitizedWeights[k] = v
413 |                 }
414 |             }
415 | 
416 |             return sanitizedWeights
417 |         }
418 |     }
419 | }
420 | 
421 | // MARK: - Processor
422 | 
423 | /// PaliGemma VLM `UserInputProcessor`.
424 | ///
425 | /// This is meant to be used with ``PaliGemma`` and is typically created by ``VLMModelFactory``.
426 | public class PaliGemmaProcessor: UserInputProcessor {
427 | 
428 |     private let config: PaliGemmaProcessorConfiguration
429 |     private let tokenizer: any Tokenizer
430 | 
431 |     public init(_ config: PaliGemmaProcessorConfiguration, tokenizer: any Tokenizer) {
432 |         self.config = config
433 |         self.tokenizer = tokenizer
434 |     }
435 | 
436 |     private func prepare(image: CIImage, processing: UserInput.Processing?) throws -> MLXArray {
437 |         // based on image_processing_siglip from transformers
438 |         var image = image
439 | 
440 |         // we want to do all of the image processing in an sRGB tone curve
441 |         // rather than a linear space as that is what transformers / torch_vision
442 |         // do (implicitly by using sRGB rasters directly)
443 |         image = MediaProcessing.inSRGBToneCurveSpace(image)
444 | 
445 |         // apply user instructions
446 |         image = MediaProcessing.apply(image, processing: processing)
447 | 
448 |         image = try MediaProcessing.resampleBicubic(image, to: config.size.cgSize)
449 |         image = MediaProcessing.normalize(
450 |             image, mean: config.imageMeanTuple, std: config.imageStdTuple)
451 | 
452 |         return MediaProcessing.asMLXArray(image)
453 |     }
454 | 
455 |     public func prepare(input: UserInput) throws -> LMInput {
456 |         switch input.images.count {
457 |         case 0: throw VLMError.imageRequired
458 |         case 1: break
459 |         default: throw VLMError.singleImageAllowed
460 |         }
461 | 
462 |         // this doesn't have a chat template so just use the last message.
463 |         var prompt = prompt(from: input)
464 | 
465 |         // based on transformers/processing_paligemma
466 |         let count = input.images.count * config.imageSequenceLength
467 |         prompt =
468 |             Array(repeating: "<image>", count: count).joined() + (tokenizer.bosToken ?? "") + prompt
469 |             + "\n"
470 | 
471 |         let promptTokens = try tokenizer.encode(text: prompt)
472 |         let promptArray = MLXArray(promptTokens).expandedDimensions(axis: 0)
473 |         let mask = ones(like: promptArray).asType(.int8)
474 | 
475 |         let pixels = try prepare(image: input.images[0].asCIImage(), processing: input.processing)
476 | 
477 |         return LMInput(text: .init(tokens: promptArray, mask: mask), image: .init(pixels: pixels))
478 |     }
479 | 
480 |     private func prompt(from userInput: UserInput) -> String {
481 |         switch userInput.prompt {
482 |         case .text(let text):
483 |             text
484 |         case .messages(let messages):
485 |             messages.last?["content"] as? String ?? ""
486 |         case .chat(let messages):
487 |             messages.last?.content ?? ""
488 |         }
489 |     }
490 | 
491 | }
492 | 
493 | // MARK: - Model
494 | 
495 | private class PaliGemmaMultiModalProjector: Module, UnaryLayer {
496 | 
497 |     @ModuleInfo var linear: Linear
498 | 
499 |     public init(_ config: PaliGemmaConfiguration.VisionConfiguration) {
500 |         self.linear = Linear(config.hiddenSize, config.projectionDimensions, bias: true)
501 |     }
502 | 
503 |     public func callAsFunction(_ x: MLXArray) -> MLXArray {
504 |         linear(x)
505 |     }
506 | }
507 | 
508 | /// PaliGemma VLM
509 | ///
510 | /// This is typically created by ``VLMModelFactory``.
511 | public class PaliGemma: Module, VLMModel, KVCacheDimensionProvider {
512 | 
513 |     @ModuleInfo(key: "vision_tower") private var visionModel: Vision.VisionModel
514 |     @ModuleInfo(key: "language_model") private var languageModel: Language.LanguageModel
515 |     @ModuleInfo(key: "multi_modal_projector") private var multiModalProjector:
516 |         PaliGemmaMultiModalProjector
517 | 
518 |     public let config: PaliGemmaConfiguration
519 | 
520 |     public var vocabularySize: Int { config.vocabularySize }
521 |     public var kvHeads: [Int] { languageModel.kvHeads }
522 | 
523 |     public func loraLinearLayers() -> MLXLMCommon.LoRALinearLayers {
524 |         languageModel.model.layers.map { ($0.attention, ["q_proj", "v_proj"]) }
525 |     }
526 | 
527 |     public init(_ config: PaliGemmaConfiguration) {
528 |         self.config = config
529 |         self._visionModel.wrappedValue = Vision.VisionModel(config.visionConfiguration)
530 |         self._languageModel.wrappedValue = Language.LanguageModel(config.textConfiguration)
531 |         self._multiModalProjector.wrappedValue = PaliGemmaMultiModalProjector(
532 |             config.visionConfiguration)
533 |     }
534 | 
535 |     private func inputEmbeddings(inputIds: MLXArray, pixelValues: MLXArray?, mask: MLXArray) -> (
536 |         MLXArray, MLXArray
537 |     ) {
538 |         guard let pixelValues else {
539 |             return (inputIds, mask)
540 |         }
541 | 
542 |         let inputEmbedding = languageModel.model.embedTokens(inputIds)
543 |         let (hiddenState, _, _) = self.visionModel(
544 |             pixelValues.transposed(0, 2, 3, 1).asType(inputEmbedding.dtype),
545 |             outputHiddenStates: true
546 |         )
547 | 
548 |         var imageFeatures = hiddenState[.newAxis, .ellipsis].asType(inputEmbedding.dtype)
549 |         imageFeatures = multiModalProjector(imageFeatures)
550 | 
551 |         return prepareInputsForMultimodal(
552 |             imageFeatures: imageFeatures, inputEmbedding: inputEmbedding,
553 |             inputIds: inputIds, attentionMask: mask)
554 |     }
555 | 
556 |     private func prepareInputsForMultimodal(
557 |         imageFeatures: MLXArray, inputEmbedding: MLXArray, inputIds: MLXArray,
558 |         attentionMask: MLXArray
559 |     ) -> (MLXArray, MLXArray) {
560 |         let embedDimension = imageFeatures.dim(2)
561 |         let (batchSize, sequenceLength) = inputIds.shape2
562 |         var scaledImageFeatures = imageFeatures / pow(Float(config.hiddenSize), 0.5)
563 | 
564 |         let textMask = (inputIds .!= config.imageTokenIndex) & (inputIds .!= config.padTokenId)
565 |         let imageMask = inputIds .== config.imageTokenIndex
566 |         let padMask = inputIds .== config.padTokenId
567 | 
568 |         // expand masks to match embedding dimension
569 |         var textMaskExpanded = expandedDimensions(textMask, axis: -1)
570 |         var padMaskExpanded = expandedDimensions(padMask, axis: -1)
571 | 
572 |         // insert padding and text token embeddings
573 |         var finalEmbedding = which(textMaskExpanded, inputEmbedding, 0)
574 |         finalEmbedding = which(padMaskExpanded, 0, finalEmbedding)
575 | 
576 |         let padSize = finalEmbedding.dim(1) - scaledImageFeatures.dim(1)
577 |         scaledImageFeatures = padded(scaledImageFeatures, widths: [0, .init((0, padSize)), 0])
578 | 
579 |         // insert image embeddings - the image mask is always less or equal to the sentence in length
580 |         var imageMaskExpanded = expandedDimensions(imageMask, axis: -1)
581 |         finalEmbedding = which(imageMaskExpanded, scaledImageFeatures, finalEmbedding)
582 | 
583 |         finalEmbedding = which(padMaskExpanded, 0, finalEmbedding)
584 | 
585 |         let attentionMaskExpanded1 = expandedDimensions(attentionMask, axis: 1)
586 |         let attentionMaskExpanded2 = expandedDimensions(attentionMask, axis: 2)
587 |         var finalAttentionMask4d = attentionMaskExpanded1 * attentionMaskExpanded2
588 |         finalAttentionMask4d = expandedDimensions(finalAttentionMask4d, axis: 1)
589 | 
590 |         return (finalEmbedding, finalAttentionMask4d)
591 |     }
592 | 
593 |     public func prepare(_ input: LMInput, cache: [any KVCache], windowSize: Int?) throws
594 |         -> PrepareResult
595 |     {
596 |         guard let image = input.image else { throw VLMError.imageRequired }
597 |         guard let mask = input.text.mask else { throw VLMError.maskRequired }
598 |         let inputIds = input.text.tokens
599 | 
600 |         let (inputEmbedding, finalAttentionMask4d) = inputEmbeddings(
601 |             inputIds: inputIds, pixelValues: image.pixels, mask: mask)
602 | 
603 |         let result = languageModel(
604 |             inputIds, cache: cache, inputEmbedding: inputEmbedding, mask: finalAttentionMask4d)
605 | 
606 |         return .logits(result)
607 |     }
608 | 
609 |     public func callAsFunction(_ inputs: MLXArray, cache: [any KVCache]?) -> MLXArray {
610 |         languageModel(inputs, cache: cache).logits
611 |     }
612 | 
613 |     public func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {
614 |         languageModel.sanitize(weights: visionModel.sanitize(weights: weights))
615 |     }
616 | }
617 | 
618 | // MARK: - Configuration
619 | 
620 | /// Confguration for ``PaliGemma``
621 | public struct PaliGemmaConfiguration: Codable, Sendable {
622 | 
623 |     public struct TextConfiguration: Codable, Sendable {
624 |         public let modelType: String
625 |         public let hiddenSize: Int
626 |         public let hiddenLayers: Int
627 |         public let intermediateSize: Int
628 |         public let attentionHeads: Int
629 |         public let kvHeads: Int
630 |         public let vocabularySize: Int
631 |         private let _rmsNormEps: Float?
632 |         public var rmsNormEps: Float { _rmsNormEps ?? 1e-6 }
633 |         private let _ropeTheta: Float?
634 |         public var ropeTheta: Float { _ropeTheta ?? 10_000 }
635 |         private let _ropeTraditional: Bool?
636 |         public var ropeTraditional: Bool { _ropeTraditional ?? false }
637 | 
638 |         enum CodingKeys: String, CodingKey {
639 |             case modelType = "model_type"
640 |             case hiddenSize = "hidden_size"
641 |             case hiddenLayers = "num_hidden_layers"
642 |             case intermediateSize = "intermediate_size"
643 |             case attentionHeads = "num_attention_heads"
644 |             case kvHeads = "num_key_value_heads"
645 |             case vocabularySize = "vocab_size"
646 |             case _rmsNormEps = "rms_norm_eps"
647 |             case _ropeTheta = "rope_theta"
648 |             case _ropeTraditional = "rope_traditional"
649 |         }
650 |     }
651 | 
652 |     public struct VisionConfiguration: Codable, Sendable {
653 |         public let modelType: String
654 |         public let hiddenSize: Int
655 |         public let hiddenLayers: Int
656 |         public let intermediateSize: Int
657 |         public let attentionHeads: Int
658 |         public let patchSize: Int
659 |         public let projectionDimensions: Int
660 |         public let imageSize: Int
661 |         private let _channels: Int?
662 |         public var channels: Int { _channels ?? 3 }
663 |         private let _layerNormEps: Float?
664 |         public var layerNormEps: Float { _layerNormEps ?? 1e-6 }
665 | 
666 |         enum CodingKeys: String, CodingKey {
667 |             case modelType = "model_type"
668 |             case hiddenSize = "hidden_size"
669 |             case hiddenLayers = "num_hidden_layers"
670 |             case intermediateSize = "intermediate_size"
671 |             case attentionHeads = "num_attention_heads"
672 |             case patchSize = "patch_size"
673 |             case projectionDimensions = "projection_dim"
674 |             case imageSize = "image_size"
675 |             case _channels = "num_channels"
676 |             case _layerNormEps = "layer_norm_eps"
677 |         }
678 |     }
679 | 
680 |     public let textConfiguration: TextConfiguration
681 |     public let visionConfiguration: VisionConfiguration
682 |     public let modelType: String
683 |     public let vocabularySize: Int
684 |     public let ignoreIndex: Int
685 |     public let imageTokenIndex: Int
686 |     public let hiddenSize: Int
687 |     public let padTokenId: Int
688 | 
689 |     enum CodingKeys: String, CodingKey {
690 |         case textConfiguration = "text_config"
691 |         case visionConfiguration = "vision_config"
692 |         case modelType = "model_type"
693 |         case vocabularySize = "vocab_size"
694 |         case ignoreIndex = "ignore_index"
695 |         case imageTokenIndex = "image_token_index"
696 |         case hiddenSize = "hidden_size"
697 |         case padTokenId = "pad_token_id"
698 |     }
699 | }
700 | 
701 | /// Configuration for ``PaliGemmaProcessor``
702 | public struct PaliGemmaProcessorConfiguration: Codable, Sendable {
703 | 
704 |     public struct Size: Codable, Sendable {
705 |         public let width: Int
706 |         public let height: Int
707 | 
708 |         var cgSize: CGSize { .init(width: width, height: height) }
709 |     }
710 | 
711 |     public let imageMean: [CGFloat]
712 |     public let imageStd: [CGFloat]
713 |     public let size: Size
714 |     public let imageSequenceLength: Int
715 | 
716 |     public var imageMeanTuple: (CGFloat, CGFloat, CGFloat) {
717 |         (imageMean[0], imageMean[1], imageMean[2])
718 |     }
719 |     public var imageStdTuple: (CGFloat, CGFloat, CGFloat) {
720 |         (imageStd[0], imageStd[1], imageStd[2])
721 |     }
722 | 
723 |     enum CodingKeys: String, CodingKey {
724 |         case imageMean = "image_mean"
725 |         case imageStd = "image_std"
726 |         case size
727 |         case imageSequenceLength = "image_seq_length"
728 |     }
729 | }
730 | 


--------------------------------------------------------------------------------
/Libraries/MLXVLM/Models/Qwen25VL.swift:
--------------------------------------------------------------------------------
   1 | // Port of https://github.com/Blaizzy/mlx-vlm/tree/main/mlx_vlm/models/qwen2_5_vl
   2 | 
   3 | import CoreImage
   4 | import Foundation
   5 | import Hub
   6 | import MLX
   7 | import MLXLMCommon
   8 | import MLXNN
   9 | import Tokenizers
  10 | 
  11 | // MARK: - Language
  12 | 
  13 | private enum Language {
  14 | 
  15 |     /// Applies Rotary Position Embedding with Multimodal Sections to the query and key tensors
  16 |     static private func applyMultimodalRotaryPositionEmbedding(
  17 |         q: MLXArray, k: MLXArray, cos: MLXArray, sin: MLXArray,
  18 |         positionIds: MLXArray, mropeSection: [Int]
  19 |     ) -> (MLXArray, MLXArray) {
  20 |         var cos = cos[positionIds]
  21 |         var sin = sin[positionIds]
  22 | 
  23 |         cos =
  24 |             concatenated(
  25 |                 // [m[i % 3] for i, m in enumerate(mx.split(cos, mrope_section, axis=-1))]
  26 |                 split(cos, indices: mropeSection, axis: -1).enumerated().map { i, m in m[i % 3] },
  27 |                 axis: -1
  28 |             )[0..., .newAxis, 0..., 0...]
  29 | 
  30 |         sin =
  31 |             concatenated(
  32 |                 split(sin, indices: mropeSection, axis: -1).enumerated().map { i, m in m[i % 3] },
  33 |                 axis: -1
  34 |             )[0..., .newAxis, 0..., 0...]
  35 | 
  36 |         // Apply rotary embedding
  37 |         let qEmbed = (q * cos) + (QwenVL.rotateHalf(q) * sin)
  38 |         let kEmbed = (k * cos) + (QwenVL.rotateHalf(k) * sin)
  39 |         return (qEmbed, kEmbed)
  40 |     }
  41 | 
  42 |     fileprivate class Attention: Module {
  43 | 
  44 |         let heads: Int
  45 |         let kvHeads: Int
  46 |         let headDim: Int
  47 |         let scale: Float
  48 |         let mropeSection: [Int]
  49 | 
  50 |         @ModuleInfo(key: "q_proj") var wq: Linear
  51 |         @ModuleInfo(key: "k_proj") var wk: Linear
  52 |         @ModuleInfo(key: "v_proj") var wv: Linear
  53 |         @ModuleInfo(key: "o_proj") var wo: Linear
  54 | 
  55 |         @ModuleInfo(key: "rotary_emb") var rotaryEmbedding: RoPE
  56 | 
  57 |         public init(_ args: Qwen25VLConfiguration.TextConfiguration) {
  58 |             let dim = args.hiddenSize
  59 |             self.heads = args.attentionHeads
  60 |             self.kvHeads = args.kvHeads
  61 |             self.headDim = dim / heads
  62 |             self.scale = pow(Float(headDim), -0.5)
  63 | 
  64 |             self._wq.wrappedValue = Linear(dim, heads * headDim, bias: true)
  65 |             self._wk.wrappedValue = Linear(dim, kvHeads * headDim, bias: true)
  66 |             self._wv.wrappedValue = Linear(dim, kvHeads * headDim, bias: true)
  67 |             self._wo.wrappedValue = Linear(heads * headDim, dim, bias: false)
  68 | 
  69 |             if let v = args.ropeScaling?["mrope_section"], let array = v.asInts() {
  70 |                 // mrope_section = np.cumsum(mrope_section * 2)[:-1].tolist()
  71 |                 self.mropeSection = sequence(state: (0, array.makeIterator())) { state in
  72 |                     if let v = state.1.next() {
  73 |                         // note the *2
  74 |                         state.0 += v * 2
  75 |                         return state.0
  76 |                     } else {
  77 |                         return nil
  78 |                     }
  79 |                 }.dropLast()
  80 |             } else {
  81 |                 fatalError("rope_scaling['mrope_section'] must be an array of integers")
  82 |             }
  83 | 
  84 |             self._rotaryEmbedding.wrappedValue = RoPE(
  85 |                 dimensions: headDim, traditional: args.ropeTraditional, base: args.ropeTheta)
  86 |         }
  87 | 
  88 |         public func callAsFunction(
  89 |             _ x: MLXArray, mask: MLXArray? = nil, cache: KVCache?
  90 |         ) -> MLXArray {
  91 |             let (B, L) = (x.dim(0), x.dim(1))
  92 | 
  93 |             var queries = wq(x)
  94 |             var keys = wk(x)
  95 |             var values = wv(x)
  96 | 
  97 |             // prepare the queries, keys and values for the attention computation
  98 |             queries = queries.reshaped(B, L, heads, headDim).transposed(0, 2, 1, 3)
  99 |             keys = keys.reshaped(B, L, kvHeads, headDim).transposed(0, 2, 1, 3)
 100 |             values = values.reshaped(B, L, kvHeads, headDim).transposed(0, 2, 1, 3)
 101 | 
 102 |             let offset = cache?.offset ?? 0
 103 |             let mask = mask?[0..., 0 ..< keys.dim(-2)]
 104 | 
 105 |             queries = rotaryEmbedding(queries, offset: offset)
 106 |             keys = rotaryEmbedding(keys, offset: offset)
 107 | 
 108 |             if let cache {
 109 |                 (keys, values) = cache.update(keys: keys, values: values)
 110 |             }
 111 | 
 112 |             let output = MLXFast.scaledDotProductAttention(
 113 |                 queries: queries, keys: keys, values: values, scale: scale, mask: mask
 114 |             )
 115 |             .transposed(0, 2, 1, 3)
 116 |             .reshaped(B, L, -1)
 117 | 
 118 |             return wo(output)
 119 |         }
 120 |     }
 121 | 
 122 |     fileprivate class MLP: Module, UnaryLayer {
 123 |         @ModuleInfo(key: "gate_proj") var gate: Linear
 124 |         @ModuleInfo(key: "up_proj") var up: Linear
 125 |         @ModuleInfo(key: "down_proj") var down: Linear
 126 | 
 127 |         public init(dimensions: Int, hiddenDimensions: Int) {
 128 |             self._gate.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)
 129 |             self._up.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)
 130 |             self._down.wrappedValue = Linear(hiddenDimensions, dimensions, bias: false)
 131 |         }
 132 | 
 133 |         public func callAsFunction(_ x: MLXArray) -> MLXArray {
 134 |             down(silu(gate(x)) * up(x))
 135 |         }
 136 |     }
 137 | 
 138 |     fileprivate class Qwen25VLDecoderLayer: Module {
 139 | 
 140 |         @ModuleInfo(key: "self_attn") var attention: Attention
 141 |         let mlp: MLP
 142 | 
 143 |         @ModuleInfo(key: "input_layernorm") var inputLayerNorm: RMSNorm
 144 |         @ModuleInfo(key: "post_attention_layernorm") var postAttentionLayerNorm: RMSNorm
 145 | 
 146 |         public init(_ args: Qwen25VLConfiguration.TextConfiguration) {
 147 |             self._attention.wrappedValue = Attention(args)
 148 |             self.mlp = MLP(dimensions: args.hiddenSize, hiddenDimensions: args.intermediateSize)
 149 |             self._inputLayerNorm.wrappedValue = RMSNorm(
 150 |                 dimensions: args.hiddenSize, eps: args.rmsNormEps)
 151 |             self._postAttentionLayerNorm.wrappedValue = RMSNorm(
 152 |                 dimensions: args.hiddenSize, eps: args.rmsNormEps)
 153 |         }
 154 | 
 155 |         public func callAsFunction(
 156 |             _ x: MLXArray, mask: MLXArray? = nil, cache: KVCache?
 157 |         ) -> MLXArray {
 158 |             var r = attention(inputLayerNorm(x), mask: mask, cache: cache)
 159 |             let h = x + r
 160 |             r = mlp(postAttentionLayerNorm(h))
 161 |             let out = h + r
 162 |             return out
 163 |         }
 164 |     }
 165 | 
 166 |     fileprivate class Qwen25Model: Module {
 167 | 
 168 |         @ModuleInfo(key: "embed_tokens") var embedTokens: Embedding
 169 | 
 170 |         fileprivate let layers: [Qwen25VLDecoderLayer]
 171 |         fileprivate let norm: RMSNorm
 172 | 
 173 |         public init(_ args: Qwen25VLConfiguration.TextConfiguration) {
 174 |             precondition(args.vocabularySize > 0)
 175 | 
 176 |             self._embedTokens.wrappedValue = Embedding(
 177 |                 embeddingCount: args.vocabularySize, dimensions: args.hiddenSize)
 178 | 
 179 |             self.layers = (0 ..< args.hiddenLayers)
 180 |                 .map { _ in
 181 |                     Qwen25VLDecoderLayer(args)
 182 |                 }
 183 |             self.norm = RMSNorm(dimensions: args.hiddenSize, eps: args.rmsNormEps)
 184 |         }
 185 | 
 186 |         public func callAsFunction(
 187 |             _ inputs: MLXArray?, cache: [KVCache]? = nil, inputEmbedding: MLXArray? = nil
 188 |         ) -> MLXArray {
 189 |             var h: MLXArray
 190 |             if let inputEmbedding {
 191 |                 h = inputEmbedding
 192 |             } else if let inputs {
 193 |                 h = embedTokens(inputs)
 194 |             } else {
 195 |                 fatalError("one of inputs or inputEmbedding must be non-nil")
 196 |             }
 197 | 
 198 |             let mask: MLXArray? = createAttentionMask(h: h, cache: cache)
 199 | 
 200 |             for (i, layer) in layers.enumerated() {
 201 |                 h = layer(h, mask: mask, cache: cache?[i])
 202 |             }
 203 | 
 204 |             return norm(h)
 205 |         }
 206 |     }
 207 | 
 208 |     fileprivate class LanguageModel: Module, KVCacheDimensionProvider {
 209 |         @ModuleInfo var model: Qwen25Model
 210 |         @ModuleInfo(key: "lm_head") var lmHead: Linear?
 211 | 
 212 |         var kvHeads: [Int]
 213 | 
 214 |         public init(_ args: Qwen25VLConfiguration.TextConfiguration) {
 215 |             self.model = Qwen25Model(args)
 216 | 
 217 |             if !args.tieWordEmbeddings {
 218 |                 _lmHead.wrappedValue = Linear(args.hiddenSize, args.vocabularySize, bias: false)
 219 |             }
 220 | 
 221 |             self.kvHeads = (0 ..< args.hiddenLayers).map { _ in args.kvHeads }
 222 |         }
 223 | 
 224 |         public func callAsFunction(
 225 |             _ inputs: MLXArray?, cache: [KVCache]? = nil, inputEmbedding: MLXArray? = nil
 226 |         ) -> LMOutput {
 227 |             var out = model(inputs, cache: cache, inputEmbedding: inputEmbedding)
 228 |             if let lmHead {
 229 |                 out = lmHead(out)
 230 |             } else {
 231 |                 out = model.embedTokens.asLinear(out)
 232 |             }
 233 |             return LMOutput(logits: out)
 234 |         }
 235 |     }
 236 | }
 237 | 
 238 | // MARK: - Vision
 239 | 
 240 | private enum Vision {
 241 | 
 242 |     static fileprivate func applyMultimodalRotaryPositionEmbedding(
 243 |         _ tensor: MLXArray, freqs: MLXArray
 244 |     ) -> MLXArray {
 245 |         var cos = cos(freqs)
 246 |         var sin = sin(freqs)
 247 | 
 248 |         cos = expandedDimensions(cos, axis: 1)
 249 |         cos = tiled(cos, repetitions: [1, 1, 2])
 250 |         cos = expandedDimensions(cos, axis: 0)
 251 | 
 252 |         sin = expandedDimensions(sin, axis: 1)
 253 |         sin = tiled(sin, repetitions: [1, 1, 2])
 254 |         sin = expandedDimensions(sin, axis: 0)
 255 | 
 256 |         let output = (tensor * cos) + (QwenVL.rotateHalf(tensor) * sin)
 257 |         return output.asType(tensor.dtype)
 258 |     }
 259 | 
 260 |     fileprivate class PatchMerger: Module, UnaryLayer {
 261 |         let hiddenSize: Int
 262 |         @ModuleInfo(key: "ln_q") var layerNormQ: RMSNorm
 263 |         @ModuleInfo var mlp: (Linear, GELU, Linear)
 264 | 
 265 |         init(dimensions: Int, contextDimensions: Int, spatialMergeSize: Int) {
 266 |             self.hiddenSize = contextDimensions * (spatialMergeSize * spatialMergeSize)
 267 |             self._layerNormQ.wrappedValue = RMSNorm(dimensions: contextDimensions, eps: 1e-6)
 268 |             self.mlp = (
 269 |                 Linear(hiddenSize, hiddenSize),
 270 |                 GELU(),
 271 |                 Linear(hiddenSize, dimensions)
 272 |             )
 273 |         }
 274 | 
 275 |         func callAsFunction(_ x: MLXArray) -> MLXArray {
 276 |             var x = layerNormQ(x).reshaped(-1, hiddenSize)
 277 |             x = mlp.0(x)
 278 |             x = mlp.1(x)
 279 |             x = mlp.2(x)
 280 |             return x
 281 |         }
 282 |     }
 283 | 
 284 |     fileprivate class Attention: Module {
 285 | 
 286 |         let numHeads: Int
 287 |         let scale: Float
 288 | 
 289 |         @ModuleInfo(key: "qkv") var qkv: Linear
 290 |         @ModuleInfo(key: "proj") var proj: Linear
 291 | 
 292 |         public init(dims: Int, numHeads: Int) {
 293 |             self.numHeads = numHeads
 294 |             let headDim = dims / numHeads
 295 |             self.scale = pow(Float(headDim), -0.5)
 296 | 
 297 |             self._qkv.wrappedValue = Linear(dims, 3 * dims, bias: true)
 298 |             self._proj.wrappedValue = Linear(dims, dims)
 299 |         }
 300 | 
 301 |         public func callAsFunction(
 302 |             _ x: MLXArray, attentionMask: MLXArray, rotaryPositionEmbedding: MLXArray
 303 |         ) -> MLXArray {
 304 |             let sequenceLength = x.dim(0)
 305 | 
 306 |             let qkv = qkv(x)
 307 |             let s = split(qkv, parts: 3, axis: -1)
 308 |             var (q, k, v) = (s[0], s[1], s[2])
 309 | 
 310 |             q = q.reshaped(sequenceLength, numHeads, -1)
 311 |             k = k.reshaped(sequenceLength, numHeads, -1)
 312 |             v = v.reshaped(sequenceLength, numHeads, -1)
 313 | 
 314 |             q = applyMultimodalRotaryPositionEmbedding(q, freqs: rotaryPositionEmbedding)
 315 |             k = applyMultimodalRotaryPositionEmbedding(k, freqs: rotaryPositionEmbedding)
 316 | 
 317 |             q = q.reshaped(1, sequenceLength, numHeads, -1).transposed(0, 2, 1, 3)
 318 |             k = k.reshaped(1, sequenceLength, numHeads, -1).transposed(0, 2, 1, 3)
 319 |             v = v.reshaped(1, sequenceLength, numHeads, -1).transposed(0, 2, 1, 3)
 320 | 
 321 |             let output = MLXFast.scaledDotProductAttention(
 322 |                 queries: q, keys: k, values: v, scale: scale, mask: attentionMask
 323 |             )
 324 |             .transposed(0, 2, 1, 3)
 325 |             .reshaped(sequenceLength, -1)
 326 | 
 327 |             return proj(output)
 328 |         }
 329 |     }
 330 | 
 331 |     fileprivate class MLP: Module, UnaryLayer {
 332 | 
 333 |         @ModuleInfo(key: "gate_proj") var gate: Linear
 334 |         @ModuleInfo(key: "up_proj") var up: Linear
 335 |         @ModuleInfo(key: "down_proj") var down: Linear
 336 | 
 337 |         public init(dimensions: Int, hiddenDimensions: Int) {
 338 |             self._gate.wrappedValue = Linear(dimensions, hiddenDimensions)
 339 |             self._up.wrappedValue = Linear(dimensions, hiddenDimensions)
 340 |             self._down.wrappedValue = Linear(hiddenDimensions, dimensions)
 341 |         }
 342 | 
 343 |         public func callAsFunction(_ x: MLXArray) -> MLXArray {
 344 |             down(silu(gate(x)) * up(x))
 345 |         }
 346 |     }
 347 | 
 348 |     fileprivate class Qwen25VLVisionBlock: Module {
 349 | 
 350 |         @ModuleInfo var norm1: RMSNorm
 351 |         @ModuleInfo var norm2: RMSNorm
 352 |         @ModuleInfo(key: "attn") var attention: Attention
 353 |         @ModuleInfo var mlp: MLP
 354 | 
 355 |         public init(_ config: Qwen25VLConfiguration.VisionConfiguration) {
 356 |             self.norm1 = RMSNorm(dimensions: config.hiddenSize, eps: 1e-6)
 357 |             self.norm2 = RMSNorm(dimensions: config.hiddenSize, eps: 1e-6)
 358 | 
 359 |             self._attention.wrappedValue = Attention(
 360 |                 dims: config.hiddenSize, numHeads: config.numHeads)
 361 | 
 362 |             self.mlp = MLP(
 363 |                 dimensions: config.hiddenSize, hiddenDimensions: config.intermediateSize)
 364 |         }
 365 | 
 366 |         func callAsFunction(
 367 |             _ hiddenStates: MLXArray, attentionMask: MLXArray, rotaryPositionEmbedding: MLXArray
 368 |         ) -> MLXArray {
 369 |             var hiddenStates =
 370 |                 hiddenStates
 371 |                 + attention(
 372 |                     norm1(hiddenStates),
 373 |                     attentionMask: attentionMask,
 374 |                     rotaryPositionEmbedding: rotaryPositionEmbedding
 375 |                 )
 376 |             hiddenStates = hiddenStates + mlp(norm2(hiddenStates))
 377 |             return hiddenStates
 378 |         }
 379 |     }
 380 | 
 381 |     fileprivate class VisionModel: Module {
 382 | 
 383 |         @ModuleInfo(key: "patch_embed") var patchEmbed: QwenVL.PatchEmbed
 384 |         @ModuleInfo(key: "rotary_pos_emb") var rotaryPositionEmbedding: QwenVL.VisionRotaryEmbedding
 385 |         @ModuleInfo(key: "blocks") var blocks: [Qwen25VLVisionBlock]
 386 |         @ModuleInfo(key: "merger") var patchMerger: PatchMerger
 387 | 
 388 |         let spatialMergeSize: Int
 389 |         let windowSize: Int
 390 |         let patchSize: Int
 391 |         let spatialMergeUnit: Int
 392 |         let fullattBlockIndexes: [Int]
 393 | 
 394 |         public init(_ config: Qwen25VLConfiguration.VisionConfiguration) {
 395 |             self.spatialMergeSize = config.spatialMergeSize
 396 |             self.windowSize = config.windowSize
 397 |             self.patchSize = config.patchSize
 398 |             self.spatialMergeUnit = config.spatialMergeSize * config.spatialMergeSize
 399 |             self.fullattBlockIndexes = config.fullattBlockIndexes
 400 | 
 401 |             self._patchEmbed.wrappedValue = QwenVL.PatchEmbed(
 402 |                 patchSize: config.patchSize,
 403 |                 temporalPatchSize: config.temporalPatchSize,
 404 |                 inChannels: config.inChannels,
 405 |                 hiddenSize: config.hiddenSize)
 406 | 
 407 |             let headDimensions = config.hiddenSize / config.numHeads
 408 |             self._rotaryPositionEmbedding.wrappedValue = QwenVL.VisionRotaryEmbedding(
 409 |                 dimensions: headDimensions / 2, theta: 10_000)
 410 | 
 411 |             self._blocks.wrappedValue = (0 ..< config.depth).map { _ in
 412 |                 Qwen25VLVisionBlock(config)
 413 |             }
 414 |             self._patchMerger.wrappedValue = PatchMerger(
 415 |                 dimensions: config.outHiddenSize, contextDimensions: config.hiddenSize,
 416 |                 spatialMergeSize: config.spatialMergeSize)
 417 |         }
 418 | 
 419 |         func rotaryPositionEmbedding(_ frames: [THW]) -> MLXArray {
 420 |             var positionIds = [MLXArray]()
 421 | 
 422 |             for row in frames {
 423 |                 let (t, h, w) = row.values
 424 | 
 425 |                 var hposIds = expandedDimensions(MLXArray(0 ..< h), axis: 1)
 426 |                 hposIds = repeated(hposIds, count: w, axis: 1)
 427 |                 hposIds =
 428 |                     hposIds
 429 |                     .reshaped(
 430 |                         h / spatialMergeSize,
 431 |                         spatialMergeSize,
 432 |                         w / spatialMergeSize,
 433 |                         spatialMergeSize
 434 |                     )
 435 |                     .transposed(0, 2, 1, 3)
 436 |                     .flattened()
 437 | 
 438 |                 var wposIds = expandedDimensions(MLXArray(0 ..< w), axis: 0)
 439 |                 wposIds = repeated(wposIds, count: h, axis: 0)
 440 |                 wposIds =
 441 |                     wposIds
 442 |                     .reshaped(
 443 |                         h / spatialMergeSize,
 444 |                         spatialMergeSize,
 445 |                         w / spatialMergeSize,
 446 |                         spatialMergeSize
 447 |                     )
 448 |                     .transposed(0, 2, 1, 3)
 449 |                     .flattened()
 450 | 
 451 |                 let stackedPosIds = stacked([hposIds, wposIds], axis: -1)
 452 |                 positionIds.append(tiled(stackedPosIds, repetitions: [t, 1]))
 453 |             }
 454 | 
 455 |             let indices = concatenated(positionIds, axis: 0)
 456 |             let maxFrameSize = frames.lazy.map { max($0.h, $0.w) }.max() ?? 0
 457 |             let rotaryPositionEmbedFull = rotaryPositionEmbedding(sequenceLength: maxFrameSize)[
 458 |                 indices]
 459 | 
 460 |             return rotaryPositionEmbedFull.reshaped(indices.dim(0), -1)
 461 |         }
 462 | 
 463 |         func getWindowIndex(_ frames: [THW]) -> (MLXArray, MLXArray) {
 464 |             var windowIndex = [MLXArray]()
 465 |             var cuWindowSeqlens = [0]
 466 |             var windowIndexId = 0
 467 |             let vitMergerWindowSize = windowSize / spatialMergeSize / patchSize
 468 | 
 469 |             for frame in frames {
 470 |                 let (gridT, gridH, gridW) = frame.values
 471 |                 let llmGridH = gridH / spatialMergeSize
 472 |                 let llmGridW = gridW / spatialMergeSize
 473 | 
 474 |                 let index = MLXArray(0 ..< (gridT * llmGridH * llmGridW)).reshaped(
 475 |                     gridT, llmGridH, llmGridW)
 476 | 
 477 |                 let padH = vitMergerWindowSize - llmGridH % vitMergerWindowSize
 478 |                 let padW = vitMergerWindowSize - llmGridW % vitMergerWindowSize
 479 |                 let numWindowsH = (llmGridH + padH) / vitMergerWindowSize
 480 |                 let numWindowsW = (llmGridW + padW) / vitMergerWindowSize
 481 | 
 482 |                 // Pad the index
 483 |                 let indexPadded = padded(
 484 |                     index,
 485 |                     widths: [[0, 0], [0, padH], [0, padW]],
 486 |                     mode: .constant,
 487 |                     value: MLXArray(-100)
 488 |                 )
 489 | 
 490 |                 // Reshape and transpose
 491 |                 let indexReshaped = indexPadded.reshaped(
 492 |                     gridT,
 493 |                     numWindowsH,
 494 |                     vitMergerWindowSize,
 495 |                     numWindowsW,
 496 |                     vitMergerWindowSize
 497 |                 )
 498 | 
 499 |                 let indexTransposed = indexReshaped.transposed(0, 1, 3, 2, 4).reshaped(
 500 |                     gridT,
 501 |                     numWindowsH * numWindowsW,
 502 |                     vitMergerWindowSize,
 503 |                     vitMergerWindowSize
 504 |                 )
 505 | 
 506 |                 // Calculate sequence lengths
 507 |                 let seqlens = sum(indexTransposed .!= -100, axes: [2, 3]).reshaped(-1)
 508 | 
 509 |                 // Get valid indices
 510 |                 let indexFlattened = indexTransposed.flattened()
 511 |                 let validIndices = indexFlattened.asArray(Int.self).enumerated()
 512 |                     .filter { $0.element != -100 }
 513 |                     .map { $0.offset }
 514 | 
 515 |                 let validValues = indexFlattened[MLXArray(validIndices)]
 516 | 
 517 |                 // Add to window index
 518 |                 windowIndex.append(validValues + windowIndexId)
 519 | 
 520 |                 // Update cumulative sequence lengths
 521 |                 let cuSeqlensTmp =
 522 |                     cumsum(seqlens, axis: 0) * spatialMergeUnit + cuWindowSeqlens.last!
 523 |                 cuWindowSeqlens.append(contentsOf: cuSeqlensTmp.asArray(Int.self))
 524 | 
 525 |                 windowIndexId += gridT * llmGridH * llmGridW
 526 |             }
 527 | 
 528 |             // Concatenate all window indices
 529 |             let combinedWindowIndex = concatenated(windowIndex, axis: 0)
 530 |             let cuWindowSeqlensArray = MLXArray(cuWindowSeqlens)
 531 | 
 532 |             // Get unique values in cuWindowSeqlens
 533 |             var seen = Set<Int>()
 534 |             var uniqueIndices = [Int]()
 535 | 
 536 |             for (i, value) in cuWindowSeqlens.enumerated() {
 537 |                 if !seen.contains(value) {
 538 |                     seen.insert(value)
 539 |                     uniqueIndices.append(i)
 540 |                 }
 541 |             }
 542 | 
 543 |             let uniqueCuWindowSeqlens = cuWindowSeqlensArray[MLXArray(uniqueIndices)]
 544 | 
 545 |             return (combinedWindowIndex, uniqueCuWindowSeqlens)
 546 |         }
 547 | 
 548 |         func attentionMask(sequenceLength: Int, cuSeqlens: MLXArray) -> MLXArray {
 549 |             // Create attention mask
 550 |             let attentionMask = full(
 551 |                 [1, sequenceLength, sequenceLength],
 552 |                 values: false)
 553 | 
 554 |             // Update mask for each sequence
 555 |             let cuSeqlens = cuSeqlens.asArray(Int.self)
 556 |             for i in 1 ..< cuSeqlens.count {
 557 |                 let start = cuSeqlens[i - 1]
 558 |                 let end = cuSeqlens[i]
 559 |                 attentionMask[0..., start ..< end, start ..< end] = MLXArray(true)
 560 |             }
 561 | 
 562 |             return attentionMask
 563 |         }
 564 | 
 565 |         public func callAsFunction(_ hiddenStates: MLXArray, frames: [THW]) -> MLXArray {
 566 |             var hiddenStates = patchEmbed(hiddenStates)
 567 |             let rotaryPosEmb = rotaryPositionEmbedding(frames)
 568 | 
 569 |             // Get window indices and sequence lengths
 570 |             let (windowIndex, cuWindowSeqlens) = getWindowIndex(frames)
 571 | 
 572 |             // prepare attention masks
 573 |             let seqLen = hiddenStates.dim(0)
 574 |             var cuSeqlens = [0]
 575 |             for frame in frames {
 576 |                 let seqLen = frame.h * frame.w
 577 |                 cuSeqlens.append(
 578 |                     contentsOf: Array(repeating: seqLen, count: frame.t).map {
 579 |                         cuSeqlens.last! + $0
 580 |                     })
 581 |             }
 582 |             let cuSeqlensArray = MLXArray(cuSeqlens)
 583 | 
 584 |             let fullAttentionMask = attentionMask(sequenceLength: seqLen, cuSeqlens: cuSeqlensArray)
 585 |             let windowAttentionMask = attentionMask(
 586 |                 sequenceLength: seqLen, cuSeqlens: cuWindowSeqlens)
 587 | 
 588 |             // Reshape and reindex hidden states
 589 |             hiddenStates = hiddenStates.reshaped(seqLen / spatialMergeUnit, spatialMergeUnit, -1)
 590 |             hiddenStates = hiddenStates[windowIndex, 0..., 0...]
 591 |             hiddenStates = hiddenStates.reshaped(seqLen, -1)
 592 | 
 593 |             // Reshape and reindex rotary position embeddings
 594 |             var rotaryPosEmbReshaped = rotaryPosEmb.reshaped(
 595 |                 seqLen / spatialMergeUnit, spatialMergeUnit, -1)
 596 |             rotaryPosEmbReshaped = rotaryPosEmbReshaped[windowIndex, 0..., 0...]
 597 |             rotaryPosEmbReshaped = rotaryPosEmbReshaped.reshaped(seqLen, -1)
 598 | 
 599 |             // Process through blocks
 600 |             for (i, block) in blocks.enumerated() {
 601 |                 // Use full attention for specific blocks, window attention for others
 602 |                 let attentionMask =
 603 |                     fullattBlockIndexes.contains(i) ? fullAttentionMask : windowAttentionMask
 604 | 
 605 |                 hiddenStates = block(
 606 |                     hiddenStates,
 607 |                     attentionMask: attentionMask,
 608 |                     rotaryPositionEmbedding: rotaryPosEmbReshaped
 609 |                 )
 610 |             }
 611 | 
 612 |             // Apply patch merger
 613 |             hiddenStates = patchMerger(hiddenStates)
 614 | 
 615 |             // Reorder back to original sequence
 616 |             let reverseIndices = argSort(windowIndex, axis: 0)
 617 |             hiddenStates = hiddenStates[reverseIndices, 0...]
 618 | 
 619 |             return hiddenStates
 620 |         }
 621 | 
 622 |         private func isMLXWeight(_ array: MLXArray) -> Bool {
 623 |             if array.ndim != 4, array.ndim != 5 {
 624 |                 return false
 625 |             }
 626 | 
 627 |             if array.dim(-1) == 3 {
 628 |                 return true
 629 |             }
 630 | 
 631 |             let (outChannels, kH, kW) = (array.dim(1), array.dim(2), array.dim(3))
 632 |             return outChannels >= kH && outChannels >= kW && kH == kW
 633 |         }
 634 | 
 635 |         func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {
 636 |             var sanitizedWeights = [String: MLXArray]()
 637 | 
 638 |             for (k, v) in weights {
 639 |                 if k.contains("position_id") {
 640 |                     // Remove unused position_ids
 641 |                     continue
 642 |                 } else if k.contains("patch_embed.proj.weight") {
 643 |                     // PyTorch conv2d weight tensors have shape:
 644 |                     //   [B, out_channels, in_channels, kH, KW]
 645 |                     // MLX conv2d expects the weight be of shape:
 646 |                     //   [B, out_channels, kH, KW, in_channels]
 647 |                     if isMLXWeight(v) {
 648 |                         sanitizedWeights[k] = v
 649 |                     } else {
 650 |                         sanitizedWeights[k] = v.transposed(0, 2, 3, 4, 1)
 651 |                     }
 652 |                 } else {
 653 |                     sanitizedWeights[k] = v
 654 |                 }
 655 |             }
 656 | 
 657 |             return sanitizedWeights
 658 |         }
 659 |     }
 660 | }
 661 | 
 662 | // MARK: - Processor
 663 | 
 664 | /// Qwen2.5VL VLM `UserInputProcessor`.
 665 | ///
 666 | /// This is meant to be used with ``Qwen25VL`` and is typically created by ``VLMModelFactory``.
 667 | public class Qwen25VLProcessor: UserInputProcessor {
 668 |     private let config: Qwen25VLProcessorConfiguration
 669 |     private let tokenizer: any Tokenizer
 670 | 
 671 |     public init(_ config: Qwen25VLProcessorConfiguration, tokenizer: any Tokenizer) {
 672 |         self.config = config
 673 |         self.tokenizer = tokenizer
 674 |     }
 675 | 
 676 |     func preprocess(image: CIImage, resizedSize: CGSize) -> CIImage {
 677 |         image
 678 |             .toSRGB()
 679 |             .resampled(to: resizedSize, method: .bicubic)
 680 |             .normalized(mean: config.imageMeanTuple, std: config.imageStdTuple)
 681 |     }
 682 | 
 683 |     public func preprocess(images: [CIImage], processing: UserInput.Processing?) throws -> (
 684 |         MLXArray, THW
 685 |     ) {
 686 |         // First apply the user requested resizing, etc. if any
 687 |         let images = images.map { MediaProcessing.apply($0, processing: processing) }
 688 | 
 689 |         // image_processing_qwen2_vl._preprocess
 690 |         let size = images[0].extent.size
 691 |         let (resizedHeight, resizedWidth) = try QwenVL.targetSize(
 692 |             height: Int(size.height), width: Int(size.width),
 693 |             factor: config.patchSize * config.mergeSize,
 694 |             minPixels: config.size.minPixels, maxPixels: config.size.maxPixels)
 695 |         let resizedSize = CGSize(width: resizedWidth, height: resizedHeight)
 696 | 
 697 |         // Process images
 698 |         let processedImages =
 699 |             try images
 700 |             .map {
 701 |                 MediaProcessing.inSRGBToneCurveSpace($0)
 702 |             }
 703 |             .map {
 704 |                 return try MediaProcessing.resampleBicubic($0, to: resizedSize)
 705 |             }
 706 |             .map {
 707 |                 MediaProcessing.normalize(
 708 |                     $0, mean: config.imageMeanTuple, std: config.imageStdTuple)
 709 |             }
 710 |             .map {
 711 |                 MediaProcessing.asMLXArray($0)
 712 |             }
 713 | 
 714 |         return try QwenVL.patchify(
 715 |             images: processedImages, mergeSize: config.mergeSize, patchSize: config.patchSize,
 716 |             temporalPatchSize: config.temporalPatchSize)
 717 |     }
 718 | 
 719 |     public func prepare(input: UserInput) async throws -> LMInput {
 720 |         let messages = Qwen2VLMessageGenerator().generate(from: input)
 721 | 
 722 |         var promptTokens = try tokenizer.applyChatTemplate(messages: messages)
 723 | 
 724 |         // Text-only input
 725 |         if input.images.isEmpty, input.videos.isEmpty {
 726 |             return LMInput(tokens: MLXArray(promptTokens))
 727 |         }
 728 | 
 729 |         // Process images if any
 730 |         var processedImage: LMInput.ProcessedImage?
 731 |         if !input.images.isEmpty {
 732 |             let imagePixelsAndFrames = try input.images.map {
 733 |                 try preprocess(images: [$0.asCIImage()], processing: input.processing)
 734 |             }
 735 |             let imagePixelsConcatenated = concatenated(imagePixelsAndFrames.map { $0.0 })
 736 |             processedImage = LMInput.ProcessedImage(
 737 |                 pixels: imagePixelsConcatenated, frames: imagePixelsAndFrames.map { $0.1 })
 738 | 
 739 |             if let imageFrames = processedImage?.frames {
 740 |                 promptTokens = try QwenVL.replacePaddingTokens(
 741 |                     in: promptTokens, frames: imageFrames, paddingToken: "<|image_pad|>",
 742 |                     mergeSize: config.mergeSize, tokenizer: tokenizer)
 743 |             }
 744 |         }
 745 | 
 746 |         // Process videos if any
 747 |         var processedVideo: LMInput.ProcessedVideo?
 748 |         if !input.videos.isEmpty {
 749 |             var videosAsImageSequences = [[MLXArray]]()
 750 |             var resizedSize: CGSize = .zero
 751 |             for video in input.videos {
 752 |                 let imageSequence = try await MediaProcessing.asProcessedSequence(
 753 |                     video.asAVAsset(), samplesPerSecond: 2
 754 |                 ) { frame in
 755 |                     // first apply the user requested resizing, etc. if any
 756 |                     let resizedImage = MediaProcessing.apply(
 757 |                         frame.frame, processing: input.processing)
 758 |                     if resizedSize == .zero {
 759 |                         let size = resizedImage.extent.size
 760 |                         let (resizedHeight, resizedWidth) = try QwenVL.targetSize(
 761 |                             height: Int(size.height), width: Int(size.width),
 762 |                             factor: config.patchSize * config.mergeSize,
 763 |                             minPixels: config.minPixels, maxPixels: config.maxPixels)
 764 |                         resizedSize = CGSize(width: resizedWidth, height: resizedHeight)
 765 |                     }
 766 |                     let processedImage = preprocess(image: resizedImage, resizedSize: resizedSize)
 767 |                     return VideoFrame(frame: processedImage, timeStamp: frame.timeStamp)
 768 |                 }
 769 |                 videosAsImageSequences.append(imageSequence.frames)
 770 |             }
 771 |             let videoPixelsAndFrames = try videosAsImageSequences.map {
 772 |                 try QwenVL.patchify(
 773 |                     images: $0, mergeSize: config.mergeSize, patchSize: config.patchSize,
 774 |                     temporalPatchSize: config.temporalPatchSize)
 775 |             }
 776 |             let videoPixelsConcatenated = concatenated(videoPixelsAndFrames.map { $0.0 })
 777 |             processedVideo = LMInput.ProcessedVideo(
 778 |                 pixels: videoPixelsConcatenated, frames: videoPixelsAndFrames.map { $0.1 })
 779 |             if let videoFrames = processedVideo?.frames {
 780 |                 promptTokens = try QwenVL.replacePaddingTokens(
 781 |                     in: promptTokens, frames: videoFrames, paddingToken: "<|video_pad|>",
 782 |                     mergeSize: config.mergeSize, tokenizer: tokenizer)
 783 |             }
 784 |         }
 785 | 
 786 |         let promptArray = MLXArray(promptTokens).expandedDimensions(axis: 0)
 787 |         let mask = ones(like: promptArray).asType(.int8)
 788 |         return LMInput(
 789 |             text: .init(tokens: promptArray, mask: mask),
 790 |             image: processedImage,
 791 |             video: processedVideo)
 792 |     }
 793 | }
 794 | 
 795 | // MARK: - Model
 796 | 
 797 | /// Qwen2.5VL VLM
 798 | ///
 799 | /// This is typically created by ``VLMModelFactory``.
 800 | public class Qwen25VL: Module, VLMModel, KVCacheDimensionProvider {
 801 | 
 802 |     @ModuleInfo(key: "vision_tower") private var visionModel: Vision.VisionModel
 803 |     @ModuleInfo(key: "language_model") private var languageModel: Language.LanguageModel
 804 | 
 805 |     public let config: Qwen25VLConfiguration
 806 | 
 807 |     public var vocabularySize: Int { config.baseConfiguration.vocabularySize }
 808 |     public var kvHeads: [Int] { languageModel.kvHeads }
 809 | 
 810 |     public func loraLinearLayers() -> MLXLMCommon.LoRALinearLayers {
 811 |         languageModel.model.layers.map { ($0.attention, ["q_proj", "v_proj"]) }
 812 |     }
 813 | 
 814 |     public init(_ config: Qwen25VLConfiguration) {
 815 |         self.config = config
 816 |         self._visionModel.wrappedValue = Vision.VisionModel(config.visionConfiguration)
 817 |         self._languageModel.wrappedValue = Language.LanguageModel(config.textConfiguration)
 818 |     }
 819 | 
 820 |     private func inputEmbeddings(inputIds: MLXArray, pixelValues: MLXArray?, frames: [THW]?)
 821 |         -> MLXArray
 822 |     {
 823 |         guard let pixelValues, let frames else {
 824 |             return languageModel.model.embedTokens(inputIds[.newAxis, .ellipsis])
 825 |         }
 826 | 
 827 |         // Get the input embeddings from the language model
 828 |         let inputEmbeds = languageModel.model.embedTokens(inputIds)
 829 | 
 830 |         // Get the ouptut hidden states from the vision model
 831 |         var hiddenStates = self.visionModel(pixelValues, frames: frames)
 832 | 
 833 |         if hiddenStates.ndim == 2 {
 834 |             hiddenStates = hiddenStates[.newAxis, 0..., 0...]
 835 |         }
 836 | 
 837 |         // Insert special image tokens in the input_ids
 838 |         return QwenVL.mergeInputIdsWithImageFeatures(
 839 |             inputIds: inputIds, inputEmbeds: inputEmbeds, imageFeatures: hiddenStates,
 840 |             imageTokenId: config.baseConfiguration.imageTokenId,
 841 |             videoTokenId: config.baseConfiguration.videoTokenId)
 842 |     }
 843 | 
 844 |     public func prepare(_ input: LMInput, cache: [any KVCache], windowSize: Int?) throws
 845 |         -> PrepareResult
 846 |     {
 847 |         let dtype = visionModel.patchEmbed.proj.weight.dtype
 848 | 
 849 |         // Process both images and videos together
 850 |         var allPixels: MLXArray?
 851 |         var allFrames: [THW] = []
 852 | 
 853 |         if let imagePixels = input.image?.pixels, let imageFrames = input.image?.frames {
 854 |             allPixels = imagePixels.asType(dtype)
 855 |             allFrames.append(contentsOf: imageFrames)
 856 |         }
 857 | 
 858 |         if let videoPixels = input.video?.pixels, let videoFrames = input.video?.frames {
 859 |             if allPixels == nil {
 860 |                 allPixels = videoPixels.asType(dtype)
 861 |             } else {
 862 |                 allPixels = concatenated([allPixels!, videoPixels.asType(dtype)])
 863 |             }
 864 |             allFrames.append(contentsOf: videoFrames)
 865 |         }
 866 | 
 867 |         let inputEmbeddings = self.inputEmbeddings(
 868 |             inputIds: input.text.tokens, pixelValues: allPixels,
 869 |             frames: allFrames.isEmpty ? nil : allFrames)
 870 | 
 871 |         let result = languageModel(nil, cache: cache, inputEmbedding: inputEmbeddings)
 872 | 
 873 |         return .logits(result)
 874 |     }
 875 | 
 876 |     public func callAsFunction(_ inputs: MLXArray, cache: [any KVCache]?) -> MLXArray {
 877 |         languageModel(inputs, cache: cache).logits
 878 |     }
 879 | 
 880 |     public func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {
 881 |         visionModel.sanitize(
 882 |             weights:
 883 |                 Dictionary(
 884 |                     uniqueKeysWithValues: weights.map { key, value in
 885 |                         var key = key
 886 |                         if !key.contains("vision_tower") {
 887 |                             key = key.replacingOccurrences(of: "visual", with: "vision_tower")
 888 |                         }
 889 |                         if !key.contains("language_model") {
 890 |                             key = key.replacingOccurrences(
 891 |                                 of: "model", with: "language_model.model")
 892 |                             key = key.replacingOccurrences(
 893 |                                 of: "lm_head", with: "language_model.lm_head")
 894 |                         }
 895 | 
 896 |                         return (key, value)
 897 |                     })
 898 |         )
 899 |     }
 900 | }
 901 | 
 902 | // MARK: - Configuration
 903 | 
 904 | /// Configuration for ``Qwen25VL``
 905 | public struct Qwen25VLConfiguration: Codable, Sendable {
 906 | 
 907 |     public struct TextConfiguration: Codable, Sendable {
 908 |         public let modelType: String
 909 |         public let hiddenSize: Int
 910 |         public let hiddenLayers: Int
 911 |         public let intermediateSize: Int
 912 |         public let attentionHeads: Int
 913 |         private let _rmsNormEps: Float?
 914 |         public var rmsNormEps: Float { _rmsNormEps ?? 1e-6 }
 915 |         public let vocabularySize: Int
 916 |         public let kvHeads: Int
 917 |         private let _maxPositionEmbeddings: Int?
 918 |         public var maxPositionEmbeddings: Int { _maxPositionEmbeddings ?? 128000 }
 919 |         private let _ropeTheta: Float?
 920 |         public var ropeTheta: Float { _ropeTheta ?? 1_000_000 }
 921 |         private let _ropeTraditional: Bool?
 922 |         public var ropeTraditional: Bool { _ropeTraditional ?? false }
 923 |         public let ropeScaling: [String: StringOrNumber]?
 924 |         private let _tieWordEmbeddings: Bool?
 925 |         public var tieWordEmbeddings: Bool { _tieWordEmbeddings ?? true }
 926 |         private let _slidingWindow: Int?
 927 |         public var slidingWindow: Int { _slidingWindow ?? 32768 }
 928 |         private let _useSlidingWindow: Bool?
 929 |         public var useSlidingWindow: Bool { _useSlidingWindow ?? false }
 930 | 
 931 |         enum CodingKeys: String, CodingKey {
 932 |             case modelType = "model_type"
 933 |             case hiddenSize = "hidden_size"
 934 |             case hiddenLayers = "num_hidden_layers"
 935 |             case intermediateSize = "intermediate_size"
 936 |             case attentionHeads = "num_attention_heads"
 937 |             case _rmsNormEps = "rms_norm_eps"
 938 |             case vocabularySize = "vocab_size"
 939 |             case kvHeads = "num_key_value_heads"
 940 |             case _maxPositionEmbeddings = "max_position_embeddings"
 941 |             case _ropeTheta = "rope_theta"
 942 |             case _ropeTraditional = "rope_traditional"
 943 |             case ropeScaling = "rope_scaling"
 944 |             case _tieWordEmbeddings = "tie_word_embeddings"
 945 |             case _slidingWindow = "sliding_window"
 946 |             case _useSlidingWindow = "use_sliding_window"
 947 |         }
 948 |     }
 949 | 
 950 |     public struct VisionConfiguration: Codable, Sendable {
 951 |         public let depth: Int
 952 |         public let hiddenSize: Int
 953 |         public let intermediateSize: Int
 954 |         public let outHiddenSize: Int
 955 |         public let numHeads: Int
 956 |         public let patchSize: Int
 957 |         private let _inChans: Int?
 958 |         public var inChannels: Int { _inChans ?? 3 }
 959 |         private let _layerNormEps: Float?
 960 |         public var layerNormEps: Float { _layerNormEps ?? 1e-6 }
 961 |         public let spatialPatchSize: Int
 962 |         public let spatialMergeSize: Int
 963 |         public let temporalPatchSize: Int
 964 |         public let windowSize: Int
 965 |         public let fullattBlockIndexes: [Int]
 966 |         public let tokensPerSecond: Int
 967 |         private let _skipVision: Bool?
 968 |         public var skipVision: Bool { _skipVision ?? false }
 969 |         private let _hiddenAct: String?
 970 |         public var hiddenAct: String { _hiddenAct ?? "silu" }
 971 | 
 972 |         enum CodingKeys: String, CodingKey {
 973 |             case depth
 974 |             case hiddenSize = "hidden_size"
 975 |             case intermediateSize = "intermediate_size"
 976 |             case outHiddenSize = "out_hidden_size"
 977 |             case numHeads = "num_heads"
 978 |             case patchSize = "patch_size"
 979 |             case _inChans = "in_chans"
 980 |             case _layerNormEps = "layer_norm_eps"  // Added this line
 981 |             case spatialPatchSize = "spatial_patch_size"
 982 |             case spatialMergeSize = "spatial_merge_size"
 983 |             case temporalPatchSize = "temporal_patch_size"
 984 |             case windowSize = "window_size"
 985 |             case fullattBlockIndexes = "fullatt_block_indexes"
 986 |             case tokensPerSecond = "tokens_per_second"
 987 |             case _skipVision = "skip_vision"
 988 |             case _hiddenAct = "hidden_act"
 989 |         }
 990 |     }
 991 | 
 992 |     public struct BaseConfiguration: Codable, Sendable {
 993 |         public let modelType: String
 994 |         public let vocabularySize: Int
 995 |         public let imageTokenId: Int
 996 |         public let videoTokenId: Int
 997 |         public let visionStartTokenId: Int
 998 |         public let visionEndTokenId: Int
 999 |         public let visionTokenId: Int
1000 |         public let hiddenSize: Int
1001 |         public let numAttentionHeads: Int
1002 |         public let numHiddenLayers: Int
1003 |         public let intermediateSize: Int
1004 |         public let numKeyValueHeads: Int
1005 |         public let slidingWindow: Int
1006 |         public let useSlidingWindow: Bool
1007 |         public let maxWindowLayers: Int
1008 | 
1009 |         enum CodingKeys: String, CodingKey {
1010 |             case modelType = "model_type"
1011 |             case vocabularySize = "vocab_size"
1012 |             case imageTokenId = "image_token_id"
1013 |             case videoTokenId = "video_token_id"
1014 |             case visionStartTokenId = "vision_start_token_id"
1015 |             case visionEndTokenId = "vision_end_token_id"
1016 |             case visionTokenId = "vision_token_id"
1017 |             case hiddenSize = "hidden_size"
1018 |             case numAttentionHeads = "num_attention_heads"
1019 |             case numHiddenLayers = "num_hidden_layers"
1020 |             case intermediateSize = "intermediate_size"
1021 |             case numKeyValueHeads = "num_key_value_heads"
1022 |             case slidingWindow = "sliding_window"
1023 |             case useSlidingWindow = "use_sliding_window"
1024 |             case maxWindowLayers = "max_window_layers"
1025 |         }
1026 |     }
1027 | 
1028 |     public let textConfiguration: TextConfiguration
1029 |     public let visionConfiguration: VisionConfiguration
1030 |     public let baseConfiguration: BaseConfiguration
1031 | 
1032 |     enum CodingKeys: String, CodingKey {
1033 |         case visionConfiguration = "vision_config"
1034 |     }
1035 | 
1036 |     public init(from decoder: any Swift.Decoder) throws {
1037 |         let container = try decoder.container(keyedBy: CodingKeys.self)
1038 | 
1039 |         // this is a sub-dictionary
1040 |         self.visionConfiguration = try container.decode(
1041 |             VisionConfiguration.self, forKey: .visionConfiguration)
1042 | 
1043 |         // these are overlaid in the top level
1044 |         self.textConfiguration = try TextConfiguration(from: decoder)
1045 |         self.baseConfiguration = try BaseConfiguration(from: decoder)
1046 |     }
1047 | }
1048 | 
1049 | /// Configuration for ``Qwen25VLProcessor``
1050 | public struct Qwen25VLProcessorConfiguration: Codable, Sendable {
1051 |     public struct Size: Codable, Sendable {
1052 |         public let maxPixels: Int
1053 |         public let minPixels: Int
1054 | 
1055 |         enum CodingKeys: String, CodingKey {
1056 |             case maxPixels = "max_pixels"
1057 |             case minPixels = "min_pixels"
1058 |         }
1059 |     }
1060 | 
1061 |     public let imageMean: [CGFloat]
1062 |     public let imageStd: [CGFloat]
1063 |     public let minPixels: Int
1064 |     public let maxPixels: Int
1065 |     public let mergeSize: Int
1066 |     public let patchSize: Int
1067 |     public let temporalPatchSize: Int
1068 |     public let imageProcessorType: String
1069 | 
1070 |     public var imageMeanTuple: (CGFloat, CGFloat, CGFloat) {
1071 |         (imageMean[0], imageMean[1], imageMean[2])
1072 |     }
1073 |     public var imageStdTuple: (CGFloat, CGFloat, CGFloat) {
1074 |         (imageStd[0], imageStd[1], imageStd[2])
1075 |     }
1076 | 
1077 |     public var size: Size {
1078 |         Size(maxPixels: maxPixels, minPixels: minPixels)
1079 |     }
1080 | 
1081 |     enum CodingKeys: String, CodingKey {
1082 |         case imageMean = "image_mean"
1083 |         case imageStd = "image_std"
1084 |         case minPixels = "min_pixels"
1085 |         case maxPixels = "max_pixels"
1086 |         case mergeSize = "merge_size"
1087 |         case patchSize = "patch_size"
1088 |         case temporalPatchSize = "temporal_patch_size"
1089 |         case imageProcessorType = "image_processor_type"
1090 |     }
1091 | }
1092 | 


--------------------------------------------------------------------------------
/Libraries/MLXVLM/Models/Qwen2VL.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | // port of https://github.com/Blaizzy/mlx-vlm/tree/main/mlx_vlm/models/qwen2_vl
  4 | 
  5 | import CoreImage
  6 | import Foundation
  7 | import Hub
  8 | import MLX
  9 | import MLXLMCommon
 10 | import MLXNN
 11 | import Tokenizers
 12 | 
 13 | // MARK: - Language
 14 | 
 15 | private enum Language {
 16 | 
 17 |     /// Applies Rotary Position Embedding with Multimodal Sections to the query and key tensors
 18 |     static private func applyMultimodalRotaryPositionEmbedding(
 19 |         q: MLXArray, k: MLXArray, cos: MLXArray, sin: MLXArray,
 20 |         positionIds: MLXArray, mropeSection: [Int]
 21 |     ) -> (MLXArray, MLXArray) {
 22 |         var cos = cos[positionIds]
 23 |         var sin = sin[positionIds]
 24 | 
 25 |         cos =
 26 |             concatenated(
 27 |                 // [m[i % 3] for i, m in enumerate(mx.split(cos, mrope_section, axis=-1))]
 28 |                 split(cos, indices: mropeSection, axis: -1).enumerated().map { i, m in m[i % 3] },
 29 |                 axis: -1
 30 |             )[0..., .newAxis, 0..., 0...]
 31 | 
 32 |         sin =
 33 |             concatenated(
 34 |                 split(sin, indices: mropeSection, axis: -1).enumerated().map { i, m in m[i % 3] },
 35 |                 axis: -1
 36 |             )[0..., .newAxis, 0..., 0...]
 37 | 
 38 |         // Apply rotary embedding
 39 |         let qEmbed = (q * cos) + (QwenVL.rotateHalf(q) * sin)
 40 |         let kEmbed = (k * cos) + (QwenVL.rotateHalf(k) * sin)
 41 |         return (qEmbed, kEmbed)
 42 |     }
 43 | 
 44 |     fileprivate class Attention: Module {
 45 | 
 46 |         let heads: Int
 47 |         let kvHeads: Int
 48 |         let headDim: Int
 49 |         let scale: Float
 50 |         let mropeSection: [Int]
 51 | 
 52 |         @ModuleInfo(key: "q_proj") var wq: Linear
 53 |         @ModuleInfo(key: "k_proj") var wk: Linear
 54 |         @ModuleInfo(key: "v_proj") var wv: Linear
 55 |         @ModuleInfo(key: "o_proj") var wo: Linear
 56 | 
 57 |         @ModuleInfo(key: "rotary_emb") var rotaryEmbedding: RoPE
 58 | 
 59 |         public init(_ args: Qwen2VLConfiguration.TextConfiguration) {
 60 |             let dim = args.hiddenSize
 61 |             self.heads = args.attentionHeads
 62 |             self.kvHeads = args.kvHeads
 63 |             self.headDim = dim / heads
 64 |             self.scale = pow(Float(headDim), -0.5)
 65 | 
 66 |             self._wq.wrappedValue = Linear(dim, heads * headDim, bias: true)
 67 |             self._wk.wrappedValue = Linear(dim, kvHeads * headDim, bias: true)
 68 |             self._wv.wrappedValue = Linear(dim, kvHeads * headDim, bias: true)
 69 |             self._wo.wrappedValue = Linear(heads * headDim, dim, bias: false)
 70 | 
 71 |             if let v = args.ropeScaling?["mrope_section"], let array = v.asInts() {
 72 |                 // mrope_section = np.cumsum(mrope_section * 2)[:-1].tolist()
 73 |                 self.mropeSection = sequence(state: (0, array.makeIterator())) { state in
 74 |                     if let v = state.1.next() {
 75 |                         // note the *2
 76 |                         state.0 += v * 2
 77 |                         return state.0
 78 |                     } else {
 79 |                         return nil
 80 |                     }
 81 |                 }.dropLast()
 82 |             } else {
 83 |                 fatalError("rope_scaling['mrope_section'] must be an array of integers")
 84 |             }
 85 | 
 86 |             self._rotaryEmbedding.wrappedValue = RoPE(
 87 |                 dimensions: headDim, traditional: args.ropeTraditional, base: args.ropeTheta)
 88 |         }
 89 | 
 90 |         public func callAsFunction(
 91 |             _ x: MLXArray, mask: MLXArray? = nil, cache: KVCache?
 92 |         ) -> MLXArray {
 93 |             let (B, L) = (x.dim(0), x.dim(1))
 94 | 
 95 |             var queries = wq(x)
 96 |             var keys = wk(x)
 97 |             var values = wv(x)
 98 | 
 99 |             // prepare the queries, keys and values for the attention computation
100 |             queries = queries.reshaped(B, L, heads, headDim).transposed(0, 2, 1, 3)
101 |             keys = keys.reshaped(B, L, kvHeads, headDim).transposed(0, 2, 1, 3)
102 |             values = values.reshaped(B, L, kvHeads, headDim).transposed(0, 2, 1, 3)
103 | 
104 |             let offset = cache?.offset ?? 0
105 |             queries = rotaryEmbedding(queries, offset: offset)
106 |             keys = rotaryEmbedding(keys, offset: offset)
107 | 
108 |             if let cache {
109 |                 (keys, values) = cache.update(keys: keys, values: values)
110 |             }
111 | 
112 |             let mask = mask?[.ellipsis, 0 ..< keys.dim(-2)]
113 | 
114 |             let output = MLXFast.scaledDotProductAttention(
115 |                 queries: queries, keys: keys, values: values, scale: scale, mask: mask
116 |             )
117 |             .transposed(0, 2, 1, 3)
118 |             .reshaped(B, L, -1)
119 | 
120 |             return wo(output)
121 |         }
122 |     }
123 | 
124 |     fileprivate class MLP: Module, UnaryLayer {
125 | 
126 |         @ModuleInfo(key: "gate_proj") var gate: Linear
127 |         @ModuleInfo(key: "down_proj") var down: Linear
128 |         @ModuleInfo(key: "up_proj") var up: Linear
129 | 
130 |         public init(dimensions: Int, hiddenDimensions: Int) {
131 |             self._gate.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)
132 |             self._down.wrappedValue = Linear(hiddenDimensions, dimensions, bias: false)
133 |             self._up.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)
134 |         }
135 | 
136 |         public func callAsFunction(_ x: MLXArray) -> MLXArray {
137 |             down(silu(gate(x)) * up(x))
138 |         }
139 |     }
140 | 
141 |     fileprivate class Qwen2VLDecoderLayer: Module {
142 | 
143 |         @ModuleInfo(key: "self_attn") var attention: Attention
144 |         let mlp: MLP
145 | 
146 |         @ModuleInfo(key: "input_layernorm") var inputLayerNorm: RMSNorm
147 |         @ModuleInfo(key: "post_attention_layernorm") var postAttentionLayerNorm: RMSNorm
148 | 
149 |         public init(_ args: Qwen2VLConfiguration.TextConfiguration) {
150 |             self._attention.wrappedValue = Attention(args)
151 |             self.mlp = MLP(dimensions: args.hiddenSize, hiddenDimensions: args.intermediateSize)
152 |             self._inputLayerNorm.wrappedValue = RMSNorm(
153 |                 dimensions: args.hiddenSize, eps: args.rmsNormEps)
154 |             self._postAttentionLayerNorm.wrappedValue = RMSNorm(
155 |                 dimensions: args.hiddenSize, eps: args.rmsNormEps)
156 |         }
157 | 
158 |         public func callAsFunction(
159 |             _ x: MLXArray, mask: MLXArray? = nil, cache: KVCache?
160 |         ) -> MLXArray {
161 |             var r = attention(inputLayerNorm(x), mask: mask, cache: cache)
162 |             let h = x + r
163 |             r = mlp(postAttentionLayerNorm(h))
164 |             let out = h + r
165 |             return out
166 |         }
167 |     }
168 | 
169 |     fileprivate class Qwen2Model: Module {
170 | 
171 |         @ModuleInfo(key: "embed_tokens") var embedTokens: Embedding
172 | 
173 |         fileprivate let layers: [Qwen2VLDecoderLayer]
174 |         fileprivate let norm: RMSNorm
175 | 
176 |         public init(_ args: Qwen2VLConfiguration.TextConfiguration) {
177 |             precondition(args.vocabularySize > 0)
178 | 
179 |             self._embedTokens.wrappedValue = Embedding(
180 |                 embeddingCount: args.vocabularySize, dimensions: args.hiddenSize)
181 | 
182 |             self.layers = (0 ..< args.hiddenLayers)
183 |                 .map { _ in
184 |                     Qwen2VLDecoderLayer(args)
185 |                 }
186 |             self.norm = RMSNorm(dimensions: args.hiddenSize, eps: args.rmsNormEps)
187 |         }
188 | 
189 |         public func callAsFunction(
190 |             _ inputs: MLXArray?, cache: [KVCache]? = nil, inputEmbedding: MLXArray? = nil
191 |         ) -> MLXArray {
192 |             var h: MLXArray
193 |             if let inputEmbedding {
194 |                 h = inputEmbedding
195 |             } else if let inputs {
196 |                 h = embedTokens(inputs)
197 |             } else {
198 |                 fatalError("one of inputs or inputEmbedding must be non-nil")
199 |             }
200 | 
201 |             let mask: MLXArray? = createAttentionMask(h: h, cache: cache)
202 | 
203 |             for (i, layer) in layers.enumerated() {
204 |                 h = layer(h, mask: mask, cache: cache?[i])
205 |             }
206 | 
207 |             return norm(h)
208 |         }
209 |     }
210 | 
211 |     fileprivate class LanguageModel: Module, KVCacheDimensionProvider {
212 |         @ModuleInfo var model: Qwen2Model
213 |         @ModuleInfo(key: "lm_head") var lmHead: Linear?
214 | 
215 |         var kvHeads: [Int]
216 | 
217 |         public init(_ args: Qwen2VLConfiguration.TextConfiguration) {
218 |             self.model = Qwen2Model(args)
219 | 
220 |             if !args.tieWordEmbeddings {
221 |                 _lmHead.wrappedValue = Linear(args.hiddenSize, args.vocabularySize, bias: false)
222 |             }
223 | 
224 |             self.kvHeads = (0 ..< args.hiddenLayers).map { _ in args.kvHeads }
225 |         }
226 | 
227 |         public func callAsFunction(
228 |             _ inputs: MLXArray?, cache: [KVCache]? = nil, inputEmbedding: MLXArray? = nil
229 |         ) -> LMOutput {
230 |             var out = model(inputs, cache: cache, inputEmbedding: inputEmbedding)
231 |             if let lmHead {
232 |                 out = lmHead(out)
233 |             } else {
234 |                 out = model.embedTokens.asLinear(out)
235 |             }
236 |             return LMOutput(logits: out)
237 |         }
238 |     }
239 | }
240 | 
241 | // MARK: - Vision
242 | 
243 | private enum Vision {
244 | 
245 |     static fileprivate func applyMultimodalRotaryPositionEmbedding(
246 |         _ tensor: MLXArray, freqs: MLXArray
247 |     ) -> MLXArray {
248 |         var cos = cos(freqs)
249 |         var sin = sin(freqs)
250 | 
251 |         cos = expandedDimensions(cos, axis: 1)
252 |         cos = tiled(cos, repetitions: [1, 1, 2])
253 |         cos = expandedDimensions(cos, axis: 0)
254 | 
255 |         sin = expandedDimensions(sin, axis: 1)
256 |         sin = tiled(sin, repetitions: [1, 1, 2])
257 |         sin = expandedDimensions(sin, axis: 0)
258 | 
259 |         let output = (tensor * cos) + (QwenVL.rotateHalf(tensor) * sin)
260 |         return output.asType(tensor.dtype)
261 |     }
262 | 
263 |     fileprivate class PatchMerger: Module, UnaryLayer {
264 |         let hiddenSize: Int
265 |         @ModuleInfo(key: "ln_q") var layerNormQ: LayerNorm
266 |         @ModuleInfo var mlp: (Linear, GELU, Linear)
267 | 
268 |         init(dimensions: Int, contextDimensions: Int, spatialMergeSize: Int) {
269 |             self.hiddenSize = contextDimensions * (spatialMergeSize * spatialMergeSize)
270 |             self._layerNormQ.wrappedValue = LayerNorm(dimensions: contextDimensions, eps: 1e-6)
271 |             self.mlp = (
272 |                 Linear(hiddenSize, hiddenSize),
273 |                 GELU(),
274 |                 Linear(hiddenSize, dimensions)
275 |             )
276 |         }
277 | 
278 |         func callAsFunction(_ x: MLXArray) -> MLXArray {
279 |             var x = layerNormQ(x).reshaped(-1, hiddenSize)
280 |             x = mlp.0(x)
281 |             x = mlp.1(x)
282 |             x = mlp.2(x)
283 |             return x
284 |         }
285 |     }
286 | 
287 |     fileprivate class Attention: Module {
288 | 
289 |         let numHeads: Int
290 |         let scale: Float
291 | 
292 |         @ModuleInfo(key: "qkv") var qkv: Linear
293 |         @ModuleInfo(key: "proj") var proj: Linear
294 | 
295 |         public init(dims: Int, numHeads: Int) {
296 |             self.numHeads = numHeads
297 |             let headDim = dims / numHeads
298 |             self.scale = pow(Float(headDim), -0.5)
299 | 
300 |             self._qkv.wrappedValue = Linear(dims, 3 * dims, bias: true)
301 |             self._proj.wrappedValue = Linear(dims, dims)
302 |         }
303 | 
304 |         public func callAsFunction(
305 |             _ x: MLXArray, frames: [THW], rotaryPositionEmbedding: MLXArray
306 |         ) -> MLXArray {
307 |             let sequenceLength = x.dim(0)
308 |             let B = frames[0].t
309 |             let L = sequenceLength / B
310 | 
311 |             let qkv = qkv(x)
312 |             let s = split(qkv, parts: 3, axis: -1)
313 |             var (q, k, v) = (s[0], s[1], s[2])
314 | 
315 |             q = q.reshaped(sequenceLength, numHeads, -1)
316 |             k = k.reshaped(sequenceLength, numHeads, -1)
317 |             v = v.reshaped(sequenceLength, numHeads, -1)
318 | 
319 |             q = applyMultimodalRotaryPositionEmbedding(q, freqs: rotaryPositionEmbedding)
320 |             k = applyMultimodalRotaryPositionEmbedding(k, freqs: rotaryPositionEmbedding)
321 | 
322 |             q = q.reshaped(B, L, numHeads, -1).transposed(0, 2, 1, 3)
323 |             k = k.reshaped(B, L, numHeads, -1).transposed(0, 2, 1, 3)
324 |             v = v.reshaped(B, L, numHeads, -1).transposed(0, 2, 1, 3)
325 | 
326 |             let output = MLXFast.scaledDotProductAttention(
327 |                 queries: q, keys: k, values: v, scale: scale, mask: nil
328 |             )
329 |             .transposed(0, 2, 1, 3)
330 |             .reshaped(sequenceLength, -1)
331 | 
332 |             return proj(output)
333 |         }
334 |     }
335 | 
336 |     fileprivate class MLP: Module, UnaryLayer {
337 | 
338 |         @ModuleInfo var activation: GELU
339 |         @ModuleInfo var fc1: Linear
340 |         @ModuleInfo var fc2: Linear
341 | 
342 |         public init(dimensions: Int, hiddenDimensions: Int) {
343 |             self.activation = GELU(approximation: .fast)
344 |             self.fc1 = Linear(dimensions, hiddenDimensions)
345 |             self.fc2 = Linear(hiddenDimensions, dimensions)
346 |         }
347 | 
348 |         public func callAsFunction(_ x: MLXArray) -> MLXArray {
349 |             fc2(activation(fc1(x)))
350 |         }
351 |     }
352 | 
353 |     fileprivate class Qwen2VLVisionBlock: Module {
354 | 
355 |         @ModuleInfo var norm1: LayerNorm
356 |         @ModuleInfo var norm2: LayerNorm
357 |         @ModuleInfo(key: "attn") var attention: Attention
358 |         @ModuleInfo var mlp: MLP
359 | 
360 |         public init(_ config: Qwen2VLConfiguration.VisionConfiguration) {
361 |             self.norm1 = LayerNorm(dimensions: config.embedDimensions, eps: 1e-6)
362 |             self.norm2 = LayerNorm(dimensions: config.embedDimensions, eps: 1e-6)
363 | 
364 |             self._attention.wrappedValue = Attention(
365 |                 dims: config.embedDimensions, numHeads: config.numHeads)
366 | 
367 |             let mlpHiddenDimensions = Int(Float(config.embedDimensions) * config.mlpRatio)
368 |             self.mlp = MLP(
369 |                 dimensions: config.embedDimensions, hiddenDimensions: mlpHiddenDimensions)
370 |         }
371 | 
372 |         func callAsFunction(
373 |             _ hiddenStates: MLXArray, frames: [THW], rotaryPositionEmbedding: MLXArray
374 |         ) -> MLXArray {
375 |             var hiddenStates =
376 |                 hiddenStates
377 |                 + attention(
378 |                     norm1(hiddenStates),
379 |                     frames: frames,
380 |                     rotaryPositionEmbedding: rotaryPositionEmbedding
381 |                 )
382 |             hiddenStates = hiddenStates + mlp(norm2(hiddenStates))
383 |             return hiddenStates
384 |         }
385 |     }
386 | 
387 |     fileprivate class VisionModel: Module {
388 | 
389 |         @ModuleInfo(key: "patch_embed") var patchEmbed: QwenVL.PatchEmbed
390 |         @ModuleInfo(key: "rotary_pos_emb") var rotaryPositionEmbedding: QwenVL.VisionRotaryEmbedding
391 |         @ModuleInfo(key: "blocks") var blocks: [Qwen2VLVisionBlock]
392 |         @ModuleInfo(key: "merger") var patchMerger: PatchMerger
393 | 
394 |         let spatialMergeSize: Int
395 | 
396 |         public init(_ config: Qwen2VLConfiguration.VisionConfiguration) {
397 |             self.spatialMergeSize = config.spatialMergeSize
398 | 
399 |             self._patchEmbed.wrappedValue = QwenVL.PatchEmbed(
400 |                 patchSize: config.patchSize,
401 |                 temporalPatchSize: config.temporalPatchSize,
402 |                 inChannels: config.inChannels,
403 |                 embedDimensions: config.embedDimensions)
404 | 
405 |             let headDimensions = config.embedDimensions / config.numHeads
406 |             self._rotaryPositionEmbedding.wrappedValue = QwenVL.VisionRotaryEmbedding(
407 |                 dimensions: headDimensions / 2, theta: 10_000)
408 | 
409 |             self._blocks.wrappedValue = (0 ..< config.depth).map { _ in
410 |                 Qwen2VLVisionBlock(config)
411 |             }
412 |             self._patchMerger.wrappedValue = PatchMerger(
413 |                 dimensions: config.hiddenSize, contextDimensions: config.embedDimensions,
414 |                 spatialMergeSize: 2)
415 |         }
416 | 
417 |         func rotaryPositionEmbedding(_ frames: [THW]) -> MLXArray {
418 |             var positionIds = [MLXArray]()
419 | 
420 |             for row in frames {
421 |                 let (t, h, w) = row.values
422 | 
423 |                 var hposIds = expandedDimensions(MLXArray(0 ..< h), axis: 1)
424 |                 hposIds = repeated(hposIds, count: w, axis: 1)
425 |                 hposIds =
426 |                     hposIds
427 |                     .reshaped(
428 |                         h / spatialMergeSize,
429 |                         spatialMergeSize,
430 |                         w / spatialMergeSize,
431 |                         spatialMergeSize
432 |                     )
433 |                     .transposed(0, 2, 1, 3)
434 |                     .flattened()
435 | 
436 |                 var wposIds = expandedDimensions(MLXArray(0 ..< w), axis: 0)
437 |                 wposIds = repeated(wposIds, count: h, axis: 0)
438 |                 wposIds =
439 |                     wposIds
440 |                     .reshaped(
441 |                         h / spatialMergeSize,
442 |                         spatialMergeSize,
443 |                         w / spatialMergeSize,
444 |                         spatialMergeSize
445 |                     )
446 |                     .transposed(0, 2, 1, 3)
447 |                     .flattened()
448 | 
449 |                 let stackedPosIds = stacked([hposIds, wposIds], axis: -1)
450 |                 positionIds.append(tiled(stackedPosIds, repetitions: [t, 1]))
451 |             }
452 | 
453 |             let indices = concatenated(positionIds, axis: 0)
454 |             let maxFrameSize = frames.lazy.map { max($0.h, $0.w) }.max() ?? 0
455 |             let rotaryPositionEmbedFull = rotaryPositionEmbedding(sequenceLength: maxFrameSize)[
456 |                 indices]
457 | 
458 |             return rotaryPositionEmbedFull.reshaped(indices.dim(0), -1)
459 |         }
460 | 
461 |         public func callAsFunction(_ hiddenStates: MLXArray, frames: [THW]) -> MLXArray {
462 |             var hiddenStates = patchEmbed(hiddenStates)
463 |             let rotaryPositionEmbedding = rotaryPositionEmbedding(frames)
464 | 
465 |             let batchSize = frames.count
466 | 
467 |             for block in blocks {
468 |                 hiddenStates = block(
469 |                     hiddenStates, frames: frames,
470 |                     rotaryPositionEmbedding: rotaryPositionEmbedding)
471 |             }
472 | 
473 |             return patchMerger(hiddenStates)
474 |         }
475 | 
476 |         private func isMLXWeight(_ array: MLXArray) -> Bool {
477 |             if array.ndim != 4, array.ndim != 5 {
478 |                 return false
479 |             }
480 | 
481 |             if array.dim(-1) == 3 {
482 |                 return true
483 |             }
484 | 
485 |             let (outChannels, kH, kW) = (array.dim(1), array.dim(2), array.dim(3))
486 |             return outChannels >= kH && outChannels >= kW && kH == kW
487 |         }
488 | 
489 |         func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {
490 |             var sanitizedWeights = [String: MLXArray]()
491 | 
492 |             for (k, v) in weights {
493 |                 if k.contains("position_id") {
494 |                     // Remove unused position_ids
495 |                     continue
496 |                 } else if k.contains("patch_embed.proj.weight") {
497 |                     // PyTorch conv2d weight tensors have shape:
498 |                     //   [B, out_channels, in_channels, kH, KW]
499 |                     // MLX conv2d expects the weight be of shape:
500 |                     //   [B, out_channels, kH, KW, in_channels]
501 |                     if isMLXWeight(v) {
502 |                         sanitizedWeights[k] = v
503 |                     } else {
504 |                         sanitizedWeights[k] = v.transposed(0, 2, 3, 4, 1)
505 |                     }
506 |                 } else {
507 |                     sanitizedWeights[k] = v
508 |                 }
509 |             }
510 | 
511 |             return sanitizedWeights
512 |         }
513 |     }
514 | }
515 | 
516 | // MARK: - Processor
517 | 
518 | /// Qwen2VL VLM `UserInputProcessor`.
519 | ///
520 | /// This is meant to be used with ``Qwen2VL`` and is typically created by ``VLMModelFactory``.
521 | public class Qwen2VLProcessor: UserInputProcessor {
522 |     private let config: Qwen2VLProcessorConfiguration
523 |     private let tokenizer: any Tokenizer
524 | 
525 |     public init(_ config: Qwen2VLProcessorConfiguration, tokenizer: any Tokenizer) {
526 |         self.config = config
527 |         self.tokenizer = tokenizer
528 |     }
529 | 
530 |     func preprocess(image: CIImage, resizedSize: CGSize) -> CIImage {
531 |         image
532 |             .toSRGB()
533 |             .resampled(to: resizedSize, method: .bicubic)
534 |             .normalized(mean: config.imageMeanTuple, std: config.imageStdTuple)
535 |     }
536 | 
537 |     public func preprocess(images: [CIImage], processing: UserInput.Processing?) throws -> (
538 |         MLXArray, THW
539 |     ) {
540 |         // first apply the user requested resizing, etc. if any
541 |         let images = images.map { MediaProcessing.apply($0, processing: processing) }
542 | 
543 |         // image_processing_qwen2_vl._preprocess
544 | 
545 |         let size = images[0].extent.size
546 |         let (resizedHeight, resizedWidth) = try QwenVL.targetSize(
547 |             height: Int(size.height), width: Int(size.width),
548 |             factor: config.patchSize * config.mergeSize,
549 |             minPixels: config.minPixels, maxPixels: config.maxPixels)
550 |         let resizedSize = CGSize(width: resizedWidth, height: resizedHeight)
551 | 
552 |         let processedImages = try images.map { image in
553 |             preprocess(image: image, resizedSize: resizedSize).asMLXArray()
554 |         }
555 | 
556 |         return try QwenVL.patchify(
557 |             images: processedImages, mergeSize: config.mergeSize, patchSize: config.patchSize,
558 |             temporalPatchSize: config.temporalPatchSize)
559 |     }
560 | 
561 |     public func prepare(input: UserInput) async throws -> LMInput {
562 |         let messages = Qwen2VLMessageGenerator().generate(from: input)
563 | 
564 |         var promptTokens = try tokenizer.applyChatTemplate(messages: messages)
565 | 
566 |         // Text-only input
567 |         if input.images.isEmpty, input.videos.isEmpty {
568 |             return LMInput(tokens: MLXArray(promptTokens))
569 |         }
570 | 
571 |         // Process images if any
572 |         var processedImage: LMInput.ProcessedImage?
573 |         if !input.images.isEmpty {
574 |             let imagePixelsAndFrames = try input.images.map {
575 |                 try preprocess(images: [$0.asCIImage()], processing: input.processing)
576 |             }
577 |             let imagePixelsConcatenated = concatenated(imagePixelsAndFrames.map { $0.0 })
578 |             processedImage = LMInput.ProcessedImage(
579 |                 pixels: imagePixelsConcatenated, frames: imagePixelsAndFrames.map { $0.1 })
580 |             if let imageFrames = processedImage?.frames {
581 |                 promptTokens = try QwenVL.replacePaddingTokens(
582 |                     in: promptTokens, frames: imageFrames, paddingToken: "<|image_pad|>",
583 |                     mergeSize: config.mergeSize, tokenizer: tokenizer)
584 |             }
585 |         }
586 | 
587 |         // Process videos if any
588 |         var processedVideo: LMInput.ProcessedVideo?
589 |         if !input.videos.isEmpty {
590 |             var videosAsImageSequences = [[MLXArray]]()
591 |             var resizedSize: CGSize = .zero
592 |             for video in input.videos {
593 |                 let imageSequence = try await MediaProcessing.asProcessedSequence(
594 |                     video.asAVAsset(), samplesPerSecond: 2
595 |                 ) { frame in
596 |                     // first apply the user requested resizing, etc. if any
597 |                     let resizedImage = MediaProcessing.apply(
598 |                         frame.frame, processing: input.processing)
599 |                     if resizedSize == .zero {
600 |                         let size = resizedImage.extent.size
601 |                         let (resizedHeight, resizedWidth) = try QwenVL.targetSize(
602 |                             height: Int(size.height), width: Int(size.width),
603 |                             factor: config.patchSize * config.mergeSize,
604 |                             minPixels: config.minPixels, maxPixels: config.maxPixels)
605 |                         resizedSize = CGSize(width: resizedWidth, height: resizedHeight)
606 |                     }
607 |                     let processedImage = preprocess(image: resizedImage, resizedSize: resizedSize)
608 |                     return VideoFrame(frame: processedImage, timeStamp: frame.timeStamp)
609 |                 }
610 |                 videosAsImageSequences.append(imageSequence.frames)
611 |             }
612 |             let videoPixelsAndFrames = try videosAsImageSequences.map {
613 |                 try QwenVL.patchify(
614 |                     images: $0, mergeSize: config.mergeSize, patchSize: config.patchSize,
615 |                     temporalPatchSize: config.temporalPatchSize)
616 |             }
617 |             let videoPixelsConcatenated = concatenated(videoPixelsAndFrames.map { $0.0 })
618 |             processedVideo = LMInput.ProcessedVideo(
619 |                 pixels: videoPixelsConcatenated, frames: videoPixelsAndFrames.map { $0.1 })
620 |             if let videoFrames = processedVideo?.frames {
621 |                 promptTokens = try QwenVL.replacePaddingTokens(
622 |                     in: promptTokens, frames: videoFrames, paddingToken: "<|video_pad|>",
623 |                     mergeSize: config.mergeSize, tokenizer: tokenizer)
624 |             }
625 |         }
626 | 
627 |         let promptArray = MLXArray(promptTokens).expandedDimensions(axis: 0)
628 |         let mask = ones(like: promptArray).asType(.int8)
629 |         return LMInput(
630 |             text: .init(tokens: promptArray, mask: mask),
631 |             image: processedImage,
632 |             video: processedVideo)
633 |     }
634 | }
635 | 
636 | // MARK: - Model
637 | 
638 | /// Qwen2VL VLM
639 | ///
640 | /// This is typically created by ``VLMModelFactory``.
641 | public class Qwen2VL: Module, VLMModel, KVCacheDimensionProvider {
642 | 
643 |     @ModuleInfo(key: "vision_tower") private var visionModel: Vision.VisionModel
644 |     @ModuleInfo(key: "language_model") private var languageModel: Language.LanguageModel
645 | 
646 |     public let config: Qwen2VLConfiguration
647 | 
648 |     public var vocabularySize: Int { config.baseConfiguration.vocabularySize }
649 |     public var kvHeads: [Int] { languageModel.kvHeads }
650 | 
651 |     public func loraLinearLayers() -> MLXLMCommon.LoRALinearLayers {
652 |         languageModel.model.layers.map { ($0.attention, ["q_proj", "v_proj"]) }
653 |     }
654 | 
655 |     public init(_ config: Qwen2VLConfiguration) {
656 |         self.config = config
657 |         self._visionModel.wrappedValue = Vision.VisionModel(config.visionConfiguration)
658 |         self._languageModel.wrappedValue = Language.LanguageModel(config.textConfiguration)
659 |     }
660 | 
661 |     private func inputEmbeddings(inputIds: MLXArray, pixelValues: MLXArray?, frames: [THW]?)
662 |         -> MLXArray
663 |     {
664 |         guard let pixelValues, let frames else {
665 |             return languageModel.model.embedTokens(inputIds[.newAxis, .ellipsis])
666 |         }
667 | 
668 |         // Get the input embeddings from the language model
669 |         let inputEmbeds = languageModel.model.embedTokens(inputIds)
670 | 
671 |         // Get the ouptut hidden states from the vision model
672 |         var hiddenStates = self.visionModel(pixelValues, frames: frames)
673 | 
674 |         if hiddenStates.ndim == 2 {
675 |             hiddenStates = hiddenStates[.newAxis, 0..., 0...]
676 |         }
677 | 
678 |         // Insert special image tokens in the input_ids
679 |         return QwenVL.mergeInputIdsWithImageFeatures(
680 |             inputIds: inputIds, inputEmbeds: inputEmbeds, imageFeatures: hiddenStates,
681 |             imageTokenId: config.baseConfiguration.imageTokenId,
682 |             videoTokenId: config.baseConfiguration.videoTokenId)
683 |     }
684 | 
685 |     public func prepare(_ input: LMInput, cache: [any KVCache], windowSize: Int?) throws
686 |         -> PrepareResult
687 |     {
688 |         let dtype = visionModel.patchEmbed.proj.weight.dtype
689 | 
690 |         // Process both images and videos together
691 |         var allPixels: MLXArray?
692 |         var allFrames: [THW] = []
693 | 
694 |         if let imagePixels = input.image?.pixels, let imageFrames = input.image?.frames {
695 |             allPixels = imagePixels.asType(dtype)
696 |             allFrames.append(contentsOf: imageFrames)
697 |         }
698 | 
699 |         if let videoPixels = input.video?.pixels, let videoFrames = input.video?.frames {
700 |             if allPixels == nil {
701 |                 allPixels = videoPixels.asType(dtype)
702 |             } else {
703 |                 allPixels = concatenated([allPixels!, videoPixels.asType(dtype)])
704 |             }
705 |             allFrames.append(contentsOf: videoFrames)
706 |         }
707 | 
708 |         let inputEmbeddings = self.inputEmbeddings(
709 |             inputIds: input.text.tokens, pixelValues: allPixels,
710 |             frames: allFrames.isEmpty ? nil : allFrames)
711 | 
712 |         let result = languageModel(nil, cache: cache, inputEmbedding: inputEmbeddings)
713 | 
714 |         return .logits(result)
715 |     }
716 | 
717 |     public func callAsFunction(_ inputs: MLXArray, cache: [any KVCache]?) -> MLXArray {
718 |         languageModel(inputs, cache: cache).logits
719 |     }
720 | 
721 |     public func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {
722 |         visionModel.sanitize(
723 |             weights:
724 |                 Dictionary(
725 |                     uniqueKeysWithValues: weights.map { key, value in
726 |                         var key = key
727 |                         if !key.contains("vision_tower") {
728 |                             key = key.replacingOccurrences(of: "visual", with: "vision_tower")
729 |                         }
730 |                         if !key.contains("language_model") {
731 |                             key = key.replacingOccurrences(
732 |                                 of: "model", with: "language_model.model")
733 |                             key = key.replacingOccurrences(
734 |                                 of: "lm_head", with: "language_model.lm_head")
735 |                         }
736 | 
737 |                         return (key, value)
738 |                     })
739 |         )
740 |     }
741 | 
742 | }
743 | 
744 | // MARK: - Configuration
745 | 
746 | /// Configuration for ``Qwen2VL``
747 | public struct Qwen2VLConfiguration: Codable, Sendable {
748 | 
749 |     public struct TextConfiguration: Codable, Sendable {
750 |         public let modelType: String
751 |         public let hiddenSize: Int
752 |         public let hiddenLayers: Int
753 |         public let intermediateSize: Int
754 |         public let attentionHeads: Int
755 |         private let _rmsNormEps: Float?
756 |         public var rmsNormEps: Float { _rmsNormEps ?? 1e-6 }
757 |         public let vocabularySize: Int
758 |         public let kvHeads: Int
759 |         private let _maxPositionEmbeddings: Int?
760 |         public var maxpPositionEmbeddings: Int { _maxPositionEmbeddings ?? 32768 }
761 |         private let _ropeTheta: Float?
762 |         public var ropeTheta: Float { _ropeTheta ?? 1_000_000 }
763 |         private let _ropeTraditional: Bool?
764 |         public var ropeTraditional: Bool { _ropeTraditional ?? false }
765 |         public let ropeScaling: [String: StringOrNumber]?
766 |         private let _tieWordEmbeddings: Bool?
767 |         public var tieWordEmbeddings: Bool { _tieWordEmbeddings ?? true }
768 | 
769 |         enum CodingKeys: String, CodingKey {
770 |             case modelType = "model_type"
771 |             case hiddenSize = "hidden_size"
772 |             case hiddenLayers = "num_hidden_layers"
773 |             case intermediateSize = "intermediate_size"
774 |             case attentionHeads = "num_attention_heads"
775 |             case _rmsNormEps = "rms_norm_eps"
776 |             case vocabularySize = "vocab_size"
777 |             case kvHeads = "num_key_value_heads"
778 |             case _maxPositionEmbeddings = "max_position_embeddings"
779 |             case _ropeTheta = "rope_theta"
780 |             case _ropeTraditional = "rope_traditional"
781 |             case ropeScaling = "rope_scaling"
782 |             case _tieWordEmbeddings = "tie_word_embeddings"
783 |         }
784 |     }
785 | 
786 |     public struct VisionConfiguration: Codable, Sendable {
787 |         public let depth: Int
788 |         public let embedDimensions: Int
789 |         public let hiddenSize: Int
790 |         public let numHeads: Int
791 |         public let patchSize: Int
792 |         public let mlpRatio: Float
793 |         public let _inChannels: Int?
794 |         public var inChannels: Int { _inChannels ?? 3 }
795 |         public let _layerNormEps: Float?
796 |         public var layerNormEps: Float { _layerNormEps ?? 1e-6 }
797 |         public let spatialPatchSize: Int
798 |         public let spatialMergeSize: Int
799 |         public let temporalPatchSize: Int
800 | 
801 |         enum CodingKeys: String, CodingKey {
802 |             case depth
803 |             case embedDimensions = "embed_dim"
804 |             case hiddenSize = "hidden_size"
805 |             case numHeads = "num_heads"
806 |             case patchSize = "patch_size"
807 |             case mlpRatio = "mlp_ratio"
808 |             case _inChannels = "in_channels"
809 |             case _layerNormEps = "layer_norm_eps"
810 |             case spatialPatchSize = "spatial_patch_size"
811 |             case spatialMergeSize = "spatial_merge_size"
812 |             case temporalPatchSize = "temporal_patch_size"
813 |         }
814 |     }
815 | 
816 |     public struct BaseConfiguration: Codable, Sendable {
817 |         public let modelType: String
818 |         public let vocabularySize: Int
819 |         public let imageTokenId: Int
820 |         public let videoTokenId: Int
821 |         public let hiddenSize: Int
822 | 
823 |         enum CodingKeys: String, CodingKey {
824 |             case modelType = "model_type"
825 |             case vocabularySize = "vocab_size"
826 |             case imageTokenId = "image_token_id"
827 |             case videoTokenId = "video_token_id"
828 |             case hiddenSize = "hidden_size"
829 |         }
830 |     }
831 | 
832 |     public let textConfiguration: TextConfiguration
833 |     public let visionConfiguration: VisionConfiguration
834 |     public let baseConfiguration: BaseConfiguration
835 | 
836 |     enum CodingKeys: String, CodingKey {
837 |         case visionConfiguration = "vision_config"
838 |     }
839 | 
840 |     public init(from decoder: any Swift.Decoder) throws {
841 |         let container = try decoder.container(keyedBy: CodingKeys.self)
842 | 
843 |         // this is a sub-dictionary
844 |         self.visionConfiguration = try container.decode(
845 |             VisionConfiguration.self, forKey: .visionConfiguration)
846 | 
847 |         // these are overlaid in the top level
848 |         self.textConfiguration = try TextConfiguration(from: decoder)
849 |         self.baseConfiguration = try BaseConfiguration(from: decoder)
850 |     }
851 | }
852 | 
853 | /// Configuration for ``Qwen2VLProcessor``
854 | public struct Qwen2VLProcessorConfiguration: Codable, Sendable {
855 | 
856 |     public struct Size: Codable, Sendable {
857 |         public let maxPixels: Int
858 |         public let minPixels: Int
859 | 
860 |         enum CodingKeys: String, CodingKey {
861 |             case maxPixels = "max_pixels"
862 |             case minPixels = "min_pixels"
863 |         }
864 |     }
865 | 
866 |     public let imageMean: [CGFloat]
867 |     public let imageStd: [CGFloat]
868 |     public let mergeSize: Int
869 |     public let patchSize: Int
870 |     public let temporalPatchSize: Int
871 | 
872 |     private let _size: Size?
873 |     private let _maxPixels: Int?
874 |     private let _minPixels: Int?
875 | 
876 |     public var minPixels: Int {
877 |         _minPixels ?? _size?.minPixels ?? 3136
878 |     }
879 |     public var maxPixels: Int {
880 |         _maxPixels ?? _size?.maxPixels ?? 12_845_056
881 |     }
882 | 
883 |     public var imageMeanTuple: (CGFloat, CGFloat, CGFloat) {
884 |         (imageMean[0], imageMean[1], imageMean[2])
885 |     }
886 |     public var imageStdTuple: (CGFloat, CGFloat, CGFloat) {
887 |         (imageStd[0], imageStd[1], imageStd[2])
888 |     }
889 | 
890 |     enum CodingKeys: String, CodingKey {
891 |         case imageMean = "image_mean"
892 |         case imageStd = "image_std"
893 |         case mergeSize = "merge_size"
894 |         case patchSize = "patch_size"
895 |         case temporalPatchSize = "temporal_patch_size"
896 |         case _maxPixels = "max_pixels"
897 |         case _minPixels = "min_pixels"
898 |         case _size = "size"
899 |     }
900 | }
901 | 
902 | /// Message Generator for Qwen2VL
903 | public struct Qwen2VLMessageGenerator: MessageGenerator {
904 |     public init() {}
905 | 
906 |     public func generate(message: Chat.Message) -> Message {
907 |         [
908 |             "role": message.role.rawValue,
909 |             "content": [
910 |                 ["type": "text", "text": message.content]
911 |             ]
912 |                 // Messages format for Qwen 2 VL, Qwen 2.5 VL. May need to be adapted for other models.
913 |                 + message.images.map { _ in
914 |                     ["type": "image"]
915 |                 }
916 |                 + message.videos.map { _ in
917 |                     ["type": "video"]
918 |                 },
919 |         ]
920 |     }
921 | }
922 | 


--------------------------------------------------------------------------------
/Libraries/MLXVLM/Models/QwenVL.swift:
--------------------------------------------------------------------------------
  1 | import CoreImage
  2 | import Foundation
  3 | import Hub
  4 | import MLX
  5 | import MLXLMCommon
  6 | import MLXNN
  7 | import Tokenizers
  8 | 
  9 | // MARK: - Common Utilities for Qwen 2 VL and Qwen 2.5 VL
 10 | 
 11 | private func debug(_ message: @autoclosure () -> String) {
 12 |     // print(message())
 13 | }
 14 | 
 15 | public struct QwenVL {
 16 |     /// Rotates half the hidden dims of the input
 17 |     static func rotateHalf(_ x: MLXArray) -> MLXArray {
 18 |         let index = x.dim(-1) / 2
 19 |         let x1 = x[.ellipsis, 0 ..< index]
 20 |         let x2 = x[.ellipsis, index...]
 21 |         return concatenated([-x2, x1], axis: -1)
 22 |     }
 23 | 
 24 |     static func mergeInputIdsWithImageFeatures(
 25 |         inputIds: MLXArray, inputEmbeds: MLXArray, imageFeatures: MLXArray,
 26 |         imageTokenId: Int, videoTokenId: Int
 27 |     ) -> MLXArray {
 28 |         var imageIndices = [Int]()
 29 |         for (i, v) in inputIds.asArray(Int.self).enumerated() {
 30 |             if v == imageTokenId || v == videoTokenId {
 31 |                 imageIndices.append(i)
 32 |             }
 33 |         }
 34 | 
 35 |         // Make sure shapes match before assignment
 36 |         var result = inputEmbeds
 37 |         if result.ndim == 2 {
 38 |             result = result[.newAxis, 0..., 0...]
 39 |         }
 40 | 
 41 |         if imageFeatures.ndim == 2 {
 42 |             let reshapedFeatures = imageFeatures[.newAxis, 0..., 0...]
 43 |             result[0..., MLXArray(imageIndices), 0...] = reshapedFeatures
 44 |         } else {
 45 |             result[0..., MLXArray(imageIndices), 0...] = imageFeatures
 46 |         }
 47 | 
 48 |         return result
 49 |     }
 50 | 
 51 |     public class VisionRotaryEmbedding {
 52 |         let dimensions: Int
 53 |         let theta: Float
 54 |         let inverseFreq: MLXArray
 55 | 
 56 |         init(dimensions: Int, theta: Float) {
 57 |             self.dimensions = dimensions
 58 |             self.theta = theta
 59 |             let p = MLXArray(stride(from: 0, to: dimensions, by: 2)).asType(.float32) / dimensions
 60 |             self.inverseFreq = 1.0 / pow(theta, p)
 61 |         }
 62 | 
 63 |         func callAsFunction(sequenceLength: Int) -> MLXArray {
 64 |             let seq = MLXArray(0 ..< sequenceLength).asType(inverseFreq.dtype)
 65 |             let freqs = outer(seq, inverseFreq)
 66 |             return freqs
 67 |         }
 68 |     }
 69 | 
 70 |     public class PatchEmbed: Module, UnaryLayer {
 71 |         @ModuleInfo var proj: Conv3d
 72 | 
 73 |         let patchSize: Int
 74 |         let temporalPatchSize: Int
 75 |         let inChannels: Int
 76 |         let outputDimensions: Int
 77 | 
 78 |         // For Qwen 2 VL
 79 |         convenience init(
 80 |             patchSize: Int, temporalPatchSize: Int, inChannels: Int, embedDimensions: Int
 81 |         ) {
 82 |             self.init(
 83 |                 patchSize: patchSize, temporalPatchSize: temporalPatchSize,
 84 |                 inChannels: inChannels, outputDimensions: embedDimensions)
 85 |         }
 86 | 
 87 |         // For Qwen 2.5 VL
 88 |         convenience init(patchSize: Int, temporalPatchSize: Int, inChannels: Int, hiddenSize: Int) {
 89 |             self.init(
 90 |                 patchSize: patchSize, temporalPatchSize: temporalPatchSize,
 91 |                 inChannels: inChannels, outputDimensions: hiddenSize)
 92 |         }
 93 | 
 94 |         // Common initializer
 95 |         init(patchSize: Int, temporalPatchSize: Int, inChannels: Int, outputDimensions: Int) {
 96 |             self.patchSize = patchSize
 97 |             self.temporalPatchSize = temporalPatchSize
 98 |             self.inChannels = inChannels
 99 |             self.outputDimensions = outputDimensions
100 | 
101 |             let kernelSize = IntOrTriple([temporalPatchSize, patchSize, patchSize])
102 |             self._proj.wrappedValue = Conv3d(
103 |                 inputChannels: inChannels,
104 |                 outputChannels: outputDimensions,
105 |                 kernelSize: kernelSize,
106 |                 stride: kernelSize,
107 |                 bias: false
108 |             )
109 |         }
110 | 
111 |         public func callAsFunction(_ hiddenStates: MLXArray) -> MLXArray {
112 |             var hiddenStates = hiddenStates.reshaped(
113 |                 -1, inChannels, temporalPatchSize, patchSize, patchSize
114 |             ).movedAxis(source: 1, destination: 4)
115 | 
116 |             hiddenStates = proj(hiddenStates)
117 |             hiddenStates = hiddenStates.reshaped(-1, outputDimensions)
118 |             return hiddenStates
119 |         }
120 |     }
121 | 
122 |     // image_processing_qwen2_vl.smart_resize
123 |     static func targetSize(height: Int, width: Int, factor: Int, minPixels: Int, maxPixels: Int)
124 |         throws
125 |         -> (Int, Int)
126 |     {
127 |         debug("Original dimensions: \(width) × \(height)")
128 |         debug("Factor: \(factor), minPixels: \(minPixels), maxPixels: \(maxPixels)")
129 | 
130 |         if height < factor {
131 |             throw VLMError.imageProcessingFailure(
132 |                 "Height: \(height) must be larger than factor: \(factor)")
133 |         }
134 |         if width < factor {
135 |             throw VLMError.imageProcessingFailure(
136 |                 "Width: \(width) must be larger than factor: \(factor)")
137 |         }
138 |         if max(height, width) / min(height, width) > 200 {
139 |             throw VLMError.imageProcessingFailure(
140 |                 "Absolute aspect ratio must be smaller than 200: \(width) × \(height)")
141 |         }
142 | 
143 |         var hBar = max(factor, Int(round(Float(height) / Float(factor))) * factor)
144 |         var wBar = max(factor, Int(round(Float(width) / Float(factor))) * factor)
145 |         debug("After rounding to factor multiples: \(wBar) × \(hBar)")
146 | 
147 |         // Scale based on total pixel count
148 |         if hBar * wBar > maxPixels {
149 |             let beta = sqrt(Float(height * width) / Float(maxPixels))
150 |             hBar = Int(floor(Float(height) / beta / Float(factor))) * factor
151 |             wBar = Int(floor(Float(width) / beta / Float(factor))) * factor
152 |             debug("After scaling down for maxPixels: \(wBar) × \(hBar)")
153 |         } else if hBar * wBar < minPixels {
154 |             let beta = sqrt(Float(minPixels) / Float(height * width))
155 |             hBar = Int(ceil(Float(height) * beta / Float(factor))) * factor
156 |             wBar = Int(ceil(Float(width) * beta / Float(factor))) * factor
157 |             debug("After scaling up for minPixels: \(wBar) × \(hBar)")
158 |         }
159 | 
160 |         // Ensure dimensions are divisible by the factor
161 |         hBar = (hBar / factor) * factor
162 |         wBar = (wBar / factor) * factor
163 |         debug("Final dimensions: \(wBar) × \(hBar)")
164 |         debug("Total pixels: \(wBar * hBar)")
165 | 
166 |         // Final sanity check
167 |         if hBar <= 0 || wBar <= 0 {
168 |             throw VLMError.imageProcessingFailure(
169 |                 "Invalid target dimensions: \(wBar) × \(hBar)")
170 |         }
171 | 
172 |         return (hBar, wBar)
173 |     }
174 | 
175 |     static func replacePaddingTokens(
176 |         in promptTokens: [Int], frames: [THW], paddingToken: String, mergeSize: Int,
177 |         tokenizer: any Tokenizer
178 |     ) throws -> [Int] {
179 |         // Replace single padding token with correct number for each image or video frame
180 |         let placeholderTokens = try tokenizer.encode(
181 |             text: "<|vision_start|>\(paddingToken)<|vision_end|>")
182 |         let placeholderRanges = promptTokens.ranges(of: placeholderTokens)
183 |         guard placeholderRanges.count == frames.count else {
184 |             throw VLMError.processing(
185 |                 "Number of placeholder tokens does not match number of frames")
186 |         }
187 |         let mergeLength = mergeSize * mergeSize
188 |         let replacementSequences = try frames.map { frame in
189 |             let paddingCount = frame.product / mergeLength
190 |             return try tokenizer.encode(
191 |                 text:
192 |                     "<|vision_start|>\(Array(repeating: paddingToken, count: paddingCount).joined())<|vision_end|>"
193 |             )
194 |         }
195 |         // Build the final array
196 |         var result: [Int] = []
197 |         var currentIndex = promptTokens.startIndex
198 |         for (range, replacement) in zip(placeholderRanges, replacementSequences) {
199 |             // Add tokens before the placeholder
200 |             result.append(contentsOf: promptTokens[currentIndex ..< range.lowerBound])
201 |             // Add replacement sequence
202 |             result.append(contentsOf: replacement)
203 |             currentIndex = range.upperBound
204 |         }
205 |         // Add any remaining tokens after the last replacement
206 |         if currentIndex < promptTokens.endIndex {
207 |             result.append(contentsOf: promptTokens[currentIndex...])
208 |         }
209 |         return result
210 |     }
211 | 
212 |     static func patchify(images: [MLXArray], mergeSize: Int, patchSize: Int, temporalPatchSize: Int)
213 |         throws -> (
214 |             MLXArray, THW
215 |         )
216 |     {
217 |         guard let firstImage = images.first else {
218 |             throw VLMError.imageProcessingFailure("No images in video sequence")
219 |         }
220 |         let resizedHeight = firstImage.dim(-2)
221 |         let resizedWidth = firstImage.dim(-1)
222 |         var patches = concatenated(images)
223 | 
224 |         // Pad to match temporal patch size if needed
225 |         let mod = patches.dim(0) % temporalPatchSize
226 |         if mod != 0 {
227 |             let lastPatch = patches[-1, .ellipsis]
228 |             let lastPatchRepeated = tiled(
229 |                 lastPatch, repetitions: [temporalPatchSize - mod, 1, 1, 1])
230 |             patches = concatenated([patches, lastPatchRepeated])
231 |         }
232 |         let channel = patches.dim(1)
233 |         let gridT = patches.dim(0) / temporalPatchSize
234 |         let gridH = resizedHeight / patchSize
235 |         let gridW = resizedWidth / patchSize
236 | 
237 |         patches = patches.reshaped(
238 |             gridT,
239 |             temporalPatchSize,
240 |             channel,
241 |             gridH / mergeSize,
242 |             mergeSize,
243 |             patchSize,
244 |             gridW / mergeSize,
245 |             mergeSize,
246 |             patchSize
247 |         )
248 |         patches = patches.transposed(0, 3, 6, 4, 7, 2, 1, 5, 8)
249 | 
250 |         let flattenedPatches = patches.reshaped(
251 |             gridT * gridH * gridW,
252 |             channel * temporalPatchSize * patchSize * patchSize
253 |         )
254 | 
255 |         return (flattenedPatches, .init(gridT, gridH, gridW))
256 |     }
257 | 
258 | }
259 | 


--------------------------------------------------------------------------------
/Libraries/MLXVLM/Models/SmolVLM2.swift:
--------------------------------------------------------------------------------
  1 | //
  2 | //  SmolVLM2.swift
  3 | //  mlx-swift-examples
  4 | //
  5 | //  Created by Pedro Cuenca on 20/3/25.
  6 | //
  7 | 
  8 | import CoreImage
  9 | import CoreMedia
 10 | import Foundation
 11 | import MLX
 12 | import MLXLMCommon
 13 | import Tokenizers
 14 | 
 15 | // MARK: - Configuration and modeling are Idefics3
 16 | 
 17 | typealias SmolVLM2Configuration = Idefics3Configuration
 18 | typealias SmolVLM2 = Idefics3
 19 | 
 20 | // MARK: - SmolVLMProcessor and configuration
 21 | 
 22 | public struct SmolVLMProcessorConfiguration: Codable, Sendable {
 23 |     public struct Size: Codable, Sendable {
 24 |         public let longestEdge: Int
 25 |         enum CodingKeys: String, CodingKey {
 26 |             case longestEdge = "longest_edge"
 27 |         }
 28 |     }
 29 | 
 30 |     public struct VideoSampling: Codable, Sendable {
 31 |         public let fps: Int
 32 |         public let maxFrames: Int
 33 |         // Intentionally ignoring videoSize because I believe it's still wrong in the config files
 34 |         //        public let videoSize: Size
 35 | 
 36 |         enum CodingKeys: String, CodingKey {
 37 |             case fps
 38 |             case maxFrames = "max_frames"
 39 |         }
 40 |     }
 41 | 
 42 |     public let imageMean: [CGFloat]
 43 |     public let imageStd: [CGFloat]
 44 |     public let size: Size
 45 |     public let maxImageSize: Size
 46 |     public let videoSampling: VideoSampling
 47 |     private let _imageSequenceLength: Int?
 48 |     // TODO: this does not come in preprocessor_config.json, verify where transformers gets it from
 49 |     public var imageSequenceLength: Int { _imageSequenceLength ?? 64 }
 50 | 
 51 |     init(
 52 |         imageMean: [CGFloat], imageStd: [CGFloat], size: Size, maxImageSize: Size,
 53 |         videoSampling: VideoSampling, imageSequenceLength: Int?
 54 |     ) {
 55 |         self.imageMean = imageMean
 56 |         self.imageStd = imageStd
 57 |         self.size = size
 58 |         self.maxImageSize = maxImageSize
 59 |         self.videoSampling = videoSampling
 60 |         self._imageSequenceLength = imageSequenceLength
 61 |     }
 62 | 
 63 |     public var imageMeanTuple: (CGFloat, CGFloat, CGFloat) {
 64 |         (imageMean[0], imageMean[1], imageMean[2])
 65 |     }
 66 |     public var imageStdTuple: (CGFloat, CGFloat, CGFloat) {
 67 |         (imageStd[0], imageStd[1], imageStd[2])
 68 |     }
 69 | 
 70 |     enum CodingKeys: String, CodingKey {
 71 |         case imageMean = "image_mean"
 72 |         case imageStd = "image_std"
 73 |         case size
 74 |         case maxImageSize = "max_image_size"
 75 |         case videoSampling = "video_sampling"
 76 |         case _imageSequenceLength = "image_seq_len"
 77 |     }
 78 | }
 79 | 
 80 | public class SmolVLMProcessor: UserInputProcessor {
 81 |     private let config: SmolVLMProcessorConfiguration
 82 |     private let tokenizer: any Tokenizer
 83 | 
 84 |     // FIXME: hardcoded values for now
 85 | 
 86 |     // Hardcode this since we can't pass it in or rely on it from the preprocessor config.
 87 |     let imageTokenId = 49190
 88 |     let imageToken = "<image>"
 89 |     let fakeImageToken = "<fake_token_around_image>"
 90 |     let globalImageToken = "<global-img>"
 91 | 
 92 |     var maxProcessingImageSize: CGFloat { CGFloat(config.size.longestEdge) }  // 2048
 93 |     var fixedImageSize: CGFloat { CGFloat(config.maxImageSize.longestEdge) }  // 384 for big models, 512 for small models (200-500M)
 94 |     var imageSequenceLength: Int { config.imageSequenceLength }
 95 |     var maxVideoFrames: Int { 20 /*config.videoSampling.maxFrames*/ }
 96 |     var targetVideoFPS: Double { Double(config.videoSampling.fps) }
 97 | 
 98 |     let defaultVideoSystemMessage =
 99 |         "You are a helpful assistant that can understand videos. Describe what type of video this is and what's happening in it."
100 | 
101 |     public init(
102 |         _ config: SmolVLMProcessorConfiguration,
103 |         tokenizer: any Tokenizer
104 |     ) {
105 |         self.config = config
106 |         self.tokenizer = tokenizer
107 |     }
108 | 
109 |     func getVideoPromptString(
110 |         frameCount: Int, timeStamps: [String], videoDuration: String, seqLen: Int,
111 |         fakeToken: String, imageToken: String, globalImageToken: String
112 |     ) -> String {
113 |         var textSplitFrames =
114 |             "You are provided the following series of \(frameCount) frames from a \(videoDuration) [H:MM:SS] video.\n"
115 |         for frameIndex in 0 ..< frameCount {
116 |             textSplitFrames += "\nFrame from \(timeStamps[frameIndex]):"
117 |             textSplitFrames +=
118 |                 (fakeToken
119 |                     + globalImageToken
120 |                     + String(repeating: imageToken, count: seqLen)
121 |                     + fakeToken)
122 |         }
123 |         textSplitFrames += "\n\n"
124 |         return textSplitFrames
125 |     }
126 | 
127 |     func getImagePromptString(
128 |         rows: Int, cols: Int, seqLen: Int, fakeToken: String, imageToken: String,
129 |         globalImageToken: String
130 |     ) -> String {
131 |         /// Prompt with expanded image tokens for when the image is split into patches.
132 |         /// This applies to image processing, not video (I think).
133 |         /// This just transliterates this: https://github.com/huggingface/transformers/blob/6a1ab634b6886b6560b0502e7a305c8cd881732e/src/transformers/models/idefics3/processing_idefics3.py#L44
134 |         var textSplitImages = ""
135 |         for h in 0 ..< rows {
136 |             for w in 0 ..< cols {
137 |                 textSplitImages +=
138 |                     (fakeToken
139 |                         + "<row_\(h + 1)_col_\(w + 1)>"
140 |                         + String(repeating: imageToken, count: seqLen))
141 |             }
142 |             textSplitImages += "\n"
143 |         }
144 |         textSplitImages +=
145 |             ("\n"
146 |                 + fakeToken
147 |                 + globalImageToken
148 |                 + String(repeating: imageToken, count: seqLen)
149 |                 + fakeToken)
150 |         return textSplitImages
151 |     }
152 | 
153 |     /// Compute the resize size with `longestEdge` for the given size
154 |     /// If `multiple` is not nil, ensures each side is a multiple of that value
155 |     func aspectRatioSize(for size: CGSize, longestEdge: CGFloat, multiple: CGFloat? = nil) -> CGSize
156 |     {
157 |         var targetSize = MediaProcessing.bestFit(
158 |             size, in: CGSize(width: longestEdge, height: longestEdge))
159 |         guard let multiple = multiple else { return targetSize }
160 |         let aspectRatio = targetSize.width / targetSize.height
161 |         if size.width >= size.height {
162 |             let width = ceil(targetSize.width / multiple) * multiple
163 |             var height = width / aspectRatio
164 |             height = ceil(height / multiple) * multiple
165 |             return CGSize(width: width, height: height)
166 |         } else {
167 |             let height = ceil(targetSize.height / multiple) * multiple
168 |             var width = height * aspectRatio
169 |             width = ceil(width / multiple) * multiple
170 |             return CGSize(width: width, height: height)
171 |         }
172 |     }
173 | 
174 |     /// Compute the resize size with `longestEdge` for the given size
175 |     /// If `multiple` is not nil, ensures each side is a multiple of that value
176 |     func aspectRatioSize(for size: CGSize, longestEdge: Int, multiple: Int? = nil) -> CGSize {
177 |         return aspectRatioSize(
178 |             for: size, longestEdge: CGFloat(longestEdge), multiple: multiple.flatMap(CGFloat.init))
179 |     }
180 | 
181 |     /// Tile image if it's larger than the maxProcessingImageSize, so the model gets to see more of it
182 |     /// TODO: disable in video mode
183 |     func tiles(from originalImage: CIImage) -> (tiles: [CIImage], rows: Int, cols: Int) {
184 |         // The original code resizes to maxProcessingImageSize, then resizes again ensuring multiples of fixedImageSize
185 |         // We do both resizes in one go
186 |         let processingSize = aspectRatioSize(
187 |             for: originalImage.extent.size, longestEdge: maxProcessingImageSize,
188 |             multiple: fixedImageSize)
189 |         let image = MediaProcessing.resampleLanczos(originalImage, to: processingSize)
190 | 
191 |         var tiles: [CIImage] = []
192 | 
193 |         // Crop nRows x nCols tiles
194 |         let nRows = Int(ceil(image.extent.size.height / CGFloat(fixedImageSize)))
195 |         let nCols = Int(ceil(image.extent.size.width / CGFloat(fixedImageSize)))
196 | 
197 |         // Warning: in CIImage, y=0 is the bottom side. We reverse the rows to match the transformers processor
198 |         let tileEdge = Int(fixedImageSize)
199 |         for row in (0 ..< nRows).reversed() {
200 |             for col in 0 ..< nCols {
201 |                 let x0 = col * tileEdge
202 |                 let y0 = row * tileEdge
203 |                 let x1 = min(x0 + tileEdge, Int(image.extent.size.width))
204 |                 let y1 = min(y0 + tileEdge, Int(image.extent.size.height))
205 | 
206 |                 let tile = image.cropped(to: CGRect(x: x0, y: y0, width: x1 - x0, height: y1 - y0))
207 |                 tiles.append(tile)
208 |             }
209 |         }
210 | 
211 |         return (tiles, nRows, nCols)
212 |     }
213 | 
214 |     func formatTimestamp(_ time: CMTime) -> String {
215 |         let totalSeconds = Int(ceil(time.seconds))
216 |         let hours = totalSeconds / 3600
217 |         let minutes = (totalSeconds % 3600) / 60
218 |         let seconds = totalSeconds % 60
219 | 
220 |         return String(format: "%d:%02d:%02d", hours, minutes, seconds)
221 |     }
222 | 
223 |     public func prepare(input: UserInput) async throws -> LMInput {
224 |         let messages = Qwen2VLMessageGenerator().generate(from: input)  // TODO: Create SmolVLM2MessageGenerator
225 | 
226 |         if input.images.isEmpty && input.videos.isEmpty {
227 |             // No image scenario
228 |             let promptTokens = try tokenizer.applyChatTemplate(messages: messages)
229 |             let tokensArray = MLXArray(promptTokens).expandedDimensions(axis: 0)
230 |             let mask = ones(like: tokensArray)
231 |             return LMInput(text: .init(tokens: tokensArray, mask: mask), image: nil)
232 |         } else if input.images.count > 0 && input.videos.isEmpty {
233 |             // Single image scenario
234 |             guard input.images.count == 1 else {
235 |                 throw VLMError.singleImageAllowed
236 |             }
237 | 
238 |             // Unfortunately we don't have a "render" option in Tokenizers yet, so decoding
239 |             let promptTokens = try tokenizer.applyChatTemplate(messages: messages)
240 |             let decoded = try tokenizer.decode(tokens: promptTokens, skipSpecialTokens: false)
241 | 
242 |             let image = try input.images[0].asCIImage().toSRGB()
243 |             let (tiles, imageRows, imageCols) = tiles(from: image)
244 | 
245 |             // Append the resized global image
246 |             // Note we are resampling from the original (potentially larger), not the processing size. It shouldn't make much difference.
247 |             let images =
248 |                 tiles + [
249 |                     image.resampled(
250 |                         to: CGSize(width: fixedImageSize, height: fixedImageSize), method: .lanczos)
251 |                 ]
252 | 
253 |             let pixelsForImages = images.map {
254 |                 $0.normalized(mean: config.imageMeanTuple, std: config.imageStdTuple).asMLXArray()
255 |             }
256 | 
257 |             // In transformers we have a batch dim plus the number of images per batch, and they get collapsed inside the model.
258 |             // Here we provide the compact version.
259 |             let pixels = concatenated(pixelsForImages, axis: 0).transposed(0, 2, 3, 1)
260 | 
261 |             let imagePromptString = getImagePromptString(
262 |                 rows: imageRows,
263 |                 cols: imageCols,
264 |                 seqLen: imageSequenceLength,
265 |                 fakeToken: fakeImageToken,
266 |                 imageToken: imageToken,
267 |                 globalImageToken: globalImageToken
268 |             )
269 | 
270 |             let splitPrompt = decoded.split(by: imageToken, options: .literal)
271 |             let prompt = splitPrompt.joined(separator: imagePromptString)
272 |             let finalPromptTokens = try tokenizer.encode(text: prompt)
273 | 
274 |             let promptArray = MLXArray(finalPromptTokens).expandedDimensions(axis: 0)
275 |             let mask = ones(like: promptArray)
276 | 
277 |             return LMInput(
278 |                 text: .init(tokens: promptArray, mask: mask),
279 |                 image: .init(pixels: pixels)
280 |             )
281 |         } else {
282 |             // Single video scenario
283 |             guard input.images.count == 0 else {
284 |                 throw VLMError.singleMediaTypeAllowed
285 |             }
286 |             guard input.videos.count == 1 else {
287 |                 throw VLMError.singleVideoAllowed
288 |             }
289 | 
290 |             // Insert a default system message if the input doesn't have one
291 |             func messagesWithSystem(_ messages: [Message]) -> [Message] {
292 |                 guard messages.filter { $0["role"] as? String == "system" }.isEmpty else {
293 |                     return messages
294 |                 }
295 | 
296 |                 var messagesWithSystem = messages
297 |                 messagesWithSystem.insert(
298 |                     [
299 |                         "role": "system",
300 |                         "content": [["type": "text", "text": defaultVideoSystemMessage]],
301 |                     ], at: 0)
302 |                 return messagesWithSystem
303 |             }
304 | 
305 |             // Unfortunately we don't have a "render" option in Tokenizers yet, so decoding
306 |             let finalMessages = messagesWithSystem(messages)
307 |             let promptTokens = try tokenizer.applyChatTemplate(
308 |                 messages: messagesWithSystem(messages))
309 |             let decoded = try tokenizer.decode(tokens: promptTokens, skipSpecialTokens: false)
310 | 
311 |             var video = try input.videos[0].asAVAsset()
312 | 
313 |             let processedFrames = await try MediaProcessing.asProcessedSequence(
314 |                 video,
315 |                 maxFrames: maxVideoFrames,
316 |                 targetFPS: { duration in
317 |                     // 1 fps for duration >= 10s, apply a multiplier if smaller
318 |                     max((10 - 0.9 * duration.seconds) * targetVideoFPS, 1)
319 |                 }
320 |             ) { frame in
321 |                 let processedFrame = frame.frame
322 |                     .toSRGB()
323 |                     .resampled(
324 |                         to: CGSize(width: fixedImageSize, height: fixedImageSize), method: .lanczos
325 |                     )
326 |                     .normalized(mean: config.imageMeanTuple, std: config.imageStdTuple)
327 |                 return VideoFrame(frame: processedFrame, timeStamp: frame.timeStamp)
328 |             }
329 | 
330 |             let thwFrames = (0 ..< processedFrames.frames.count).map {
331 |                 THW($0, Int(fixedImageSize), Int(fixedImageSize))
332 |             }
333 | 
334 |             let stackedFrames = concatenated(processedFrames.frames, axis: 0)
335 |             let transposedFrames = stackedFrames.transposed(0, 2, 3, 1)
336 | 
337 |             let videoPromptString = getVideoPromptString(
338 |                 frameCount: processedFrames.frames.count,
339 |                 timeStamps: processedFrames.timestamps.map(formatTimestamp),
340 |                 videoDuration: formatTimestamp(processedFrames.totalDuration),
341 |                 seqLen: imageSequenceLength,
342 |                 fakeToken: fakeImageToken, imageToken: imageToken,
343 |                 globalImageToken: globalImageToken)
344 | 
345 |             let splitPrompt = decoded.split(by: "User: ", options: .literal)
346 |             let prompt = splitPrompt[0] + "User: " + videoPromptString + splitPrompt[1]
347 |             let finalPromptTokens = try tokenizer.encode(text: prompt)
348 | 
349 |             let promptArray = MLXArray(finalPromptTokens).expandedDimensions(axis: 0)
350 |             let mask = ones(like: promptArray)
351 |             return LMInput(
352 |                 text: .init(tokens: promptArray, mask: mask),
353 |                 image: .init(pixels: transposedFrames, frames: thwFrames)
354 |             )
355 |         }
356 |     }
357 | }
358 | 


--------------------------------------------------------------------------------
/Libraries/MLXVLM/README.md:
--------------------------------------------------------------------------------
  1 | # MLXVLM
  2 | 
  3 | # Documentation
  4 | 
  5 | - [Porting and implementing models](https://swiftpackageindex.com/ml-explore/mlx-swift-examples/main/documentation/mlxlmcommon/porting)
  6 | - [MLXLLMCommon](https://swiftpackageindex.com/ml-explore/mlx-swift-examples/main/documentation/mlxlmcommon) -- common API for LLM and VLM
  7 | - [MLXLLM](https://swiftpackageindex.com/ml-explore/mlx-swift-examples/main/documentation/mlxllm) -- large language model example implementations
  8 | - [MLXVLM](https://swiftpackageindex.com/ml-explore/mlx-swift-examples/main/documentation/mlxvlm) -- vision language model example implementations
  9 | 
 10 | # Quick Start
 11 | 
 12 | Using LLMs and VLMs from MLXLMCommon is as easy as:
 13 | 
 14 | ```swift
 15 | let model = try await loadModel(id: "mlx-community/Qwen2.5-VL-3B-Instruct-4bit")
 16 | let session = ChatSession(model)
 17 | 
 18 | let answer1 = try await session.respond(
 19 |     to: "what kind of creature is in the picture?"
 20 |     image: .url(URL(fileURLWithPath: "support/test.jpg"))
 21 | )
 22 | print(answer1)
 23 | 
 24 | // we can ask a followup question referring back to the previous image
 25 | let answer2 = try await session.respond(
 26 |     to: "What is behind the dog?"
 27 | )
 28 | print(answer2)
 29 | ```
 30 | 
 31 | For more information see 
 32 | [Evaluation](https://swiftpackageindex.com/ml-explore/mlx-swift-examples/main/documentation/mlxlmcommon/evaluation)
 33 | or [Using Models](https://swiftpackageindex.com/ml-explore/mlx-swift-examples/main/documentation/mlxlmcommon/using-model)
 34 | for more advanced API.
 35 | 
 36 | # Contents
 37 | 
 38 | This is a port of several models from:
 39 | 
 40 | - https://github.com/Blaizzy/mlx-vlm
 41 | 
 42 | using the Hugging Face swift transformers package to provide tokenization:
 43 | 
 44 | - https://github.com/huggingface/swift-transformers
 45 | 
 46 | The [VLMModelFactory.swift](VLMModelFactory.swift) provides minor overrides and customization --
 47 | if you require overrides for the tokenizer or prompt customizations they can be
 48 | added there.
 49 | 
 50 | This is set up to load models from Hugging Face, e.g. https://huggingface.co/mlx-community
 51 | 
 52 | The following models have been tried:
 53 | 
 54 | - mlx-community/paligemma-3b-mix-448-8bit
 55 | - mlx-community/Qwen2-VL-2B-Instruct-4bit
 56 | 
 57 | Currently supported model types are:
 58 | 
 59 | - paligemma
 60 | - qwen2_vl
 61 | 
 62 | See [llm-tool](../../Tools/llm-tool)
 63 | 
 64 | # Adding a Model
 65 | 
 66 | If the model follows the typical VLM pattern:
 67 | 
 68 | - `config.json`, `tokenizer.json`, and `tokenizer_config.json`
 69 | - `*.safetensors`
 70 | 
 71 | You can follow the pattern of the models in the [Models](Models) directory
 72 | and create a `.swift` file for your new model:
 73 | 
 74 | ## Create a Model Configuration
 75 | 
 76 | Create a configuration struct for both the Text and Vision models
 77 | that matches the structure in `config.json`. A struct like this
 78 | is recommended:
 79 | 
 80 | ```swift
 81 | public struct YourModelConfiguration: Codable, Sendable {
 82 |     public struct TextConfiguration: Codable, Sendable {
 83 |         public let hiddenSize: Int
 84 | 
 85 |         // use this pattern for values that need defaults
 86 |         public let _layerNormEps: Float?
 87 |         public var layerNormEps: Float { _layerNormEps ?? 1e-6 }
 88 |         
 89 |         enum CodingKeys: String, CodingKey {
 90 |             case hiddenSize = "hidden_size"
 91 |             case _layerNormEps = "layer_norm_eps"
 92 |         }
 93 |     }
 94 |     
 95 |     public struct VisionConfiguration: Codable, Sendable {
 96 |         ...
 97 |     }
 98 |     
 99 |     public let textConfiguration: TextConfiguration
100 |     public let visionConfiguration: VisionConfiguration
101 |     public let vocabularySize: Int
102 | 
103 |     enum CodingKeys: String, CodingKey {
104 |         case textConfiguration = "text_config"
105 |         case visionConfiguration = "vision_config"
106 |         case vocabularySize = "vocab_size"
107 |     }
108 | }
109 | ```
110 | 
111 | ## Create a Processor Configuration
112 | 
113 | VLMs also require a image/video preprocessor. Create a configuration to match 
114 | the `preprocessor_config.json` file:
115 | 
116 | ```swift
117 | public struct YourModelProcessorConfiguration: Codable, Sendable {
118 | 
119 |     public struct Size: Codable, Sendable {
120 |         public let width: Int
121 |         public let height: Int
122 | 
123 |         var cgSize: CGSize { .init(width: width, height: height) }
124 |     }
125 | 
126 |     public let imageMean: [CGFloat]
127 |     public let imageStd: [CGFloat]
128 |     public let size: Size
129 | 
130 |     public var imageMeanTuple: (CGFloat, CGFloat, CGFloat) {
131 |         (imageMean[0], imageMean[1], imageMean[2])
132 |     }
133 |     public var imageStdTuple: (CGFloat, CGFloat, CGFloat) {
134 |         (imageStd[0], imageStd[1], imageStd[2])
135 |     }
136 | 
137 |     enum CodingKeys: String, CodingKey {
138 |         case imageMean = "image_mean"
139 |         case imageStd = "image_std"
140 |         case size
141 |     }
142 | }
143 | ```
144 | 
145 | this will be consumed by:
146 | 
147 | ```swift
148 | public class YourModelProcessor: UserInputProcessor {
149 | ...
150 | ```
151 | 
152 | discussed later.
153 | 
154 | ## Create the Vision, Text and VLM Classes
155 | 
156 | VLMs have language and vision models that are aggregated into a single
157 | top level model.
158 | 
159 | For purposes of name spacing you might put the Language and Vision
160 | models into an `enum` to create something structured like this:
161 | 
162 | ```swift
163 | // MARK: - Language
164 | 
165 | private enum Language {
166 | 
167 |     fileprivate class Attention: Module {
168 |         ...
169 |     }
170 |     
171 |     ...
172 |     
173 |     fileprivate class LanguageModel: Module, KVCacheDimensionProvider {
174 |         @ModuleInfo var model: YourModel
175 | 
176 |         var kvHeads: [Int]
177 |         var headDim: MLX.IntOrPair
178 | 
179 |         public init(_ args: YourModelConfiguration.TextConfiguration) {
180 |             self.model = YourModel(args)
181 | 
182 |             self.kvHeads = (0 ..< args.hiddenLayers).map { _ in args.kvHeads }
183 |             }
184 | 
185 |         public func callAsFunction(
186 |             _ inputs: MLXArray, cache: [KVCache]? = nil, inputEmbedding: MLXArray? = nil,
187 |             mask: MLXArray? = nil
188 |         ) -> LMOutput {
189 |             ...
190 |             return LMOutput(logits: ...)
191 |         }
192 |     }
193 | }
194 | ```
195 | 
196 | Similarly the Vision model can go into an `enum` namespace:
197 | 
198 | ```swift
199 | // MARK: - Vision
200 | 
201 | private enum Vision {
202 | 
203 |     fileprivate class Attention: Module {
204 |         ...
205 |     }
206 |     
207 |     fileprivate class VisionModel: Module {
208 | 
209 |         @ModuleInfo(key: "vision_model") var visionModel: InternalVisionModel
210 | 
211 |         public init(_ config: YourModelConfiguration.VisionConfiguration) {
212 |             self._visionModel.wrappedValue = InternalVisionModel(config)
213 |         }
214 | 
215 |         public func callAsFunction(_ x: MLXArray, outputHiddenStates: Bool = false) -> (
216 |             MLXArray, MLXArray, MLXArray?
217 |         ) {
218 |             visionModel(x, outputHiddenStates: outputHiddenStates)
219 |         }
220 |     }
221 | }
222 | ```
223 | 
224 | The exact signatures on the `init()` and `callAsFunction()` can vary as needed --
225 | these models are not exposed to callers.
226 | 
227 | The top level model is the only piece of the model with public API and it
228 | should implement `VLMModel` (aka `LanguageModel`). Here is an outline of how
229 | the top level model might work:
230 | 
231 | ```swift
232 | public class YourModel: Module, VLMModel, KVCacheDimensionProvider {
233 | 
234 |     @ModuleInfo(key: "vision_tower") private var visionModel: Vision.VisionModel
235 |     @ModuleInfo(key: "language_model") private var languageModel: Language.LanguageModel
236 | 
237 |     public let config: YourModelConfiguration
238 | 
239 |     public var vocabularySize: Int { config.vocabularySize }
240 |     public var kvHeads: [Int] { languageModel.kvHeads }
241 |     public var headDim: MLX.IntOrPair { languageModel.headDim }
242 | 
243 |     public func loraLinearLayers() -> MLXLMCommon.LoRALinearLayers {
244 |         languageModel.model.layers.map { ($0.attention, ["q_proj", "v_proj"]) }
245 |     }
246 | 
247 |     public init(_ config: YourModelConfiguration) {
248 |         self.config = config
249 |         self._visionModel.wrappedValue = Vision.VisionModel(config.visionConfiguration)
250 |         self._languageModel.wrappedValue = Language.LanguageModel(config.textConfiguration)
251 |     }
252 | 
253 |     public func prepare(_ input: LMInput, cache: [any KVCache], windowSize: Int?) throws
254 |         -> PrepareResult
255 |     {
256 |         // TODO prepare the cache and resulting logits based on the
257 |         // text prompt and any media assets
258 |         guard let image = input.image else { throw VLMError.imageRequired }
259 |         guard let mask = input.text.mask else { throw VLMError.maskRequired }
260 |         let inputIds = input.text.tokens
261 | 
262 |         let inputEmbedding = inputEmbeddings(
263 |             inputIds: inputIds, pixelValues: image.pixels, mask: mask)
264 | 
265 |         let result = languageModel(
266 |             inputIds, cache: cache, inputEmbedding: inputEmbedding, mask: mask)
267 | 
268 |         return .logits(result)
269 |     }
270 | 
271 |     public func callAsFunction(_ inputs: MLXArray, cache: [any KVCache]?) -> MLXArray {
272 |         // TODO evaluate a step in the language model
273 |         languageModel(inputs, cache: cache).logits
274 |     }
275 | }
276 | ```
277 | 
278 | ## Create the UserInputProcessor
279 | 
280 | VLMs require custom `UserInputProcessor` instances to manipulate the prompts and
281 | media as needed. For example it might:
282 | 
283 | - apply resampling and normalization to the images
284 | - convert the images into an `MLXArray` and build a `THW` struct describing the layout
285 | - modify the prompt by injecting `<image>` tokens that the model expects
286 | 
287 | In the python implementations, much of this code typically lives in the `transformers`
288 | package from huggingface -- inspection will be required to determine which code
289 | is called and what it does. You can examine the processors in the `Models` directory:
290 | they reference the files and functions that they are based on.
291 | 
292 | The `UserInputProcessor` is initialized with the `ProcessorConfiguration` (defined above)
293 | and has a prepare method:
294 | 
295 | ```swift
296 | public func prepare(input: UserInput) throws -> LMInput
297 | ```
298 | 
299 | This is a slight paraphrase of the `PaligemmaUserInputProcessor` as an example:
300 | 
301 | ```swift
302 | public class YourModelProcessor: UserInputProcessor {
303 | 
304 |     private let config: YourModelProcessorConfiguration
305 |     private let tokenizer: any Tokenizer
306 | 
307 |     public init(_ config: YourModelProcessorConfiguration, tokenizer: any Tokenizer) {
308 |         self.config = config
309 |         self.tokenizer = tokenizer
310 |     }
311 | 
312 |     private func prepare(image: CIImage, processing: UserInput.Processing?) -> MLXArray {
313 |         // based on image_processing_siglip from transformers
314 |         var image = image
315 | 
316 |         // we want to do all of the image processing in an sRGB tone curve
317 |         // rather than a linear space as that is what transformers / torch_vision
318 |         // do (implicitly by using sRGB rasters directly)
319 |         image = MediaProcessing.inSRGBToneCurveSpace(image)
320 | 
321 |         // apply user instructions
322 |         image = MediaProcessing.apply(image, processing: processing)
323 | 
324 |         image = MediaProcessing.resampleBicubic(image, to: config.size.cgSize)
325 |         image = MediaProcessing.normalize(
326 |             image, mean: config.imageMeanTuple, std: config.imageStdTuple)
327 | 
328 |         return MediaProcessing.asMLXArray(image)
329 |     }
330 | 
331 |     public func prepare(input: UserInput) throws -> LMInput {
332 |         switch input.images.count {
333 |         case 0: throw VLMError.imageRequired
334 |         case 1: break
335 |         default: throw VLMError.singleImageAllowed
336 |         }
337 | 
338 |         // this doesn't have a chat template so just use the last message.
339 |         var prompt = input.prompt.asMessages().last?["content"] ?? ""
340 | 
341 |         // based on transformers/processing_paligemma
342 |         let count = input.images.count * config.imageSequenceLength
343 |         prompt =
344 |             Array(repeating: "<image>", count: count).joined() + (tokenizer.bosToken ?? "") + prompt
345 |             + "\n"
346 | 
347 |         let promptTokens = try tokenizer.encode(text: prompt)
348 |         let promptArray = MLXArray(promptTokens).expandedDimensions(axis: 0)
349 |         let mask = ones(like: promptArray)
350 | 
351 |         let pixels = try prepare(image: input.images[0].asCIImage(), processing: input.processing)
352 | 
353 |         return LMInput(text: .init(tokens: promptArray, mask: mask), image: .init(pixels: pixels))
354 |     }
355 | 
356 | }
357 | ```
358 | 
359 | Note that the python code may rely on the chat template to inject the image tokens
360 | (paligemma does not). This may have to be expressed in swift code as the current
361 | interface does not support the structured parameters used for this (see Qwen2VL 
362 | processor for an example).
363 | 
364 | ## Register the Model
365 | 
366 | In [VLMModelFactory.swift](VLMModelFactory.swift) register the model type itself
367 | (this is independent of the model id):
368 | 
369 | ```swift
370 | public class VLMTypeRegistry: @unchecked Sendable {
371 | ...
372 |     private var creators: [String: @Sendable (URL) throws -> any LanguageModel] = [
373 |         "yourModel": create(YourModelConfiguration.self, YourModel.init),
374 | ```
375 | 
376 | Similarly, register the UserInputProcessor type (`preprocessor_config.json`):
377 | 
378 | ```swift
379 | public class VLMProcessorTypeRegistry: @unchecked Sendable {
380 | ...
381 |     private var creators:
382 |         [String: @Sendable (URL, any Tokenizer) throws -> any UserInputProcessor] = [
383 |             "YourModelProcessor": create(
384 |                 YourModelProcessorConfiguration.self, YourModelProcessor.init),
385 | ```
386 | 
387 | Add a constant for the model in the VLMRegistry (not strictly required but useful
388 | for callers to refer to it in code):
389 | 
390 | ```swift
391 | public class VLMRegistry: @unchecked Sendable {
392 | ...
393 |     static public let yourModel_4bit = ModelConfiguration(
394 |         id: "mlx-community/YourModel-4bit",
395 |         defaultPrompt: "Describe the image in English"
396 |     )
397 | ```
398 | 
399 | and finally add it to the all list -- this will let users find the model
400 | configuration by id:
401 | 
402 | ```swift
403 |     private static func all() -> [ModelConfiguration] {
404 |         [
405 |             paligemma3bMix4488bit,
406 | ...
407 |             yourModel_4bit,
408 | ```
409 | 
410 | # Using a Model
411 | 
412 | See [MLXLMCommon/README.md](../MLXLMCommon/README.md#using-a-model).
413 | 


--------------------------------------------------------------------------------
/Libraries/MLXVLM/VLMModel.swift:
--------------------------------------------------------------------------------
1 | // Copyright © 2024 Apple Inc.
2 | 
3 | import MLX
4 | import MLXLMCommon
5 | 
6 | public protocol VLMModel: LanguageModel, LoRAModel {
7 | }
8 | 


--------------------------------------------------------------------------------
/Libraries/MLXVLM/VLMModelFactory.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import Foundation
  4 | import Hub
  5 | import MLX
  6 | import MLXLMCommon
  7 | import Tokenizers
  8 | 
  9 | public enum VLMError: LocalizedError {
 10 |     case imageRequired
 11 |     case maskRequired
 12 |     case singleImageAllowed
 13 |     case singleVideoAllowed
 14 |     case singleMediaTypeAllowed
 15 |     case imageProcessingFailure(String)
 16 |     case processing(String)
 17 | 
 18 |     public var errorDescription: String? {
 19 |         switch self {
 20 |         case .imageRequired:
 21 |             return String(localized: "An image is required for this operation.")
 22 |         case .maskRequired:
 23 |             return String(localized: "An image mask is required for this operation.")
 24 |         case .singleImageAllowed:
 25 |             return String(localized: "Only a single image is allowed for this operation.")
 26 |         case .singleVideoAllowed:
 27 |             return String(localized: "Only a single video is allowed for this operation.")
 28 |         case .singleMediaTypeAllowed:
 29 |             return String(
 30 |                 localized:
 31 |                     "Only a single media type (image or video) is allowed for this operation.")
 32 |         case .imageProcessingFailure(let details):
 33 |             return String(localized: "Failed to process the image: \(details)")
 34 |         case .processing(let details):
 35 |             return String(localized: "Processing error: \(details)")
 36 |         }
 37 |     }
 38 | }
 39 | 
 40 | public struct BaseProcessorConfiguration: Codable, Sendable {
 41 |     public let processorClass: String
 42 | 
 43 |     enum CodingKeys: String, CodingKey {
 44 |         case processorClass = "processor_class"
 45 |     }
 46 | }
 47 | 
 48 | /// Creates a function that loads a configuration file and instantiates a model with the proper configuration
 49 | private func create<C: Codable, M>(
 50 |     _ configurationType: C.Type, _ modelInit: @escaping (C) -> M
 51 | ) -> (URL) throws -> M {
 52 |     { url in
 53 |         let configuration = try JSONDecoder().decode(
 54 |             C.self, from: Data(contentsOf: url))
 55 |         return modelInit(configuration)
 56 |     }
 57 | }
 58 | 
 59 | private func create<C: Codable, P>(
 60 |     _ configurationType: C.Type,
 61 |     _ processorInit: @escaping (
 62 |         C,
 63 |         any Tokenizer
 64 |     ) -> P
 65 | ) -> (URL, any Tokenizer) throws -> P {
 66 |     { url, tokenizer in
 67 |         let configuration = try JSONDecoder().decode(
 68 |             C.self, from: Data(contentsOf: url))
 69 |         return processorInit(configuration, tokenizer)
 70 |     }
 71 | }
 72 | 
 73 | /// Registry of model type, e.g 'llama', to functions that can instantiate the model from configuration.
 74 | ///
 75 | /// Typically called via ``LLMModelFactory/load(hub:configuration:progressHandler:)``.
 76 | public class VLMTypeRegistry: ModelTypeRegistry, @unchecked Sendable {
 77 | 
 78 |     /// Shared instance with default model types.
 79 |     public static let shared: VLMTypeRegistry = .init(creators: all())
 80 | 
 81 |     /// All predefined model types
 82 |     private static func all() -> [String: @Sendable (URL) throws -> any LanguageModel] {
 83 |         [
 84 |             "paligemma": create(PaliGemmaConfiguration.self, PaliGemma.init),
 85 |             "qwen2_vl": create(Qwen2VLConfiguration.self, Qwen2VL.init),
 86 |             "qwen2_5_vl": create(Qwen25VLConfiguration.self, Qwen25VL.init),
 87 |             "idefics3": create(Idefics3Configuration.self, Idefics3.init),
 88 |             "gemma3": create(Gemma3Configuration.self, Gemma3.init),
 89 |             "smolvlm": create(SmolVLM2Configuration.self, SmolVLM2.init),
 90 |         ]
 91 |     }
 92 | }
 93 | 
 94 | public class VLMProcessorTypeRegistry: ProcessorTypeRegistry, @unchecked Sendable {
 95 | 
 96 |     /// Shared instance with default processor types.
 97 |     public static let shared: VLMProcessorTypeRegistry = .init(creators: all())
 98 | 
 99 |     /// All predefined processor types.
100 |     private static func all() -> [String: @Sendable (URL, any Tokenizer) throws ->
101 |         any UserInputProcessor]
102 |     {
103 |         [
104 |             "PaliGemmaProcessor": create(
105 |                 PaliGemmaProcessorConfiguration.self, PaliGemmaProcessor.init),
106 |             "Qwen2VLProcessor": create(
107 |                 Qwen2VLProcessorConfiguration.self, Qwen2VLProcessor.init),
108 |             "Qwen2_5_VLProcessor": create(
109 |                 Qwen25VLProcessorConfiguration.self, Qwen25VLProcessor.init),
110 |             "Idefics3Processor": create(
111 |                 Idefics3ProcessorConfiguration.self, Idefics3Processor.init),
112 |             "Gemma3Processor": create(
113 |                 Gemma3ProcessorConfiguration.self, Gemma3Processor.init),
114 |             "SmolVLMProcessor": create(
115 |                 SmolVLMProcessorConfiguration.self, SmolVLMProcessor.init),
116 |         ]
117 |     }
118 | }
119 | 
120 | /// Registry of models and any overrides that go with them, e.g. prompt augmentation.
121 | /// If asked for an unknown configuration this will use the model/tokenizer as-is.
122 | ///
123 | /// The python tokenizers have a very rich set of implementations and configuration. The
124 | /// swift-tokenizers code handles a good chunk of that and this is a place to augment that
125 | /// implementation, if needed.
126 | public class VLMRegistry: AbstractModelRegistry, @unchecked Sendable {
127 | 
128 |     /// Shared instance with default model configurations.
129 |     public static let shared: VLMRegistry = .init(modelConfigurations: all())
130 | 
131 |     static public let paligemma3bMix448_8bit = ModelConfiguration(
132 |         id: "mlx-community/paligemma-3b-mix-448-8bit",
133 |         defaultPrompt: "Describe the image in English"
134 |     )
135 | 
136 |     static public let qwen2VL2BInstruct4Bit = ModelConfiguration(
137 |         id: "mlx-community/Qwen2-VL-2B-Instruct-4bit",
138 |         defaultPrompt: "Describe the image in English"
139 |     )
140 | 
141 |     static public let qwen2_5VL3BInstruct4Bit = ModelConfiguration(
142 |         id: "mlx-community/Qwen2.5-VL-3B-Instruct-4bit",
143 |         defaultPrompt: "Describe the image in English"
144 |     )
145 | 
146 |     static public let smolvlminstruct4bit = ModelConfiguration(
147 |         id: "mlx-community/SmolVLM-Instruct-4bit",
148 |         defaultPrompt: "Describe the image in English"
149 |     )
150 | 
151 |     static public let gemma3_4B_qat_4bit = ModelConfiguration(
152 |         id: "mlx-community/gemma-3-4b-it-qat-4bit",
153 |         defaultPrompt: "Describe the image in English",
154 |         extraEOSTokens: ["<end_of_turn>"]
155 |     )
156 | 
157 |     static public let gemma3_12B_qat_4bit = ModelConfiguration(
158 |         id: "mlx-community/gemma-3-12b-it-qat-4bit",
159 |         defaultPrompt: "Describe the image in English",
160 |         extraEOSTokens: ["<end_of_turn>"]
161 |     )
162 | 
163 |     static public let gemma3_27B_qat_4bit = ModelConfiguration(
164 |         id: "mlx-community/gemma-3-27b-it-qat-4bit",
165 |         defaultPrompt: "Describe the image in English",
166 |         extraEOSTokens: ["<end_of_turn>"]
167 |     )
168 | 
169 |     static public let smolvlm = ModelConfiguration(
170 |         id: "HuggingFaceTB/SmolVLM2-500M-Video-Instruct-mlx",
171 |         defaultPrompt:
172 |             "What is the main action or notable event happening in this segment? Describe it in one brief sentence."
173 |     )
174 | 
175 |     static public func all() -> [ModelConfiguration] {
176 |         [
177 |             paligemma3bMix448_8bit,
178 |             qwen2VL2BInstruct4Bit,
179 |             qwen2_5VL3BInstruct4Bit,
180 |             smolvlminstruct4bit,
181 |             gemma3_4B_qat_4bit,
182 |             gemma3_12B_qat_4bit,
183 |             gemma3_27B_qat_4bit,
184 |             smolvlm,
185 |         ]
186 |     }
187 | 
188 | }
189 | 
190 | @available(*, deprecated, renamed: "VLMRegistry", message: "Please use VLMRegistry directly.")
191 | public typealias ModelRegistry = VLMRegistry
192 | 
193 | /// Factory for creating new LLMs.
194 | ///
195 | /// Callers can use the `shared` instance or create a new instance if custom configuration
196 | /// is required.
197 | ///
198 | /// ```swift
199 | /// let modelContainer = try await VLMModelFactory.shared.loadContainer(
200 | ///     configuration: VLMRegistry.paligemma3bMix4488bit)
201 | /// ```
202 | public class VLMModelFactory: ModelFactory {
203 | 
204 |     public init(
205 |         typeRegistry: ModelTypeRegistry, processorRegistry: ProcessorTypeRegistry,
206 |         modelRegistry: AbstractModelRegistry
207 |     ) {
208 |         self.typeRegistry = typeRegistry
209 |         self.processorRegistry = processorRegistry
210 |         self.modelRegistry = modelRegistry
211 |     }
212 | 
213 |     /// Shared instance with default behavior.
214 |     public static let shared = VLMModelFactory(
215 |         typeRegistry: VLMTypeRegistry.shared, processorRegistry: VLMProcessorTypeRegistry.shared,
216 |         modelRegistry: VLMRegistry.shared)
217 | 
218 |     /// registry of model type, e.g. configuration value `paligemma` -> configuration and init methods
219 |     public let typeRegistry: ModelTypeRegistry
220 | 
221 |     /// registry of input processor type, e.g. configuration value `PaliGemmaProcessor` -> configuration and init methods
222 |     public let processorRegistry: ProcessorTypeRegistry
223 | 
224 |     /// registry of model id to configuration, e.g. `mlx-community/paligemma-3b-mix-448-8bit`
225 |     public let modelRegistry: AbstractModelRegistry
226 | 
227 |     public func _load(
228 |         hub: HubApi, configuration: ModelConfiguration,
229 |         progressHandler: @Sendable @escaping (Progress) -> Void
230 |     ) async throws -> sending ModelContext {
231 |         // download weights and config
232 |         let modelDirectory = try await downloadModel(
233 |             hub: hub, configuration: configuration, progressHandler: progressHandler)
234 | 
235 |         // load the generic config to understand which model and how to load the weights
236 |         let configurationURL = modelDirectory.appending(
237 |             component: "config.json"
238 |         )
239 | 
240 |         let baseConfig: BaseConfiguration
241 |         do {
242 |             baseConfig = try JSONDecoder().decode(
243 |                 BaseConfiguration.self, from: Data(contentsOf: configurationURL))
244 |         } catch let error as DecodingError {
245 |             throw ModelFactoryError.configurationDecodingError(
246 |                 configurationURL.lastPathComponent, configuration.name, error)
247 |         }
248 | 
249 |         let model: LanguageModel
250 |         do {
251 |             model = try typeRegistry.createModel(
252 |                 configuration: configurationURL, modelType: baseConfig.modelType)
253 |         } catch let error as DecodingError {
254 |             throw ModelFactoryError.configurationDecodingError(
255 |                 configurationURL.lastPathComponent, configuration.name, error)
256 |         }
257 | 
258 |         // apply the weights to the bare model
259 |         try loadWeights(
260 |             modelDirectory: modelDirectory, model: model,
261 |             perLayerQuantization: baseConfig.perLayerQuantization)
262 | 
263 |         let tokenizer = try await loadTokenizer(
264 |             configuration: configuration,
265 |             hub: hub
266 |         )
267 | 
268 |         let processorConfigurationURL = modelDirectory.appending(
269 |             component: "preprocessor_config.json"
270 |         )
271 | 
272 |         let baseProcessorConfig: BaseProcessorConfiguration
273 |         do {
274 |             baseProcessorConfig = try JSONDecoder().decode(
275 |                 BaseProcessorConfiguration.self,
276 |                 from: Data(contentsOf: processorConfigurationURL)
277 |             )
278 |         } catch let error as DecodingError {
279 |             throw ModelFactoryError.configurationDecodingError(
280 |                 processorConfigurationURL.lastPathComponent, configuration.name, error)
281 |         }
282 | 
283 |         let processor = try processorRegistry.createModel(
284 |             configuration: processorConfigurationURL,
285 |             processorType: baseProcessorConfig.processorClass, tokenizer: tokenizer)
286 | 
287 |         return .init(
288 |             configuration: configuration, model: model, processor: processor, tokenizer: tokenizer)
289 |     }
290 | 
291 | }
292 | 
293 | public class TrampolineModelFactory: NSObject, ModelFactoryTrampoline {
294 |     public static func modelFactory() -> (any MLXLMCommon.ModelFactory)? {
295 |         VLMModelFactory.shared
296 |     }
297 | }
298 | 


--------------------------------------------------------------------------------
/Libraries/StableDiffusion/Clip.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import Foundation
  4 | import MLX
  5 | import MLXNN
  6 | 
  7 | // port of https://github.com/ml-explore/mlx-examples/blob/main/stable_diffusion/stable_diffusion/clip.py
  8 | 
  9 | struct CLIPOutput {
 10 |     /// The lastHiddenState indexed at the EOS token and possibly projected if
 11 |     /// the model has a projection layer
 12 |     public var pooledOutput: MLXArray
 13 | 
 14 |     /// The full sequence output of the transformer after the final layernorm
 15 |     public var lastHiddenState: MLXArray
 16 | 
 17 |     /// A list of hidden states corresponding to the outputs of the transformer layers
 18 |     public var hiddenStates: [MLXArray]
 19 | }
 20 | 
 21 | /// The transformer encoder layer from CLIP
 22 | class CLIPEncoderLayer: Module {
 23 | 
 24 |     @ModuleInfo(key: "layer_norm1") var layerNorm1: LayerNorm
 25 |     @ModuleInfo(key: "layer_norm2") var layerNorm2: LayerNorm
 26 | 
 27 |     let attention: MultiHeadAttention
 28 | 
 29 |     @ModuleInfo var linear1: Linear
 30 |     @ModuleInfo var linear2: Linear
 31 | 
 32 |     let activation: (MLXArray) -> MLXArray
 33 | 
 34 |     init(modelDimensions: Int, numHeads: Int, activation: @escaping (MLXArray) -> MLXArray) {
 35 |         self._layerNorm1.wrappedValue = LayerNorm(dimensions: modelDimensions)
 36 |         self._layerNorm2.wrappedValue = LayerNorm(dimensions: modelDimensions)
 37 | 
 38 |         self.attention = MultiHeadAttention(
 39 |             dimensions: modelDimensions, numHeads: numHeads, bias: true)
 40 | 
 41 |         self.linear1 = Linear(modelDimensions, 4 * modelDimensions)
 42 |         self.linear2 = Linear(4 * modelDimensions, modelDimensions)
 43 | 
 44 |         self.activation = activation
 45 |     }
 46 | 
 47 |     func callAsFunction(_ x: MLXArray, attentionMask: MLXArray? = nil) -> MLXArray {
 48 |         var y = layerNorm1(x)
 49 |         y = attention(y, keys: y, values: y, mask: attentionMask)
 50 |         var x = y + x
 51 | 
 52 |         y = layerNorm2(x)
 53 |         y = linear1(y)
 54 |         y = activation(y)
 55 |         y = linear2(y)
 56 |         x = y + x
 57 | 
 58 |         return x
 59 |     }
 60 | }
 61 | 
 62 | /// Implements the text encoder transformer from CLIP
 63 | class CLIPTextModel: Module {
 64 | 
 65 |     @ModuleInfo(key: "token_embedding") var tokenEmbedding: Embedding
 66 |     @ModuleInfo(key: "position_embedding") var positionEmbedding: Embedding
 67 | 
 68 |     let layers: [CLIPEncoderLayer]
 69 | 
 70 |     @ModuleInfo(key: "final_layer_norm") var finalLayerNorm: LayerNorm
 71 | 
 72 |     @ModuleInfo(key: "text_projection") var textProjection: Linear?
 73 | 
 74 |     init(configuration: CLIPTextModelConfiguration) {
 75 |         self._tokenEmbedding.wrappedValue = Embedding(
 76 |             embeddingCount: configuration.vocabularySize, dimensions: configuration.modelDimensions)
 77 |         self._positionEmbedding.wrappedValue = Embedding(
 78 |             embeddingCount: configuration.maxLength, dimensions: configuration.modelDimensions)
 79 | 
 80 |         self.layers = (0 ..< configuration.numLayers)
 81 |             .map { _ in
 82 |                 CLIPEncoderLayer(
 83 |                     modelDimensions: configuration.modelDimensions,
 84 |                     numHeads: configuration.numHeads,
 85 |                     activation: configuration.hiddenActivation.activation)
 86 |             }
 87 | 
 88 |         self._finalLayerNorm.wrappedValue = LayerNorm(dimensions: configuration.modelDimensions)
 89 | 
 90 |         if let projectionDimensions = configuration.projectionDimensions {
 91 |             self._textProjection.wrappedValue = Linear(
 92 |                 configuration.modelDimensions, projectionDimensions, bias: false)
 93 |         } else {
 94 |             self._textProjection.wrappedValue = nil
 95 |         }
 96 |     }
 97 | 
 98 |     func mask(_ N: Int, _ dType: DType) -> MLXArray {
 99 |         let indices = MLXArray(0 ..< Int32(N))
100 |         var mask = indices[0..., .newAxis] .< indices[.newAxis]
101 |         mask = mask.asType(dType) * (dType == .float16 ? -6e4 : -1e9)
102 |         return mask
103 |     }
104 | 
105 |     func callAsFunction(_ x: MLXArray) -> CLIPOutput {
106 |         var x = x
107 |         let (_, N) = x.shape2
108 |         let eosTokens = x.argMax(axis: -1)
109 | 
110 |         // compute the embeddings
111 |         x = tokenEmbedding(x)
112 |         x = x + positionEmbedding.weight[..<N]
113 | 
114 |         // compute the features from the transformer
115 |         let mask = mask(N, x.dtype)
116 |         var hiddenStates = [MLXArray]()
117 |         for l in layers {
118 |             x = l(x, attentionMask: mask)
119 |             hiddenStates.append(x)
120 |         }
121 | 
122 |         // apply the final layernorm
123 |         x = finalLayerNorm(x)
124 |         let lastHiddenState = x
125 | 
126 |         // select the EOS token
127 |         var pooledOutput = x[MLXArray(0 ..< x.count), eosTokens]
128 |         if let textProjection {
129 |             pooledOutput = textProjection(pooledOutput)
130 |         }
131 | 
132 |         return CLIPOutput(
133 |             pooledOutput: pooledOutput, lastHiddenState: lastHiddenState, hiddenStates: hiddenStates
134 |         )
135 |     }
136 | }
137 | 


--------------------------------------------------------------------------------
/Libraries/StableDiffusion/Configuration.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import Foundation
  4 | import MLX
  5 | import MLXNN
  6 | 
  7 | // port of https://github.com/ml-explore/mlx-examples/blob/main/stable_diffusion/stable_diffusion/config.py
  8 | 
  9 | /// Configuration for ``Autoencoder``
 10 | struct AutoencoderConfiguration: Codable {
 11 | 
 12 |     public var inputChannels = 3
 13 |     public var outputChannels = 3
 14 |     public var latentChannelsOut: Int { latentChannelsIn * 2 }
 15 |     public var latentChannelsIn = 4
 16 |     public var blockOutChannels = [128, 256, 512, 512]
 17 |     public var layersPerBlock = 2
 18 |     public var normNumGroups = 32
 19 |     public var scalingFactor: Float = 0.18215
 20 | 
 21 |     enum CodingKeys: String, CodingKey {
 22 |         case inputChannels = "in_channels"
 23 |         case outputChannels = "out_channels"
 24 |         case latentChannelsIn = "latent_channels"
 25 |         case blockOutChannels = "block_out_channels"
 26 |         case layersPerBlock = "layers_per_block"
 27 |         case normNumGroups = "norm_num_groups"
 28 |         case scalingFactor = "scaling_factor"
 29 |     }
 30 | 
 31 |     public init(from decoder: any Decoder) throws {
 32 |         let container: KeyedDecodingContainer<AutoencoderConfiguration.CodingKeys> =
 33 |             try decoder.container(keyedBy: AutoencoderConfiguration.CodingKeys.self)
 34 | 
 35 |         // load_autoencoder()
 36 | 
 37 |         self.scalingFactor =
 38 |             try container.decodeIfPresent(Float.self, forKey: .scalingFactor) ?? 0.18215
 39 | 
 40 |         self.inputChannels = try container.decode(Int.self, forKey: .inputChannels)
 41 |         self.outputChannels = try container.decode(Int.self, forKey: .outputChannels)
 42 |         self.latentChannelsIn = try container.decode(Int.self, forKey: .latentChannelsIn)
 43 |         self.blockOutChannels = try container.decode([Int].self, forKey: .blockOutChannels)
 44 |         self.layersPerBlock = try container.decode(Int.self, forKey: .layersPerBlock)
 45 |         self.normNumGroups = try container.decode(Int.self, forKey: .normNumGroups)
 46 |     }
 47 | 
 48 |     public func encode(to encoder: any Encoder) throws {
 49 |         var container: KeyedEncodingContainer<AutoencoderConfiguration.CodingKeys> =
 50 |             encoder.container(keyedBy: AutoencoderConfiguration.CodingKeys.self)
 51 | 
 52 |         try container.encode(self.inputChannels, forKey: .inputChannels)
 53 |         try container.encode(self.outputChannels, forKey: .outputChannels)
 54 |         try container.encode(self.latentChannelsIn, forKey: .latentChannelsIn)
 55 |         try container.encode(self.blockOutChannels, forKey: .blockOutChannels)
 56 |         try container.encode(self.layersPerBlock, forKey: .layersPerBlock)
 57 |         try container.encode(self.normNumGroups, forKey: .normNumGroups)
 58 |         try container.encode(self.scalingFactor, forKey: .scalingFactor)
 59 |     }
 60 | }
 61 | 
 62 | /// Configuration for ``CLIPTextModel``
 63 | struct CLIPTextModelConfiguration: Codable {
 64 | 
 65 |     public enum ClipActivation: String, Codable {
 66 |         case fast = "quick_gelu"
 67 |         case gelu = "gelu"
 68 | 
 69 |         var activation: (MLXArray) -> MLXArray {
 70 |             switch self {
 71 |             case .fast: MLXNN.geluFastApproximate
 72 |             case .gelu: MLXNN.gelu
 73 |             }
 74 |         }
 75 |     }
 76 | 
 77 |     public var numLayers = 23
 78 |     public var modelDimensions = 1024
 79 |     public var numHeads = 16
 80 |     public var maxLength = 77
 81 |     public var vocabularySize = 49408
 82 |     public var projectionDimensions: Int? = nil
 83 |     public var hiddenActivation: ClipActivation = .fast
 84 | 
 85 |     enum CodingKeys: String, CodingKey {
 86 |         case numLayers = "num_hidden_layers"
 87 |         case modelDimensions = "hidden_size"
 88 |         case numHeads = "num_attention_heads"
 89 |         case maxLength = "max_position_embeddings"
 90 |         case vocabularySize = "vocab_size"
 91 |         case projectionDimensions = "projection_dim"
 92 |         case hiddenActivation = "hidden_act"
 93 |         case architectures = "architectures"
 94 |     }
 95 | 
 96 |     public init(from decoder: any Decoder) throws {
 97 |         let container: KeyedDecodingContainer<CLIPTextModelConfiguration.CodingKeys> =
 98 |             try decoder.container(keyedBy: CLIPTextModelConfiguration.CodingKeys.self)
 99 | 
100 |         // see load_text_encoder
101 | 
102 |         let architectures = try container.decode([String].self, forKey: .architectures)
103 |         let withProjection = architectures[0].contains("WithProjection")
104 | 
105 |         self.projectionDimensions =
106 |             withProjection
107 |             ? try container.decodeIfPresent(Int.self, forKey: .projectionDimensions) : nil
108 |         self.hiddenActivation =
109 |             try container.decodeIfPresent(
110 |                 CLIPTextModelConfiguration.ClipActivation.self, forKey: .hiddenActivation) ?? .fast
111 | 
112 |         self.numLayers = try container.decode(Int.self, forKey: .numLayers)
113 |         self.modelDimensions = try container.decode(Int.self, forKey: .modelDimensions)
114 |         self.numHeads = try container.decode(Int.self, forKey: .numHeads)
115 |         self.maxLength = try container.decode(Int.self, forKey: .maxLength)
116 |         self.vocabularySize = try container.decode(Int.self, forKey: .vocabularySize)
117 |     }
118 | 
119 |     public func encode(to encoder: any Encoder) throws {
120 |         var container: KeyedEncodingContainer<CLIPTextModelConfiguration.CodingKeys> =
121 |             encoder.container(keyedBy: CLIPTextModelConfiguration.CodingKeys.self)
122 | 
123 |         if projectionDimensions != nil {
124 |             try container.encode(["WithProjection"], forKey: .architectures)
125 |         } else {
126 |             try container.encode(["Other"], forKey: .architectures)
127 |         }
128 | 
129 |         try container.encode(self.numLayers, forKey: .numLayers)
130 |         try container.encode(self.modelDimensions, forKey: .modelDimensions)
131 |         try container.encode(self.numHeads, forKey: .numHeads)
132 |         try container.encode(self.maxLength, forKey: .maxLength)
133 |         try container.encode(self.vocabularySize, forKey: .vocabularySize)
134 |         try container.encodeIfPresent(self.projectionDimensions, forKey: .projectionDimensions)
135 |         try container.encode(self.hiddenActivation, forKey: .hiddenActivation)
136 |     }
137 | }
138 | 
139 | /// Configuration for ``UNetModel``
140 | struct UNetConfiguration: Codable {
141 | 
142 |     public var inputChannels = 4
143 |     public var outputChannels = 4
144 |     public var convolutionInKernel = 3
145 |     public var convolutionOutKernel = 3
146 |     public var blockOutChannels = [320, 640, 1280, 1280]
147 |     public var layersPerBlock = [2, 2, 2, 2]
148 |     public var midBlockLayers = 2
149 |     public var transformerLayersPerBlock = [2, 2, 2, 2]
150 |     public var numHeads = [5, 10, 20, 20]
151 |     public var crossAttentionDimension = [1024, 1024, 1024, 1024]
152 |     public var normNumGroups = 32
153 |     public var downBlockTypes: [String] = []
154 |     public var upBlockTypes: [String] = []
155 |     public var additionEmbedType: String? = nil
156 |     public var additionTimeEmbedDimension: Int? = nil
157 |     public var projectionClassEmbeddingsInputDimension: Int? = nil
158 | 
159 |     enum CodingKeys: String, CodingKey {
160 |         case inputChannels = "in_channels"
161 |         case outputChannels = "out_channels"
162 |         case convolutionInKernel = "conv_in_kernel"
163 |         case convolutionOutKernel = "conv_out_kernel"
164 |         case blockOutChannels = "block_out_channels"
165 |         case layersPerBlock = "layers_per_block"
166 |         case midBlockLayers = "mid_block_layers"
167 |         case transformerLayersPerBlock = "transformer_layers_per_block"
168 |         case numHeads = "attention_head_dim"
169 |         case crossAttentionDimension = "cross_attention_dim"
170 |         case normNumGroups = "norm_num_groups"
171 |         case downBlockTypes = "down_block_types"
172 |         case upBlockTypes = "up_block_types"
173 |         case additionEmbedType = "addition_embed_type"
174 |         case additionTimeEmbedDimension = "addition_time_embed_dim"
175 |         case projectionClassEmbeddingsInputDimension = "projection_class_embeddings_input_dim"
176 |     }
177 | 
178 |     public init() {
179 |     }
180 | 
181 |     public init(from decoder: Decoder) throws {
182 |         let container: KeyedDecodingContainer<UNetConfiguration.CodingKeys> = try decoder.container(
183 |             keyedBy: UNetConfiguration.CodingKeys.self)
184 | 
185 |         // customizations based on def load_unet(key: str = _DEFAULT_MODEL, float16: bool = False):
186 |         //
187 |         // Note: the encode() writes out the internal format (and this can load it back in)
188 | 
189 |         self.blockOutChannels = try container.decode([Int].self, forKey: .blockOutChannels)
190 |         let nBlocks = blockOutChannels.count
191 | 
192 |         self.layersPerBlock =
193 |             try (try? container.decode([Int].self, forKey: .layersPerBlock))
194 |             ?? Array(repeating: container.decode(Int.self, forKey: .layersPerBlock), count: nBlocks)
195 |         self.transformerLayersPerBlock =
196 |             (try? container.decode([Int].self, forKey: .transformerLayersPerBlock)) ?? [1, 1, 1, 1]
197 |         self.numHeads =
198 |             try (try? container.decodeIfPresent([Int].self, forKey: .numHeads))
199 |             ?? Array(repeating: container.decode(Int.self, forKey: .numHeads), count: nBlocks)
200 |         self.crossAttentionDimension =
201 |             try (try? container.decode([Int].self, forKey: .crossAttentionDimension))
202 |             ?? Array(
203 |                 repeating: container.decode(Int.self, forKey: .crossAttentionDimension),
204 |                 count: nBlocks)
205 |         self.upBlockTypes = try container.decode([String].self, forKey: .upBlockTypes).reversed()
206 | 
207 |         self.convolutionInKernel =
208 |             try container.decodeIfPresent(Int.self, forKey: .convolutionInKernel) ?? 3
209 |         self.convolutionOutKernel =
210 |             try container.decodeIfPresent(Int.self, forKey: .convolutionOutKernel) ?? 3
211 |         self.midBlockLayers = try container.decodeIfPresent(Int.self, forKey: .midBlockLayers) ?? 2
212 | 
213 |         self.inputChannels = try container.decode(Int.self, forKey: .inputChannels)
214 |         self.outputChannels = try container.decode(Int.self, forKey: .outputChannels)
215 |         self.normNumGroups = try container.decode(Int.self, forKey: .normNumGroups)
216 |         self.downBlockTypes = try container.decode([String].self, forKey: .downBlockTypes)
217 |         self.additionEmbedType = try container.decodeIfPresent(
218 |             String.self, forKey: .additionEmbedType)
219 |         self.additionTimeEmbedDimension = try container.decodeIfPresent(
220 |             Int.self, forKey: .additionTimeEmbedDimension)
221 |         self.projectionClassEmbeddingsInputDimension = try container.decodeIfPresent(
222 |             Int.self, forKey: .projectionClassEmbeddingsInputDimension)
223 |     }
224 | 
225 |     public func encode(to encoder: Encoder) throws {
226 |         var container: KeyedEncodingContainer<UNetConfiguration.CodingKeys> = encoder.container(
227 |             keyedBy: UNetConfiguration.CodingKeys.self)
228 | 
229 |         try container.encode(self.upBlockTypes.reversed(), forKey: .upBlockTypes)
230 | 
231 |         try container.encode(self.inputChannels, forKey: .inputChannels)
232 |         try container.encode(self.outputChannels, forKey: .outputChannels)
233 |         try container.encode(self.convolutionInKernel, forKey: .convolutionInKernel)
234 |         try container.encode(self.convolutionOutKernel, forKey: .convolutionOutKernel)
235 |         try container.encode(self.blockOutChannels, forKey: .blockOutChannels)
236 |         try container.encode(self.layersPerBlock, forKey: .layersPerBlock)
237 |         try container.encode(self.midBlockLayers, forKey: .midBlockLayers)
238 |         try container.encode(self.transformerLayersPerBlock, forKey: .transformerLayersPerBlock)
239 |         try container.encode(self.numHeads, forKey: .numHeads)
240 |         try container.encode(self.crossAttentionDimension, forKey: .crossAttentionDimension)
241 |         try container.encode(self.normNumGroups, forKey: .normNumGroups)
242 |         try container.encode(self.downBlockTypes, forKey: .downBlockTypes)
243 |         try container.encodeIfPresent(self.additionEmbedType, forKey: .additionEmbedType)
244 |         try container.encodeIfPresent(
245 |             self.additionTimeEmbedDimension, forKey: .additionTimeEmbedDimension)
246 |         try container.encodeIfPresent(
247 |             self.projectionClassEmbeddingsInputDimension,
248 |             forKey: .projectionClassEmbeddingsInputDimension)
249 |     }
250 | }
251 | 
252 | /// Configuration for ``StableDiffusion``
253 | public struct DiffusionConfiguration: Codable {
254 | 
255 |     public enum BetaSchedule: String, Codable {
256 |         case linear = "linear"
257 |         case scaledLinear = "scaled_linear"
258 |     }
259 | 
260 |     public var betaSchedule = BetaSchedule.scaledLinear
261 |     public var betaStart: Float = 0.00085
262 |     public var betaEnd: Float = 0.012
263 |     public var trainSteps = 3
264 | 
265 |     enum CodingKeys: String, CodingKey {
266 |         case betaSchedule = "beta_schedule"
267 |         case betaStart = "beta_start"
268 |         case betaEnd = "beta_end"
269 |         case trainSteps = "num_train_timesteps"
270 |     }
271 | }
272 | 


--------------------------------------------------------------------------------
/Libraries/StableDiffusion/Image.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import CoreGraphics
  4 | import CoreImage
  5 | import Foundation
  6 | import ImageIO
  7 | import MLX
  8 | import UniformTypeIdentifiers
  9 | 
 10 | enum ImageError: LocalizedError {
 11 |     case failedToSave
 12 |     case unableToOpen
 13 | 
 14 |     var errorDescription: String? {
 15 |         switch self {
 16 |         case .failedToSave:
 17 |             return String(localized: "Failed to save the image to the specified location.")
 18 |         case .unableToOpen:
 19 |             return String(localized: "Unable to open the image file.")
 20 |         }
 21 |     }
 22 | }
 23 | 
 24 | /// Conversion utilities for moving between `MLXArray`, `CGImage` and files.
 25 | public struct Image {
 26 | 
 27 |     public let data: MLXArray
 28 | 
 29 |     /// Create an Image from a MLXArray with ndim == 3
 30 |     public init(_ data: MLXArray) {
 31 |         precondition(data.ndim == 3)
 32 |         self.data = data
 33 |     }
 34 | 
 35 |     /// Create an Image by loading from a file
 36 |     public init(url: URL, maximumEdge: Int? = nil) throws {
 37 |         guard let source = CGImageSourceCreateWithURL(url as CFURL, nil),
 38 |             let image = CGImageSourceCreateImageAtIndex(source, 0, nil)
 39 |         else {
 40 |             throw ImageError.unableToOpen
 41 |         }
 42 | 
 43 |         self.init(image: image)
 44 |     }
 45 | 
 46 |     /// Create an image from a CGImage
 47 |     public init(image: CGImage, maximumEdge: Int? = nil) {
 48 |         // ensure the sizes ar multiples of 64 -- this doesn't worry about
 49 |         // the aspect ratio
 50 | 
 51 |         var width = image.width
 52 |         var height = image.height
 53 | 
 54 |         if let maximumEdge {
 55 |             func scale(_ edge: Int, _ maxEdge: Int) -> Int {
 56 |                 Int(round(Float(maximumEdge) / Float(maxEdge) * Float(edge)))
 57 |             }
 58 | 
 59 |             // aspect fit inside the given maximum
 60 |             if width >= height {
 61 |                 width = scale(width, image.width)
 62 |                 height = scale(height, image.width)
 63 |             } else {
 64 |                 width = scale(width, image.height)
 65 |                 height = scale(height, image.height)
 66 |             }
 67 |         }
 68 | 
 69 |         // size must be multiples of 64 -- coerce without regard to aspect ratio
 70 |         width = width - width % 64
 71 |         height = height - height % 64
 72 | 
 73 |         var raster = Data(count: width * 4 * height)
 74 |         raster.withUnsafeMutableBytes { ptr in
 75 |             let cs = CGColorSpace(name: CGColorSpace.sRGB)!
 76 |             let context = CGContext(
 77 |                 data: ptr.baseAddress, width: width, height: height, bitsPerComponent: 8,
 78 |                 bytesPerRow: width * 4, space: cs,
 79 |                 bitmapInfo: CGImageAlphaInfo.noneSkipLast.rawValue
 80 |                     | CGBitmapInfo.byteOrder32Big.rawValue)!
 81 | 
 82 |             context.draw(
 83 |                 image, in: CGRect(origin: .zero, size: .init(width: width, height: height)))
 84 |         }
 85 | 
 86 |         self.data = MLXArray(raster, [height, width, 4], type: UInt8.self)[0..., 0..., ..<3]
 87 |     }
 88 | 
 89 |     /// Convert the image data to a CGImage
 90 |     public func asCGImage() -> CGImage {
 91 |         var raster = data
 92 | 
 93 |         // we need 4 bytes per pixel
 94 |         if data.dim(-1) == 3 {
 95 |             raster = padded(raster, widths: [0, 0, [0, 1]])
 96 |         }
 97 | 
 98 |         class DataHolder {
 99 |             var data: Data
100 |             init(_ data: Data) {
101 |                 self.data = data
102 |             }
103 |         }
104 | 
105 |         let holder = DataHolder(raster.asData(access: .copy).data)
106 | 
107 |         let payload = Unmanaged.passRetained(holder).toOpaque()
108 |         func release(payload: UnsafeMutableRawPointer?, data: UnsafeMutableRawPointer?) {
109 |             Unmanaged<DataHolder>.fromOpaque(payload!).release()
110 |         }
111 | 
112 |         return holder.data.withUnsafeMutableBytes { ptr in
113 |             let (H, W, C) = raster.shape3
114 |             let cs = CGColorSpace(name: CGColorSpace.sRGB)!
115 | 
116 |             let context = CGContext(
117 |                 data: ptr.baseAddress, width: W, height: H, bitsPerComponent: 8, bytesPerRow: W * C,
118 |                 space: cs,
119 |                 bitmapInfo: CGImageAlphaInfo.noneSkipLast.rawValue
120 |                     | CGBitmapInfo.byteOrder32Big.rawValue, releaseCallback: release,
121 |                 releaseInfo: payload)!
122 |             return context.makeImage()!
123 |         }
124 |     }
125 | 
126 |     /// Convert the image data to a CIImage
127 |     public func asCIImage() -> CIImage {
128 |         // we need 4 bytes per pixel
129 |         var raster = data
130 |         if data.dim(-1) == 3 {
131 |             raster = padded(raster, widths: [0, 0, [0, 1]], value: MLXArray(255))
132 |         }
133 | 
134 |         let arrayData = raster.asData()
135 |         let (H, W, C) = raster.shape3
136 |         let cs = CGColorSpace(name: CGColorSpace.sRGB)!
137 | 
138 |         return CIImage(
139 |             bitmapData: arrayData.data, bytesPerRow: W * 4, size: .init(width: W, height: H),
140 |             format: .RGBA8, colorSpace: cs)
141 |     }
142 | 
143 |     /// Save the image
144 |     public func save(url: URL) throws {
145 |         let uti = UTType(filenameExtension: url.pathExtension) ?? UTType.png
146 | 
147 |         let destination = CGImageDestinationCreateWithURL(
148 |             url as CFURL, uti.identifier as CFString, 1, nil)!
149 |         CGImageDestinationAddImage(destination, asCGImage(), nil)
150 |         if !CGImageDestinationFinalize(destination) {
151 |             throw ImageError.failedToSave
152 |         }
153 |     }
154 | }
155 | 


--------------------------------------------------------------------------------
/Libraries/StableDiffusion/Load.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import Foundation
  4 | import Hub
  5 | import MLX
  6 | import MLXNN
  7 | 
  8 | // port of https://github.com/ml-explore/mlx-examples/blob/main/stable_diffusion/stable_diffusion/model_io.py
  9 | 
 10 | /// Configuration for loading stable diffusion weights.
 11 | ///
 12 | /// These options can be tuned to conserve memory.
 13 | public struct LoadConfiguration: Sendable {
 14 | 
 15 |     /// convert weights to float16
 16 |     public var float16 = true
 17 | 
 18 |     /// quantize weights
 19 |     public var quantize = false
 20 | 
 21 |     public var dType: DType {
 22 |         float16 ? .float16 : .float32
 23 |     }
 24 | 
 25 |     public init(float16: Bool = true, quantize: Bool = false) {
 26 |         self.float16 = float16
 27 |         self.quantize = quantize
 28 |     }
 29 | }
 30 | 
 31 | /// Parameters for evaluating a stable diffusion prompt and generating latents
 32 | public struct EvaluateParameters: Sendable {
 33 | 
 34 |     /// `cfg` value from the preset
 35 |     public var cfgWeight: Float
 36 | 
 37 |     /// number of steps -- default is from the preset
 38 |     public var steps: Int
 39 | 
 40 |     /// number of images to generate at a time
 41 |     public var imageCount = 1
 42 |     public var decodingBatchSize = 1
 43 | 
 44 |     /// size of the latent tensor -- the result image is a factor of 8 larger than this
 45 |     public var latentSize = [64, 64]
 46 | 
 47 |     public var seed: UInt64
 48 |     public var prompt = ""
 49 |     public var negativePrompt = ""
 50 | 
 51 |     public init(
 52 |         cfgWeight: Float, steps: Int, imageCount: Int = 1, decodingBatchSize: Int = 1,
 53 |         latentSize: [Int] = [64, 64], seed: UInt64? = nil, prompt: String = "",
 54 |         negativePrompt: String = ""
 55 |     ) {
 56 |         self.cfgWeight = cfgWeight
 57 |         self.steps = steps
 58 |         self.imageCount = imageCount
 59 |         self.decodingBatchSize = decodingBatchSize
 60 |         self.latentSize = latentSize
 61 |         self.seed = seed ?? UInt64(Date.timeIntervalSinceReferenceDate * 1000)
 62 |         self.prompt = prompt
 63 |         self.negativePrompt = negativePrompt
 64 |     }
 65 | }
 66 | 
 67 | /// File types for ``StableDiffusionConfiguration/files``. Used by the presets to provide
 68 | /// relative file paths for different types of files.
 69 | enum FileKey {
 70 |     case unetConfig
 71 |     case unetWeights
 72 |     case textEncoderConfig
 73 |     case textEncoderWeights
 74 |     case textEncoderConfig2
 75 |     case textEncoderWeights2
 76 |     case vaeConfig
 77 |     case vaeWeights
 78 |     case diffusionConfig
 79 |     case tokenizerVocabulary
 80 |     case tokenizerMerges
 81 |     case tokenizerVocabulary2
 82 |     case tokenizerMerges2
 83 | }
 84 | 
 85 | /// Stable diffusion configuration -- this selects the model to load.
 86 | ///
 87 | /// Use the preset values:
 88 | /// - ``presetSDXLTurbo``
 89 | /// - ``presetStableDiffusion21Base``
 90 | ///
 91 | /// or use the enum (convenient for command line tools):
 92 | ///
 93 | /// - ``Preset/sdxlTurbo``
 94 | /// - ``Preset/sdxlTurbo``
 95 | ///
 96 | /// Call ``download(hub:progressHandler:)`` to download the weights, then
 97 | /// ``textToImageGenerator(hub:configuration:)`` or
 98 | /// ``imageToImageGenerator(hub:configuration:)`` to produce the ``ImageGenerator``.
 99 | ///
100 | /// The ``ImageGenerator`` has a method to generate the latents:
101 | /// - ``TextToImageGenerator/generateLatents(parameters:)``
102 | /// - ``ImageToImageGenerator/generateLatents(image:parameters:strength:)``
103 | ///
104 | /// Evaluate each of the latents from that iterator and use the decoder to turn the last latent
105 | /// into an image:
106 | ///
107 | /// - ``ImageGenerator/decode(xt:)``
108 | ///
109 | /// Finally use ``Image`` to save it to a file or convert to a CGImage for display.
110 | public struct StableDiffusionConfiguration: Sendable {
111 |     public let id: String
112 |     let files: [FileKey: String]
113 |     public let defaultParameters: @Sendable () -> EvaluateParameters
114 |     let factory:
115 |         @Sendable (HubApi, StableDiffusionConfiguration, LoadConfiguration) throws ->
116 |             StableDiffusion
117 | 
118 |     public func download(
119 |         hub: HubApi = HubApi(), progressHandler: @escaping (Progress) -> Void = { _ in }
120 |     ) async throws {
121 |         let repo = Hub.Repo(id: self.id)
122 |         try await hub.snapshot(
123 |             from: repo, matching: Array(files.values), progressHandler: progressHandler)
124 |     }
125 | 
126 |     public func textToImageGenerator(hub: HubApi = HubApi(), configuration: LoadConfiguration)
127 |         throws -> TextToImageGenerator?
128 |     {
129 |         try factory(hub, self, configuration) as? TextToImageGenerator
130 |     }
131 | 
132 |     public func imageToImageGenerator(hub: HubApi = HubApi(), configuration: LoadConfiguration)
133 |         throws -> ImageToImageGenerator?
134 |     {
135 |         try factory(hub, self, configuration) as? ImageToImageGenerator
136 |     }
137 | 
138 |     public enum Preset: String, Codable, CaseIterable, Sendable {
139 |         case base
140 |         case sdxlTurbo = "sdxl-turbo"
141 | 
142 |         public var configuration: StableDiffusionConfiguration {
143 |             switch self {
144 |             case .base: presetStableDiffusion21Base
145 |             case .sdxlTurbo: presetSDXLTurbo
146 |             }
147 |         }
148 |     }
149 | 
150 |     /// See https://huggingface.co/stabilityai/sdxl-turbo for the model details and license
151 |     public static let presetSDXLTurbo = StableDiffusionConfiguration(
152 |         id: "stabilityai/sdxl-turbo",
153 |         files: [
154 |             .unetConfig: "unet/config.json",
155 |             .unetWeights: "unet/diffusion_pytorch_model.safetensors",
156 |             .textEncoderConfig: "text_encoder/config.json",
157 |             .textEncoderWeights: "text_encoder/model.safetensors",
158 |             .textEncoderConfig2: "text_encoder_2/config.json",
159 |             .textEncoderWeights2: "text_encoder_2/model.safetensors",
160 |             .vaeConfig: "vae/config.json",
161 |             .vaeWeights: "vae/diffusion_pytorch_model.safetensors",
162 |             .diffusionConfig: "scheduler/scheduler_config.json",
163 |             .tokenizerVocabulary: "tokenizer/vocab.json",
164 |             .tokenizerMerges: "tokenizer/merges.txt",
165 |             .tokenizerVocabulary2: "tokenizer_2/vocab.json",
166 |             .tokenizerMerges2: "tokenizer_2/merges.txt",
167 |         ],
168 |         defaultParameters: { EvaluateParameters(cfgWeight: 0, steps: 2) },
169 |         factory: { hub, sdConfiguration, loadConfiguration in
170 |             let sd = try StableDiffusionXL(
171 |                 hub: hub, configuration: sdConfiguration, dType: loadConfiguration.dType)
172 |             if loadConfiguration.quantize {
173 |                 quantize(model: sd.textEncoder, filter: { k, m in m is Linear })
174 |                 quantize(model: sd.textEncoder2, filter: { k, m in m is Linear })
175 |                 quantize(model: sd.unet, groupSize: 32, bits: 8)
176 |             }
177 |             return sd
178 |         }
179 |     )
180 | 
181 |     /// See https://huggingface.co/stabilityai/stable-diffusion-2-1-base for the model details and license
182 |     public static let presetStableDiffusion21Base = StableDiffusionConfiguration(
183 |         id: "stabilityai/stable-diffusion-2-1-base",
184 |         files: [
185 |             .unetConfig: "unet/config.json",
186 |             .unetWeights: "unet/diffusion_pytorch_model.safetensors",
187 |             .textEncoderConfig: "text_encoder/config.json",
188 |             .textEncoderWeights: "text_encoder/model.safetensors",
189 |             .vaeConfig: "vae/config.json",
190 |             .vaeWeights: "vae/diffusion_pytorch_model.safetensors",
191 |             .diffusionConfig: "scheduler/scheduler_config.json",
192 |             .tokenizerVocabulary: "tokenizer/vocab.json",
193 |             .tokenizerMerges: "tokenizer/merges.txt",
194 |         ],
195 |         defaultParameters: { EvaluateParameters(cfgWeight: 7.5, steps: 50) },
196 |         factory: { hub, sdConfiguration, loadConfiguration in
197 |             let sd = try StableDiffusionBase(
198 |                 hub: hub, configuration: sdConfiguration, dType: loadConfiguration.dType)
199 |             if loadConfiguration.quantize {
200 |                 quantize(model: sd.textEncoder, filter: { k, m in m is Linear })
201 |                 quantize(model: sd.unet, groupSize: 32, bits: 8)
202 |             }
203 |             return sd
204 |         }
205 |     )
206 | 
207 | }
208 | 
209 | // MARK: - Key Mapping
210 | 
211 | func keyReplace(_ replace: String, _ with: String) -> @Sendable (String) -> String? {
212 |     return { [replace, with] key in
213 |         if key.contains(replace) {
214 |             return key.replacingOccurrences(of: replace, with: with)
215 |         }
216 |         return nil
217 |     }
218 | }
219 | 
220 | func dropPrefix(_ prefix: String) -> @Sendable (String) -> String? {
221 |     return { [prefix] key in
222 |         if key.hasPrefix(prefix) {
223 |             return String(key.dropFirst(prefix.count))
224 |         }
225 |         return nil
226 |     }
227 | }
228 | 
229 | // see map_unet_weights()
230 | 
231 | let unetRules: [@Sendable (String) -> String?] = [
232 |     // Map up/downsampling
233 |     keyReplace("downsamplers.0.conv", "downsample"),
234 |     keyReplace("upsamplers.0.conv", "upsample"),
235 | 
236 |     // Map the mid block
237 |     keyReplace("mid_block.resnets.0", "mid_blocks.0"),
238 |     keyReplace("mid_block.attentions.0", "mid_blocks.1"),
239 |     keyReplace("mid_block.resnets.1", "mid_blocks.2"),
240 | 
241 |     // Map attention layers
242 |     keyReplace("to_k", "key_proj"),
243 |     keyReplace("to_out.0", "out_proj"),
244 |     keyReplace("to_q", "query_proj"),
245 |     keyReplace("to_v", "value_proj"),
246 | 
247 |     // Map transformer ffn
248 |     keyReplace("ff.net.2", "linear3"),
249 | ]
250 | 
251 | func unetRemap(key: String, value: MLXArray) -> [(String, MLXArray)] {
252 |     var key = key
253 |     var value = value
254 | 
255 |     for rule in unetRules {
256 |         key = rule(key) ?? key
257 |     }
258 | 
259 |     // Map transformer ffn
260 |     if key.contains("ff.net.0") {
261 |         let k1 = key.replacingOccurrences(of: "ff.net.0.proj", with: "linear1")
262 |         let k2 = key.replacingOccurrences(of: "ff.net.0.proj", with: "linear2")
263 |         let (v1, v2) = value.split()
264 |         return [(k1, v1), (k2, v2)]
265 |     }
266 | 
267 |     if key.contains("conv_shortcut.weight") {
268 |         value = value.squeezed()
269 |     }
270 | 
271 |     // Transform the weights from 1x1 convs to linear
272 |     if value.ndim == 4 && (key.contains("proj_in") || key.contains("proj_out")) {
273 |         value = value.squeezed()
274 |     }
275 | 
276 |     if value.ndim == 4 {
277 |         value = value.transposed(0, 2, 3, 1)
278 |         value = value.reshaped(-1).reshaped(value.shape)
279 |     }
280 | 
281 |     return [(key, value)]
282 | }
283 | 
284 | let clipRules: [@Sendable (String) -> String?] = [
285 |     dropPrefix("text_model."),
286 |     dropPrefix("embeddings."),
287 |     dropPrefix("encoder."),
288 | 
289 |     // Map attention layers
290 |     keyReplace("self_attn.", "attention."),
291 |     keyReplace("q_proj.", "query_proj."),
292 |     keyReplace("k_proj.", "key_proj."),
293 |     keyReplace("v_proj.", "value_proj."),
294 | 
295 |     // Map ffn layers
296 |     keyReplace("mlp.fc1", "linear1"),
297 |     keyReplace("mlp.fc2", "linear2"),
298 | ]
299 | 
300 | func clipRemap(key: String, value: MLXArray) -> [(String, MLXArray)] {
301 |     var key = key
302 | 
303 |     for rule in clipRules {
304 |         key = rule(key) ?? key
305 |     }
306 | 
307 |     // not used
308 |     if key == "position_ids" {
309 |         return []
310 |     }
311 | 
312 |     return [(key, value)]
313 | }
314 | 
315 | let vaeRules: [@Sendable (String) -> String?] = [
316 |     // Map up/downsampling
317 |     keyReplace("downsamplers.0.conv", "downsample"),
318 |     keyReplace("upsamplers.0.conv", "upsample"),
319 | 
320 |     // Map attention layers
321 |     keyReplace("to_k", "key_proj"),
322 |     keyReplace("to_out.0", "out_proj"),
323 |     keyReplace("to_q", "query_proj"),
324 |     keyReplace("to_v", "value_proj"),
325 | 
326 |     // Map the mid block
327 |     keyReplace("mid_block.resnets.0", "mid_blocks.0"),
328 |     keyReplace("mid_block.attentions.0", "mid_blocks.1"),
329 |     keyReplace("mid_block.resnets.1", "mid_blocks.2"),
330 | 
331 |     keyReplace("mid_blocks.1.key.", "mid_blocks.1.key_proj."),
332 |     keyReplace("mid_blocks.1.query.", "mid_blocks.1.query_proj."),
333 |     keyReplace("mid_blocks.1.value.", "mid_blocks.1.value_proj."),
334 |     keyReplace("mid_blocks.1.proj_attn.", "mid_blocks.1.out_proj."),
335 | 
336 | ]
337 | 
338 | func vaeRemap(key: String, value: MLXArray) -> [(String, MLXArray)] {
339 |     var key = key
340 |     var value = value
341 | 
342 |     for rule in vaeRules {
343 |         key = rule(key) ?? key
344 |     }
345 | 
346 |     // Map the quant/post_quant layers
347 |     if key.contains("quant_conv") {
348 |         key = key.replacingOccurrences(of: "quant_conv", with: "quant_proj")
349 |         value = value.squeezed()
350 |     }
351 | 
352 |     // Map the conv_shortcut to linear
353 |     if key.contains("conv_shortcut.weight") {
354 |         value = value.squeezed()
355 |     }
356 | 
357 |     if value.ndim == 4 {
358 |         value = value.transposed(0, 2, 3, 1)
359 |         value = value.reshaped(-1).reshaped(value.shape)
360 |     }
361 | 
362 |     return [(key, value)]
363 | }
364 | 
365 | func loadWeights(
366 |     url: URL, model: Module, mapper: (String, MLXArray) -> [(String, MLXArray)], dType: DType
367 | ) throws {
368 |     let weights = try loadArrays(url: url).flatMap { mapper($0.key, $0.value.asType(dType)) }
369 | 
370 |     // Note: not using verifier because some shapes change upon load
371 |     try model.update(parameters: ModuleParameters.unflattened(weights), verify: .none)
372 | }
373 | 
374 | // MARK: - Loading
375 | 
376 | func resolve(hub: HubApi, configuration: StableDiffusionConfiguration, key: FileKey) -> URL {
377 |     precondition(
378 |         configuration.files[key] != nil, "configuration \(configuration.id) missing key: \(key)")
379 |     let repo = Hub.Repo(id: configuration.id)
380 |     let directory = hub.localRepoLocation(repo)
381 |     return directory.appending(component: configuration.files[key]!)
382 | }
383 | 
384 | func loadConfiguration<T: Decodable>(
385 |     hub: HubApi, configuration: StableDiffusionConfiguration, key: FileKey, type: T.Type
386 | ) throws -> T {
387 |     let url = resolve(hub: hub, configuration: configuration, key: key)
388 |     return try JSONDecoder().decode(T.self, from: Data(contentsOf: url))
389 | }
390 | 
391 | func loadUnet(hub: HubApi, configuration: StableDiffusionConfiguration, dType: DType) throws
392 |     -> UNetModel
393 | {
394 |     let unetConfiguration = try loadConfiguration(
395 |         hub: hub, configuration: configuration, key: .unetConfig, type: UNetConfiguration.self)
396 |     let model = UNetModel(configuration: unetConfiguration)
397 | 
398 |     let weightsURL = resolve(hub: hub, configuration: configuration, key: .unetWeights)
399 |     try loadWeights(url: weightsURL, model: model, mapper: unetRemap, dType: dType)
400 | 
401 |     return model
402 | }
403 | 
404 | func loadTextEncoder(
405 |     hub: HubApi, configuration: StableDiffusionConfiguration,
406 |     configKey: FileKey = .textEncoderConfig, weightsKey: FileKey = .textEncoderWeights, dType: DType
407 | ) throws -> CLIPTextModel {
408 |     let clipConfiguration = try loadConfiguration(
409 |         hub: hub, configuration: configuration, key: configKey,
410 |         type: CLIPTextModelConfiguration.self)
411 |     let model = CLIPTextModel(configuration: clipConfiguration)
412 | 
413 |     let weightsURL = resolve(hub: hub, configuration: configuration, key: weightsKey)
414 |     try loadWeights(url: weightsURL, model: model, mapper: clipRemap, dType: dType)
415 | 
416 |     return model
417 | }
418 | 
419 | func loadAutoEncoder(hub: HubApi, configuration: StableDiffusionConfiguration, dType: DType) throws
420 |     -> Autoencoder
421 | {
422 |     let autoEncoderConfiguration = try loadConfiguration(
423 |         hub: hub, configuration: configuration, key: .vaeConfig, type: AutoencoderConfiguration.self
424 |     )
425 |     let model = Autoencoder(configuration: autoEncoderConfiguration)
426 | 
427 |     let weightsURL = resolve(hub: hub, configuration: configuration, key: .vaeWeights)
428 |     try loadWeights(url: weightsURL, model: model, mapper: vaeRemap, dType: dType)
429 | 
430 |     return model
431 | }
432 | 
433 | func loadDiffusionConfiguration(hub: HubApi, configuration: StableDiffusionConfiguration) throws
434 |     -> DiffusionConfiguration
435 | {
436 |     try loadConfiguration(
437 |         hub: hub, configuration: configuration, key: .diffusionConfig,
438 |         type: DiffusionConfiguration.self)
439 | }
440 | 
441 | // MARK: - Tokenizer
442 | 
443 | func loadTokenizer(
444 |     hub: HubApi, configuration: StableDiffusionConfiguration,
445 |     vocabulary: FileKey = .tokenizerVocabulary, merges: FileKey = .tokenizerMerges
446 | ) throws -> CLIPTokenizer {
447 |     let vocabularyURL = resolve(hub: hub, configuration: configuration, key: vocabulary)
448 |     let mergesURL = resolve(hub: hub, configuration: configuration, key: merges)
449 | 
450 |     let vocabulary = try JSONDecoder().decode(
451 |         [String: Int].self, from: Data(contentsOf: vocabularyURL))
452 |     let merges = try String(contentsOf: mergesURL)
453 |         .components(separatedBy: .newlines)
454 |         // first line is a comment
455 |         .dropFirst()
456 |         .filter { !$0.isEmpty }
457 | 
458 |     return CLIPTokenizer(merges: merges, vocabulary: vocabulary)
459 | }
460 | 


--------------------------------------------------------------------------------
/Libraries/StableDiffusion/README.md:
--------------------------------------------------------------------------------
 1 | #  Stable Diffusion
 2 | 
 3 | Stable Diffusion in MLX. The implementation was ported from Hugging Face's
 4 | [diffusers](https://huggingface.co/docs/diffusers/index) and 
 5 | [mlx-examples/stable_diffusion](https://github.com/ml-explore/mlx-examples/tree/main/stable_diffusion).
 6 | Model weights are downloaded directly from the Hugging Face hub. The implementation currently
 7 | supports the following models:
 8 | 
 9 | - [stabilityai/sdxl-turbo](https://huggingface.co/stabilityai/sdxl-turbo)
10 | - [stabilitiai/stable-diffusion-2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1)
11 | 
12 | ## Usage
13 | 
14 | See [StableDiffusionExample](../../Applications/StableDiffusionExample) and
15 | [image-tool](../../Tools/image-tool) for examples of using this code.
16 | 
17 | The basic sequence is:
18 | 
19 | - download & load the model
20 | - generate latents
21 | - evaluate the latents one by one
22 | - decode the last latent generated
23 | - you have an image!
24 | 
25 | ```swift
26 | let configuration = StableDiffusionConfiguration.presetSDXLTurbo
27 | 
28 | let generator = try configuration.textToImageGenerator(
29 |     configuration: model.loadConfiguration)
30 | 
31 | generator.ensureLoaded()
32 | 
33 | // Generate the latents, which are the iterations for generating
34 | // the output image. This is just generating the evaluation graph
35 | let parameters = generate.evaluateParameters(configuration: configuration)
36 | let latents = generator.generateLatents(parameters: parameters)
37 | 
38 | // evaluate the latents (evalue the graph) and keep the last value generated
39 | var lastXt: MLXArray?
40 | for xt in latents {
41 |     eval(xt)
42 |     lastXt = xt
43 | }
44 | 
45 | // decode the final latent into an image
46 | if let lastXt {
47 |     var raster = decoder(lastXt[0])
48 |     raster = (image * 255).asType(.uint8).squeezed()
49 |     eval(raster)
50 |     
51 |     // turn it into a CGImage
52 |     let image = Image(raster).asCGImage()
53 |     
54 |     // or write it out
55 |     try Image(raster).save(url: url)
56 | }
57 | ```
58 | 


--------------------------------------------------------------------------------
/Libraries/StableDiffusion/Sampler.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import Foundation
  4 | import MLX
  5 | 
  6 | // port of https://github.com/ml-explore/mlx-examples/blob/main/stable_diffusion/stable_diffusion/sampler.py
  7 | 
  8 | /// Interpolate the function defined by `(0 ..< y.count) y)` at positions `xNew`.
  9 | func interpolate(y: MLXArray, xNew: MLXArray) -> MLXArray {
 10 |     let xLow = xNew.asType(.int32)
 11 |     let xHigh = minimum(xLow + 1, y.count - 1)
 12 | 
 13 |     let yLow = y[xLow]
 14 |     let yHigh = y[xHigh]
 15 |     let deltaX = xNew - xLow
 16 |     let yNew = yLow * (1 - deltaX) + deltaX * yHigh
 17 | 
 18 |     return yNew
 19 | }
 20 | 
 21 | /// A simple Euler integrator that can be used to sample from our diffusion models.
 22 | ///
 23 | /// The method ``step()`` performs one Euler step from `x_t` to `x_t_prev`.
 24 | class SimpleEulerSampler {
 25 | 
 26 |     let sigmas: MLXArray
 27 | 
 28 |     public init(configuration: DiffusionConfiguration) {
 29 |         let betas: MLXArray
 30 | 
 31 |         // compute the noise schedule
 32 |         switch configuration.betaSchedule {
 33 |         case .linear:
 34 |             betas = MLXArray.linspace(
 35 |                 configuration.betaStart, configuration.betaEnd, count: configuration.trainSteps)
 36 |         case .scaledLinear:
 37 |             betas = MLXArray.linspace(
 38 |                 sqrt(configuration.betaStart), sqrt(configuration.betaEnd),
 39 |                 count: configuration.trainSteps
 40 |             ).square()
 41 |         }
 42 | 
 43 |         let alphas = 1 - betas
 44 |         let alphasCumprod = cumprod(alphas)
 45 | 
 46 |         self.sigmas = concatenated([
 47 |             MLXArray.zeros([1]), ((1 - alphasCumprod) / alphasCumprod).sqrt(),
 48 |         ])
 49 |     }
 50 | 
 51 |     public var maxTime: Int {
 52 |         sigmas.count - 1
 53 |     }
 54 | 
 55 |     public func samplePrior(shape: [Int], dType: DType = .float32, key: MLXArray? = nil) -> MLXArray
 56 |     {
 57 |         let noise = MLXRandom.normal(shape, key: key)
 58 |         return (noise * sigmas[-1] * (sigmas[-1].square() + 1).rsqrt()).asType(dType)
 59 |     }
 60 | 
 61 |     public func addNoise(x: MLXArray, t: MLXArray, key: MLXArray? = nil) -> MLXArray {
 62 |         let noise = MLXRandom.normal(x.shape, key: key)
 63 |         let s = sigmas(t)
 64 |         return (x + noise * s) * (s.square() + 1).rsqrt()
 65 |     }
 66 | 
 67 |     public func sigmas(_ t: MLXArray) -> MLXArray {
 68 |         interpolate(y: sigmas, xNew: t)
 69 |     }
 70 | 
 71 |     public func timeSteps(steps: Int, start: Int? = nil, dType: DType = .float32) -> [(
 72 |         MLXArray, MLXArray
 73 |     )] {
 74 |         let start = start ?? (sigmas.count - 1)
 75 |         precondition(0 < start)
 76 |         precondition(start <= sigmas.count - 1)
 77 |         let steps = MLX.linspace(start, 0, count: steps + 1).asType(dType)
 78 | 
 79 |         return Array(zip(steps, steps[1...]))
 80 |     }
 81 | 
 82 |     open func step(epsPred: MLXArray, xt: MLXArray, t: MLXArray, tPrev: MLXArray) -> MLXArray {
 83 |         let dtype = epsPred.dtype
 84 |         let sigma = sigmas(t).asType(dtype)
 85 |         let sigmaPrev = sigmas(tPrev).asType(dtype)
 86 | 
 87 |         let dt = sigmaPrev - sigma
 88 |         var xtPrev = (sigma.square() + 1).sqrt() * xt + epsPred * dt
 89 |         xtPrev = xtPrev * (sigmaPrev.square() + 1).rsqrt()
 90 | 
 91 |         return xtPrev
 92 |     }
 93 | }
 94 | 
 95 | class SimpleEulerAncestralSampler: SimpleEulerSampler {
 96 | 
 97 |     open override func step(epsPred: MLXArray, xt: MLXArray, t: MLXArray, tPrev: MLXArray)
 98 |         -> MLXArray
 99 |     {
100 |         let dtype = epsPred.dtype
101 |         let sigma = sigmas(t).asType(dtype)
102 |         let sigmaPrev = sigmas(tPrev).asType(dtype)
103 | 
104 |         let sigma2 = sigma.square()
105 |         let sigmaPrev2 = sigmaPrev.square()
106 |         let sigmaUp = (sigmaPrev2 * (sigma2 - sigmaPrev2) / sigma2).sqrt()
107 |         let sigmaDown = (sigmaPrev2 - sigmaUp ** 2).sqrt()
108 | 
109 |         let dt = sigmaDown - sigma
110 |         var xtPrev = (sigma2 + 1).sqrt() * xt + epsPred * dt
111 |         let noise = MLXRandom.normal(xtPrev.shape).asType(xtPrev.dtype)
112 |         xtPrev = xtPrev + noise * sigmaUp
113 |         xtPrev = xtPrev * (sigmaPrev2 + 1).rsqrt()
114 | 
115 |         return xtPrev
116 |     }
117 | }
118 | 


--------------------------------------------------------------------------------
/Libraries/StableDiffusion/StableDiffusion.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import Foundation
  4 | import Hub
  5 | import MLX
  6 | import MLXNN
  7 | 
  8 | // port of https://github.com/ml-explore/mlx-examples/blob/main/stable_diffusion/stable_diffusion/__init__.py
  9 | 
 10 | /// Iterator that produces latent images.
 11 | ///
 12 | /// Created by:
 13 | ///
 14 | /// - ``TextToImageGenerator/generateLatents(parameters:)``
 15 | /// - ``ImageToImageGenerator/generateLatents(image:parameters:strength:)``
 16 | public struct DenoiseIterator: Sequence, IteratorProtocol {
 17 | 
 18 |     let sd: StableDiffusion
 19 | 
 20 |     var xt: MLXArray
 21 | 
 22 |     let conditioning: MLXArray
 23 |     let cfgWeight: Float
 24 |     let textTime: (MLXArray, MLXArray)?
 25 | 
 26 |     var i: Int
 27 |     let steps: [(MLXArray, MLXArray)]
 28 | 
 29 |     init(
 30 |         sd: StableDiffusion, xt: MLXArray, t: Int, conditioning: MLXArray, steps: Int,
 31 |         cfgWeight: Float, textTime: (MLXArray, MLXArray)? = nil
 32 |     ) {
 33 |         self.sd = sd
 34 |         self.steps = sd.sampler.timeSteps(steps: steps, start: t, dType: sd.dType)
 35 |         self.i = 0
 36 |         self.xt = xt
 37 |         self.conditioning = conditioning
 38 |         self.cfgWeight = cfgWeight
 39 |         self.textTime = textTime
 40 |     }
 41 | 
 42 |     public var underestimatedCount: Int {
 43 |         steps.count
 44 |     }
 45 | 
 46 |     mutating public func next() -> MLXArray? {
 47 |         guard i < steps.count else {
 48 |             return nil
 49 |         }
 50 | 
 51 |         let (t, tPrev) = steps[i]
 52 |         i += 1
 53 | 
 54 |         xt = sd.step(
 55 |             xt: xt, t: t, tPrev: tPrev, conditioning: conditioning, cfgWeight: cfgWeight,
 56 |             textTime: textTime)
 57 |         return xt
 58 |     }
 59 | }
 60 | 
 61 | /// Type for the _decoder_ step.
 62 | public typealias ImageDecoder = (MLXArray) -> MLXArray
 63 | 
 64 | public protocol ImageGenerator {
 65 |     func ensureLoaded()
 66 | 
 67 |     /// Return a detached decoder -- this is useful if trying to conserve memory.
 68 |     ///
 69 |     /// The decoder can be used independently of the ImageGenerator to transform
 70 |     /// latents into raster images.
 71 |     func detachedDecoder() -> ImageDecoder
 72 | 
 73 |     /// the equivalent to the ``detachedDecoder()`` but without the detatching
 74 |     func decode(xt: MLXArray) -> MLXArray
 75 | }
 76 | 
 77 | /// Public interface for transforming a text prompt into an image.
 78 | ///
 79 | /// Steps:
 80 | ///
 81 | /// - ``generateLatents(parameters:)``
 82 | /// - evaluate each of the latents from the iterator
 83 | /// - ``ImageGenerator/decode(xt:)`` or ``ImageGenerator/detachedDecoder()`` to convert the final latent into an image
 84 | /// - use ``Image`` to save the image
 85 | public protocol TextToImageGenerator: ImageGenerator {
 86 |     func generateLatents(parameters: EvaluateParameters) -> DenoiseIterator
 87 | }
 88 | 
 89 | /// Public interface for transforming a text prompt into an image.
 90 | ///
 91 | /// Steps:
 92 | ///
 93 | /// - ``generateLatents(image:parameters:strength:)``
 94 | /// - evaluate each of the latents from the iterator
 95 | /// - ``ImageGenerator/decode(xt:)`` or ``ImageGenerator/detachedDecoder()`` to convert the final latent into an image
 96 | /// - use ``Image`` to save the image
 97 | public protocol ImageToImageGenerator: ImageGenerator {
 98 |     func generateLatents(image: MLXArray, parameters: EvaluateParameters, strength: Float)
 99 |         -> DenoiseIterator
100 | }
101 | 
102 | enum ModelContainerError: LocalizedError {
103 |     /// Unable to create the particular type of model, e.g. it doesn't support image to image
104 |     case unableToCreate(String, String)
105 |     /// When operating in conserveMemory mode, it tried to use a model that had been discarded
106 |     case modelDiscarded
107 | 
108 |     var errorDescription: String? {
109 |         switch self {
110 |         case .unableToCreate(let modelId, let generatorType):
111 |             return String(
112 |                 localized:
113 |                     "Unable to create a \(generatorType) with model ID '\(modelId)'. The model may not support this operation type."
114 |             )
115 |         case .modelDiscarded:
116 |             return String(
117 |                 localized:
118 |                     "The model has been discarded to conserve memory and is no longer available. Please recreate the model container."
119 |             )
120 |         }
121 |     }
122 | }
123 | 
124 | /// Container for models that guarantees single threaded access.
125 | public actor ModelContainer<M> {
126 | 
127 |     enum State {
128 |         case discarded
129 |         case loaded(M)
130 |     }
131 | 
132 |     var state: State
133 | 
134 |     /// if true this will discard the model in ``performTwoStage(first:second:)``
135 |     var conserveMemory = false
136 | 
137 |     private init(model: M) {
138 |         self.state = .loaded(model)
139 |     }
140 | 
141 |     /// create a ``ModelContainer`` that supports ``TextToImageGenerator``
142 |     static public func createTextToImageGenerator(
143 |         configuration: StableDiffusionConfiguration, loadConfiguration: LoadConfiguration = .init()
144 |     ) throws -> ModelContainer<TextToImageGenerator> {
145 |         if let model = try configuration.textToImageGenerator(configuration: loadConfiguration) {
146 |             return .init(model: model)
147 |         } else {
148 |             throw ModelContainerError.unableToCreate(configuration.id, "TextToImageGenerator")
149 |         }
150 |     }
151 | 
152 |     /// create a ``ModelContainer`` that supports ``ImageToImageGenerator``
153 |     static public func createImageToImageGenerator(
154 |         configuration: StableDiffusionConfiguration, loadConfiguration: LoadConfiguration = .init()
155 |     ) throws -> ModelContainer<ImageToImageGenerator> {
156 |         if let model = try configuration.imageToImageGenerator(configuration: loadConfiguration) {
157 |             return .init(model: model)
158 |         } else {
159 |             throw ModelContainerError.unableToCreate(configuration.id, "ImageToImageGenerator")
160 |         }
161 |     }
162 | 
163 |     public func setConserveMemory(_ conserveMemory: Bool) {
164 |         self.conserveMemory = conserveMemory
165 |     }
166 | 
167 |     /// Perform an action on the model and/or tokenizer. Callers _must_ eval any `MLXArray` before returning as
168 |     /// `MLXArray` is not `Sendable`.
169 |     public func perform<R>(_ action: @Sendable (M) throws -> R) throws -> R {
170 |         switch state {
171 |         case .discarded:
172 |             throw ModelContainerError.modelDiscarded
173 |         case .loaded(let m):
174 |             try action(m)
175 |         }
176 |     }
177 | 
178 |     /// Perform a two stage action where the first stage returns values passed to the second stage.
179 |     ///
180 |     /// If ``setConservativeMemory(_:)`` is `true` this will discard the model in between
181 |     /// the `first` and `second` blocks. The container will have to be recreated if a caller
182 |     /// wants to use it again.
183 |     ///
184 |     /// If `false` this will just run them in sequence and the container can be reused.
185 |     ///
186 |     /// Callers _must_ eval any `MLXArray` before returning as `MLXArray` is not `Sendable`.
187 |     public func performTwoStage<R1, R2>(
188 |         first: @Sendable (M) throws -> R1, second: @Sendable (R1) throws -> R2
189 |     ) throws -> R2 {
190 |         let r1 =
191 |             switch state {
192 |             case .discarded:
193 |                 throw ModelContainerError.modelDiscarded
194 |             case .loaded(let m):
195 |                 try first(m)
196 |             }
197 |         if conserveMemory {
198 |             self.state = .discarded
199 |         }
200 |         return try second(r1)
201 |     }
202 | 
203 | }
204 | 
205 | /// Base class for Stable Diffusion.
206 | open class StableDiffusion {
207 | 
208 |     let dType: DType
209 |     let diffusionConfiguration: DiffusionConfiguration
210 |     let unet: UNetModel
211 |     let textEncoder: CLIPTextModel
212 |     let autoencoder: Autoencoder
213 |     let sampler: SimpleEulerSampler
214 |     let tokenizer: CLIPTokenizer
215 | 
216 |     internal init(
217 |         hub: HubApi, configuration: StableDiffusionConfiguration, dType: DType,
218 |         diffusionConfiguration: DiffusionConfiguration? = nil, unet: UNetModel? = nil,
219 |         textEncoder: CLIPTextModel? = nil, autoencoder: Autoencoder? = nil,
220 |         sampler: SimpleEulerSampler? = nil, tokenizer: CLIPTokenizer? = nil
221 |     ) throws {
222 |         self.dType = dType
223 |         self.diffusionConfiguration =
224 |             try diffusionConfiguration
225 |             ?? loadDiffusionConfiguration(hub: hub, configuration: configuration)
226 |         self.unet = try unet ?? loadUnet(hub: hub, configuration: configuration, dType: dType)
227 |         self.textEncoder =
228 |             try textEncoder ?? loadTextEncoder(hub: hub, configuration: configuration, dType: dType)
229 | 
230 |         // note: autoencoder uses float32 weights
231 |         self.autoencoder =
232 |             try autoencoder
233 |             ?? loadAutoEncoder(hub: hub, configuration: configuration, dType: .float32)
234 | 
235 |         if let sampler {
236 |             self.sampler = sampler
237 |         } else {
238 |             self.sampler = SimpleEulerSampler(configuration: self.diffusionConfiguration)
239 |         }
240 |         self.tokenizer = try tokenizer ?? loadTokenizer(hub: hub, configuration: configuration)
241 |     }
242 | 
243 |     open func ensureLoaded() {
244 |         eval(unet, textEncoder, autoencoder)
245 |     }
246 | 
247 |     func tokenize(tokenizer: CLIPTokenizer, text: String, negativeText: String?) -> MLXArray {
248 |         var tokens = [tokenizer.tokenize(text: text)]
249 |         if let negativeText {
250 |             tokens.append(tokenizer.tokenize(text: negativeText))
251 |         }
252 | 
253 |         let c = tokens.count
254 |         let max = tokens.map { $0.count }.max() ?? 0
255 |         let mlxTokens = MLXArray(
256 |             tokens
257 |                 .map {
258 |                     ($0 + Array(repeating: 0, count: max - $0.count))
259 |                 }
260 |                 .flatMap { $0 }
261 |         )
262 |         .reshaped(c, max)
263 | 
264 |         return mlxTokens
265 |     }
266 | 
267 |     open func step(
268 |         xt: MLXArray, t: MLXArray, tPrev: MLXArray, conditioning: MLXArray, cfgWeight: Float,
269 |         textTime: (MLXArray, MLXArray)?
270 |     ) -> MLXArray {
271 |         let xtUnet = cfgWeight > 1 ? concatenated([xt, xt], axis: 0) : xt
272 |         let tUnet = broadcast(t, to: [xtUnet.count])
273 | 
274 |         var epsPred = unet(xtUnet, timestep: tUnet, encoderX: conditioning, textTime: textTime)
275 | 
276 |         if cfgWeight > 1 {
277 |             let (epsText, epsNeg) = epsPred.split()
278 |             epsPred = epsNeg + cfgWeight * (epsText - epsNeg)
279 |         }
280 | 
281 |         return sampler.step(epsPred: epsPred, xt: xt, t: t, tPrev: tPrev)
282 |     }
283 | 
284 |     public func detachedDecoder() -> ImageDecoder {
285 |         let autoencoder = self.autoencoder
286 |         func decode(xt: MLXArray) -> MLXArray {
287 |             var x = autoencoder.decode(xt)
288 |             x = clip(x / 2 + 0.5, min: 0, max: 1)
289 |             return x
290 |         }
291 |         return decode(xt:)
292 |     }
293 | 
294 |     public func decode(xt: MLXArray) -> MLXArray {
295 |         detachedDecoder()(xt)
296 |     }
297 | }
298 | 
299 | /// Implementation of ``StableDiffusion`` for the `stabilityai/stable-diffusion-2-1-base` model.
300 | open class StableDiffusionBase: StableDiffusion, TextToImageGenerator {
301 | 
302 |     public init(hub: HubApi, configuration: StableDiffusionConfiguration, dType: DType) throws {
303 |         try super.init(hub: hub, configuration: configuration, dType: dType)
304 |     }
305 | 
306 |     func conditionText(text: String, imageCount: Int, cfgWeight: Float, negativeText: String?)
307 |         -> MLXArray
308 |     {
309 |         // tokenize the text
310 |         let tokens = tokenize(
311 |             tokenizer: tokenizer, text: text, negativeText: cfgWeight > 1 ? negativeText : nil)
312 | 
313 |         // compute the features
314 |         var conditioning = textEncoder(tokens).lastHiddenState
315 | 
316 |         // repeat the conditioning for each of the generated images
317 |         if imageCount > 1 {
318 |             conditioning = repeated(conditioning, count: imageCount, axis: 0)
319 |         }
320 | 
321 |         return conditioning
322 |     }
323 | 
324 |     public func generateLatents(parameters: EvaluateParameters) -> DenoiseIterator {
325 |         MLXRandom.seed(parameters.seed)
326 | 
327 |         let conditioning = conditionText(
328 |             text: parameters.prompt, imageCount: parameters.imageCount,
329 |             cfgWeight: parameters.cfgWeight, negativeText: parameters.negativePrompt)
330 | 
331 |         let xt = sampler.samplePrior(
332 |             shape: [parameters.imageCount] + parameters.latentSize + [autoencoder.latentChannels],
333 |             dType: dType)
334 | 
335 |         return DenoiseIterator(
336 |             sd: self, xt: xt, t: sampler.maxTime, conditioning: conditioning,
337 |             steps: parameters.steps, cfgWeight: parameters.cfgWeight)
338 |     }
339 | 
340 | }
341 | 
342 | /// Implementation of ``StableDiffusion`` for the `stabilityai/sdxl-turbo` model.
343 | open class StableDiffusionXL: StableDiffusion, TextToImageGenerator, ImageToImageGenerator {
344 | 
345 |     let textEncoder2: CLIPTextModel
346 |     let tokenizer2: CLIPTokenizer
347 | 
348 |     public init(hub: HubApi, configuration: StableDiffusionConfiguration, dType: DType) throws {
349 |         let diffusionConfiguration = try loadConfiguration(
350 |             hub: hub, configuration: configuration, key: .diffusionConfig,
351 |             type: DiffusionConfiguration.self)
352 |         let sampler = SimpleEulerAncestralSampler(configuration: diffusionConfiguration)
353 | 
354 |         self.textEncoder2 = try loadTextEncoder(
355 |             hub: hub, configuration: configuration, configKey: .textEncoderConfig2,
356 |             weightsKey: .textEncoderWeights2, dType: dType)
357 | 
358 |         self.tokenizer2 = try loadTokenizer(
359 |             hub: hub, configuration: configuration, vocabulary: .tokenizerVocabulary2,
360 |             merges: .tokenizerMerges2)
361 | 
362 |         try super.init(
363 |             hub: hub, configuration: configuration, dType: dType,
364 |             diffusionConfiguration: diffusionConfiguration, sampler: sampler)
365 |     }
366 | 
367 |     open override func ensureLoaded() {
368 |         super.ensureLoaded()
369 |         eval(textEncoder2)
370 |     }
371 | 
372 |     func conditionText(text: String, imageCount: Int, cfgWeight: Float, negativeText: String?) -> (
373 |         MLXArray, MLXArray
374 |     ) {
375 |         let tokens1 = tokenize(
376 |             tokenizer: tokenizer, text: text, negativeText: cfgWeight > 1 ? negativeText : nil)
377 |         let tokens2 = tokenize(
378 |             tokenizer: tokenizer2, text: text, negativeText: cfgWeight > 1 ? negativeText : nil)
379 | 
380 |         let conditioning1 = textEncoder(tokens1)
381 |         let conditioning2 = textEncoder2(tokens2)
382 |         var conditioning = concatenated(
383 |             [
384 |                 conditioning1.hiddenStates.dropLast().last!,
385 |                 conditioning2.hiddenStates.dropLast().last!,
386 |             ],
387 |             axis: -1)
388 |         var pooledConditionng = conditioning2.pooledOutput
389 | 
390 |         if imageCount > 1 {
391 |             conditioning = repeated(conditioning, count: imageCount, axis: 0)
392 |             pooledConditionng = repeated(pooledConditionng, count: imageCount, axis: 0)
393 |         }
394 | 
395 |         return (conditioning, pooledConditionng)
396 |     }
397 | 
398 |     public func generateLatents(parameters: EvaluateParameters) -> DenoiseIterator {
399 |         MLXRandom.seed(parameters.seed)
400 | 
401 |         let (conditioning, pooledConditioning) = conditionText(
402 |             text: parameters.prompt, imageCount: parameters.imageCount,
403 |             cfgWeight: parameters.cfgWeight, negativeText: parameters.negativePrompt)
404 | 
405 |         let textTime = (
406 |             pooledConditioning,
407 |             repeated(
408 |                 MLXArray(converting: [512.0, 512, 0, 0, 512, 512]).reshaped(1, -1),
409 |                 count: pooledConditioning.count, axis: 0)
410 |         )
411 | 
412 |         let xt = sampler.samplePrior(
413 |             shape: [parameters.imageCount] + parameters.latentSize + [autoencoder.latentChannels],
414 |             dType: dType)
415 | 
416 |         return DenoiseIterator(
417 |             sd: self, xt: xt, t: sampler.maxTime, conditioning: conditioning,
418 |             steps: parameters.steps, cfgWeight: parameters.cfgWeight, textTime: textTime)
419 |     }
420 | 
421 |     public func generateLatents(image: MLXArray, parameters: EvaluateParameters, strength: Float)
422 |         -> DenoiseIterator
423 |     {
424 |         MLXRandom.seed(parameters.seed)
425 | 
426 |         // Define the num steps and start step
427 |         let startStep = Float(sampler.maxTime) * strength
428 |         let numSteps = Int(Float(parameters.steps) * strength)
429 | 
430 |         let (conditioning, pooledConditioning) = conditionText(
431 |             text: parameters.prompt, imageCount: parameters.imageCount,
432 |             cfgWeight: parameters.cfgWeight, negativeText: parameters.negativePrompt)
433 | 
434 |         let textTime = (
435 |             pooledConditioning,
436 |             repeated(
437 |                 MLXArray(converting: [512.0, 512, 0, 0, 512, 512]).reshaped(1, -1),
438 |                 count: pooledConditioning.count, axis: 0)
439 |         )
440 | 
441 |         // Get the latents from the input image and add noise according to the
442 |         // start time.
443 | 
444 |         var (x0, _) = autoencoder.encode(image[.newAxis])
445 |         x0 = broadcast(x0, to: [parameters.imageCount] + x0.shape.dropFirst())
446 |         let xt = sampler.addNoise(x: x0, t: MLXArray(startStep))
447 | 
448 |         return DenoiseIterator(
449 |             sd: self, xt: xt, t: sampler.maxTime, conditioning: conditioning, steps: numSteps,
450 |             cfgWeight: parameters.cfgWeight, textTime: textTime)
451 |     }
452 | }
453 | 


--------------------------------------------------------------------------------
/Libraries/StableDiffusion/Tokenizer.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import Foundation
  4 | 
  5 | struct Bigram: Hashable {
  6 |     let a: String
  7 |     let b: String
  8 | 
  9 |     init(_ s: String) {
 10 |         let pieces = s.split(separator: " ")
 11 |         precondition(pieces.count == 2, "BPEPair expected two pieces for '\(s)'")
 12 |         self.a = String(pieces[0])
 13 |         self.b = String(pieces[1])
 14 |     }
 15 | 
 16 |     init(_ a: String, _ b: String) {
 17 |         self.a = a
 18 |         self.b = b
 19 |     }
 20 | 
 21 |     init(_ v: (String, String)) {
 22 |         self.a = v.0
 23 |         self.b = v.1
 24 |     }
 25 | }
 26 | 
 27 | /// A CLIP tokenizer.
 28 | ///
 29 | /// Ported from:
 30 | ///
 31 | /// - https://github.com/ml-explore/mlx-examples/blob/main/stable_diffusion/stable_diffusion/tokenizer.py
 32 | /// - https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/tokenization_clip.py
 33 | ///
 34 | /// Ideally this would be a tokenizer from `swift-transformers` but this is too special purpose to be representable in
 35 | /// what exists there (at time of writing).
 36 | class CLIPTokenizer {
 37 | 
 38 |     let pattern =
 39 |         #/<\|startoftext\|>|<\|endoftext\|>|'s|'t|'re|'ve|'m|'ll|'d|[\p{L}]+|[\p{N}]|[^\s\p{L}\p{N}]+/#
 40 |     let bpeRanks: [Bigram: Int]
 41 |     let vocabulary: [String: Int]
 42 | 
 43 |     let bos = "<|startoftext|>"
 44 |     let eos = "<|endoftext|>"
 45 | 
 46 |     let bosToken: Int
 47 |     let eosToken: Int
 48 | 
 49 |     var cache = [String: [String]]()
 50 | 
 51 |     init(merges: [String], vocabulary: [String: Int]) {
 52 |         self.bpeRanks = Dictionary(
 53 |             uniqueKeysWithValues:
 54 |                 merges
 55 |                 .map { Bigram($0) }
 56 |                 .enumerated()
 57 |                 .map { ($0.element, $0.offset) })
 58 | 
 59 |         self.vocabulary = vocabulary
 60 |         self.cache[bos] = [bos]
 61 |         self.cache[eos] = [eos]
 62 |         self.bosToken = vocabulary[bos]!
 63 |         self.eosToken = vocabulary[eos]!
 64 |     }
 65 | 
 66 |     func bpe(text: String) -> [String] {
 67 |         if let result = cache[text] {
 68 |             return result
 69 |         }
 70 | 
 71 |         precondition(!text.isEmpty)
 72 | 
 73 |         var unigrams = text.dropLast().map { String($0) } + ["\(text.last!)</w>"]
 74 |         var uniqueBigrams = Set(zip(unigrams, unigrams.dropFirst()).map { Bigram($0) })
 75 | 
 76 |         // In every iteration try to merge the two most likely bigrams. If none
 77 |         // was merged we are done
 78 | 
 79 |         while !uniqueBigrams.isEmpty {
 80 |             let (bigram, _) =
 81 |                 uniqueBigrams
 82 |                 .map { ($0, bpeRanks[$0] ?? Int.max) }
 83 |                 .min { $0.1 < $1.1 }!
 84 | 
 85 |             if bpeRanks[bigram] == nil {
 86 |                 break
 87 |             }
 88 | 
 89 |             var newUnigrams = [String]()
 90 |             var skip = false
 91 | 
 92 |             for (a, b) in zip(unigrams, unigrams.dropFirst()) {
 93 |                 if skip {
 94 |                     skip = false
 95 |                     continue
 96 |                 }
 97 | 
 98 |                 if Bigram(a, b) == bigram {
 99 |                     newUnigrams.append(a + b)
100 |                     skip = true
101 |                 } else {
102 |                     newUnigrams.append(a)
103 |                 }
104 |             }
105 | 
106 |             if !skip, let last = unigrams.last {
107 |                 newUnigrams.append(last)
108 |             }
109 | 
110 |             unigrams = newUnigrams
111 |             uniqueBigrams = Set(zip(unigrams, unigrams.dropFirst()).map { Bigram($0) })
112 |         }
113 | 
114 |         cache[text] = unigrams
115 | 
116 |         return unigrams
117 |     }
118 | 
119 |     public func tokenize(text: String) -> [Int32] {
120 |         // Lower case cleanup and split according to self.pat. Hugging Face does
121 |         // a much more thorough job here but this should suffice for 95% of
122 |         // cases.
123 | 
124 |         let clean = text.lowercased().replacing(#/\s+/#, with: " ")
125 |         let tokens = clean.matches(of: pattern).map { $0.description }
126 | 
127 |         // Split the tokens according to the byte-pair merge file
128 |         let bpeTokens = tokens.flatMap { bpe(text: String($0)) }
129 | 
130 |         // Map to token ids and return
131 |         let result = [bosToken] + bpeTokens.compactMap { vocabulary[$0] } + [eosToken]
132 | 
133 |         return result.map { Int32($0) }
134 |     }
135 | }
136 | 


--------------------------------------------------------------------------------
/Libraries/StableDiffusion/UNet.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import Foundation
  4 | import MLX
  5 | import MLXNN
  6 | 
  7 | // port of https://github.com/ml-explore/mlx-examples/blob/main/stable_diffusion/stable_diffusion/unet.py
  8 | 
  9 | func upsampleNearest(_ x: MLXArray, scale: Int = 2) -> MLXArray {
 10 |     precondition(x.ndim == 4)
 11 |     let (B, H, W, C) = x.shape4
 12 |     var x = broadcast(
 13 |         x[0..., 0..., .newAxis, 0..., .newAxis, 0...], to: [B, H, scale, W, scale, C])
 14 |     x = x.reshaped(B, H * scale, W * scale, C)
 15 |     return x
 16 | }
 17 | 
 18 | class TimestepEmbedding: Module, UnaryLayer {
 19 | 
 20 |     @ModuleInfo(key: "linear_1") var linear1: Linear
 21 |     @ModuleInfo(key: "linear_2") var linear2: Linear
 22 | 
 23 |     init(inputChannels: Int, timeEmbedDimensions: Int) {
 24 |         self._linear1.wrappedValue = Linear(inputChannels, timeEmbedDimensions)
 25 |         self._linear2.wrappedValue = Linear(timeEmbedDimensions, timeEmbedDimensions)
 26 |     }
 27 | 
 28 |     func callAsFunction(_ x: MLXArray) -> MLXArray {
 29 |         var x = linear1(x)
 30 |         x = silu(x)
 31 |         x = linear2(x)
 32 | 
 33 |         return x
 34 |     }
 35 | }
 36 | 
 37 | class TransformerBlock: Module {
 38 | 
 39 |     let norm1: LayerNorm
 40 |     let attn1: MultiHeadAttention
 41 | 
 42 |     let norm2: LayerNorm
 43 |     let attn2: MultiHeadAttention
 44 | 
 45 |     let norm3: LayerNorm
 46 |     @ModuleInfo var linear1: Linear
 47 |     @ModuleInfo var linear2: Linear
 48 |     @ModuleInfo var linear3: Linear
 49 | 
 50 |     init(
 51 |         modelDimensions: Int, numHeads: Int, hiddenDimensions: Int? = nil,
 52 |         memoryDimensions: Int? = nil
 53 |     ) {
 54 |         norm1 = LayerNorm(dimensions: modelDimensions)
 55 |         attn1 = MultiHeadAttention(dimensions: modelDimensions, numHeads: numHeads)
 56 | 
 57 |         // we want to self.attn1.out_proj.bias = mx.zeros(model_dims) turn enable the
 58 |         // bias in one of the four Linears attached to attn1. Since bias is nil we can't
 59 |         // update it so just replace the layer.
 60 |         attn1.update(
 61 |             modules: ModuleChildren(
 62 |                 values: ["out_proj": .value(Linear(modelDimensions, modelDimensions, bias: true))]))
 63 | 
 64 |         let memoryDimensions = memoryDimensions ?? modelDimensions
 65 |         self.norm2 = LayerNorm(dimensions: modelDimensions)
 66 |         self.attn2 = MultiHeadAttention(
 67 |             dimensions: modelDimensions, numHeads: numHeads, keyInputDimensions: memoryDimensions)
 68 |         attn2.update(
 69 |             modules: ModuleChildren(
 70 |                 values: ["out_proj": .value(Linear(modelDimensions, modelDimensions, bias: true))]))
 71 | 
 72 |         let hiddenDimensions = hiddenDimensions ?? (4 * modelDimensions)
 73 |         self.norm3 = LayerNorm(dimensions: modelDimensions)
 74 |         self.linear1 = Linear(modelDimensions, hiddenDimensions)
 75 |         self.linear2 = Linear(modelDimensions, hiddenDimensions)
 76 |         self.linear3 = Linear(hiddenDimensions, modelDimensions)
 77 |     }
 78 | 
 79 |     func callAsFunction(
 80 |         _ x: MLXArray, memory: MLXArray, attentionMask: MLXArray?, memoryMask: MLXArray?
 81 |     ) -> MLXArray {
 82 |         var x = x
 83 | 
 84 |         // self attention
 85 |         var y = norm1(x)
 86 |         y = attn1(y, keys: y, values: y, mask: attentionMask)
 87 |         x = x + y
 88 | 
 89 |         // cross attention
 90 |         y = norm2(x)
 91 |         y = attn2(y, keys: memory, values: memory, mask: memoryMask)
 92 |         x = x + y
 93 | 
 94 |         // FFN
 95 |         y = norm3(x)
 96 |         let ya = linear1(y)
 97 |         let yb = linear2(y)
 98 |         y = ya * gelu(yb)
 99 |         y = linear3(y)
100 |         x = x + y
101 | 
102 |         return x
103 |     }
104 | }
105 | 
106 | /// A transformer model for inputs with 2 spatial dimensions
107 | class Transformer2D: Module {
108 | 
109 |     let norm: GroupNorm
110 |     @ModuleInfo(key: "proj_in") var projectIn: Linear
111 |     @ModuleInfo(key: "transformer_blocks") var transformerBlocks: [TransformerBlock]
112 |     @ModuleInfo(key: "proj_out") var projectOut: Linear
113 | 
114 |     init(
115 |         inputChannels: Int, modelDimensions: Int, encoderDimensions: Int, numHeads: Int,
116 |         numLayers: Int, groupCount: Int = 32
117 |     ) {
118 |         self.norm = GroupNorm(
119 |             groupCount: groupCount, dimensions: inputChannels, pytorchCompatible: true)
120 |         self._projectIn.wrappedValue = Linear(inputChannels, modelDimensions)
121 |         self._transformerBlocks.wrappedValue = (0 ..< numLayers)
122 |             .map { _ in
123 |                 TransformerBlock(
124 |                     modelDimensions: modelDimensions, numHeads: numHeads,
125 |                     memoryDimensions: encoderDimensions)
126 |             }
127 |         self._projectOut.wrappedValue = Linear(modelDimensions, inputChannels)
128 |     }
129 | 
130 |     func callAsFunction(
131 |         _ x: MLXArray, encoderX: MLXArray, attentionMask: MLXArray?, encoderAttentionMask: MLXArray?
132 |     ) -> MLXArray {
133 |         let inputX = x
134 |         let dtype = x.dtype
135 |         var x = x
136 | 
137 |         // Perform the input norm and projection
138 |         let (B, H, W, C) = x.shape4
139 |         x = norm(x).reshaped(B, -1, C)
140 |         x = projectIn(x)
141 | 
142 |         // apply the transformer
143 |         for block in transformerBlocks {
144 |             x = block(
145 |                 x, memory: encoderX, attentionMask: attentionMask, memoryMask: encoderAttentionMask)
146 |         }
147 | 
148 |         // apply the output projection and reshape
149 |         x = projectOut(x)
150 |         x = x.reshaped(B, H, W, C)
151 | 
152 |         return x + inputX
153 |     }
154 | }
155 | 
156 | class ResnetBlock2D: Module {
157 | 
158 |     let norm1: GroupNorm
159 |     let conv1: Conv2d
160 | 
161 |     @ModuleInfo(key: "time_emb_proj") var timeEmbedProjection: Linear?
162 | 
163 |     let norm2: GroupNorm
164 |     let conv2: Conv2d
165 | 
166 |     @ModuleInfo(key: "conv_shortcut") var convolutionShortcut: Linear?
167 | 
168 |     init(
169 |         inputChannels: Int, outputChannels: Int? = nil, groupCount: Int = 32,
170 |         timeEmbedChannels: Int? = nil
171 |     ) {
172 |         let outputChannels = outputChannels ?? inputChannels
173 | 
174 |         self.norm1 = GroupNorm(
175 |             groupCount: groupCount, dimensions: inputChannels, pytorchCompatible: true)
176 |         self.conv1 = Conv2d(
177 |             inputChannels: inputChannels, outputChannels: outputChannels,
178 |             kernelSize: 3, stride: 1, padding: 1)
179 | 
180 |         if let timeEmbedChannels {
181 |             self._timeEmbedProjection.wrappedValue = Linear(timeEmbedChannels, outputChannels)
182 |         }
183 | 
184 |         self.norm2 = GroupNorm(
185 |             groupCount: groupCount, dimensions: outputChannels, pytorchCompatible: true)
186 |         self.conv2 = Conv2d(
187 |             inputChannels: outputChannels, outputChannels: outputChannels,
188 |             kernelSize: 3, stride: 1, padding: 1)
189 | 
190 |         if inputChannels != outputChannels {
191 |             self._convolutionShortcut.wrappedValue = Linear(inputChannels, outputChannels)
192 |         }
193 |     }
194 | 
195 |     func callAsFunction(_ x: MLXArray, timeEmbedding: MLXArray? = nil) -> MLXArray {
196 |         let dtype = x.dtype
197 | 
198 |         var y = norm1(x)
199 |         y = silu(y)
200 |         y = conv1(y)
201 | 
202 |         if var timeEmbedding, let timeEmbedProjection {
203 |             timeEmbedding = timeEmbedProjection(silu(timeEmbedding))
204 |             y = y + timeEmbedding[0..., .newAxis, .newAxis, 0...]
205 |         }
206 | 
207 |         y = norm2(y)
208 |         y = silu(y)
209 |         y = conv2(y)
210 | 
211 |         if let convolutionShortcut {
212 |             return y + convolutionShortcut(x)
213 |         } else {
214 |             return y + x
215 |         }
216 |     }
217 | }
218 | 
219 | class UNetBlock2D: Module {
220 | 
221 |     let resnets: [ResnetBlock2D]
222 |     let attentions: [Transformer2D]?
223 |     let downsample: Conv2d?
224 |     let upsample: Conv2d?
225 | 
226 |     init(
227 |         inputChannels: Int, outputChannels: Int, timeEmbedChannels: Int,
228 |         previousOutChannels: Int? = nil, numLayers: Int = 1, transformerLayersPerBlock: Int = 1,
229 |         numHeads: Int = 8, crossAttentionDimension: Int = 1280, resnetGroups: Int = 32,
230 |         addDownSample: Bool = true, addUpSample: Bool = true, addCrossAttention: Bool = true
231 |     ) {
232 | 
233 |         // Prepare the inputChannelsArray for the resnets
234 |         let inputChannelsArray: [Int]
235 |         if let previousOutChannels {
236 |             let inputChannelsBuild =
237 |                 [previousOutChannels] + Array(repeating: outputChannels, count: numLayers - 1)
238 |             let resChannelsArray =
239 |                 Array(repeating: outputChannels, count: numLayers - 1) + [inputChannels]
240 |             inputChannelsArray = zip(inputChannelsBuild, resChannelsArray).map { $0.0 + $0.1 }
241 |         } else {
242 |             inputChannelsArray =
243 |                 [inputChannels] + Array(repeating: outputChannels, count: numLayers - 1)
244 |         }
245 | 
246 |         // Add resnet blocks that also process the time embedding
247 |         self.resnets =
248 |             inputChannelsArray
249 |             .map { ic in
250 |                 ResnetBlock2D(
251 |                     inputChannels: ic, outputChannels: outputChannels, groupCount: resnetGroups,
252 |                     timeEmbedChannels: timeEmbedChannels)
253 |             }
254 | 
255 |         // Add optional cross attention layers
256 |         if addCrossAttention {
257 |             self.attentions = (0 ..< numLayers)
258 |                 .map { _ in
259 |                     Transformer2D(
260 |                         inputChannels: outputChannels, modelDimensions: outputChannels,
261 |                         encoderDimensions: crossAttentionDimension, numHeads: numHeads,
262 |                         numLayers: transformerLayersPerBlock)
263 |                 }
264 |         } else {
265 |             self.attentions = nil
266 |         }
267 | 
268 |         // Add an optional downsampling layer
269 |         if addDownSample {
270 |             self.downsample = Conv2d(
271 |                 inputChannels: outputChannels, outputChannels: outputChannels, kernelSize: 3,
272 |                 stride: 2, padding: 1)
273 |         } else {
274 |             self.downsample = nil
275 |         }
276 | 
277 |         // or upsampling layer
278 |         if addUpSample {
279 |             self.upsample = Conv2d(
280 |                 inputChannels: outputChannels, outputChannels: outputChannels, kernelSize: 3,
281 |                 stride: 1, padding: 1)
282 |         } else {
283 |             self.upsample = nil
284 |         }
285 |     }
286 | 
287 |     func callAsFunction(
288 |         _ x: MLXArray, encoderX: MLXArray, timeEmbedding: MLXArray? = nil,
289 |         attentionMask: MLXArray? = nil, encoderAttentionMask: MLXArray? = nil,
290 |         residualHiddenStates: [MLXArray]? = nil
291 |     ) -> (MLXArray, [MLXArray], [MLXArray]) {
292 |         var x = x
293 |         var outputStates = [MLXArray]()
294 |         var residualHiddenStates = residualHiddenStates
295 | 
296 |         for i in 0 ..< resnets.count {
297 |             if residualHiddenStates != nil {
298 |                 x = concatenated([x, residualHiddenStates!.removeLast()], axis: -1)
299 |             }
300 | 
301 |             x = resnets[i](x, timeEmbedding: timeEmbedding)
302 | 
303 |             if let attentions {
304 |                 x = attentions[i](
305 |                     x, encoderX: encoderX, attentionMask: attentionMask,
306 |                     encoderAttentionMask: encoderAttentionMask)
307 |             }
308 | 
309 |             outputStates.append(x)
310 |         }
311 | 
312 |         if let downsample {
313 |             x = downsample(x)
314 |             outputStates.append(x)
315 |         }
316 | 
317 |         if let upsample {
318 |             x = upsample(upsampleNearest(x))
319 |             outputStates.append(x)
320 |         }
321 | 
322 |         if let residualHiddenStates {
323 |             return (x, outputStates, residualHiddenStates)
324 |         } else {
325 |             return (x, outputStates, [])
326 |         }
327 |     }
328 | }
329 | 
330 | class UNetModel: Module {
331 | 
332 |     @ModuleInfo(key: "conv_in") var convIn: Conv2d
333 |     let timesteps: SinusoidalPositionalEncoding
334 |     @ModuleInfo(key: "time_embedding") var timeEmbedding: TimestepEmbedding
335 | 
336 |     @ModuleInfo(key: "addition_embed_type") var addTimeProj: SinusoidalPositionalEncoding?
337 |     @ModuleInfo(key: "add_embedding") var addEmbedding: TimestepEmbedding?
338 | 
339 |     @ModuleInfo(key: "down_blocks") var downBlocks: [UNetBlock2D]
340 |     @ModuleInfo(key: "mid_blocks") var midBlocks: (ResnetBlock2D, Transformer2D, ResnetBlock2D)
341 |     @ModuleInfo(key: "up_blocks") var upBlocks: [UNetBlock2D]
342 | 
343 |     @ModuleInfo(key: "conv_norm_out") var convNormOut: GroupNorm
344 |     @ModuleInfo(key: "conv_out") var convOut: Conv2d
345 | 
346 |     init(configuration: UNetConfiguration) {
347 |         let channels0 = configuration.blockOutChannels[0]
348 | 
349 |         self._convIn.wrappedValue = Conv2d(
350 |             inputChannels: configuration.inputChannels, outputChannels: channels0,
351 |             kernelSize: .init(configuration.convolutionInKernel),
352 |             padding: .init((configuration.convolutionInKernel - 1) / 2))
353 | 
354 |         self.timesteps = SinusoidalPositionalEncoding(
355 |             dimensions: channels0,
356 |             minFrequency: exp(-log(10_000) + 2 * log(10_000) / Float(channels0)),
357 |             maxFrequency: 1, scale: 1, cosineFirst: true, fullTurns: false)
358 | 
359 |         self._timeEmbedding.wrappedValue = TimestepEmbedding(
360 |             inputChannels: channels0, timeEmbedDimensions: channels0 * 4)
361 | 
362 |         if configuration.additionEmbedType == "text_time",
363 |             let additionTimeEmbedDimension = configuration.additionTimeEmbedDimension,
364 |             let projectionClassEmbeddingsInputDimension = configuration
365 |                 .projectionClassEmbeddingsInputDimension
366 |         {
367 |             self._addTimeProj.wrappedValue = SinusoidalPositionalEncoding(
368 |                 dimensions: additionTimeEmbedDimension,
369 |                 minFrequency: exp(
370 |                     -log(10_000) + 2 * log(10_000) / Float(additionTimeEmbedDimension)),
371 |                 maxFrequency: 1,
372 |                 scale: 1, cosineFirst: true, fullTurns: false)
373 | 
374 |             self._addEmbedding.wrappedValue = TimestepEmbedding(
375 |                 inputChannels: projectionClassEmbeddingsInputDimension,
376 |                 timeEmbedDimensions: channels0 * 4)
377 |         }
378 | 
379 |         // make the downsampling blocks
380 |         let downblockChannels = [channels0] + configuration.blockOutChannels
381 |         self._downBlocks.wrappedValue = zip(downblockChannels, downblockChannels.dropFirst())
382 |             .enumerated()
383 |             .map { (i, pair) in
384 |                 let (inChannels, outChannels) = pair
385 |                 return UNetBlock2D(
386 |                     inputChannels: inChannels,
387 |                     outputChannels: outChannels,
388 |                     timeEmbedChannels: channels0 * 4,
389 |                     numLayers: configuration.layersPerBlock[i],
390 |                     transformerLayersPerBlock: configuration.transformerLayersPerBlock[i],
391 |                     numHeads: configuration.numHeads[i],
392 |                     crossAttentionDimension: configuration.crossAttentionDimension[i],
393 |                     resnetGroups: configuration.normNumGroups,
394 |                     addDownSample: i < configuration.blockOutChannels.count - 1,
395 |                     addUpSample: false,
396 |                     addCrossAttention: configuration.downBlockTypes[i].contains("CrossAttn")
397 |                 )
398 |             }
399 | 
400 |         // make the middle block
401 |         let channelsLast = configuration.blockOutChannels.last!
402 |         self._midBlocks.wrappedValue = (
403 |             ResnetBlock2D(
404 |                 inputChannels: channelsLast,
405 |                 outputChannels: channelsLast,
406 |                 groupCount: configuration.normNumGroups,
407 |                 timeEmbedChannels: channels0 * 4
408 |             ),
409 |             Transformer2D(
410 |                 inputChannels: channelsLast,
411 |                 modelDimensions: channelsLast,
412 |                 encoderDimensions: configuration.crossAttentionDimension.last!,
413 |                 numHeads: configuration.numHeads.last!,
414 |                 numLayers: configuration.transformerLayersPerBlock.last!
415 |             ),
416 |             ResnetBlock2D(
417 |                 inputChannels: channelsLast,
418 |                 outputChannels: channelsLast,
419 |                 groupCount: configuration.normNumGroups,
420 |                 timeEmbedChannels: channels0 * 4
421 |             )
422 |         )
423 | 
424 |         // make the upsampling blocks
425 |         let upblockChannels =
426 |             [channels0] + configuration.blockOutChannels + [configuration.blockOutChannels.last!]
427 |         self._upBlocks.wrappedValue =
428 |             zip(upblockChannels, zip(upblockChannels.dropFirst(), upblockChannels.dropFirst(2)))
429 |             .enumerated()
430 |             .reversed()
431 |             .map { (i, triple) in
432 |                 let (inChannels, (outChannels, prevOutChannels)) = triple
433 |                 return UNetBlock2D(
434 |                     inputChannels: inChannels,
435 |                     outputChannels: outChannels,
436 |                     timeEmbedChannels: channels0 * 4,
437 |                     previousOutChannels: prevOutChannels,
438 |                     numLayers: configuration.layersPerBlock[i] + 1,
439 |                     transformerLayersPerBlock: configuration.transformerLayersPerBlock[i],
440 |                     numHeads: configuration.numHeads[i],
441 |                     crossAttentionDimension: configuration.crossAttentionDimension[i],
442 |                     resnetGroups: configuration.normNumGroups,
443 |                     addDownSample: false,
444 |                     addUpSample: i > 0,
445 |                     addCrossAttention: configuration.upBlockTypes[i].contains("CrossAttn")
446 |                 )
447 |             }
448 | 
449 |         self._convNormOut.wrappedValue = GroupNorm(
450 |             groupCount: configuration.normNumGroups, dimensions: channels0, pytorchCompatible: true)
451 |         self._convOut.wrappedValue = Conv2d(
452 |             inputChannels: channels0, outputChannels: configuration.outputChannels,
453 |             kernelSize: .init(configuration.convolutionOutKernel),
454 |             padding: .init((configuration.convolutionOutKernel - 1) / 2))
455 |     }
456 | 
457 |     func callAsFunction(
458 |         _ x: MLXArray, timestep: MLXArray, encoderX: MLXArray, attentionMask: MLXArray? = nil,
459 |         encoderAttentionMask: MLXArray? = nil, textTime: (MLXArray, MLXArray)? = nil
460 |     ) -> MLXArray {
461 |         // compute the time embeddings
462 |         var temb = timesteps(timestep).asType(x.dtype)
463 |         temb = timeEmbedding(temb)
464 | 
465 |         // add the extra textTime conditioning
466 |         if let (textEmbedding, timeIds) = textTime,
467 |             let addTimeProj, let addEmbedding
468 |         {
469 |             var emb = addTimeProj(timeIds).flattened(start: 1).asType(x.dtype)
470 |             emb = concatenated([textEmbedding, emb], axis: -1)
471 |             emb = addEmbedding(emb)
472 |             temb = temb + emb
473 |         }
474 | 
475 |         // preprocess the input
476 |         var x = convIn(x)
477 | 
478 |         // run the downsampling part of the unet
479 |         var residuals = [x]
480 |         for block in self.downBlocks {
481 |             let res: [MLXArray]
482 |             (x, res, _) = block(
483 |                 x, encoderX: encoderX, timeEmbedding: temb, attentionMask: attentionMask,
484 |                 encoderAttentionMask: encoderAttentionMask)
485 |             residuals.append(contentsOf: res)
486 |         }
487 | 
488 |         // run the middle part of the unet
489 |         x = midBlocks.0(x, timeEmbedding: temb)
490 |         x = midBlocks.1(
491 |             x, encoderX: encoderX, attentionMask: attentionMask,
492 |             encoderAttentionMask: encoderAttentionMask)
493 |         x = midBlocks.2(x, timeEmbedding: temb)
494 | 
495 |         // run the upsampling part of the unet
496 |         for block in self.upBlocks {
497 |             (x, _, residuals) = block(
498 |                 x, encoderX: encoderX, timeEmbedding: temb, attentionMask: attentionMask,
499 |                 encoderAttentionMask: encoderAttentionMask, residualHiddenStates: residuals)
500 |         }
501 | 
502 |         // postprocess the output
503 |         let dtype = x.dtype
504 |         x = convNormOut(x)
505 |         x = silu(x)
506 |         x = convOut(x)
507 | 
508 |         return x
509 |     }
510 | }
511 | 


--------------------------------------------------------------------------------
/Libraries/StableDiffusion/VAE.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import Foundation
  4 | import MLX
  5 | import MLXNN
  6 | 
  7 | // port of https://github.com/ml-explore/mlx-examples/blob/main/stable_diffusion/stable_diffusion/vae.py
  8 | 
  9 | class Attention: Module, UnaryLayer {
 10 | 
 11 |     @ModuleInfo(key: "group_norm") public var groupNorm: GroupNorm
 12 | 
 13 |     @ModuleInfo(key: "query_proj") public var queryProjection: Linear
 14 |     @ModuleInfo(key: "key_proj") public var keyProjection: Linear
 15 |     @ModuleInfo(key: "value_proj") public var valueProjection: Linear
 16 |     @ModuleInfo(key: "out_proj") public var outProjection: Linear
 17 | 
 18 |     init(dimensions: Int, groupCount: Int = 32) {
 19 |         self._groupNorm.wrappedValue = GroupNorm(
 20 |             groupCount: groupCount, dimensions: dimensions, pytorchCompatible: true)
 21 | 
 22 |         self._queryProjection.wrappedValue = Linear(dimensions, dimensions)
 23 |         self._keyProjection.wrappedValue = Linear(dimensions, dimensions)
 24 |         self._valueProjection.wrappedValue = Linear(dimensions, dimensions)
 25 |         self._outProjection.wrappedValue = Linear(dimensions, dimensions)
 26 |     }
 27 | 
 28 |     func callAsFunction(_ x: MLXArray) -> MLXArray {
 29 |         let (B, H, W, C) = x.shape4
 30 | 
 31 |         var y = groupNorm(x)
 32 | 
 33 |         let queries = queryProjection(y).reshaped(B, H * W, C)
 34 |         let keys = keyProjection(y).reshaped(B, H * W, C)
 35 |         let values = valueProjection(y).reshaped(B, H * W, C)
 36 | 
 37 |         let scale = 1 / sqrt(Float(queries.dim(-1)))
 38 |         let scores = (queries * scale).matmul(keys.transposed(0, 2, 1))
 39 |         let attention = softmax(scores, axis: -1)
 40 | 
 41 |         y = matmul(attention, values).reshaped(B, H, W, C)
 42 |         y = outProjection(y)
 43 | 
 44 |         return x + y
 45 |     }
 46 | }
 47 | 
 48 | class EncoderDecoderBlock2D: Module, UnaryLayer {
 49 | 
 50 |     let resnets: [ResnetBlock2D]
 51 |     let downsample: Conv2d?
 52 |     let upsample: Conv2d?
 53 | 
 54 |     init(
 55 |         inputChannels: Int, outputChannels: Int, numLayers: Int = 1, resnetGroups: Int = 32,
 56 |         addDownSample: Bool = true, addUpSample: Bool = true
 57 |     ) {
 58 |         // Add the resnet blocks
 59 |         self.resnets = (0 ..< numLayers)
 60 |             .map { i in
 61 |                 ResnetBlock2D(
 62 |                     inputChannels: i == 0 ? inputChannels : outputChannels,
 63 |                     outputChannels: outputChannels,
 64 |                     groupCount: resnetGroups)
 65 |             }
 66 | 
 67 |         // Add an optional downsampling layer
 68 |         if addDownSample {
 69 |             self.downsample = Conv2d(
 70 |                 inputChannels: outputChannels, outputChannels: outputChannels, kernelSize: 3,
 71 |                 stride: 2, padding: 0)
 72 |         } else {
 73 |             self.downsample = nil
 74 |         }
 75 | 
 76 |         // or upsampling layer
 77 |         if addUpSample {
 78 |             self.upsample = Conv2d(
 79 |                 inputChannels: outputChannels, outputChannels: outputChannels, kernelSize: 3,
 80 |                 stride: 1, padding: 1)
 81 |         } else {
 82 |             self.upsample = nil
 83 |         }
 84 |     }
 85 | 
 86 |     func callAsFunction(_ x: MLXArray) -> MLXArray {
 87 |         var x = x
 88 | 
 89 |         for resnet in resnets {
 90 |             x = resnet(x)
 91 |         }
 92 | 
 93 |         if let downsample {
 94 |             x = padded(x, widths: [[0, 0], [0, 1], [0, 1], [0, 0]])
 95 |             x = downsample(x)
 96 |         }
 97 | 
 98 |         if let upsample {
 99 |             x = upsample(upsampleNearest(x))
100 |         }
101 | 
102 |         return x
103 |     }
104 | }
105 | 
106 | /// Implements the encoder side of the Autoencoder
107 | class VAEncoder: Module, UnaryLayer {
108 | 
109 |     @ModuleInfo(key: "conv_in") var convIn: Conv2d
110 |     @ModuleInfo(key: "down_blocks") var downBlocks: [EncoderDecoderBlock2D]
111 |     @ModuleInfo(key: "mid_blocks") var midBlocks: (ResnetBlock2D, Attention, ResnetBlock2D)
112 |     @ModuleInfo(key: "conv_norm_out") var convNormOut: GroupNorm
113 |     @ModuleInfo(key: "conv_out") var convOut: Conv2d
114 | 
115 |     init(
116 |         inputChannels: Int, outputChannels: Int, blockOutChannels: [Int] = [64],
117 |         layersPerBlock: Int = 2, resnetGroups: Int = 32
118 |     ) {
119 |         let channels0 = blockOutChannels[0]
120 | 
121 |         self._convIn.wrappedValue = Conv2d(
122 |             inputChannels: inputChannels, outputChannels: channels0, kernelSize: 3, stride: 1,
123 |             padding: 1)
124 | 
125 |         let downblockChannels = [channels0] + blockOutChannels
126 |         self._downBlocks.wrappedValue = zip(downblockChannels, downblockChannels.dropFirst())
127 |             .enumerated()
128 |             .map { (i, pair) in
129 |                 let (inChannels, outChannels) = pair
130 |                 return EncoderDecoderBlock2D(
131 |                     inputChannels: inChannels, outputChannels: outChannels,
132 |                     numLayers: layersPerBlock, resnetGroups: resnetGroups,
133 |                     addDownSample: i < blockOutChannels.count - 1,
134 |                     addUpSample: false
135 |                 )
136 |             }
137 | 
138 |         let channelsLast = blockOutChannels.last!
139 |         self._midBlocks.wrappedValue = (
140 |             ResnetBlock2D(
141 |                 inputChannels: channelsLast,
142 |                 outputChannels: channelsLast,
143 |                 groupCount: resnetGroups
144 |             ),
145 |             Attention(dimensions: channelsLast, groupCount: resnetGroups),
146 |             ResnetBlock2D(
147 |                 inputChannels: channelsLast,
148 |                 outputChannels: channelsLast,
149 |                 groupCount: resnetGroups
150 |             )
151 |         )
152 | 
153 |         self._convNormOut.wrappedValue = GroupNorm(
154 |             groupCount: resnetGroups, dimensions: channelsLast, pytorchCompatible: true)
155 |         self._convOut.wrappedValue = Conv2d(
156 |             inputChannels: channelsLast, outputChannels: outputChannels,
157 |             kernelSize: 3,
158 |             padding: 1)
159 |     }
160 | 
161 |     func callAsFunction(_ x: MLXArray) -> MLXArray {
162 |         var x = convIn(x)
163 | 
164 |         for l in downBlocks {
165 |             x = l(x)
166 |         }
167 | 
168 |         x = midBlocks.0(x)
169 |         x = midBlocks.1(x)
170 |         x = midBlocks.2(x)
171 | 
172 |         x = convNormOut(x)
173 |         x = silu(x)
174 |         x = convOut(x)
175 | 
176 |         return x
177 |     }
178 | }
179 | 
180 | /// Implements the decoder side of the Autoencoder
181 | class VADecoder: Module, UnaryLayer {
182 | 
183 |     @ModuleInfo(key: "conv_in") var convIn: Conv2d
184 |     @ModuleInfo(key: "mid_blocks") var midBlocks: (ResnetBlock2D, Attention, ResnetBlock2D)
185 |     @ModuleInfo(key: "up_blocks") var upBlocks: [EncoderDecoderBlock2D]
186 |     @ModuleInfo(key: "conv_norm_out") var convNormOut: GroupNorm
187 |     @ModuleInfo(key: "conv_out") var convOut: Conv2d
188 | 
189 |     init(
190 |         inputChannels: Int, outputChannels: Int, blockOutChannels: [Int] = [64],
191 |         layersPerBlock: Int = 2, resnetGroups: Int = 32
192 |     ) {
193 |         let channels0 = blockOutChannels[0]
194 |         let channelsLast = blockOutChannels.last!
195 | 
196 |         self._convIn.wrappedValue = Conv2d(
197 |             inputChannels: inputChannels, outputChannels: channelsLast, kernelSize: 3, stride: 1,
198 |             padding: 1)
199 | 
200 |         self._midBlocks.wrappedValue = (
201 |             ResnetBlock2D(
202 |                 inputChannels: channelsLast,
203 |                 outputChannels: channelsLast,
204 |                 groupCount: resnetGroups
205 |             ),
206 |             Attention(dimensions: channelsLast, groupCount: resnetGroups),
207 |             ResnetBlock2D(
208 |                 inputChannels: channelsLast,
209 |                 outputChannels: channelsLast,
210 |                 groupCount: resnetGroups
211 |             )
212 |         )
213 | 
214 |         let channels = [channelsLast] + blockOutChannels.reversed()
215 |         self._upBlocks.wrappedValue = zip(channels, channels.dropFirst())
216 |             .enumerated()
217 |             .map { (i, pair) in
218 |                 let (inChannels, outChannels) = pair
219 |                 return EncoderDecoderBlock2D(
220 |                     inputChannels: inChannels,
221 |                     outputChannels: outChannels,
222 |                     numLayers: layersPerBlock,
223 |                     resnetGroups: resnetGroups,
224 |                     addDownSample: false,
225 |                     addUpSample: i < blockOutChannels.count - 1
226 |                 )
227 |             }
228 | 
229 |         self._convNormOut.wrappedValue = GroupNorm(
230 |             groupCount: resnetGroups, dimensions: channels0, pytorchCompatible: true)
231 |         self._convOut.wrappedValue = Conv2d(
232 |             inputChannels: channels0, outputChannels: outputChannels,
233 |             kernelSize: 3,
234 |             padding: 1)
235 |     }
236 | 
237 |     func callAsFunction(_ x: MLXArray) -> MLXArray {
238 |         var x = convIn(x)
239 | 
240 |         x = midBlocks.0(x)
241 |         x = midBlocks.1(x)
242 |         x = midBlocks.2(x)
243 | 
244 |         for l in upBlocks {
245 |             x = l(x)
246 |         }
247 | 
248 |         x = convNormOut(x)
249 |         x = silu(x)
250 |         x = convOut(x)
251 | 
252 |         return x
253 |     }
254 | }
255 | 
256 | /// The autoencoder that allows us to perform diffusion in the latent space
257 | class Autoencoder: Module {
258 | 
259 |     let latentChannels: Int
260 |     let scalingFactor: Float
261 |     let encoder: VAEncoder
262 |     let decoder: VADecoder
263 | 
264 |     @ModuleInfo(key: "quant_proj") public var quantProjection: Linear
265 |     @ModuleInfo(key: "post_quant_proj") public var postQuantProjection: Linear
266 | 
267 |     init(configuration: AutoencoderConfiguration) {
268 |         self.latentChannels = configuration.latentChannelsIn
269 |         self.scalingFactor = configuration.scalingFactor
270 |         self.encoder = VAEncoder(
271 |             inputChannels: configuration.inputChannels,
272 |             outputChannels: configuration.latentChannelsOut,
273 |             blockOutChannels: configuration.blockOutChannels,
274 |             layersPerBlock: configuration.layersPerBlock,
275 |             resnetGroups: configuration.normNumGroups)
276 |         self.decoder = VADecoder(
277 |             inputChannels: configuration.latentChannelsIn,
278 |             outputChannels: configuration.outputChannels,
279 |             blockOutChannels: configuration.blockOutChannels,
280 |             layersPerBlock: configuration.layersPerBlock + 1,
281 |             resnetGroups: configuration.normNumGroups)
282 | 
283 |         self._quantProjection.wrappedValue = Linear(
284 |             configuration.latentChannelsIn, configuration.latentChannelsOut)
285 |         self._postQuantProjection.wrappedValue = Linear(
286 |             configuration.latentChannelsIn, configuration.latentChannelsIn)
287 |     }
288 | 
289 |     func decode(_ z: MLXArray) -> MLXArray {
290 |         let z = z / scalingFactor
291 |         return decoder(postQuantProjection(z))
292 |     }
293 | 
294 |     func encode(_ x: MLXArray) -> (MLXArray, MLXArray) {
295 |         var x = encoder(x)
296 |         x = quantProjection(x)
297 |         var (mean, logvar) = x.split(axis: -1)
298 |         mean = mean * scalingFactor
299 |         logvar = logvar + 2 * log(scalingFactor)
300 | 
301 |         return (mean, logvar)
302 |     }
303 | 
304 |     struct Result {
305 |         let xHat: MLXArray
306 |         let z: MLXArray
307 |         let mean: MLXArray
308 |         let logvar: MLXArray
309 |     }
310 | 
311 |     func callAsFunction(_ x: MLXArray, key: MLXArray? = nil) -> Result {
312 |         let (mean, logvar) = encode(x)
313 |         let z = MLXRandom.normal(mean.shape, key: key) * exp(0.5 * logvar) + mean
314 |         let xHat = decode(z)
315 | 
316 |         return Result(xHat: xHat, z: z, mean: mean, logvar: logvar)
317 |     }
318 | }
319 | 


--------------------------------------------------------------------------------
/Package.resolved:
--------------------------------------------------------------------------------
 1 | {
 2 |   "pins" : [
 3 |     {
 4 |       "identity" : "gzipswift",
 5 |       "kind" : "remoteSourceControl",
 6 |       "location" : "https://github.com/1024jp/GzipSwift",
 7 |       "state" : {
 8 |         "revision" : "731037f6cc2be2ec01562f6597c1d0aa3fe6fd05",
 9 |         "version" : "6.0.1"
10 |       }
11 |     },
12 |     {
13 |       "identity" : "jinja",
14 |       "kind" : "remoteSourceControl",
15 |       "location" : "https://github.com/johnmai-dev/Jinja",
16 |       "state" : {
17 |         "revision" : "31c4dd39bcdc07eaa42a384bdc88ea599022b800",
18 |         "version" : "1.1.2"
19 |       }
20 |     },
21 |     {
22 |       "identity" : "mlx-swift",
23 |       "kind" : "remoteSourceControl",
24 |       "location" : "https://github.com/ml-explore/mlx-swift",
25 |       "state" : {
26 |         "revision" : "b94473af8c50010edba87a48bbd60c3d7f949852",
27 |         "version" : "0.25.4"
28 |       }
29 |     },
30 |     {
31 |       "identity" : "swift-argument-parser",
32 |       "kind" : "remoteSourceControl",
33 |       "location" : "https://github.com/apple/swift-argument-parser.git",
34 |       "state" : {
35 |         "revision" : "0fbc8848e389af3bb55c182bc19ca9d5dc2f255b",
36 |         "version" : "1.4.0"
37 |       }
38 |     },
39 |     {
40 |       "identity" : "swift-collections",
41 |       "kind" : "remoteSourceControl",
42 |       "location" : "https://github.com/apple/swift-collections.git",
43 |       "state" : {
44 |         "revision" : "c1805596154bb3a265fd91b8ac0c4433b4348fb0",
45 |         "version" : "1.2.0"
46 |       }
47 |     },
48 |     {
49 |       "identity" : "swift-numerics",
50 |       "kind" : "remoteSourceControl",
51 |       "location" : "https://github.com/apple/swift-numerics",
52 |       "state" : {
53 |         "revision" : "0a5bc04095a675662cf24757cc0640aa2204253b",
54 |         "version" : "1.0.2"
55 |       }
56 |     },
57 |     {
58 |       "identity" : "swift-transformers",
59 |       "kind" : "remoteSourceControl",
60 |       "location" : "https://github.com/huggingface/swift-transformers",
61 |       "state" : {
62 |         "revision" : "c2f302a74cca59cbde683b1425ab43c05685515a",
63 |         "version" : "0.1.21"
64 |       }
65 |     }
66 |   ],
67 |   "version" : 2
68 | }
69 | 


--------------------------------------------------------------------------------
/Package.swift:
--------------------------------------------------------------------------------
  1 | // swift-tools-version: 5.9
  2 | // The swift-tools-version declares the minimum version of Swift required to build this package.
  3 | 
  4 | import PackageDescription
  5 | 
  6 | let package = Package(
  7 |     name: "mlx-libraries",
  8 |     platforms: [.macOS(.v14), .iOS(.v16)],
  9 |     products: [
 10 |         .library(
 11 |             name: "MLXLLM",
 12 |             targets: ["MLXLLM"]),
 13 |         .library(
 14 |             name: "MLXVLM",
 15 |             targets: ["MLXVLM"]),
 16 |         .library(
 17 |             name: "MLXLMCommon",
 18 |             targets: ["MLXLMCommon"]),
 19 |         .library(
 20 |             name: "MLXMNIST",
 21 |             targets: ["MLXMNIST"]),
 22 |         .library(
 23 |             name: "MLXEmbedders",
 24 |             targets: ["MLXEmbedders"]),
 25 |         .library(
 26 |             name: "StableDiffusion",
 27 |             targets: ["StableDiffusion"]),
 28 |     ],
 29 |     dependencies: [
 30 |         .package(url: "https://github.com/ml-explore/mlx-swift", .upToNextMinor(from: "0.25.4")),
 31 |         .package(
 32 |             url: "https://github.com/huggingface/swift-transformers", .upToNextMinor(from: "0.1.21")
 33 |         ),
 34 |         .package(url: "https://github.com/1024jp/GzipSwift", "6.0.1" ... "6.0.1"),  // Only needed by MLXMNIST
 35 |     ],
 36 |     targets: [
 37 |         .target(
 38 |             name: "MLXLLM",
 39 |             dependencies: [
 40 |                 "MLXLMCommon",
 41 |                 .product(name: "MLX", package: "mlx-swift"),
 42 |                 .product(name: "MLXFast", package: "mlx-swift"),
 43 |                 .product(name: "MLXNN", package: "mlx-swift"),
 44 |                 .product(name: "MLXOptimizers", package: "mlx-swift"),
 45 |                 .product(name: "MLXRandom", package: "mlx-swift"),
 46 |                 .product(name: "Transformers", package: "swift-transformers"),
 47 |             ],
 48 |             path: "Libraries/MLXLLM",
 49 |             exclude: [
 50 |                 "README.md"
 51 |             ],
 52 |             swiftSettings: [
 53 |                 .enableExperimentalFeature("StrictConcurrency")
 54 |             ]
 55 |         ),
 56 |         .target(
 57 |             name: "MLXVLM",
 58 |             dependencies: [
 59 |                 "MLXLMCommon",
 60 |                 .product(name: "MLX", package: "mlx-swift"),
 61 |                 .product(name: "MLXFast", package: "mlx-swift"),
 62 |                 .product(name: "MLXNN", package: "mlx-swift"),
 63 |                 .product(name: "MLXOptimizers", package: "mlx-swift"),
 64 |                 .product(name: "MLXRandom", package: "mlx-swift"),
 65 |                 .product(name: "Transformers", package: "swift-transformers"),
 66 |             ],
 67 |             path: "Libraries/MLXVLM",
 68 |             exclude: [
 69 |                 "README.md"
 70 |             ],
 71 |             swiftSettings: [
 72 |                 .enableExperimentalFeature("StrictConcurrency")
 73 |             ]
 74 |         ),
 75 |         .target(
 76 |             name: "MLXLMCommon",
 77 |             dependencies: [
 78 |                 .product(name: "MLX", package: "mlx-swift"),
 79 |                 .product(name: "MLXNN", package: "mlx-swift"),
 80 |                 .product(name: "MLXOptimizers", package: "mlx-swift"),
 81 |                 .product(name: "MLXRandom", package: "mlx-swift"),
 82 |                 .product(name: "MLXLinalg", package: "mlx-swift"),
 83 |                 .product(name: "Transformers", package: "swift-transformers"),
 84 |             ],
 85 |             path: "Libraries/MLXLMCommon",
 86 |             exclude: [
 87 |                 "README.md"
 88 |             ],
 89 |             swiftSettings: [
 90 |                 .enableExperimentalFeature("StrictConcurrency")
 91 |             ]
 92 |         ),
 93 |         .testTarget(
 94 |             name: "MLXLMTests",
 95 |             dependencies: [
 96 |                 .product(name: "MLX", package: "mlx-swift"),
 97 |                 .product(name: "MLXNN", package: "mlx-swift"),
 98 |                 .product(name: "MLXOptimizers", package: "mlx-swift"),
 99 |                 .product(name: "MLXRandom", package: "mlx-swift"),
100 |                 .product(name: "Transformers", package: "swift-transformers"),
101 |                 "MLXLMCommon",
102 |                 "MLXLLM",
103 |                 "MLXVLM",
104 |             ],
105 |             path: "Tests/MLXLMTests",
106 |             exclude: [
107 |                 "README.md"
108 |             ],
109 |             swiftSettings: [
110 |                 .enableExperimentalFeature("StrictConcurrency")
111 |             ]
112 |         ),
113 |         .target(
114 |             name: "MLXEmbedders",
115 |             dependencies: [
116 |                 .product(name: "MLX", package: "mlx-swift"),
117 |                 .product(name: "MLXFast", package: "mlx-swift"),
118 |                 .product(name: "MLXNN", package: "mlx-swift"),
119 |                 .product(name: "Transformers", package: "swift-transformers"),
120 |                 .product(name: "MLXLinalg", package: "mlx-swift"),
121 |             ],
122 |             path: "Libraries/Embedders",
123 |             exclude: [
124 |                 "README.md"
125 |             ]
126 |         ),
127 |         .target(
128 |             name: "MLXMNIST",
129 |             dependencies: [
130 |                 .product(name: "MLX", package: "mlx-swift"),
131 |                 .product(name: "MLXFast", package: "mlx-swift"),
132 |                 .product(name: "MLXNN", package: "mlx-swift"),
133 |                 .product(name: "MLXOptimizers", package: "mlx-swift"),
134 |                 .product(name: "MLXRandom", package: "mlx-swift"),
135 |                 .product(name: "Transformers", package: "swift-transformers"),
136 |                 .product(name: "Gzip", package: "GzipSwift"),
137 |             ],
138 |             path: "Libraries/MLXMNIST",
139 |             exclude: [
140 |                 "README.md"
141 |             ],
142 |             swiftSettings: [
143 |                 .enableExperimentalFeature("StrictConcurrency")
144 |             ]
145 |         ),
146 |         .target(
147 |             name: "StableDiffusion",
148 |             dependencies: [
149 |                 .product(name: "MLX", package: "mlx-swift"),
150 |                 .product(name: "MLXNN", package: "mlx-swift"),
151 |                 .product(name: "MLXRandom", package: "mlx-swift"),
152 |                 .product(name: "Transformers", package: "swift-transformers"),
153 |             ],
154 |             path: "Libraries/StableDiffusion",
155 |             exclude: [
156 |                 "README.md"
157 |             ],
158 |             swiftSettings: [
159 |                 .enableExperimentalFeature("StrictConcurrency")
160 |             ]
161 |         ),
162 |     ]
163 | )
164 | 
165 | if Context.environment["MLX_SWIFT_BUILD_DOC"] == "1"
166 |     || Context.environment["SPI_GENERATE_DOCS"] == "1"
167 | {
168 |     // docc builder
169 |     package.dependencies.append(
170 |         .package(url: "https://github.com/apple/swift-docc-plugin", from: "1.3.0")
171 |     )
172 | }
173 | 


--------------------------------------------------------------------------------
/README.md:
--------------------------------------------------------------------------------
  1 | # Documentation
  2 | 
  3 | Developers can use these examples in their own programs -- just import the swift package!
  4 | 
  5 | - [Porting and implementing models](https://swiftpackageindex.com/ml-explore/mlx-swift-examples/main/documentation/mlxlmcommon/porting)
  6 | - [MLXLLMCommon](https://swiftpackageindex.com/ml-explore/mlx-swift-examples/main/documentation/mlxlmcommon) -- common API for LLM and VLM
  7 | - [MLXLLM](https://swiftpackageindex.com/ml-explore/mlx-swift-examples/main/documentation/mlxllm) -- large language model example implementations
  8 | - [MLXVLM](https://swiftpackageindex.com/ml-explore/mlx-swift-examples/main/documentation/mlxvlm) -- vision language model example implementations
  9 | - [MLXEmbedders](https://swiftpackageindex.com/ml-explore/mlx-swift-examples/main/documentation/mlxembedders) -- popular Encoders / Embedding models example implementations
 10 | - [StableDiffusion](https://swiftpackageindex.com/ml-explore/mlx-swift-examples/main/documentation/stablediffusion) -- SDXL Turbo and Stable Diffusion mdeol example implementations
 11 | - [MLXMNIST](https://swiftpackageindex.com/ml-explore/mlx-swift-examples/main/documentation/mlxmnist) -- MNIST implementation for all your digit recognition needs
 12 | 
 13 | # MLX Swift Examples
 14 | 
 15 | Example [MLX Swift](https://github.com/ml-explore/mlx-swift) programs.
 16 | 
 17 | - [MNISTTrainer](Applications/MNISTTrainer/README.md): An example that runs on
 18 |   both iOS and macOS that downloads MNIST training data and trains a
 19 |   [LeNet](https://en.wikipedia.org/wiki/LeNet).
 20 | 
 21 | - [LLMEval](Applications/LLMEval/README.md): An example that runs on both iOS
 22 |   and macOS that downloads an LLM and tokenizer from Hugging Face and
 23 |   generates text from a given prompt.
 24 | 
 25 | - [VLMEval](Applications/VLMEval/README.md): An example that runs on iOS, macOS and visionOS to download a VLM and tokenizer from Hugging Face and
 26 |   analyzes the given image and describe it in text.
 27 | 
 28 | - [MLXChatExample](Applications/MLXChatExample/README.md): An example chat app that runs on both iOS and macOS that supports LLMs and VLMs.
 29 | 
 30 | - [LinearModelTraining](Tools/LinearModelTraining/README.md): An example that
 31 |   trains a simple linear model.
 32 | 
 33 | - [StableDiffusionExample](Applications/StableDiffusionExample/README.md): An
 34 |   example that runs on both iOS and macOS that downloads a stable diffusion model
 35 |   from Hugging Face and  and generates an image from a given prompt.
 36 | 
 37 | - [llm-tool](Tools/llm-tool/README.md): A command line tool for generating text
 38 |   using a variety of LLMs available on the Hugging Face hub.
 39 | 
 40 | - [ExampleLLM](Tools/ExampleLLM/README.md): A command line tool using the simplified API to interact with LLMs.
 41 | 
 42 | - [image-tool](Tools/image-tool/README.md): A command line tool for generating images
 43 |   using a stable diffusion model from Hugging Face.
 44 | 
 45 | - [mnist-tool](Tools/mnist-tool/README.md): A command line tool for training a
 46 |   a LeNet on MNIST.
 47 | 
 48 | ## Interacting with LLMs
 49 | 
 50 | See also [MLXLMCommon](Libraries/MLXLMCommon).  You can easily use
 51 | a wide variety of open weight LLM and VLMs in your code.  You can use
 52 | this simplified API:
 53 | 
 54 | ```swift
 55 | let model = try await loadModel(id: "mlx-community/Qwen3-4B-4bit")
 56 | let session = ChatSession(model)
 57 | print(try await session.respond(to: "What are two things to see in San Francisco?")
 58 | print(try await session.respond(to: "How about a great place to eat?")
 59 | ```
 60 | 
 61 | Or use the underlying API to control everything aspect of the evaluation.
 62 | 
 63 | ## Running
 64 | 
 65 | The application and command line tool examples can be run from Xcode or from
 66 | the command line:
 67 | 
 68 | ```
 69 | ./mlx-run llm-tool --prompt "swift programming language"
 70 | ```
 71 | 
 72 | Note: `mlx-run` is a shell script that uses `xcode` command line tools to
 73 | locate the built binaries. It is equivalent to running from Xcode itself.
 74 | 
 75 | See also:
 76 | 
 77 | - [MLX troubleshooting](https://swiftpackageindex.com/ml-explore/mlx-swift/main/documentation/mlx/troubleshooting)
 78 | 
 79 | ## Installation of libraries
 80 | 
 81 | The MLXLLM, MLXVLM, MLXLMCommon, MLXMNIST, MLXEmbedders, and StableDiffusion libraries in the example repo are available
 82 | as Swift Packages.
 83 | 
 84 | 
 85 | Add the following dependency to your Package.swift
 86 | 
 87 | ```swift
 88 | .package(url: "https://github.com/ml-explore/mlx-swift-examples/", branch: "main"),
 89 | ```
 90 | 
 91 | Then add one or more libraries to the target as a dependency:
 92 | 
 93 | ```swift
 94 | .target(
 95 |     name: "YourTargetName",
 96 |     dependencies: [
 97 |         .product(name: "MLXLLM", package: "mlx-swift-examples")
 98 |     ]),
 99 | ```
100 | 
101 | Alternatively, add `https://github.com/ml-explore/mlx-swift-examples/` to the `Project Dependencies` and set the `Dependency Rule` to `Branch` and `main` in Xcode.
102 | 


--------------------------------------------------------------------------------
/Tests/MLXLMTests/BaseConfigurationTests.swift:
--------------------------------------------------------------------------------
 1 | // Copyright © 2025 Apple Inc.
 2 | 
 3 | import Foundation
 4 | import MLXLMCommon
 5 | import XCTest
 6 | 
 7 | public class BaseConfigurationTests: XCTestCase {
 8 | 
 9 |     func testQuantization() throws {
10 |         let json =
11 |             """
12 |             {
13 |                 "model_type": "Test",
14 |                 "quantization": {
15 |                     "group_size": 128,
16 |                     "bits": 4
17 |                 }
18 |             }
19 |             """
20 | 
21 |         let config = try JSONDecoder().decode(
22 |             BaseConfiguration.self, from: json.data(using: .utf8)!)
23 | 
24 |         XCTAssertEqual(config.quantization, .init(groupSize: 128, bits: 4))
25 |         XCTAssertEqual(
26 |             config.perLayerQuantization?.quantization(layer: "x"), .init(groupSize: 128, bits: 4))
27 |     }
28 | 
29 |     func testHeterogenousQuantization() throws {
30 |         // from https://huggingface.co/mlx-community/Qwen3-1.7B-4bit-AWQ/blob/main/config.json#L20
31 |         let json =
32 |             """
33 |             {
34 |                 "model_type": "Test",
35 |                 "quantization": {
36 |                     "group_size": 64,
37 |                     "bits": 4,
38 |                     "model.embed_tokens": {
39 |                         "group_size": 32,
40 |                         "bits": 4
41 |                     },
42 |                     "model.layers.0.self_attn.q_norm": false,
43 |                     "true_layer": true
44 |                 }
45 |             }
46 |             """
47 | 
48 |         let config = try JSONDecoder().decode(
49 |             BaseConfiguration.self, from: json.data(using: .utf8)!)
50 | 
51 |         XCTAssertEqual(config.quantization, .init(groupSize: 64, bits: 4))
52 | 
53 |         // a random layer -- no specific configuration gets default
54 |         XCTAssertEqual(
55 |             config.perLayerQuantization?.quantization(layer: "x"),
56 |             .init(groupSize: 64, bits: 4))
57 | 
58 |         // layer with an override
59 |         XCTAssertEqual(
60 |             config.perLayerQuantization?.quantization(layer: "model.embed_tokens"),
61 |             .init(groupSize: 32, bits: 4))
62 | 
63 |         // layer with an override -- not quant
64 |         XCTAssertNil(
65 |             config.perLayerQuantization?.quantization(layer: "model.layers.0.self_attn.q_norm"))
66 | 
67 |         // layer with an override -- true, use the default
68 |         XCTAssertEqual(
69 |             config.perLayerQuantization?.quantization(layer: "true_layer"),
70 |             .init(groupSize: 64, bits: 4))
71 |     }
72 | 
73 | }
74 | 


--------------------------------------------------------------------------------
/Tests/MLXLMTests/EvalTests.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2025 Apple Inc.
  2 | 
  3 | import Foundation
  4 | import MLX
  5 | import MLXLLM
  6 | import MLXLMCommon
  7 | import MLXNN
  8 | import MLXOptimizers
  9 | import Tokenizers
 10 | import XCTest
 11 | 
 12 | ///
 13 | public class EvalTests: XCTestCase {
 14 | 
 15 |     func testLlamaEval() throws {
 16 |         let config = LlamaConfiguration(
 17 |             hiddenSize: 64, hiddenLayers: 16, intermediateSize: 512, attentionHeads: 32,
 18 |             rmsNormEps: 0.00001, vocabularySize: 100, kvHeads: 8)
 19 |         let model = LlamaModel(config)
 20 |         quantize(model: model, groupSize: 64, bits: 4)
 21 | 
 22 |         let input = MLXArray([1, 2, 3, 4, 5])[.newAxis, .ellipsis]
 23 |         let output = model.callAsFunction(input, cache: nil)
 24 | 
 25 |         XCTAssertEqual(output.shape, [1, 5, 100])
 26 |     }
 27 | 
 28 |     func testLlamaLora() throws {
 29 |         let config = LlamaConfiguration(
 30 |             hiddenSize: 64, hiddenLayers: 16, intermediateSize: 512, attentionHeads: 32,
 31 |             rmsNormEps: 0.00001, vocabularySize: 100, kvHeads: 8)
 32 |         let model = LlamaModel(config)
 33 |         quantize(model: model, groupSize: 64, bits: 4)
 34 | 
 35 |         LoRATrain.convert(model: model, layers: model.loraLinearLayers(4))
 36 | 
 37 |         let optimizer = Adam(learningRate: 1e-5)
 38 | 
 39 |         let train = ["a", "b", "c"]
 40 |         let valid = ["x", "y", "z"]
 41 | 
 42 |         let tokenizer = TestTokenizer()
 43 |         let parameters = LoRATrain.Parameters(iterations: 5)
 44 | 
 45 |         try LoRATrain.train(
 46 |             model: model, train: train, validate: valid, optimizer: optimizer,
 47 |             tokenizer: tokenizer,
 48 |             parameters: parameters
 49 |         ) { progress in
 50 |             print(progress)
 51 |             return .more
 52 |         }
 53 | 
 54 |         let input = MLXArray([1, 2, 3, 4, 5])[.newAxis, .ellipsis]
 55 |         let output = model.callAsFunction(input, cache: nil)
 56 | 
 57 |         XCTAssertEqual(output.shape, [1, 5, 100])
 58 |     }
 59 | 
 60 | }
 61 | 
 62 | struct TestTokenizer: Tokenizer {
 63 | 
 64 |     let length = 8
 65 | 
 66 |     var vocabulary: [Int: String]
 67 | 
 68 |     init(vocabularySize: Int = 100) {
 69 |         let letters = "abcdefghijklmnopqrstuvwxyz"
 70 |         self.vocabulary = Dictionary(
 71 |             uniqueKeysWithValues: (0 ..< vocabularySize)
 72 |                 .map { t in
 73 |                     (
 74 |                         t,
 75 |                         String(
 76 |                             (0 ..< ((3 ..< 8).randomElement() ?? 3)).compactMap { _ in
 77 |                                 letters.randomElement()
 78 |                             })
 79 |                     )
 80 |                 }
 81 |         )
 82 |     }
 83 | 
 84 |     func tokenize(text: String) -> [String] {
 85 |         text.split(separator: " ").map { String($0) }
 86 |     }
 87 | 
 88 |     func encode(text: String) -> [Int] {
 89 |         (0 ..< length).map { _ in
 90 |             Int.random(in: 0 ..< 100)
 91 |         }
 92 |     }
 93 | 
 94 |     func encode(text: String, addSpecialTokens: Bool) -> [Int] {
 95 |         encode(text: text)
 96 |     }
 97 | 
 98 |     func decode(tokens: [Int], skipSpecialTokens: Bool) -> String {
 99 |         var tokens = tokens
100 |         if tokens.count > 50 {
101 |             tokens.append(19)
102 |         }
103 |         return tokens.map { convertIdToToken($0) ?? "" }.joined(separator: " ")
104 |     }
105 | 
106 |     func convertTokenToId(_ token: String) -> Int? {
107 |         Int.random(in: 0 ..< 100)
108 |     }
109 | 
110 |     func convertIdToToken(_ id: Int) -> String? {
111 |         if id == 19 {
112 |             return "EOS"
113 |         }
114 |         return vocabulary[id]
115 |     }
116 | 
117 |     var bosToken: String? = nil
118 | 
119 |     var bosTokenId: Int? = 0
120 | 
121 |     var eosToken: String? = nil
122 | 
123 |     var eosTokenId: Int? = 0
124 | 
125 |     var unknownToken: String? = nil
126 | 
127 |     var unknownTokenId: Int? = 0
128 | 
129 |     func applyChatTemplate(messages: [Tokenizers.Message]) throws -> [Int] {
130 |         encode(text: "")
131 |     }
132 | 
133 |     func applyChatTemplate(messages: [Tokenizers.Message], tools: [Tokenizers.ToolSpec]?) throws
134 |         -> [Int]
135 |     {
136 |         encode(text: "")
137 |     }
138 | 
139 |     func applyChatTemplate(
140 |         messages: [Tokenizers.Message], tools: [Tokenizers.ToolSpec]?,
141 |         additionalContext: [String: Any]?
142 |     ) throws -> [Int] {
143 |         encode(text: "")
144 |     }
145 | 
146 |     func applyChatTemplate(
147 |         messages: [Tokenizers.Message], chatTemplate: Tokenizers.ChatTemplateArgument
148 |     ) throws -> [Int] {
149 |         encode(text: "")
150 |     }
151 | 
152 |     func applyChatTemplate(messages: [Tokenizers.Message], chatTemplate: String) throws -> [Int] {
153 |         encode(text: "")
154 |     }
155 | 
156 |     func applyChatTemplate(
157 |         messages: [Tokenizers.Message], chatTemplate: Tokenizers.ChatTemplateArgument?,
158 |         addGenerationPrompt: Bool, truncation: Bool, maxLength: Int?, tools: [Tokenizers.ToolSpec]?
159 |     ) throws -> [Int] {
160 |         encode(text: "")
161 |     }
162 | 
163 |     func applyChatTemplate(
164 |         messages: [Tokenizers.Message], chatTemplate: Tokenizers.ChatTemplateArgument?,
165 |         addGenerationPrompt: Bool, truncation: Bool, maxLength: Int?, tools: [Tokenizers.ToolSpec]?,
166 |         additionalContext: [String: Any]?
167 |     ) throws -> [Int] {
168 |         encode(text: "")
169 |     }
170 | 
171 | }
172 | 


--------------------------------------------------------------------------------
/Tests/MLXLMTests/README.md:
--------------------------------------------------------------------------------
https://raw.githubusercontent.com/ml-explore/mlx-swift-examples/2cceb87794bcce960dfc17e79a91946f41c02317/Tests/MLXLMTests/README.md


--------------------------------------------------------------------------------
/Tests/MLXLMTests/StreamlinedTests.swift:
--------------------------------------------------------------------------------
 1 | // Copyright © 2025 Apple Inc.
 2 | 
 3 | import Foundation
 4 | import MLX
 5 | import MLXLLM
 6 | import MLXLMCommon
 7 | import MLXNN
 8 | import MLXOptimizers
 9 | import Tokenizers
10 | import XCTest
11 | 
12 | /// Tests for the streamlined API
13 | public class StreamlinedTests: XCTestCase {
14 | 
15 |     /// for tests we don't download a model but do execute one
16 |     func model() -> LanguageModel {
17 |         let config = LlamaConfiguration(
18 |             hiddenSize: 64, hiddenLayers: 16, intermediateSize: 64, attentionHeads: 8,
19 |             rmsNormEps: 0.00001, vocabularySize: 100, kvHeads: 8)
20 |         let model = LlamaModel(config)
21 |         quantize(model: model, groupSize: 64, bits: 4)
22 |         return model
23 |     }
24 | 
25 |     /// This is equivalent to:
26 |     ///
27 |     /// ```swift
28 |     /// let model = LLMModelFactory.load("test")
29 |     /// ```
30 |     func modelContainer() -> ModelContainer {
31 |         let context = ModelContext(
32 |             configuration: .init(id: "test", extraEOSTokens: ["EOS"]), model: model(),
33 |             processor: TestUserInputProcessor(), tokenizer: TestTokenizer())
34 |         return ModelContainer(context: context)
35 |     }
36 | 
37 |     func testOneShot() async throws {
38 |         let model = modelContainer()
39 |         let result = try await ChatSession(model).respond(to: "Tell me about things")
40 |         print(result)
41 |     }
42 | 
43 |     func testOneShotStream() async throws {
44 |         let model = modelContainer()
45 |         for try await token in ChatSession(model).streamResponse(to: "Tell me about things") {
46 |             print(token, terminator: "")
47 |         }
48 |     }
49 | 
50 |     func testChat() async throws {
51 |         let model = modelContainer()
52 |         let session = ChatSession(model)
53 | 
54 |         print(try await session.respond(to: "what color is the sky?"))
55 |         print(try await session.respond(to: "why is that?"))
56 |         print(try await session.respond(to: "describe this image", image: .ciImage(CIImage.red)))
57 |     }
58 | }
59 | 
60 | private struct TestUserInputProcessor: UserInputProcessor {
61 |     func prepare(input: UserInput) throws -> LMInput {
62 |         LMInput(tokens: MLXRandom.randInt(0 ..< 1000, [100]))
63 |     }
64 | }
65 | 


--------------------------------------------------------------------------------
/Tests/MLXLMTests/ToolTests.swift:
--------------------------------------------------------------------------------
 1 | import Foundation
 2 | import MLXLMCommon
 3 | import Testing
 4 | 
 5 | struct ToolTests {
 6 |     @Test("Test Weather Tool Schema Generation")
 7 |     func testWeatherToolSchemaGeneration() throws {
 8 |         struct WeatherInput: Codable {
 9 |             let location: String
10 |             let unit: String?
11 |         }
12 | 
13 |         struct WeatherOutput: Codable {
14 |             let temperature: Double
15 |             let conditions: String
16 |         }
17 | 
18 |         let tool = Tool<WeatherInput, WeatherOutput>(
19 |             name: "get_current_weather",
20 |             description: "Get the current weather in a given location",
21 |             parameters: [
22 |                 .required(
23 |                     "location", type: .string, description: "The city, e.g. Istanbul"
24 |                 ),
25 |                 .optional(
26 |                     "unit",
27 |                     type: .string,
28 |                     description: "The unit of temperature",
29 |                     extraProperties: [
30 |                         "enum": ["celsius", "fahrenheit"]
31 |                     ]
32 |                 ),
33 |             ]
34 |         ) { input in
35 |             WeatherOutput(temperature: 14.0, conditions: "Sunny")
36 |         }
37 | 
38 |         let actual = tool.schema as NSDictionary
39 | 
40 |         let expected: NSDictionary = [
41 |             "type": "function",
42 |             "function": [
43 |                 "name": "get_current_weather",
44 |                 "description": "Get the current weather in a given location",
45 |                 "parameters": [
46 |                     "type": "object",
47 |                     "properties": [
48 |                         "location": [
49 |                             "type": "string",
50 |                             "description": "The city, e.g. Istanbul",
51 |                         ],
52 |                         "unit": [
53 |                             "type": "string",
54 |                             "description": "The unit of temperature",
55 |                             "enum": ["celsius", "fahrenheit"],
56 |                         ],
57 |                     ],
58 |                     "required": ["location"],
59 |                 ],
60 |             ],
61 |         ]
62 | 
63 |         #expect(actual == expected)
64 |     }
65 | 
66 |     @Test("Test Tool Call Detection in Generated Text")
67 |     func testToolCallDetection() throws {
68 |         let processor = ToolCallProcessor()
69 |         let chunks: [String] = [
70 |             "<tool", "_", "call>", "{", "\"", "name", "\"", ":", " ", "\"", "get", "_", "current",
71 |             "_", "weather", "\"", ",", " ", "\"", "arguments", "\"", ":", " ", "{", "\"",
72 |             "location", "\"", ":", " ", "\"", "San", " Francisco", "\"", ",", " ", "\"", "unit",
73 |             "\"", ":", " ", "\"", "celsius", "\"", "}", "}", "</tool", "_", "call>",
74 |         ]
75 | 
76 |         for chunk in chunks {
77 |             let result = processor.processChunk(chunk)
78 |             #expect(result == nil)
79 |         }
80 | 
81 |         #expect(processor.toolCalls.count == 1)
82 |         let toolCall = try #require(processor.toolCalls.first)
83 | 
84 |         #expect(toolCall.function.name == "get_current_weather")
85 |         #expect(toolCall.function.arguments["location"] == .string("San Francisco"))
86 |         #expect(toolCall.function.arguments["unit"] == .string("celsius"))
87 |     }
88 | }
89 | 


--------------------------------------------------------------------------------
/Tests/MLXLMTests/UserInputTests.swift:
--------------------------------------------------------------------------------
  1 | import Foundation
  2 | import MLX
  3 | import MLXLMCommon
  4 | import MLXVLM
  5 | import XCTest
  6 | 
  7 | func assertEqual(
  8 |     _ v1: Any, _ v2: Any, path: [String] = [], file: StaticString = #filePath, line: UInt = #line
  9 | ) {
 10 |     switch (v1, v2) {
 11 |     case let (v1, v2) as (String, String):
 12 |         XCTAssertEqual(v1, v2, file: file, line: line)
 13 | 
 14 |     case let (v1, v2) as ([Any], [Any]):
 15 |         XCTAssertEqual(
 16 |             v1.count, v2.count, "Arrays not equal size at \(path)", file: file, line: line)
 17 | 
 18 |         for (index, (v1v, v2v)) in zip(v1, v2).enumerated() {
 19 |             assertEqual(v1v, v2v, path: path + [index.description], file: file, line: line)
 20 |         }
 21 | 
 22 |     case let (v1, v2) as ([String: Any], [String: Any]):
 23 |         XCTAssertEqual(
 24 |             v1.keys.sorted(), v2.keys.sorted(),
 25 |             "\(String(describing: v1.keys.sorted())) and \(String(describing: v2.keys.sorted())) not equal at \(path)",
 26 |             file: file, line: line)
 27 | 
 28 |         for (k, v1v) in v1 {
 29 |             if let v2v = v2[k] {
 30 |                 assertEqual(v1v, v2v, path: path + [k], file: file, line: line)
 31 |             } else {
 32 |                 XCTFail("Missing value for \(k) at \(path)", file: file, line: line)
 33 |             }
 34 |         }
 35 |     default:
 36 |         XCTFail(
 37 |             "Unable to compare \(String(describing: v1)) and \(String(describing: v2)) at \(path)",
 38 |             file: file, line: line)
 39 |     }
 40 | }
 41 | 
 42 | public class UserInputTests: XCTestCase {
 43 | 
 44 |     public func testStandardConversion() {
 45 |         let chat: [Chat.Message] = [
 46 |             .system("You are a useful agent."),
 47 |             .user("Tell me a story."),
 48 |         ]
 49 | 
 50 |         let messages = DefaultMessageGenerator().generate(messages: chat)
 51 | 
 52 |         let expected = [
 53 |             [
 54 |                 "role": "system",
 55 |                 "content": "You are a useful agent.",
 56 |             ],
 57 |             [
 58 |                 "role": "user",
 59 |                 "content": "Tell me a story.",
 60 |             ],
 61 |         ]
 62 | 
 63 |         XCTAssertEqual(expected, messages as? [[String: String]])
 64 |     }
 65 | 
 66 |     public func testQwen2ConversionText() {
 67 |         let chat: [Chat.Message] = [
 68 |             .system("You are a useful agent."),
 69 |             .user("Tell me a story."),
 70 |         ]
 71 | 
 72 |         let messages = Qwen2VLMessageGenerator().generate(messages: chat)
 73 | 
 74 |         let expected = [
 75 |             [
 76 |                 "role": "system",
 77 |                 "content": [
 78 |                     [
 79 |                         "type": "text",
 80 |                         "text": "You are a useful agent.",
 81 |                     ]
 82 |                 ],
 83 |             ],
 84 |             [
 85 |                 "role": "user",
 86 |                 "content": [
 87 |                     [
 88 |                         "type": "text",
 89 |                         "text": "Tell me a story.",
 90 |                     ]
 91 |                 ],
 92 |             ],
 93 |         ]
 94 | 
 95 |         assertEqual(expected, messages)
 96 |     }
 97 | 
 98 |     public func testQwen2ConversionImage() {
 99 |         let chat: [Chat.Message] = [
100 |             .system("You are a useful agent."),
101 |             .user(
102 |                 "What is this?",
103 |                 images: [
104 |                     .url(
105 |                         URL(
106 |                             string: "https://opensource.apple.com/images/projects/mlx.f5c59d8b.png")!
107 |                     )
108 |                 ]),
109 |         ]
110 | 
111 |         let messages = Qwen2VLMessageGenerator().generate(messages: chat)
112 | 
113 |         let expected = [
114 |             [
115 |                 "role": "system",
116 |                 "content": [
117 |                     [
118 |                         "type": "text",
119 |                         "text": "You are a useful agent.",
120 |                     ]
121 |                 ],
122 |             ],
123 |             [
124 |                 "role": "user",
125 |                 "content": [
126 |                     [
127 |                         "type": "text",
128 |                         "text": "What is this?",
129 |                     ],
130 |                     [
131 |                         "type": "image"
132 |                     ],
133 |                 ],
134 |             ],
135 |         ]
136 | 
137 |         assertEqual(expected, messages)
138 | 
139 |         let userInput = UserInput(chat: chat)
140 |         XCTAssertEqual(userInput.images.count, 1)
141 |     }
142 | 
143 | }
144 | 


--------------------------------------------------------------------------------
/Tests/mlx-libraries-Package.xctestplan:
--------------------------------------------------------------------------------
 1 | {
 2 |   "configurations" : [
 3 |     {
 4 |       "id" : "CCE21325-5E68-419C-8EB1-B868EF7688F7",
 5 |       "name" : "Test Scheme Action",
 6 |       "options" : {
 7 | 
 8 |       }
 9 |     }
10 |   ],
11 |   "defaultOptions" : {
12 | 
13 |   },
14 |   "testTargets" : [
15 |     {
16 |       "target" : {
17 |         "containerPath" : "container:mlx-swift-examples.xcodeproj",
18 |         "identifier" : "C3208E6D2DB19451006AE6CA",
19 |         "name" : "MLXLMTests"
20 |       }
21 |     }
22 |   ],
23 |   "version" : 1
24 | }
25 | 


--------------------------------------------------------------------------------
/Tools/ExampleLLM/README.md:
--------------------------------------------------------------------------------
 1 | #  ExampleLLM
 2 | 
 3 | An example that uses the simplified APIs to load and evaluate an LLM in only a few lines of
 4 | code:
 5 | 
 6 | ```swift
 7 | let model = try await loadModel(id: "mlx-community/Qwen3-4B-4bit")
 8 | let session = ChatSession(model)
 9 | print(try await session.respond(to: "What are two things to see in San Francisco?")
10 | print(try await session.respond(to: "How about a great place to eat?")
11 | ```
12 | 
13 | See various READMEs:
14 | 
15 | - [MLXLMCommon](../../Libraries/MLXLMCommon/README.md) -- common LM code
16 | - [MLXLLM](../../Libraries/MLXLLM/README.md) -- large language models
17 | - [MLXVLM](../../Libraries/MLXVLM/README.md) -- vision language models
18 | 
19 | ### Building
20 | 
21 | Build the `ExampleLLM` scheme in Xcode.
22 | 
23 | ### Running: Xcode
24 | 
25 | Just press cmd-r to run!
26 | 
27 | ### Running: Command Line
28 | 
29 | Use the `mlx-run` script to run the command line tools:
30 | 
31 | ```
32 | ./mlx-run ExampleLLM
33 | ```
34 | 
35 | Note: `mlx-run` is a shell script that uses `xcode` command line tools to
36 | locate the built binaries.  It is equivalent to running from Xcode itself.
37 | 
38 | By default this will find and run the tools built in _Release_ configuration.  Specify `--debug`
39 | to find and run the tool built in _Debug_ configuration.
40 | 
41 | 


--------------------------------------------------------------------------------
/Tools/ExampleLLM/main.swift:
--------------------------------------------------------------------------------
 1 | import MLXLMCommon
 2 | 
 3 | let model = try await loadModel(id: "mlx-community/Qwen3-4B-4bit")
 4 | 
 5 | let prompt = "What are three things to see in Paris?"
 6 | 
 7 | // MARK: - one-shot print out full response
 8 | print(
 9 |     """
10 |     ================
11 |     \(prompt)
12 | 
13 |     """)
14 | print(try await ChatSession(model).respond(to: prompt))
15 | 
16 | // MARK: - one-shot streaming output
17 | print(
18 |     """
19 |     ================
20 |     \(prompt)
21 | 
22 |     """)
23 | for try await item in ChatSession(model).streamResponse(to: prompt) {
24 |     print(item, terminator: "")
25 | }
26 | print()
27 | 
28 | // MARK: - conversation with follow-on questions
29 | let session = ChatSession(model)
30 | 
31 | let questions = [
32 |     "What are two things to see in San Francisco?",
33 |     "How about a great place to eat?",
34 |     "What city are we talking about?  I forgot!",
35 | ]
36 | 
37 | for question in questions {
38 |     print(
39 |         """
40 |         ================
41 |         \(question)
42 | 
43 |         """)
44 | 
45 |     for try await item in session.streamResponse(to: question) {
46 |         print(item, terminator: "")
47 |     }
48 |     print()
49 | }
50 | 


--------------------------------------------------------------------------------
/Tools/LinearModelTraining/LinearModelTraining.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import ArgumentParser
  4 | import Foundation
  5 | import MLX
  6 | import MLXNN
  7 | import MLXOptimizers
  8 | 
  9 | #if swift(>=5.10)
 10 |     extension MLX.DeviceType: @retroactive ExpressibleByArgument {
 11 |         public init?(argument: String) {
 12 |             self.init(rawValue: argument)
 13 |         }
 14 |     }
 15 | #else
 16 |     extension MLX.DeviceType: ExpressibleByArgument {
 17 |         public init?(argument: String) {
 18 |             self.init(rawValue: argument)
 19 |         }
 20 |     }
 21 | #endif
 22 | 
 23 | @main
 24 | struct Train: AsyncParsableCommand {
 25 | 
 26 |     @Option var epochs = 20
 27 |     @Option var batchSize = 8
 28 | 
 29 |     @Option var m: Float = 0.25
 30 |     @Option var b: Float = 7
 31 | 
 32 |     @Flag var compile = false
 33 | 
 34 |     @Option var device = DeviceType.cpu
 35 | 
 36 |     func run() async throws {
 37 |         Device.setDefault(device: Device(device))
 38 | 
 39 |         // A very simple model that implements the equation
 40 |         // for a linear function: y = mx + b. This can be trained
 41 |         // to match data – in this case, an unknown (to the model)
 42 |         // linear function.
 43 |         //
 44 |         // This is a nice example because most people know how
 45 |         // linear functions work and we can see how the slope
 46 |         // and intercept converge.
 47 |         class LinearFunctionModel: Module, UnaryLayer {
 48 |             let m = MLXRandom.uniform(low: -5.0, high: 5.0)
 49 |             let b = MLXRandom.uniform(low: -5.0, high: 5.0)
 50 | 
 51 |             func callAsFunction(_ x: MLXArray) -> MLXArray {
 52 |                 m * x + b
 53 |             }
 54 |         }
 55 | 
 56 |         // Measure the distance from the prediction (model(x)) and the
 57 |         // ground truth (y). This gives feedback on how close the
 58 |         // prediction is from matching the truth.
 59 |         func loss(model: LinearFunctionModel, x: MLXArray, y: MLXArray) -> MLXArray {
 60 |             mseLoss(predictions: model(x), targets: y, reduction: .mean)
 61 |         }
 62 | 
 63 |         let model = LinearFunctionModel()
 64 |         eval(model.parameters())
 65 | 
 66 |         let lg = valueAndGrad(model: model, loss)
 67 | 
 68 |         // The optimizer will use the gradients update the model parameters
 69 |         let optimizer = SGD(learningRate: 1e-1)
 70 | 
 71 |         // The function to train our model against. It doesn't have
 72 |         // to be linear, but matching what the model models is easy
 73 |         // to understand.
 74 |         func f(_ x: MLXArray) -> MLXArray {
 75 |             // These are the target parameters
 76 |             let m = self.m
 77 |             let b = self.b
 78 | 
 79 |             // Our actual function
 80 |             return m * x + b
 81 |         }
 82 | 
 83 |         func step(_ x: MLXArray, _ y: MLXArray) -> MLXArray {
 84 |             let (loss, grads) = lg(model, x, y)
 85 |             optimizer.update(model: model, gradients: grads)
 86 |             return loss
 87 |         }
 88 | 
 89 |         let resolvedStep =
 90 |             self.compile
 91 |             ? MLX.compile(inputs: [model, optimizer], outputs: [model, optimizer], step) : step
 92 | 
 93 |         for _ in 0 ..< epochs {
 94 |             // We expect that the parameters will approach the targets
 95 |             print("target: b = \(b), m = \(m)")
 96 |             print("parameters: \(model.parameters())")
 97 | 
 98 |             // Generate random training data along with the ground truth.
 99 |             // Notice that the shape is [B, 1] where B is the batch
100 |             // dimension. This allows us to train on several samples simultaneously.
101 |             //
102 |             // Note: A very large batch size will take longer to converge because
103 |             // the gradient will be representing too many samples down into
104 |             // a single float parameter.
105 |             let x = MLXRandom.uniform(low: -5.0, high: 5.0, [batchSize, 1])
106 |             let y = f(x)
107 |             eval(x, y)
108 | 
109 |             // Compute the loss and gradients. Use the optimizer
110 |             // to adjust the parameters closer to the target.
111 |             let loss = resolvedStep(x, y)
112 | 
113 |             eval(model, optimizer)
114 | 
115 |             // We should see this converge toward 0
116 |             print("loss: \(loss)")
117 |         }
118 | 
119 |     }
120 | }
121 | 


--------------------------------------------------------------------------------
/Tools/LinearModelTraining/README.md:
--------------------------------------------------------------------------------
 1 | #  LinearModelTraining
 2 | 
 3 | A command line tool that creates a Model that represents:
 4 | 
 5 |     f(x) = mx + b
 6 |     
 7 | and trains it against an unknown linear function. Very
 8 | simple but illustrates:
 9 | 
10 | - a very simple model with parameters
11 | - a loss function
12 | - the gradient
13 | - use of an optimizers
14 | - the training loop
15 | 


--------------------------------------------------------------------------------
/Tools/Tutorial/Tutorial.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import Foundation
  4 | import MLX
  5 | 
  6 | /// mlx-swift tutorial based on:
  7 | /// https://github.com/ml-explore/mlx/blob/main/examples/cpp/tutorial.cpp
  8 | @main
  9 | struct Tutorial {
 10 | 
 11 |     static func scalarBasics() {
 12 |         // create a scalar array
 13 |         let x = MLXArray(1.0)
 14 | 
 15 |         // the datatype is .float32
 16 |         let dtype = x.dtype
 17 |         assert(dtype == .float32)
 18 | 
 19 |         // get the value
 20 |         let s = x.item(Float.self)
 21 |         assert(s == 1.0)
 22 | 
 23 |         // reading the value with a different type is a fatal error
 24 |         // let i = x.item(Int.self)
 25 | 
 26 |         // scalars have a size of 1
 27 |         let size = x.size
 28 |         assert(size == 1)
 29 | 
 30 |         // scalars have 0 dimensions
 31 |         let ndim = x.ndim
 32 |         assert(ndim == 0)
 33 | 
 34 |         // scalar shapes are empty arrays
 35 |         let shape = x.shape
 36 |         assert(shape == [])
 37 |     }
 38 | 
 39 |     static func arrayBasics() {
 40 |         // make a multidimensional array.
 41 |         //
 42 |         // Note: the argument is a [Double] array literal, which is not
 43 |         // a supported type, but we can explicitly convert it to [Float]
 44 |         // when we create the MLXArray.
 45 |         let x = MLXArray(converting: [1.0, 2.0, 3.0, 4.0], [2, 2])
 46 | 
 47 |         // mlx is row-major by default so the first row of this array
 48 |         // is [1.0, 2.0] and the second row is [3.0, 4.0]
 49 |         print(x[0])
 50 |         print(x[1])
 51 | 
 52 |         // make an array of shape [2, 2] filled with ones
 53 |         let y = MLXArray.ones([2, 2])
 54 | 
 55 |         // pointwise add x and y
 56 |         let z = x + y
 57 | 
 58 |         // mlx is lazy by default. At this point `z` only
 59 |         // has a shape and a type but no actual data
 60 |         assert(z.dtype == .float32)
 61 |         assert(z.shape == [2, 2])
 62 | 
 63 |         // To actually run the computation you must evaluate `z`.
 64 |         // Under the hood, mlx records operations in a graph.
 65 |         // The variable `z` is a node in the graph which points to its operation
 66 |         // and inputs. When `eval` is called on an array (or arrays), the array and
 67 |         // all of its dependencies are recursively evaluated to produce the result.
 68 |         // Once an array is evaluated, it has data and is detached from its inputs.
 69 | 
 70 |         // Note: this is being called for demonstration purposes -- all reads
 71 |         // ensure the array is evaluated.
 72 |         z.eval()
 73 | 
 74 |         // this implicitly evaluates z before converting to a description
 75 |         print(z)
 76 |     }
 77 | 
 78 |     static func automaticDifferentiation() {
 79 |         func fn(_ x: MLXArray) -> MLXArray {
 80 |             x.square()
 81 |         }
 82 | 
 83 |         let gradFn = grad(fn)
 84 | 
 85 |         let x = MLXArray(1.5)
 86 |         let dfdx = gradFn(x)
 87 |         print(dfdx)
 88 | 
 89 |         assert(dfdx.item() == Float(2 * 1.5))
 90 | 
 91 |         let df2dx2 = grad(grad(fn))(x)
 92 |         print(df2dx2)
 93 | 
 94 |         assert(df2dx2.item() == Float(2))
 95 |     }
 96 | 
 97 |     static func main() {
 98 |         scalarBasics()
 99 |         arrayBasics()
100 |         automaticDifferentiation()
101 |     }
102 | }
103 | 


--------------------------------------------------------------------------------
/Tools/image-tool/Arguments.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import ArgumentParser
  4 | import Foundation
  5 | import MLX
  6 | 
  7 | #if swift(>=5.10)
  8 |     /// Extension to allow URL command line arguments.
  9 |     extension URL: @retroactive ExpressibleByArgument {
 10 |         public init?(argument: String) {
 11 |             if argument.contains("://") {
 12 |                 self.init(string: argument)
 13 |             } else {
 14 |                 self.init(filePath: argument)
 15 |             }
 16 |         }
 17 |     }
 18 | #else
 19 |     /// Extension to allow URL command line arguments.
 20 |     extension URL: ExpressibleByArgument {
 21 |         public init?(argument: String) {
 22 |             if argument.contains("://") {
 23 |                 self.init(string: argument)
 24 |             } else {
 25 |                 self.init(filePath: argument)
 26 |             }
 27 |         }
 28 |     }
 29 | #endif
 30 | 
 31 | /// Argument package for adjusting and reporting memory use.
 32 | struct MemoryArguments: ParsableArguments, Sendable {
 33 | 
 34 |     @Flag(name: .long, help: "Show memory stats")
 35 |     var memoryStats = false
 36 | 
 37 |     @Option(name: .long, help: "Maximum cache size in M")
 38 |     var cacheSize = 1024
 39 | 
 40 |     @Option(name: .long, help: "Maximum memory size in M")
 41 |     var memorySize: Int?
 42 | 
 43 |     var startMemory: GPU.Snapshot?
 44 | 
 45 |     mutating func start<L>(_ load: () async throws -> L) async throws -> L {
 46 |         GPU.set(cacheLimit: cacheSize * 1024 * 1024)
 47 | 
 48 |         if let memorySize {
 49 |             GPU.set(memoryLimit: memorySize * 1024 * 1024)
 50 |         }
 51 | 
 52 |         let result = try await load()
 53 |         startMemory = GPU.snapshot()
 54 | 
 55 |         return result
 56 |     }
 57 | 
 58 |     mutating func start() {
 59 |         GPU.set(cacheLimit: cacheSize * 1024 * 1024)
 60 | 
 61 |         if let memorySize {
 62 |             GPU.set(memoryLimit: memorySize * 1024 * 1024)
 63 |         }
 64 | 
 65 |         startMemory = GPU.snapshot()
 66 |     }
 67 | 
 68 |     func reportCurrent() {
 69 |         if memoryStats {
 70 |             let memory = GPU.snapshot()
 71 |             print(memory.description)
 72 |         }
 73 |     }
 74 | 
 75 |     func reportMemoryStatistics() {
 76 |         if memoryStats, let startMemory {
 77 |             let endMemory = GPU.snapshot()
 78 | 
 79 |             print("=======")
 80 |             print("Memory size: \(GPU.memoryLimit / 1024)K")
 81 |             print("Cache size:  \(GPU.cacheLimit / 1024)K")
 82 | 
 83 |             print("")
 84 |             print("=======")
 85 |             print("Starting memory")
 86 |             print(startMemory.description)
 87 | 
 88 |             print("")
 89 |             print("=======")
 90 |             print("Ending memory")
 91 |             print(endMemory.description)
 92 | 
 93 |             print("")
 94 |             print("=======")
 95 |             print("Growth")
 96 |             print(startMemory.delta(endMemory).description)
 97 | 
 98 |         }
 99 |     }
100 | }
101 | 


--------------------------------------------------------------------------------
/Tools/image-tool/ImageTool.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import ArgumentParser
  4 | import Foundation
  5 | import MLX
  6 | import Progress
  7 | import StableDiffusion
  8 | 
  9 | @main
 10 | struct ImageTool: AsyncParsableCommand {
 11 |     static let configuration = CommandConfiguration(
 12 |         abstract: "Command line tool for working with images and MLX",
 13 |         subcommands: [StableDiffusionTool.self])
 14 | }
 15 | 
 16 | struct StableDiffusionTool: AsyncParsableCommand {
 17 | 
 18 |     static let configuration = CommandConfiguration(
 19 |         commandName: "sd",
 20 |         abstract: "Stable diffusion related commands",
 21 |         subcommands: [TextToImageCommand.self, ImageToImageCommand.self]
 22 |     )
 23 | }
 24 | 
 25 | #if swift(>=5.10)
 26 |     extension StableDiffusionConfiguration.Preset: @retroactive ExpressibleByArgument {}
 27 | #else
 28 |     extension StableDiffusionConfiguration.Preset: ExpressibleByArgument {}
 29 | #endif
 30 | 
 31 | struct ModelArguments: ParsableArguments, Sendable {
 32 | 
 33 |     @Option(name: .long, help: "stable diffusion model")
 34 |     var model: StableDiffusionConfiguration.Preset = .sdxlTurbo
 35 | 
 36 |     @Flag(name: .long, inversion: .prefixedNo, help: "Disable float16 conversion")
 37 |     var float16 = true
 38 | 
 39 |     @Flag(name: .long, help: "Enable quantization")
 40 |     var quantize = false
 41 | 
 42 |     var loadConfiguration: LoadConfiguration {
 43 |         LoadConfiguration(float16: float16, quantize: quantize)
 44 |     }
 45 | 
 46 |     func download() async throws -> StableDiffusionConfiguration {
 47 |         let configuration = model.configuration
 48 | 
 49 |         var progressBar: ProgressBar?
 50 |         try await configuration.download { progress in
 51 |             if progressBar == nil {
 52 |                 let complete = progress.fractionCompleted
 53 |                 if complete < 0.99 {
 54 |                     progressBar = ProgressBar(count: 1000)
 55 |                     if complete > 0 {
 56 |                         print("Resuming download (\(Int(complete * 100))% complete)")
 57 |                     } else {
 58 |                         print("Downloading")
 59 |                     }
 60 |                     print()
 61 |                 }
 62 |             }
 63 | 
 64 |             let complete = Int(progress.fractionCompleted * 1000)
 65 |             progressBar?.setValue(complete)
 66 |         }
 67 | 
 68 |         return configuration
 69 |     }
 70 | }
 71 | 
 72 | /// Command line arguments for controlling generation of images
 73 | struct GenerateArguments: ParsableArguments, Sendable {
 74 | 
 75 |     @Option(name: .shortAndLong, help: "The message to be processed by the model")
 76 |     var prompt = "purple cow on the moon"
 77 | 
 78 |     @Option(name: .shortAndLong, help: "Negative prompt (requires cfg to be > 1)")
 79 |     var negativePrompt = ""
 80 | 
 81 |     @Option(name: .long, help: "cfg weight")
 82 |     var cfg: Float?
 83 | 
 84 |     @Option(name: .long, help: "number of images")
 85 |     var imageCount: Int = 1
 86 | 
 87 |     @Option(name: .long, help: "decoding batch size")
 88 |     var batchSize: Int = 1
 89 | 
 90 |     @Option(name: .long, help: "latent width (output size is 8x this value)")
 91 |     var latentWidth: Int = 64
 92 | 
 93 |     @Option(name: .long, help: "latent height (output size is 8x this value)")
 94 |     var latentHeight: Int = 64
 95 | 
 96 |     @Option(name: .long, help: "number of rows of images in the output")
 97 |     var rows: Int = 1
 98 | 
 99 |     @Option(name: .long, help: "number of steps")
100 |     var steps: Int?
101 | 
102 |     @Option(name: .long, help: "The PRNG seed")
103 |     var seed: UInt64?
104 | 
105 |     func evaluateParameters(configuration: StableDiffusionConfiguration) -> EvaluateParameters {
106 |         var parameters = configuration.defaultParameters()
107 |         parameters.prompt = prompt
108 |         parameters.negativePrompt = negativePrompt
109 |         if let cfg {
110 |             parameters.cfgWeight = cfg
111 |         }
112 |         parameters.imageCount = imageCount
113 |         parameters.decodingBatchSize = batchSize
114 |         parameters.latentSize = [latentHeight, latentWidth]
115 |         if let steps {
116 |             parameters.steps = steps
117 |         }
118 |         if let seed {
119 |             parameters.seed = seed
120 |         }
121 |         print("using seed: \(parameters.seed)")
122 |         return parameters
123 |     }
124 | 
125 | }
126 | 
127 | func makeGrid(images: [MLXArray], rows: Int) -> MLXArray {
128 |     var x = concatenated(images, axis: 0)
129 |     x = padded(x, widths: [[0, 0], [8, 8], [8, 8], [0, 0]])
130 |     let (B, H, W, C) = x.shape4
131 |     x = x.reshaped(rows, B / rows, H, W, C).transposed(0, 2, 1, 3, 4)
132 |     x = x.reshaped(rows * H, B / rows * W, C)
133 |     x = (x * 255).asType(.uint8)
134 |     return x
135 | }
136 | 
137 | struct TextToImageCommand: AsyncParsableCommand {
138 | 
139 |     static let configuration = CommandConfiguration(
140 |         commandName: "text",
141 |         abstract: "Text to image command"
142 |     )
143 | 
144 |     @Option(name: .long, help: "output image")
145 |     var output = URL(filePath: "/tmp/out.png")
146 | 
147 |     @OptionGroup var model: ModelArguments
148 |     @OptionGroup var memory: MemoryArguments
149 |     @OptionGroup var generate: GenerateArguments
150 | 
151 |     mutating func generateLatents(configuration: StableDiffusionConfiguration) throws -> (
152 |         EvaluateParameters, ImageDecoder, MLXArray
153 |     ) {
154 |         // Download and prepare the model
155 |         guard
156 |             let generator = try configuration.textToImageGenerator(
157 |                 configuration: model.loadConfiguration)
158 |         else {
159 |             fatalError("Unable to produce TextToImageGenerator from \(configuration.id)")
160 |         }
161 | 
162 |         generator.ensureLoaded()
163 |         memory.start()
164 | 
165 |         // Generate the latents. These are the iterations for generating
166 |         // the output image. This is just generating the evaluation graph.
167 |         let parameters = generate.evaluateParameters(configuration: configuration)
168 |         let latents = generator.generateLatents(parameters: parameters)
169 | 
170 |         // Evaluate the latents (evalue the graph) and keep the last value generated
171 |         var lastXt: MLXArray!
172 |         for xt in Progress(latents) {
173 |             eval(xt)
174 |             lastXt = xt
175 |         }
176 | 
177 |         return (parameters, generator.detachedDecoder(), lastXt)
178 |     }
179 | 
180 |     @MainActor
181 |     mutating func run() async throws {
182 |         let configuration = try await model.download()
183 | 
184 |         let (parameters, decoder, xt) = try generateLatents(configuration: configuration)
185 | 
186 |         var decoded = [MLXArray]()
187 |         for i in Progress(
188 |             stride(from: 0, to: parameters.imageCount, by: parameters.decodingBatchSize))
189 |         {
190 |             let image = decoder(xt[i ..< i + parameters.decodingBatchSize])
191 |             eval(image)
192 |             decoded.append(image)
193 |         }
194 | 
195 |         let grid = makeGrid(images: decoded, rows: generate.rows)
196 |         try Image(grid).save(url: output)
197 |     }
198 | }
199 | 
200 | struct ImageToImageCommand: AsyncParsableCommand {
201 | 
202 |     static let configuration = CommandConfiguration(
203 |         commandName: "image",
204 |         abstract: "Image to image command"
205 |     )
206 | 
207 |     @Option(name: .long, help: "input image")
208 |     var input: URL
209 | 
210 |     @Option(name: .long, help: "maximum edge of the input image -- scale to fit this size")
211 |     var maxEdge: Int = 1024
212 | 
213 |     @Option(name: .long, help: "output image")
214 |     var output = URL(filePath: "/tmp/out.png")
215 | 
216 |     @Option(name: .long, help: "noise strength")
217 |     var strength: Float = 0.9
218 | 
219 |     @OptionGroup var model: ModelArguments
220 |     @OptionGroup var memory: MemoryArguments
221 |     @OptionGroup var generate: GenerateArguments
222 | 
223 |     mutating func generateLatents(configuration: StableDiffusionConfiguration) throws -> (
224 |         EvaluateParameters, ImageDecoder, MLXArray
225 |     ) {
226 | 
227 |         let image = try Image(url: self.input, maximumEdge: maxEdge)
228 |         let input = (image.data.asType(.float32) / 255) * 2 - 1
229 | 
230 |         guard
231 |             let generator = try configuration.imageToImageGenerator(
232 |                 configuration: model.loadConfiguration)
233 |         else {
234 |             fatalError("Unable to produce TextToImageGenerator from \(configuration.id)")
235 |         }
236 | 
237 |         generator.ensureLoaded()
238 |         memory.start()
239 | 
240 |         // Adjust the steps based on the strength
241 |         if Int(Float(generate.evaluateParameters(configuration: configuration).steps) * strength)
242 |             < 1
243 |         {
244 |             generate.steps = Int(ceil(1 / strength))
245 |         }
246 | 
247 |         let parameters = generate.evaluateParameters(configuration: configuration)
248 |         let latents = generator.generateLatents(
249 |             image: input, parameters: parameters, strength: strength)
250 | 
251 |         var lastXt: MLXArray!
252 |         for xt in Progress(latents) {
253 |             eval(xt)
254 |             lastXt = xt
255 |         }
256 | 
257 |         return (parameters, generator.detachedDecoder(), lastXt)
258 |     }
259 | 
260 |     @MainActor
261 |     mutating func run() async throws {
262 |         let configuration = try await model.download()
263 | 
264 |         let (parameters, decoder, xt) = try generateLatents(configuration: configuration)
265 | 
266 |         var decoded = [MLXArray]()
267 |         for i in Progress(
268 |             stride(from: 0, to: parameters.imageCount, by: parameters.decodingBatchSize))
269 |         {
270 |             let image = decoder(xt[i ..< i + parameters.decodingBatchSize])
271 |             eval(image)
272 |             decoded.append(image)
273 |         }
274 | 
275 |         let grid = makeGrid(images: decoded, rows: generate.rows)
276 |         try Image(grid).save(url: output)
277 |     }
278 | }
279 | 


--------------------------------------------------------------------------------
/Tools/llm-tool/Arguments.swift:
--------------------------------------------------------------------------------
 1 | // Copyright © 2024 Apple Inc.
 2 | 
 3 | import ArgumentParser
 4 | import Foundation
 5 | 
 6 | /// Extension to allow URL command line arguments.
 7 | #if swift(>=5.10)
 8 |     extension URL: @retroactive ExpressibleByArgument {
 9 |         public init?(argument: String) {
10 |             if argument.contains("://") {
11 |                 self.init(string: argument)
12 |             } else {
13 |                 self.init(filePath: argument)
14 |             }
15 |         }
16 |     }
17 | #else
18 |     extension URL: ExpressibleByArgument {
19 |         public init?(argument: String) {
20 |             if argument.contains("://") {
21 |                 self.init(string: argument)
22 |             } else {
23 |                 self.init(filePath: argument)
24 |             }
25 |         }
26 |     }
27 | #endif
28 | 


--------------------------------------------------------------------------------
/Tools/llm-tool/Chat.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2025 Apple Inc.
  2 | 
  3 | import ArgumentParser
  4 | import Foundation
  5 | import MLX
  6 | import MLXLLM
  7 | import MLXLMCommon
  8 | import MLXVLM
  9 | 
 10 | struct ChatCommand: AsyncParsableCommand {
 11 |     static let configuration = CommandConfiguration(
 12 |         commandName: "chat",
 13 |         abstract: "interactive chat with model"
 14 |     )
 15 | 
 16 |     @OptionGroup var args: ModelArguments
 17 |     @OptionGroup var memory: MemoryArguments
 18 |     @OptionGroup var generate: GenerateArguments
 19 |     @OptionGroup var media: MediaArguments
 20 | 
 21 |     struct State {
 22 |         var parameters: GenerateParameters
 23 |         var processing: UserInput.Processing
 24 | 
 25 |         var images: [UserInput.Image]
 26 |         var videos: [UserInput.Video]
 27 | 
 28 |         var chat: [Chat.Message]
 29 | 
 30 |         var cache: [KVCache]
 31 | 
 32 |         var printStats = false
 33 |     }
 34 | 
 35 |     @MainActor
 36 |     mutating func run() async throws {
 37 |         let defaultModel = MLXLLM.LLMRegistry.mistral7B4bit
 38 | 
 39 |         // Load the model
 40 |         let modelContainer = try await memory.start { [args] in
 41 |             do {
 42 |                 return try await args.load(
 43 |                     defaultModel: defaultModel.name, modelFactory: LLMModelFactory.shared)
 44 |             } catch ModelFactoryError.unsupportedModelType {
 45 |                 return try await args.load(
 46 |                     defaultModel: defaultModel.name, modelFactory: VLMModelFactory.shared)
 47 |             }
 48 |         }
 49 | 
 50 |         // update the context/configuration with any command line parameters
 51 |         await modelContainer.update { [generate] context in
 52 |             generate.prepare(&context)
 53 |         }
 54 | 
 55 |         try await chat(modelContainer: modelContainer)
 56 |     }
 57 | 
 58 |     func chat(modelContainer: ModelContainer) async throws {
 59 |         try await modelContainer.perform { context in
 60 |             let parameters = generate.generateParameters
 61 |             let initialState = State(
 62 |                 parameters: parameters,
 63 |                 processing: media.processing,
 64 |                 images: media.images, videos: media.videos,
 65 |                 chat: [.system(generate.system)],
 66 |                 cache: context.model.newCache(parameters: parameters))
 67 | 
 68 |             var state = initialState
 69 | 
 70 |             print("> ", terminator: "")
 71 |             while let line = readLine() {
 72 |                 if line.hasPrefix("/") {
 73 |                     // handle commands
 74 |                     switch command(line: line, state: &state) {
 75 |                     case .exit:
 76 |                         return
 77 |                     case .reset:
 78 |                         state = initialState
 79 |                         state.cache = context.model.newCache(parameters: parameters)
 80 |                         continue
 81 |                     case .inference:
 82 |                         // continue and run inference
 83 |                         break
 84 |                     case .handled:
 85 |                         print("\n\n> ", terminator: "")
 86 |                         continue
 87 |                     }
 88 |                 } else {
 89 |                     // chat input
 90 |                     state.chat.append(.user(line, images: state.images, videos: state.videos))
 91 |                 }
 92 | 
 93 |                 // consume the media, if any
 94 |                 state.images.removeAll()
 95 |                 state.videos.removeAll()
 96 | 
 97 |                 // convert UserInput to LMInput
 98 |                 let userInput = UserInput(chat: state.chat, processing: state.processing)
 99 |                 let input = try await context.processor.prepare(input: userInput)
100 | 
101 |                 // generate the output
102 |                 var output = ""
103 |                 var result: GenerateCompletionInfo?
104 |                 for await item in try MLXLMCommon.generate(
105 |                     input: input, cache: state.cache, parameters: parameters, context: context
106 |                 ) {
107 |                     switch item {
108 |                     case .chunk(let string):
109 |                         output += string
110 |                         print(string, terminator: "")
111 |                     case .info(let info):
112 |                         result = info
113 |                     case .toolCall:
114 |                         break
115 |                     }
116 |                 }
117 | 
118 |                 // the kvcache now contains this context
119 |                 state.chat.removeAll()
120 | 
121 |                 if state.printStats, let result {
122 |                     print(
123 |                         "\ntime to first token: \(result.promptTime.formatted()) tps: \(result.tokensPerSecond.formatted())"
124 |                     )
125 |                 }
126 |                 print("\n\n> ", terminator: "")
127 |             }
128 |         }
129 |     }
130 | 
131 |     enum CommandDisposition {
132 |         case exit
133 |         case reset
134 |         case inference
135 |         case handled
136 |     }
137 | 
138 |     func help() {
139 |         print(
140 |             """
141 |             /help -- this message
142 |             /quit -- terminate the chat
143 |             /memory -- print memory stats
144 |             /stats -- toggle token stats
145 |             /reset -- reset the chat session to initial state
146 |             /image [pathOrURL] -- provide an image
147 |             /video [pathOrURL] -- provide a video
148 |             /again -- rerun inference for last response
149 |             /parameters -- print generation parametes
150 |             /temperature [number] -- set the sampling temperature
151 |             /topP [number] -- set the top p sampling
152 |             /maxTokens [number] -- set the maximum number of tokens to generate or no number to remove limit
153 |             """)
154 |     }
155 | 
156 |     func command(line: String, state: inout State) -> CommandDisposition {
157 |         let command = line.split(separator: " ")[0]
158 |         let rest = String(
159 |             line.dropFirst(command.count).trimmingCharacters(in: .whitespaces))
160 | 
161 |         func url(_ string: String) -> URL? {
162 |             if string.hasPrefix("/") {
163 |                 URL(filePath: string)
164 |             } else {
165 |                 URL(string: string)
166 |             }
167 |         }
168 | 
169 |         switch command {
170 |         case "/help":
171 |             help()
172 | 
173 |         case "/quit":
174 |             return .exit
175 | 
176 |         case "/memory":
177 |             let memory = GPU.snapshot()
178 |             print("Memory size: \(GPU.memoryLimit / 1024)K")
179 |             print("Cache size:  \(GPU.cacheLimit / 1024)K")
180 |             print(memory.description)
181 | 
182 |         case "/stats":
183 |             state.printStats.toggle()
184 |             print("Token stats: \(state.printStats ? "ON" : "OFF")")
185 | 
186 |         case "/reset":
187 |             return .reset
188 | 
189 |         case "/image":
190 |             if let url = url(rest) {
191 |                 state.images.append(UserInput.Image.url(url))
192 |             }
193 |         case "/video":
194 |             if let url = url(rest) {
195 |                 state.videos.append(UserInput.Video.url(url))
196 |             }
197 | 
198 |         case "/again":
199 |             state.chat.removeLast()
200 |             return .inference
201 | 
202 |         case "/parameters":
203 |             print(state.parameters)
204 |         case "/temperature":
205 |             if let value = Float(rest) {
206 |                 state.parameters.temperature = value
207 |                 print(state.parameters)
208 |             }
209 |         case "/topP":
210 |             if let value = Float(rest) {
211 |                 state.parameters.topP = value
212 |                 print(state.parameters)
213 |             }
214 |         case "/maxTokens":
215 |             state.parameters.maxTokens = Int(rest)
216 |             print(state.parameters)
217 | 
218 |         default:
219 |             help()
220 |         }
221 | 
222 |         return .handled
223 |     }
224 | }
225 | 


--------------------------------------------------------------------------------
/Tools/llm-tool/LLMTool.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import ArgumentParser
  4 | import CoreImage
  5 | import Foundation
  6 | import Hub
  7 | import MLX
  8 | import MLXLLM
  9 | import MLXLMCommon
 10 | import MLXVLM
 11 | import Tokenizers
 12 | 
 13 | @main
 14 | struct LLMTool: AsyncParsableCommand {
 15 |     static let configuration = CommandConfiguration(
 16 |         abstract: "Command line tool for generating text and manipulating LLMs",
 17 |         subcommands: [
 18 |             EvaluateCommand.self, ChatCommand.self, LoRACommand.self,
 19 |             ListCommands.self,
 20 |         ],
 21 |         defaultSubcommand: EvaluateCommand.self)
 22 | }
 23 | 
 24 | /// Command line arguments for loading a model.
 25 | struct ModelArguments: ParsableArguments, Sendable {
 26 | 
 27 |     @Option(name: .long, help: "Name of the Hugging Face model or absolute path to directory")
 28 |     var model: String?
 29 | 
 30 |     @Option(help: "Hub download directory")
 31 |     var download: URL?
 32 | 
 33 |     @Sendable
 34 |     func load(defaultModel: String, modelFactory: ModelFactory) async throws -> ModelContainer {
 35 |         let modelConfiguration: ModelConfiguration
 36 | 
 37 |         let modelName = self.model ?? defaultModel
 38 | 
 39 |         print("Loading \(modelName)...")
 40 | 
 41 |         if modelName.hasPrefix("/") {
 42 |             // path
 43 |             modelConfiguration = ModelConfiguration(directory: URL(filePath: modelName))
 44 |         } else {
 45 |             // identifier
 46 |             modelConfiguration = modelFactory.configuration(id: modelName)
 47 |         }
 48 | 
 49 |         let hub =
 50 |             if let download {
 51 |                 HubApi(downloadBase: download)
 52 |             } else {
 53 |                 HubApi()
 54 |             }
 55 | 
 56 |         return try await modelFactory.loadContainer(hub: hub, configuration: modelConfiguration)
 57 |     }
 58 | }
 59 | 
 60 | struct PromptArguments: ParsableArguments, Sendable {
 61 |     @Option(
 62 |         name: .shortAndLong,
 63 |         help:
 64 |             "The message to be processed by the model. Use @path,@path to load from files, e.g. @/tmp/prompt.txt"
 65 |     )
 66 |     var prompt: String?
 67 | 
 68 |     func resolvePrompt(configuration: ModelConfiguration) throws -> String {
 69 |         let prompt = self.prompt ?? configuration.defaultPrompt
 70 |         if prompt.hasPrefix("@") {
 71 |             let names = prompt.split(separator: ",").map { String($0.dropFirst()) }
 72 |             return try names.map { try String(contentsOfFile: $0) }.joined(separator: "\n")
 73 |         } else {
 74 |             return prompt
 75 |         }
 76 |     }
 77 | }
 78 | 
 79 | /// Argument package for supplying media files
 80 | struct MediaArguments: ParsableArguments, Sendable {
 81 | 
 82 |     @Option(parsing: .upToNextOption, help: "Resize images to this size (width, height)")
 83 |     var resize: [Int] = []
 84 | 
 85 |     @Option(parsing: .upToNextOption, help: "Paths or URLs for input images")
 86 |     var image: [URL] = []
 87 | 
 88 |     @Option(parsing: .upToNextOption, help: "Paths or URLs for input videos")
 89 |     var video: [URL] = []
 90 | 
 91 |     var images: [UserInput.Image] {
 92 |         image.map { UserInput.Image.url($0) }
 93 |     }
 94 |     var videos: [UserInput.Video] {
 95 |         video.map { UserInput.Video.url($0) }
 96 |     }
 97 | 
 98 |     var processing: UserInput.Processing {
 99 |         var processing = UserInput.Processing()
100 |         if !resize.isEmpty {
101 |             let size: CGSize
102 |             if resize.count == 1 {
103 |                 // Single value represents width/height
104 |                 let v = resize[0]
105 |                 size = CGSize(width: v, height: v)
106 |             } else {
107 |                 let v0 = resize[0]
108 |                 let v1 = resize[1]
109 |                 size = CGSize(width: v0, height: v1)
110 |             }
111 |             processing.resize = size
112 |         }
113 |         return processing
114 |     }
115 | }
116 | 
117 | /// Command line arguments for controlling generation of text.
118 | struct GenerateArguments: ParsableArguments, Sendable {
119 | 
120 |     @Option(
121 |         name: .shortAndLong,
122 |         help:
123 |             "The system prompt"
124 |     )
125 |     var system: String = ""
126 | 
127 |     @Option(name: .shortAndLong, help: "Maximum number of tokens to generate")
128 |     var maxTokens = 100
129 | 
130 |     @Option(name: .shortAndLong, help: "The sampling temperature")
131 |     var temperature: Float = 0.6
132 | 
133 |     @Option(name: .long, help: "The top p sampling")
134 |     var topP: Float = 1.0
135 | 
136 |     @Option(name: .long, help: "The penalty factor for repeating tokens")
137 |     var repetitionPenalty: Float?
138 | 
139 |     @Option(name: .long, help: "The number of tokens to consider for repetition penalty")
140 |     var repetitionContextSize: Int = 20
141 | 
142 |     @Option(name: .long, help: "Additional end-of-sequence token to stop generation")
143 |     var extraEosToken: String?
144 | 
145 |     @Option(name: .long, help: "The PRNG seed")
146 |     var seed: UInt64 = 0
147 | 
148 |     @Option(name: .long, help: "Number of bits for KV cache quantization (nil = no quantization)")
149 |     var kvBits: Int?
150 | 
151 |     @Option(name: .long, help: "Group size for KV cache quantization")
152 |     var kvGroupSize: Int = 64
153 | 
154 |     @Option(name: .long, help: "Step to begin using quantized KV cache when kv-bits is set")
155 |     var quantizedKvStart: Int = 0
156 | 
157 |     @Flag(name: .shortAndLong, help: "If true only print the generated output")
158 |     var quiet = false
159 | 
160 |     var generateParameters: GenerateParameters {
161 |         GenerateParameters(
162 |             maxTokens: maxTokens,
163 |             kvBits: kvBits,
164 |             kvGroupSize: kvGroupSize,
165 |             quantizedKVStart: quantizedKvStart,
166 |             temperature: temperature, topP: topP, repetitionPenalty: repetitionPenalty,
167 |             repetitionContextSize: repetitionContextSize)
168 |     }
169 | 
170 |     func prepare(
171 |         _ context: inout ModelContext
172 |     ) {
173 |         if let extraEosToken {
174 |             context.configuration.extraEOSTokens.insert(extraEosToken)
175 |         }
176 |     }
177 | 
178 |     func generate(
179 |         input: LMInput, context: ModelContext
180 |     ) async throws -> (GenerateCompletionInfo, String) {
181 |         var output = ""
182 |         for await item in try MLXLMCommon.generate(
183 |             input: input, parameters: generateParameters, context: context)
184 |         {
185 |             switch item {
186 |             case .chunk(let string):
187 |                 output += string
188 |                 print(string, terminator: "")
189 |             case .info(let info):
190 |                 return (info, output)
191 |             case .toolCall:
192 |                 break
193 |             }
194 |         }
195 |         fatalError("exited loop without seeing .info")
196 |     }
197 | }
198 | 
199 | /// Argument package for adjusting and reporting memory use.
200 | struct MemoryArguments: ParsableArguments, Sendable {
201 | 
202 |     @Flag(name: .long, help: "Show memory stats")
203 |     var memoryStats = false
204 | 
205 |     @Option(name: .long, help: "Maximum cache size in M")
206 |     var cacheSize: Int?
207 | 
208 |     @Option(name: .long, help: "Maximum memory size in M")
209 |     var memorySize: Int?
210 | 
211 |     var startMemory: GPU.Snapshot?
212 | 
213 |     mutating func start<L>(_ load: @Sendable () async throws -> L) async throws -> L {
214 |         if let cacheSize {
215 |             GPU.set(cacheLimit: cacheSize * 1024 * 1024)
216 |         }
217 | 
218 |         if let memorySize {
219 |             GPU.set(memoryLimit: memorySize * 1024 * 1024)
220 |         }
221 | 
222 |         let result = try await load()
223 |         startMemory = GPU.snapshot()
224 | 
225 |         return result
226 |     }
227 | 
228 |     mutating func start() {
229 |         if let cacheSize {
230 |             GPU.set(cacheLimit: cacheSize * 1024 * 1024)
231 |         }
232 | 
233 |         if let memorySize {
234 |             GPU.set(memoryLimit: memorySize * 1024 * 1024)
235 |         }
236 | 
237 |         startMemory = GPU.snapshot()
238 |     }
239 | 
240 |     func reportCurrent() {
241 |         if memoryStats {
242 |             let memory = GPU.snapshot()
243 |             print(memory.description)
244 |         }
245 |     }
246 | 
247 |     func reportMemoryStatistics() {
248 |         if memoryStats, let startMemory {
249 |             let endMemory = GPU.snapshot()
250 | 
251 |             print("=======")
252 |             print("Memory size: \(GPU.memoryLimit / 1024)K")
253 |             print("Cache size:  \(GPU.cacheLimit / 1024)K")
254 | 
255 |             print("")
256 |             print("=======")
257 |             print("Starting memory")
258 |             print(startMemory.description)
259 | 
260 |             print("")
261 |             print("=======")
262 |             print("Ending memory")
263 |             print(endMemory.description)
264 | 
265 |             print("")
266 |             print("=======")
267 |             print("Growth")
268 |             print(startMemory.delta(endMemory).description)
269 | 
270 |         }
271 |     }
272 | }
273 | 
274 | struct EvaluateCommand: AsyncParsableCommand {
275 |     static let configuration = CommandConfiguration(
276 |         commandName: "eval",
277 |         abstract: "evaluate prompt and generate text"
278 |     )
279 | 
280 |     @OptionGroup var args: ModelArguments
281 |     @OptionGroup var memory: MemoryArguments
282 |     @OptionGroup var generate: GenerateArguments
283 |     @OptionGroup var prompt: PromptArguments
284 |     @OptionGroup var media: MediaArguments
285 | 
286 |     private func userInput(modelConfiguration: ModelConfiguration) -> UserInput {
287 |         let prompt =
288 |             (try? self.prompt.resolvePrompt(configuration: modelConfiguration))
289 |             ?? modelConfiguration.defaultPrompt
290 | 
291 |         return UserInput(
292 |             chat: [
293 |                 .system(generate.system),
294 |                 .user(prompt, images: media.images, videos: media.videos),
295 |             ],
296 |             processing: media.processing
297 |         )
298 |     }
299 | 
300 |     @MainActor
301 |     mutating func run() async throws {
302 |         let modelFactory: ModelFactory
303 |         let defaultModel: ModelConfiguration
304 | 
305 |         // Switch between LLM and VLM based on presence of media
306 |         let vlm = !media.image.isEmpty || !media.video.isEmpty
307 |         if vlm {
308 |             modelFactory = VLMModelFactory.shared
309 |             defaultModel = MLXVLM.VLMRegistry.qwen2VL2BInstruct4Bit
310 |         } else {
311 |             modelFactory = LLMModelFactory.shared
312 |             defaultModel = MLXLLM.LLMRegistry.mistral7B4bit
313 |         }
314 | 
315 |         // Load the model
316 |         let modelContainer = try await memory.start { [args] in
317 |             try await args.load(defaultModel: defaultModel.name, modelFactory: modelFactory)
318 |         }
319 | 
320 |         // update the context/configuration with any command line parameters
321 |         await modelContainer.update { [generate] context in
322 |             generate.prepare(&context)
323 |         }
324 | 
325 |         // Get the resolved configuration (this has the default prompt)
326 |         let modelConfiguration = await modelContainer.configuration
327 | 
328 |         if !generate.quiet {
329 |             print("Loaded \(modelConfiguration.name)")
330 |         }
331 | 
332 |         let userInput = self.userInput(modelConfiguration: modelConfiguration)
333 | 
334 |         if !generate.quiet {
335 |             print("Starting generation ...")
336 |             print(userInput.prompt, terminator: " ")
337 |         }
338 | 
339 |         let (result, _) = try await modelContainer.perform { [generate] context in
340 |             let input = try await context.processor.prepare(input: userInput)
341 |             return try await generate.generate(input: input, context: context)
342 |         }
343 | 
344 |         // wait for any asynchronous cleanup, e.g. tearing down compiled functions
345 |         // before the task exits -- this would race with mlx::core shutdown
346 |         try await Task.sleep(for: .milliseconds(30))
347 | 
348 |         if !generate.quiet {
349 |             print("------")
350 |             print(result.summary())
351 | 
352 |             memory.reportMemoryStatistics()
353 |         }
354 |     }
355 | }
356 | 


--------------------------------------------------------------------------------
/Tools/llm-tool/ListCommands.swift:
--------------------------------------------------------------------------------
 1 | // Copyright © 2025 Apple Inc.
 2 | 
 3 | import ArgumentParser
 4 | import Foundation
 5 | import MLXLLM
 6 | import MLXVLM
 7 | 
 8 | struct ListCommands: AsyncParsableCommand {
 9 | 
10 |     static let configuration = CommandConfiguration(
11 |         commandName: "list",
12 |         abstract: "list registered model configurations",
13 |         subcommands: [
14 |             ListLLMCommand.self, ListVLMCommand.self,
15 |         ]
16 |     )
17 | }
18 | 
19 | struct ListLLMCommand: AsyncParsableCommand {
20 | 
21 |     static let configuration = CommandConfiguration(
22 |         commandName: "llms",
23 |         abstract: "List registered LLM model configurations"
24 |     )
25 | 
26 |     func run() async throws {
27 |         for configuration in LLMRegistry.shared.models {
28 |             switch configuration.id {
29 |             case .id(let id): print(id)
30 |             case .directory: break
31 |             }
32 |         }
33 |     }
34 | }
35 | 
36 | struct ListVLMCommand: AsyncParsableCommand {
37 | 
38 |     static let configuration = CommandConfiguration(
39 |         commandName: "vlms",
40 |         abstract: "List registered VLM model configurations"
41 |     )
42 | 
43 |     func run() async throws {
44 |         for configuration in VLMRegistry.shared.models {
45 |             switch configuration.id {
46 |             case .id(let id): print(id)
47 |             case .directory: break
48 |             }
49 |         }
50 |     }
51 | }
52 | 


--------------------------------------------------------------------------------
/Tools/llm-tool/LoraCommands.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import ArgumentParser
  4 | import Foundation
  5 | import Hub
  6 | import MLX
  7 | import MLXLLM
  8 | import MLXLMCommon
  9 | import MLXNN
 10 | import MLXOptimizers
 11 | import Tokenizers
 12 | 
 13 | struct LoRACommand: AsyncParsableCommand {
 14 | 
 15 |     static let configuration = CommandConfiguration(
 16 |         commandName: "lora",
 17 |         abstract: "LoRA commands",
 18 |         subcommands: [
 19 |             LoRATrainCommand.self, LoRAFuseCommand.self, LoRATestCommand.self, LoRAEvalCommand.self,
 20 |         ]
 21 |     )
 22 | }
 23 | 
 24 | private let defaultModel = MLXLLM.LLMRegistry.mistral7B4bit.name
 25 | 
 26 | /// Common arguments for loading a LoRA mdoel with adapter weights
 27 | struct LoRAModelArguments: ParsableArguments, Sendable {
 28 | 
 29 |     @OptionGroup var args: ModelArguments
 30 | 
 31 |     @Option(name: .long, help: "Save/load path for the trained adapter weights")
 32 |     public var adapter: URL = URL(filePath: "adapters.safetensors")
 33 | 
 34 |     @Option(name: .long, help: "Number of layers to fine-tune")
 35 |     public var loraLayers = 16
 36 | 
 37 |     /// Load the model and apply the LoRA adapters.
 38 |     ///
 39 |     /// This does not load the adapter weights as they may not exist yet.
 40 |     func load(
 41 |         defaultModel: String = defaultModel,
 42 |         modelFactory: ModelFactory = LLMModelFactory.shared
 43 |     ) async throws -> ModelContainer {
 44 |         let modelContainer = try await args.load(
 45 |             defaultModel: defaultModel, modelFactory: modelFactory)
 46 | 
 47 |         // convert some of the Linear layers to LoRALinear
 48 |         await modelContainer.perform { context in
 49 |             guard let lora = context.model as? LoRAModel else {
 50 |                 fatalError("Model \(context.configuration.name) is not a LoRAModel")
 51 |             }
 52 |             LoRATrain.convert(model: context.model, layers: lora.loraLinearLayers(loraLayers))
 53 |         }
 54 | 
 55 |         return modelContainer
 56 |     }
 57 | 
 58 |     func describe(model: Module) {
 59 |         let totalParameterCount = model.numParameters()
 60 |         let trainableParameterCount = model.trainableParameters()
 61 |             .flattenedValues().map { $0.size }.reduce(0, +)
 62 | 
 63 |         print("Model: \(args.model ?? defaultModel)")
 64 |         print("Total parameters: \((totalParameterCount / 1_000_000).formatted())M")
 65 |         print(
 66 |             "Trainable parameters: \((Float(trainableParameterCount) / 1_000_000).formatted(.number.precision(.significantDigits(1 ..< 4))))M"
 67 |         )
 68 | 
 69 |     }
 70 | }
 71 | 
 72 | struct LoRATrainCommand: AsyncParsableCommand {
 73 | 
 74 |     static let configuration = CommandConfiguration(
 75 |         commandName: "train",
 76 |         abstract: "LoRA training"
 77 |     )
 78 | 
 79 |     @OptionGroup var args: LoRAModelArguments
 80 |     @OptionGroup var memory: MemoryArguments
 81 | 
 82 |     @Flag(help: "Resume training with the given adapter file")
 83 |     public var resume = false
 84 | 
 85 |     @Option(name: .long, help: "Directory with {train, valid, test}.{jsonl,txt} files")
 86 |     public var data: URL = URL(filePath: "data")
 87 | 
 88 |     @Option(name: .long, help: "Learning rate for the optimizer")
 89 |     public var learningRate: Float = 1e-5
 90 | 
 91 |     @Option(name: .long, help: "Number of dataset items to evaluate per iteration (batch)")
 92 |     public var batchSize = 4
 93 | 
 94 |     @Option(name: .long, help: "Number iterations to train for")
 95 |     public var iterations = 1000
 96 | 
 97 |     @Option(name: .long, help: "Number of iterations between loss reporting")
 98 |     public var stepsPerReport = 10
 99 | 
100 |     @Option(name: .long, help: "Number of iterations between validations")
101 |     public var stepsPerEval = 100
102 | 
103 |     @Option(name: .long, help: "Number of validation batches, 0 uses the entire set")
104 |     public var validationBatches = 10
105 | 
106 |     @Option(name: .long, help: "Number of iterations between checkpointing the adapter weights")
107 |     public var saveEvery = 100
108 | 
109 |     var parameters: LoRATrain.Parameters {
110 |         var p = LoRATrain.Parameters()
111 |         p.batchSize = self.batchSize
112 |         p.iterations = self.iterations
113 |         p.stepsPerReport = self.stepsPerReport
114 |         p.stepsPerEval = self.stepsPerEval
115 |         p.validationBatches = self.validationBatches
116 |         p.saveEvery = self.saveEvery
117 |         p.adapterURL = args.adapter
118 |         return p
119 |     }
120 | 
121 |     @MainActor
122 |     mutating func run() async throws {
123 |         let modelContainer = try await args.load()
124 |         await modelContainer.perform { [args] context in
125 |             args.describe(model: context.model)
126 |         }
127 | 
128 |         memory.start()
129 | 
130 |         if resume {
131 |             print("Loading pretrained adapters from \(args.adapter.path())")
132 |             try await modelContainer.perform { [args] context in
133 |                 try LoRATrain.loadLoRAWeights(model: context.model, url: args.adapter)
134 |             }
135 |         }
136 | 
137 |         // load the train/validation data
138 |         let train = try loadLoRAData(directory: data, name: "train")
139 |         let valid = try loadLoRAData(directory: data, name: "valid")
140 | 
141 |         if train.isEmpty {
142 |             fatalError("Training set is empty: \(data.path()))")
143 |         }
144 |         if valid.isEmpty {
145 |             fatalError("Validation set is empty: \(data.path()))")
146 |         }
147 | 
148 |         // train
149 |         try await modelContainer.perform { [args, parameters, learningRate] context in
150 |             let optimizer = Adam(learningRate: learningRate)
151 |             try LoRATrain.train(
152 |                 model: context.model, train: train, validate: valid, optimizer: optimizer,
153 |                 tokenizer: context.tokenizer,
154 |                 parameters: parameters
155 |             ) { progress in
156 |                 print(progress)
157 |                 return .more
158 |             }
159 |             try LoRATrain.saveLoRAWeights(model: context.model, url: args.adapter)
160 |         }
161 |     }
162 | }
163 | 
164 | struct LoRAFuseCommand: AsyncParsableCommand {
165 | 
166 |     static let configuration = CommandConfiguration(
167 |         commandName: "fuse",
168 |         abstract: "Fuse lora adapter weights back in to original model"
169 |     )
170 | 
171 |     @OptionGroup var args: LoRAModelArguments
172 | 
173 |     @Flag(name: .long, help: "De-quantize QuantizedLinear layers back into Linear")
174 |     var deQuantize = false
175 | 
176 |     @Option(name: .long, help: "Hub ID (mlx-community/mistral-lora) or path (/tmp/mistral-lora)")
177 |     var output: String
178 | 
179 |     @MainActor
180 |     mutating func run() async throws {
181 |         let outputURL: URL
182 |         if output.hasPrefix("/") {
183 |             outputURL = URL(filePath: output)
184 |         } else {
185 |             let repo = HubApi.Repo(id: output)
186 |             outputURL = HubApi().localRepoLocation(repo)
187 |         }
188 | 
189 |         let modelContainer = try await args.load()
190 | 
191 |         // load the prepared weights
192 |         try await modelContainer.perform { [args] context in
193 |             try LoRATrain.loadLoRAWeights(model: context.model, url: args.adapter)
194 |         }
195 | 
196 |         // fuse them back into Linear/QuantizedLinear
197 |         await modelContainer.perform { [args, deQuantize] context in
198 |             guard let lora = context.model as? LoRAModel else {
199 |                 fatalError("Model \(context.configuration.name) is not a LoRAModel")
200 |             }
201 | 
202 |             LoRATrain.fuse(
203 |                 model: context.model, layers: lora.loraLinearLayers(args.loraLayers),
204 |                 deQuantize: deQuantize)
205 |         }
206 | 
207 |         // make the new directory and copy files from source model
208 |         try FileManager.default.createDirectory(at: outputURL, withIntermediateDirectories: true)
209 |         let inputURL = await modelContainer.configuration.modelDirectory()
210 |         let enumerator = FileManager.default.enumerator(
211 |             at: inputURL, includingPropertiesForKeys: nil)!
212 |         for url in enumerator.allObjects.compactMap({ $0 as? URL }) {
213 |             // copy everything except the model weights -- we will write out the fused one below
214 |             if url.pathExtension == "safetensors" {
215 |                 continue
216 |             }
217 | 
218 |             try FileManager.default.copyItem(
219 |                 at: url, to: outputURL.appending(component: url.lastPathComponent))
220 |         }
221 | 
222 |         // write them back out
223 |         try await modelContainer.perform { context in
224 |             let weights = Dictionary(uniqueKeysWithValues: context.model.parameters().flattened())
225 |             try save(arrays: weights, url: outputURL.appending(component: "weights.safetensors"))
226 |         }
227 | 
228 |         print("Fused weights written to \(outputURL.path())")
229 |         print("Use with:\n\tllm-tool eval --model \(output)")
230 |     }
231 | 
232 | }
233 | 
234 | struct LoRATestCommand: AsyncParsableCommand {
235 | 
236 |     static let configuration = CommandConfiguration(
237 |         commandName: "test",
238 |         abstract: "LoRA testing"
239 |     )
240 | 
241 |     @OptionGroup var args: LoRAModelArguments
242 |     @OptionGroup var memory: MemoryArguments
243 | 
244 |     @Option(name: .long, help: "Directory with {train, valid, test}.{jsonl,txt} files")
245 |     public var data: URL = URL(filePath: "data")
246 | 
247 |     @Option(name: .long, help: "Minibatch size")
248 |     public var batchSize = 4
249 | 
250 |     @MainActor
251 |     mutating func run() async throws {
252 |         let modelContainer = try await args.load()
253 |         await modelContainer.perform { [args] context in
254 |             args.describe(model: context.model)
255 |         }
256 |         try await modelContainer.perform { [args] context in
257 |             try LoRATrain.loadLoRAWeights(model: context.model, url: args.adapter)
258 |         }
259 | 
260 |         memory.start()
261 | 
262 |         let test = try loadLoRAData(directory: data, name: "test")
263 |         let loss = await modelContainer.perform { [batchSize] context in
264 |             LoRATrain.evaluate(
265 |                 model: context.model, dataset: test,
266 |                 tokenizer: context.tokenizer, batchSize: batchSize,
267 |                 batchCount: 0)
268 |         }
269 | 
270 |         print("Test loss \(loss.formatted()), ppl \(exp(loss).formatted())")
271 |     }
272 | 
273 | }
274 | 
275 | struct LoRAEvalCommand: AsyncParsableCommand {
276 | 
277 |     static let configuration = CommandConfiguration(
278 |         commandName: "eval",
279 |         abstract: "LoRA evaluation"
280 |     )
281 | 
282 |     @OptionGroup var args: LoRAModelArguments
283 |     @OptionGroup var memory: MemoryArguments
284 |     @OptionGroup var generate: GenerateArguments
285 |     @OptionGroup var prompt: PromptArguments
286 | 
287 |     @MainActor
288 |     mutating func run() async throws {
289 |         let modelContainer = try await args.load()
290 |         await modelContainer.perform { [args] context in
291 |             args.describe(model: context.model)
292 |         }
293 |         try await modelContainer.perform { [args] context in
294 |             try LoRATrain.loadLoRAWeights(model: context.model, url: args.adapter)
295 |         }
296 | 
297 |         memory.start()
298 | 
299 |         let defaultPrompt = await modelContainer.configuration.defaultPrompt
300 |         let prompt = prompt.prompt ?? defaultPrompt
301 | 
302 |         if !generate.quiet {
303 |             print("Starting generation ...")
304 |             print(prompt, terminator: "")
305 |         }
306 | 
307 |         // generate and print the result
308 |         let (result, _) = try await modelContainer.perform { [generate] context in
309 |             let input = try await context.processor.prepare(input: .init(prompt: prompt))
310 |             return try await generate.generate(input: input, context: context)
311 |         }
312 | 
313 |         if !generate.quiet {
314 |             print("------")
315 |             print(result.summary())
316 | 
317 |             memory.reportMemoryStatistics()
318 |         }
319 |     }
320 | }
321 | 


--------------------------------------------------------------------------------
/Tools/llm-tool/README.md:
--------------------------------------------------------------------------------
  1 | # llm-tool
  2 | 
  3 | See various READMEs:
  4 | 
  5 | - [MLXLMCommon](../../Libraries/MLXLMCommon/README.md) -- common LM code
  6 | - [MLXLLM](../../Libraries/MLXLLM/README.md) -- large language models
  7 | - [MLXVLM](../../Libraries/MLXVLM/README.md) -- vision language models
  8 | 
  9 | ### Building
 10 | 
 11 | Build the `llm-tool` scheme in Xcode.
 12 | 
 13 | ### Running: Xcode
 14 | 
 15 | To run this in Xcode simply press cmd-opt-r to set the scheme arguments. For example:
 16 | 
 17 | ```
 18 | --model mlx-community/Mistral-7B-Instruct-v0.3-4bit
 19 | --prompt "swift programming language"
 20 | --max-tokens 50
 21 | ```
 22 | 
 23 | Then cmd-r to run.
 24 | 
 25 | > Note: you may be prompted for access to your Documents directory -- this is where
 26 | the Hugging Face HubApi stores the downloaded files.
 27 | 
 28 | The model should be a path in the Hugging Face repository, e.g.:
 29 | 
 30 | - `mlx-community/Mistral-7B-Instruct-v0.3-4bit`
 31 | - `mlx-community/phi-2-hf-4bit-mlx`
 32 | 
 33 | See [LLM](../../Libraries/MLXLLM/README.md) for more info.
 34 | 
 35 | ### Running: Command Line
 36 | 
 37 | Use the `mlx-run` script to run the command line tools:
 38 | 
 39 | ```
 40 | ./mlx-run llm-tool --prompt "swift programming language"
 41 | ```
 42 | 
 43 | Note: `mlx-run` is a shell script that uses `xcode` command line tools to
 44 | locate the built binaries. It is equivalent to running from Xcode itself.
 45 | 
 46 | By default this will find and run the tools built in _Release_ configuration. Specify `--debug`
 47 | to find and run the tool built in _Debug_ configuration.
 48 | 
 49 | See also:
 50 | 
 51 | - [MLX troubleshooting](https://swiftpackageindex.com/ml-explore/mlx-swift/main/documentation/mlx/troubleshooting)
 52 | 
 53 | ### Troubleshooting
 54 | 
 55 | If the program crashes with a very deep stack trace you may need to build
 56 | in Release configuration. This seems to depend on the size of the model.
 57 | 
 58 | There are a couple options:
 59 | 
 60 | - build Release
 61 | - force the model evaluation to run on the main thread, e.g. using @MainActor
 62 | - build `Cmlx` with optimizations by modifying `mlx/Package.swift` and adding `.unsafeFlags(["-O"]),` around line 87
 63 | 
 64 | Building in Release / optimizations will remove a lot of tail calls in the C++ 
 65 | layer. These lead to the stack overflows.
 66 | 
 67 | See discussion here: https://github.com/ml-explore/mlx-swift-examples/issues/3
 68 | 
 69 | ## LoRA
 70 | 
 71 | `llm-tool` provides an example LoRA driver based on:
 72 | 
 73 | - https://github.com/ml-explore/mlx-examples/blob/main/lora/README.md
 74 | 
 75 | This is an example of using MLX to fine-tune an LLM with low rank adaptation
 76 | (LoRA) for a target task.[^lora] The example also supports quantized LoRA
 77 | (QLoRA).[^qlora] The example works with Llama and Mistral style models
 78 | available on Hugging Face.
 79 | 
 80 | In this example we'll use the WikiSQL[^wikisql] dataset to train the LLM to
 81 | generate SQL queries from natural language. However, the example is intended to
 82 | be general should you wish to use a custom dataset.
 83 | 
 84 | > Note: Some of the prompts have newlines in them which is difficult to achieve via running in Xcode.
 85 | 
 86 | Running `llm-tool lora` will produce help:
 87 | 
 88 | ```
 89 | SUBCOMMANDS:
 90 |   train                   LoRA training
 91 |   fuse                    Fuse lora adapter weights back in to original model
 92 |   test                    LoRA testing
 93 |   eval                    LoRA evaluation
 94 | ```
 95 | 
 96 | ### Training
 97 | 
 98 | The first step will be training the LoRA adapter. Example training data
 99 | is available in $SRCROOT/Data/lora. You can use your
100 | own data in either `jsonl` or `txt` format with one entry per line.
101 | 
102 | We need to specify a number of parameters:
103 | 
104 | - `--model` -- which model to use. This can be quantized [^qlora] or not [^lora]
105 | - `--data` -- directory with the test, train and valid files. These can be either `jsonl` or `txt` files
106 | - `--adapter` -- path to a safetensors file to write the fine tuned parameters into
107 | 
108 | Additionally the performance of the fine tuning can be controlled with:
109 | 
110 | - `--batch-size` -- size of the minibatches to run in the training loop, e.g. how many prompts to process per iteration
111 | - `--lora-layers` -- the number of layers in the Attention section of the model to adapt and train
112 | - `--iterations` -- the number of iterations to train for
113 | 
114 | If desired, the amount of memory used can be adjusted with:
115 | 
116 | - `--cache-size` -- the number shown below limits the cache size to 1024M 
117 | 
118 | Here is an example run using adapters on the last 4 layers of the model:
119 | 
120 | ```
121 | ./mlx-run llm-tool lora train \
122 |     --model mlx-community/Mistral-7B-v0.1-hf-4bit-mlx \
123 |     --data Data/lora \
124 |     --adapter /tmp/lora-layers-4.safetensors \
125 |     --batch-size 1 --lora-layers 4 \
126 |     --cache-size 1024
127 | ```
128 | 
129 | giving output like this:
130 | 
131 | ```
132 | Model: mlx-community/Mistral-7B-Instruct-v0.3-4bit
133 | Total parameters: 1,242M
134 | Trainable parameters: 0.426M
135 | Iteration 1: validation loss 2.443872, validation time 3.330629s
136 | Iteration 10: training loss 2.356848, iterations/sec 2.640604, Tokens/sec 260.363581
137 | Iteration 20: training loss 2.063395, iterations/sec 2.294999, Tokens/sec 232.483365
138 | Iteration 30: training loss 1.63846, iterations/sec 2.279401, Tokens/sec 225.204788
139 | Iteration 40: training loss 1.66366, iterations/sec 2.493669, Tokens/sec 218.196057
140 | Iteration 50: training loss 1.470927, iterations/sec 2.301153, Tokens/sec 231.72614
141 | Iteration 60: training loss 1.396581, iterations/sec 2.400012, Tokens/sec 230.401195
142 | Iteration 70: training loss 1.587023, iterations/sec 2.422193, Tokens/sec 218.966258
143 | Iteration 80: training loss 1.376895, iterations/sec 2.111973, Tokens/sec 216.477187
144 | Iteration 90: training loss 1.245127, iterations/sec 2.383802, Tokens/sec 214.065436
145 | Iteration 100: training loss 1.344523, iterations/sec 2.424746, Tokens/sec 223.076649
146 | Iteration 100: validation loss 1.400582, validation time 3.489797s
147 | Iteration 100: saved weights to /tmp/lora.safetensors
148 | ...
149 | Iteration 910: training loss 1.181306, iterations/sec 2.355085, Tokens/sec 212.428628
150 | Iteration 920: training loss 1.042286, iterations/sec 2.374377, Tokens/sec 222.479127
151 | Iteration 930: training loss 0.920768, iterations/sec 2.475088, Tokens/sec 220.035347
152 | Iteration 940: training loss 1.140762, iterations/sec 2.119886, Tokens/sec 227.039828
153 | Iteration 950: training loss 1.068073, iterations/sec 2.523047, Tokens/sec 218.495903
154 | Iteration 960: training loss 1.106662, iterations/sec 2.339293, Tokens/sec 221.063186
155 | Iteration 970: training loss 0.833658, iterations/sec 2.474683, Tokens/sec 213.56517
156 | Iteration 980: training loss 0.844026, iterations/sec 2.441064, Tokens/sec 210.663791
157 | Iteration 990: training loss 0.903735, iterations/sec 2.253876, Tokens/sec 218.175162
158 | Iteration 1000: training loss 0.872615, iterations/sec 2.343899, Tokens/sec 219.62336
159 | Iteration 1000: validation loss 0.714194, validation time 3.470462s
160 | Iteration 1000: saved weights to /tmp/lora-layers-4.safetensors
161 | ```
162 | 
163 | ### Testing
164 | 
165 | You can test the LoRA adapated model against the `test` dataset using this command:
166 | 
167 | ```
168 | ./mlx-run llm-tool lora test \ 
169 |     --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \
170 |     --data Data/lora \
171 |     --adapter /tmp/lora-layers-4.safetensors \
172 |     --batch-size 1 --lora-layers 4 \
173 |     --cache-size 1024
174 | ```
175 | 
176 | This will run all the items (100 in the example data we are using) in the test set and compute the loss:
177 | 
178 | ```
179 | Model: mlx-community/Mistral-7B-Instruct-v0.3-4bit
180 | Total parameters: 1,242M
181 | Trainable parameters: 0.426M
182 | Test loss 1.327623, ppl 3.772065
183 | ```
184 | 
185 | ### Evaluate
186 | 
187 | Next you can evaluate your own prompts with the fine tuned LoRA adapters. It is important to
188 | follow the prompt example from the training data to match the format:
189 | 
190 | ```
191 | {"text": "table: 1-10015132-1\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What school did player number 6 come from?\nA: SELECT School/Club Team FROM 1-10015132-1 WHERE No. = '6'"}
192 | ```
193 | 
194 | Given that format you might issue a command like this:
195 | 
196 | ```
197 | ./mlx-run llm-tool lora eval \
198 |     --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \
199 |     --adapter /tmp/lora-layers-4.safetensors \
200 |     --lora-layers 4 \
201 |     --prompt "table: 1-10015132-16
202 | columns: Player, No., Nationality, Position, Years in Toronto, School/Club Team
203 | Q: What is terrence ross' nationality
204 | A: "
205 | ```
206 | 
207 | > Note: the prompt has newlines in it to match the format of the fine tuned prompts -- this may be easier to do with the command line than Xcode.
208 | 
209 | You might be treated to a response like this:
210 | 
211 | ```
212 | Model: mlx-community/Mistral-7B-Instruct-v0.3-4bit
213 | Total parameters: 1,242M
214 | Trainable parameters: 0.426M
215 | Starting generation ...
216 | table: 1-10015132-16
217 | columns: Player, No., Nationality, Position, Years in Toronto, School/Club Team
218 | Q: What is terrence ross' nationality
219 | A: SELECT Nationality FROM 1-10015132-16 WHERE Player = 'Terrence Ross' AND No. = 1
220 | ```
221 | 
222 | ### Fusing
223 | 
224 | Once the adapter weights are trained you can produce new weights with the original achitecture that
225 | have the adapter weights merged in:
226 | 
227 | ```
228 | ./mlx-run llm-tool lora fuse \
229 |     --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \
230 |     --adapter /tmp/lora-layers-4.safetensors \
231 |     --output mlx-community/mistral-lora
232 | ```
233 | 
234 | outputs:
235 | 
236 | ```
237 | Total parameters: 1,244M
238 | Trainable parameters: 0.426M
239 | Use with:
240 |     llm-tool eval --model mlx-community/mistral-lora
241 | ```
242 | 
243 | As noted in the output these new weights can be used with the original model architecture.
244 | 
245 | 
246 | [^lora]: Refer to the [arXiv paper](https://arxiv.org/abs/2106.09685) for more details on LoRA.
247 | [^qlora]: Refer to the paper [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
248 | [^wikisql]: Refer to the [GitHub repo](https://github.com/salesforce/WikiSQL/tree/master) for more information about WikiSQL.
249 | 


--------------------------------------------------------------------------------
/Tools/mnist-tool/MNISTTool.swift:
--------------------------------------------------------------------------------
  1 | // Copyright © 2024 Apple Inc.
  2 | 
  3 | import ArgumentParser
  4 | import Foundation
  5 | import MLX
  6 | import MLXMNIST
  7 | import MLXNN
  8 | import MLXOptimizers
  9 | 
 10 | @main
 11 | struct MNISTTool: AsyncParsableCommand {
 12 |     static let configuration = CommandConfiguration(
 13 |         abstract: "Command line tool for training mnist models",
 14 |         subcommands: [Train.self],
 15 |         defaultSubcommand: Train.self)
 16 | }
 17 | 
 18 | #if swift(>=5.10)
 19 |     extension MLX.DeviceType: @retroactive ExpressibleByArgument {
 20 |         public init?(argument: String) {
 21 |             self.init(rawValue: argument)
 22 |         }
 23 |     }
 24 | #else
 25 |     extension MLX.DeviceType: ExpressibleByArgument {
 26 |         public init?(argument: String) {
 27 |             self.init(rawValue: argument)
 28 |         }
 29 |     }
 30 | #endif
 31 | 
 32 | struct Train: AsyncParsableCommand {
 33 | 
 34 |     @Option(name: .long, help: "Directory with the training data")
 35 |     var data: String
 36 | 
 37 |     @Option(name: .long, help: "The PRNG seed")
 38 |     var seed: UInt64 = 0
 39 | 
 40 |     @Option var batchSize = 256
 41 |     @Option var epochs = 20
 42 |     @Option var learningRate: Float = 1e-1
 43 | 
 44 |     @Option var device = DeviceType.gpu
 45 | 
 46 |     @Flag var compile = false
 47 | 
 48 |     func run() async throws {
 49 |         Device.setDefault(device: Device(device))
 50 | 
 51 |         MLXRandom.seed(seed)
 52 |         var generator: RandomNumberGenerator = SplitMix64(seed: seed)
 53 | 
 54 |         // load the data
 55 |         let url = URL(filePath: data)
 56 | 
 57 |         try FileManager.default.createDirectory(at: url, withIntermediateDirectories: true)
 58 |         try await download(into: url)
 59 | 
 60 |         let data = try load(from: url)
 61 | 
 62 |         let trainImages = data[.init(.training, .images)]!
 63 |         let trainLabels = data[.init(.training, .labels)]!
 64 |         let testImages = data[.init(.test, .images)]!
 65 |         let testLabels = data[.init(.test, .labels)]!
 66 | 
 67 |         // create the model
 68 |         let model = LeNet()
 69 |         eval(model.parameters())
 70 | 
 71 |         let lg = valueAndGrad(model: model, loss)
 72 |         let optimizer = SGD(learningRate: learningRate)
 73 | 
 74 |         func step(_ x: MLXArray, _ y: MLXArray) -> MLXArray {
 75 |             let (loss, grads) = lg(model, x, y)
 76 |             optimizer.update(model: model, gradients: grads)
 77 |             return loss
 78 |         }
 79 | 
 80 |         let resolvedStep =
 81 |             compile
 82 |             ? MLX.compile(inputs: [model, optimizer], outputs: [model, optimizer], step) : step
 83 | 
 84 |         for e in 0 ..< epochs {
 85 |             let start = Date.timeIntervalSinceReferenceDate
 86 | 
 87 |             for (x, y) in iterateBatches(
 88 |                 batchSize: batchSize, x: trainImages, y: trainLabels, using: &generator)
 89 |             {
 90 |                 _ = resolvedStep(x, y)
 91 | 
 92 |                 // eval the parameters so the next iteration is independent
 93 |                 eval(model, optimizer)
 94 |             }
 95 | 
 96 |             let accuracy = eval(model: model, x: testImages, y: testLabels)
 97 | 
 98 |             let end = Date.timeIntervalSinceReferenceDate
 99 | 
100 |             print(
101 |                 """
102 |                 Epoch \(e): test accuracy \(accuracy.item(Float.self).formatted())
103 |                 Time: \((end - start).formatted())
104 | 
105 |                 """
106 |             )
107 |         }
108 |     }
109 | }
110 | 


--------------------------------------------------------------------------------
/Tools/mnist-tool/README.md:
--------------------------------------------------------------------------------
 1 | #  mnist-tool
 2 | 
 3 | See the [MNIST README.md](../../Libraries/MNIST/README.md).
 4 | 
 5 | ### Building
 6 | 
 7 | `mnist-tool` has no dependencies outside of the package dependencies
 8 | represented in xcode.
 9 | 
10 | When you run the tool it will download the test/train datasets and
11 | store them in a specified directory (see run arguments -- default is /tmp).
12 | 
13 | Simply build the project in xcode.
14 | 
15 | ### Running (Xcode)
16 | 
17 | To run this in Xcode simply press cmd-opt-r to set the scheme arguments. For example:
18 | 
19 | ```
20 | --data /tmp
21 | ```
22 | 
23 | Then cmd-r to run.
24 | 
25 | ### Running (CommandLine)
26 | 
27 | Use the `mlx-run` script to run the command line tools:
28 | 
29 | ```
30 | ./mlx-run mnist-tool --data /tmp
31 | ```
32 | 
33 | By default this will find and run the tools built in _Release_ configuration. Specify `--debug`
34 | to find and run the tool built in _Debug_ configuration.
35 | 
36 | See also:
37 | 
38 | - [MLX troubleshooting](https://swiftpackageindex.com/ml-explore/mlx-swift/main/documentation/mlx/troubleshooting)
39 | 


--------------------------------------------------------------------------------
/mlx-run:
--------------------------------------------------------------------------------
 1 | #!/bin/sh
 2 | 
 3 | # Wrapper to help run command line tools -- this will find the build directory
 4 | # and set the DYLD_FRAMEWORK_PATH so that command line tools that link frameworks
 5 | # can be run.
 6 | #
 7 | # Example:
 8 | # ./mlx-run --debug llm-tool --help
 9 | 
10 | if [ "$#" -lt 1 ]; then
11 | 	echo "usage: mlx-run [--debug/--release] <tool-name> arguments"
12 | 	exit 1
13 | fi
14 | 
15 | CONFIGURATION=Release
16 | if [ "$1" == "--release" ]; then
17 | 	CONFIGURATION=Release
18 | 	shift
19 | fi
20 | if [ "$1" == "--debug" ]; then
21 | 	CONFIGURATION=Debug
22 | 	shift
23 | fi
24 | if [ "$1" == "--list" ]; then
25 | 	xcodebuild -list
26 | 	exit 0
27 | fi
28 | 
29 | COMMAND="$1"
30 | shift
31 | 
32 | BUILD_DIR=`xcodebuild -configuration $CONFIGURATION -showBuildSettings -scheme $COMMAND | grep 'BUILT_PRODUCTS_DIR = /' | sed -e 's/^[^=]*= //g'`
33 | 
34 | if [ -d "$BUILD_DIR/$COMMAND.app" ]; then
35 | 	exec $BUILD_DIR/$COMMAND.app/Contents/MacOS/$COMMAND "$@" &
36 | fi
37 | 
38 | if [ -f "$BUILD_DIR/$COMMAND" ]; then
39 | 	export DYLD_FRAMEWORK_PATH=$BUILD_DIR/PackageFrameworks:$BUILD_DIR
40 | 	exec "$BUILD_DIR/$COMMAND" "$@"
41 | else
42 | 	echo "$BUILD_DIR/$COMMAND does not exist -- check build configuration ($CONFIGURATION)"
43 | 	exit 1
44 | fi
45 | 
46 | 


--------------------------------------------------------------------------------
/mlx-swift-examples.xcodeproj/project.pbxproj:
--------------------------------------------------------------------------------
   1 | // !$*UTF8*$!
   2 | {
   3 | 	archiveVersion = 1;
   4 | 	classes = {
   5 | 	};
   6 | 	objectVersion = 70;
   7 | 	objects = {
   8 | 
   9 | /* Begin PBXBuildFile section */
  10 | 		0A8284222D13863900BEF338 /* MLXVLM in Frameworks */ = {isa = PBXBuildFile; productRef = 0AC74ED92D136223003C90A7 /* MLXVLM */; };
  11 | 		0AC74ECF2D13622A003C90A7 /* Preview Assets.xcassets in Resources */ = {isa = PBXBuildFile; fileRef = 0AC74EC92D13622A003C90A7 /* Preview Assets.xcassets */; };
  12 | 		0AC74ED02D13622A003C90A7 /* Assets.xcassets in Resources */ = {isa = PBXBuildFile; fileRef = 0AC74ECB2D13622A003C90A7 /* Assets.xcassets */; };
  13 | 		0AC74ED12D13622A003C90A7 /* ContentView.swift in Sources */ = {isa = PBXBuildFile; fileRef = 0AC74ECC2D13622A003C90A7 /* ContentView.swift */; };
  14 | 		0AC74ED22D13622A003C90A7 /* VLMEvalApp.swift in Sources */ = {isa = PBXBuildFile; fileRef = 0AC74ECD2D13622A003C90A7 /* VLMEvalApp.swift */; };
  15 | 		0AC74ED32D136265003C90A7 /* DeviceStat.swift in Sources */ = {isa = PBXBuildFile; fileRef = 819BEFF62BAF8B4E0002CCEE /* DeviceStat.swift */; };
  16 | 		0F5AD8002DB70E6300745C06 /* MLXLLM in Frameworks */ = {isa = PBXBuildFile; productRef = C32A17FC2CFFB98A0092A5B6 /* MLXLLM */; };
  17 | 		0F5AD8012DB70E6300745C06 /* MLXVLM in Frameworks */ = {isa = PBXBuildFile; productRef = C32A17FE2CFFB98A0092A5B6 /* MLXVLM */; };
  18 | 		12305EAF2B9D864400C92FEE /* PredictionView.swift in Sources */ = {isa = PBXBuildFile; fileRef = 12305EAE2B9D864400C92FEE /* PredictionView.swift */; };
  19 | 		81695B412BA373D300F260D8 /* MarkdownUI in Frameworks */ = {isa = PBXBuildFile; productRef = 81695B402BA373D300F260D8 /* MarkdownUI */; };
  20 | 		819BEFF82BAF8B4E0002CCEE /* DeviceStat.swift in Sources */ = {isa = PBXBuildFile; fileRef = 819BEFF62BAF8B4E0002CCEE /* DeviceStat.swift */; };
  21 | 		C3056BAE2BCD97B700A31D04 /* LoRATrainingExampleApp.swift in Sources */ = {isa = PBXBuildFile; fileRef = C3056BAD2BCD97B700A31D04 /* LoRATrainingExampleApp.swift */; };
  22 | 		C3056BB02BCD97B700A31D04 /* ContentView.swift in Sources */ = {isa = PBXBuildFile; fileRef = C3056BAF2BCD97B700A31D04 /* ContentView.swift */; };
  23 | 		C3056BB22BCD97B800A31D04 /* Assets.xcassets in Resources */ = {isa = PBXBuildFile; fileRef = C3056BB12BCD97B800A31D04 /* Assets.xcassets */; };
  24 | 		C3056BB62BCD97B800A31D04 /* Preview Assets.xcassets in Resources */ = {isa = PBXBuildFile; fileRef = C3056BB52BCD97B800A31D04 /* Preview Assets.xcassets */; };
  25 | 		C3056BBA2BCD981900A31D04 /* train.jsonl in Resources */ = {isa = PBXBuildFile; fileRef = C3056BA22BCD973400A31D04 /* train.jsonl */; };
  26 | 		C3056BBB2BCD981900A31D04 /* test.jsonl in Resources */ = {isa = PBXBuildFile; fileRef = C3056BA12BCD973400A31D04 /* test.jsonl */; };
  27 | 		C3056BBC2BCD981900A31D04 /* valid.jsonl in Resources */ = {isa = PBXBuildFile; fileRef = C3056BA32BCD973400A31D04 /* valid.jsonl */; };
  28 | 		C3132C2C2DFB301A00270E8E /* main.swift in Sources */ = {isa = PBXBuildFile; fileRef = C3132C2A2DFB301A00270E8E /* main.swift */; };
  29 | 		C3132C2E2DFB317C00270E8E /* MLXVLM in Frameworks */ = {isa = PBXBuildFile; productRef = C3132C2D2DFB317C00270E8E /* MLXVLM */; };
  30 | 		C3132C302DFB317C00270E8E /* MLXLLM in Frameworks */ = {isa = PBXBuildFile; productRef = C3132C2F2DFB317C00270E8E /* MLXLLM */; };
  31 | 		C3208E762DB1945D006AE6CA /* MLX in Frameworks */ = {isa = PBXBuildFile; productRef = C3208E752DB1945D006AE6CA /* MLX */; };
  32 | 		C3208E7A2DB1945D006AE6CA /* MLXNN in Frameworks */ = {isa = PBXBuildFile; productRef = C3208E792DB1945D006AE6CA /* MLXNN */; };
  33 | 		C3208E7B2DB194ED006AE6CA /* MLXLLM in Frameworks */ = {isa = PBXBuildFile; productRef = C32A17FC2CFFB98A0092A5B6 /* MLXLLM */; };
  34 | 		C3208ED02DB195A7006AE6CA /* MLXVLM in Frameworks */ = {isa = PBXBuildFile; productRef = C32A17FE2CFFB98A0092A5B6 /* MLXVLM */; };
  35 | 		C3288D762B6D9313009FF608 /* LinearModelTraining.swift in Sources */ = {isa = PBXBuildFile; fileRef = C3288D752B6D9313009FF608 /* LinearModelTraining.swift */; };
  36 | 		C3288D7B2B6D9339009FF608 /* ArgumentParser in Frameworks */ = {isa = PBXBuildFile; productRef = C3288D7A2B6D9339009FF608 /* ArgumentParser */; };
  37 | 		C32A17FD2CFFB98A0092A5B6 /* MLXLLM in Frameworks */ = {isa = PBXBuildFile; productRef = C32A17FC2CFFB98A0092A5B6 /* MLXLLM */; };
  38 | 		C32A17FF2CFFB98A0092A5B6 /* MLXVLM in Frameworks */ = {isa = PBXBuildFile; productRef = C32A17FE2CFFB98A0092A5B6 /* MLXVLM */; };
  39 | 		C32A18012CFFD1810092A5B6 /* MLXMNIST in Frameworks */ = {isa = PBXBuildFile; productRef = C32A18002CFFD1810092A5B6 /* MLXMNIST */; };
  40 | 		C32A18032CFFD1920092A5B6 /* MLXMNIST in Frameworks */ = {isa = PBXBuildFile; productRef = C32A18022CFFD1920092A5B6 /* MLXMNIST */; };
  41 | 		C32A18052CFFD19F0092A5B6 /* MLXLLM in Frameworks */ = {isa = PBXBuildFile; productRef = C32A18042CFFD19F0092A5B6 /* MLXLLM */; };
  42 | 		C32A18072CFFD1AA0092A5B6 /* MLXLLM in Frameworks */ = {isa = PBXBuildFile; productRef = C32A18062CFFD1AA0092A5B6 /* MLXLLM */; };
  43 | 		C32A18092CFFD1B70092A5B6 /* StableDiffusion in Frameworks */ = {isa = PBXBuildFile; productRef = C32A18082CFFD1B70092A5B6 /* StableDiffusion */; };
  44 | 		C32A18462D00E1490092A5B6 /* MLX in Frameworks */ = {isa = PBXBuildFile; productRef = C32A18452D00E1490092A5B6 /* MLX */; };
  45 | 		C32A18482D00E1540092A5B6 /* MLX in Frameworks */ = {isa = PBXBuildFile; productRef = C32A18472D00E1540092A5B6 /* MLX */; };
  46 | 		C32A184A2D00E1540092A5B6 /* MLXNN in Frameworks */ = {isa = PBXBuildFile; productRef = C32A18492D00E1540092A5B6 /* MLXNN */; };
  47 | 		C32A184C2D00E1540092A5B6 /* MLXOptimizers in Frameworks */ = {isa = PBXBuildFile; productRef = C32A184B2D00E1540092A5B6 /* MLXOptimizers */; };
  48 | 		C32B4C6D2DA7136000EF663D /* AsyncAlgorithms in Frameworks */ = {isa = PBXBuildFile; productRef = C32B4C6C2DA7136000EF663D /* AsyncAlgorithms */; };
  49 | 		C32B4C6F2DA71ADC00EF663D /* AsyncAlgorithms in Frameworks */ = {isa = PBXBuildFile; productRef = C32B4C6E2DA71ADC00EF663D /* AsyncAlgorithms */; };
  50 | 		C34E48F52B696F0B00FCB841 /* LLMTool.swift in Sources */ = {isa = PBXBuildFile; fileRef = C34E48F42B696F0B00FCB841 /* LLMTool.swift */; };
  51 | 		C34E49242B6A026F00FCB841 /* MNISTTool.swift in Sources */ = {isa = PBXBuildFile; fileRef = C34E49232B6A026F00FCB841 /* MNISTTool.swift */; };
  52 | 		C34E49292B6A028100FCB841 /* ArgumentParser in Frameworks */ = {isa = PBXBuildFile; productRef = C34E49282B6A028100FCB841 /* ArgumentParser */; };
  53 | 		C36BEFB52BBDEAD8002D4AFE /* LoraCommands.swift in Sources */ = {isa = PBXBuildFile; fileRef = C36BEFB32BBDEA69002D4AFE /* LoraCommands.swift */; };
  54 | 		C36BEFB82BBDED51002D4AFE /* Arguments.swift in Sources */ = {isa = PBXBuildFile; fileRef = C36BEFB62BBDECBC002D4AFE /* Arguments.swift */; };
  55 | 		C36BEFE32BC32988002D4AFE /* ImageTool.swift in Sources */ = {isa = PBXBuildFile; fileRef = C36BEFE22BC32988002D4AFE /* ImageTool.swift */; };
  56 | 		C36BEFEF2BC329C5002D4AFE /* ArgumentParser in Frameworks */ = {isa = PBXBuildFile; productRef = C36BEFEE2BC329C5002D4AFE /* ArgumentParser */; };
  57 | 		C36BEFF22BC32A9A002D4AFE /* Progress in Frameworks */ = {isa = PBXBuildFile; productRef = C36BEFF12BC32A9A002D4AFE /* Progress */; };
  58 | 		C36BF0042BC5CE55002D4AFE /* StableDiffusionExampleApp.swift in Sources */ = {isa = PBXBuildFile; fileRef = C36BF0032BC5CE55002D4AFE /* StableDiffusionExampleApp.swift */; };
  59 | 		C36BF0062BC5CE55002D4AFE /* ContentView.swift in Sources */ = {isa = PBXBuildFile; fileRef = C36BF0052BC5CE55002D4AFE /* ContentView.swift */; };
  60 | 		C36BF0082BC5CE56002D4AFE /* Assets.xcassets in Resources */ = {isa = PBXBuildFile; fileRef = C36BF0072BC5CE56002D4AFE /* Assets.xcassets */; };
  61 | 		C36BF00C2BC5CE56002D4AFE /* Preview Assets.xcassets in Resources */ = {isa = PBXBuildFile; fileRef = C36BF00B2BC5CE56002D4AFE /* Preview Assets.xcassets */; };
  62 | 		C36BF0352BC70F11002D4AFE /* Arguments.swift in Sources */ = {isa = PBXBuildFile; fileRef = C36BF0342BC70F11002D4AFE /* Arguments.swift */; };
  63 | 		C37133A22DD6524B00D19830 /* ListCommands.swift in Sources */ = {isa = PBXBuildFile; fileRef = C37133A12DD6524B00D19830 /* ListCommands.swift */; };
  64 | 		C38BA3AA2DB8321600BAFA88 /* Chat.swift in Sources */ = {isa = PBXBuildFile; fileRef = C38BA3A92DB8321600BAFA88 /* Chat.swift */; };
  65 | 		C392737D2B606A1D00368D5D /* Tutorial.swift in Sources */ = {isa = PBXBuildFile; fileRef = C392737C2B606A1D00368D5D /* Tutorial.swift */; };
  66 | 		C397C59C2B62C6D0004B084D /* ArgumentParser in Frameworks */ = {isa = PBXBuildFile; productRef = C397C59B2B62C6D0004B084D /* ArgumentParser */; };
  67 | 		C3A8B3CB2B92951E0002EFB8 /* Assets.xcassets in Resources */ = {isa = PBXBuildFile; fileRef = C3A8B3C32B92951E0002EFB8 /* Assets.xcassets */; };
  68 | 		C3A8B3CC2B92951E0002EFB8 /* MNISTTrainerApp.swift in Sources */ = {isa = PBXBuildFile; fileRef = C3A8B3C42B92951E0002EFB8 /* MNISTTrainerApp.swift */; };
  69 | 		C3A8B3CD2B92951E0002EFB8 /* Preview Assets.xcassets in Resources */ = {isa = PBXBuildFile; fileRef = C3A8B3C62B92951E0002EFB8 /* Preview Assets.xcassets */; };
  70 | 		C3A8B3CF2B92951E0002EFB8 /* ContentView.swift in Sources */ = {isa = PBXBuildFile; fileRef = C3A8B3C92B92951E0002EFB8 /* ContentView.swift */; };
  71 | 		C3A8B3F32B92A2A90002EFB8 /* Assets.xcassets in Resources */ = {isa = PBXBuildFile; fileRef = C3A8B3EC2B92A2A90002EFB8 /* Assets.xcassets */; };
  72 | 		C3A8B3F42B92A2A90002EFB8 /* LLMEvalApp.swift in Sources */ = {isa = PBXBuildFile; fileRef = C3A8B3ED2B92A2A90002EFB8 /* LLMEvalApp.swift */; };
  73 | 		C3A8B3F52B92A2A90002EFB8 /* Preview Assets.xcassets in Resources */ = {isa = PBXBuildFile; fileRef = C3A8B3EF2B92A2A90002EFB8 /* Preview Assets.xcassets */; };
  74 | 		C3A8B3F72B92A2A90002EFB8 /* ContentView.swift in Sources */ = {isa = PBXBuildFile; fileRef = C3A8B3F22B92A2A90002EFB8 /* ContentView.swift */; };
  75 | 		C3C7C4FB2DB19026000373CF /* AVKit.framework in Frameworks */ = {isa = PBXBuildFile; fileRef = C3C7C4FA2DB19026000373CF /* AVKit.framework */; };
  76 | 		C3E7D94D2CF6C9B20056C095 /* StableDiffusion in Frameworks */ = {isa = PBXBuildFile; productRef = C3E7D94C2CF6C9B20056C095 /* StableDiffusion */; };
  77 | /* End PBXBuildFile section */
  78 | 
  79 | /* Begin PBXCopyFilesBuildPhase section */
  80 | 		C3056BC12BCD984F00A31D04 /* Embed Frameworks */ = {
  81 | 			isa = PBXCopyFilesBuildPhase;
  82 | 			buildActionMask = 2147483647;
  83 | 			dstPath = "";
  84 | 			dstSubfolderSpec = 10;
  85 | 			files = (
  86 | 			);
  87 | 			name = "Embed Frameworks";
  88 | 			runOnlyForDeploymentPostprocessing = 0;
  89 | 		};
  90 | 		C3132C212DFB2FFB00270E8E /* CopyFiles */ = {
  91 | 			isa = PBXCopyFilesBuildPhase;
  92 | 			buildActionMask = 2147483647;
  93 | 			dstPath = /usr/share/man/man1/;
  94 | 			dstSubfolderSpec = 0;
  95 | 			files = (
  96 | 			);
  97 | 			runOnlyForDeploymentPostprocessing = 1;
  98 | 		};
  99 | 		C3288D712B6D9313009FF608 /* CopyFiles */ = {
 100 | 			isa = PBXCopyFilesBuildPhase;
 101 | 			buildActionMask = 2147483647;
 102 | 			dstPath = /usr/share/man/man1/;
 103 | 			dstSubfolderSpec = 0;
 104 | 			files = (
 105 | 			);
 106 | 			runOnlyForDeploymentPostprocessing = 1;
 107 | 		};
 108 | 		C34E491F2B6A026F00FCB841 /* CopyFiles */ = {
 109 | 			isa = PBXCopyFilesBuildPhase;
 110 | 			buildActionMask = 2147483647;
 111 | 			dstPath = /usr/share/man/man1/;
 112 | 			dstSubfolderSpec = 0;
 113 | 			files = (
 114 | 			);
 115 | 			runOnlyForDeploymentPostprocessing = 1;
 116 | 		};
 117 | 		C34E492E2B6A028800FCB841 /* Embed Frameworks */ = {
 118 | 			isa = PBXCopyFilesBuildPhase;
 119 | 			buildActionMask = 2147483647;
 120 | 			dstPath = "";
 121 | 			dstSubfolderSpec = 10;
 122 | 			files = (
 123 | 			);
 124 | 			name = "Embed Frameworks";
 125 | 			runOnlyForDeploymentPostprocessing = 0;
 126 | 		};
 127 | 		C36BEFDE2BC32988002D4AFE /* CopyFiles */ = {
 128 | 			isa = PBXCopyFilesBuildPhase;
 129 | 			buildActionMask = 2147483647;
 130 | 			dstPath = /usr/share/man/man1/;
 131 | 			dstSubfolderSpec = 0;
 132 | 			files = (
 133 | 			);
 134 | 			runOnlyForDeploymentPostprocessing = 1;
 135 | 		};
 136 | 		C36BEFEB2BC329AB002D4AFE /* Embed Frameworks */ = {
 137 | 			isa = PBXCopyFilesBuildPhase;
 138 | 			buildActionMask = 2147483647;
 139 | 			dstPath = "";
 140 | 			dstSubfolderSpec = 10;
 141 | 			files = (
 142 | 			);
 143 | 			name = "Embed Frameworks";
 144 | 			runOnlyForDeploymentPostprocessing = 0;
 145 | 		};
 146 | 		C36BF0142BC5CF17002D4AFE /* Embed Frameworks */ = {
 147 | 			isa = PBXCopyFilesBuildPhase;
 148 | 			buildActionMask = 2147483647;
 149 | 			dstPath = "";
 150 | 			dstSubfolderSpec = 10;
 151 | 			files = (
 152 | 			);
 153 | 			name = "Embed Frameworks";
 154 | 			runOnlyForDeploymentPostprocessing = 0;
 155 | 		};
 156 | 		C38935DB2B869CCE0037B833 /* Embed Frameworks */ = {
 157 | 			isa = PBXCopyFilesBuildPhase;
 158 | 			buildActionMask = 2147483647;
 159 | 			dstPath = "";
 160 | 			dstSubfolderSpec = 10;
 161 | 			files = (
 162 | 			);
 163 | 			name = "Embed Frameworks";
 164 | 			runOnlyForDeploymentPostprocessing = 0;
 165 | 		};
 166 | 		C39273722B606A0A00368D5D /* CopyFiles */ = {
 167 | 			isa = PBXCopyFilesBuildPhase;
 168 | 			buildActionMask = 2147483647;
 169 | 			dstPath = /usr/share/man/man1/;
 170 | 			dstSubfolderSpec = 0;
 171 | 			files = (
 172 | 			);
 173 | 			runOnlyForDeploymentPostprocessing = 1;
 174 | 		};
 175 | 		C397C5892B62C6A9004B084D /* CopyFiles */ = {
 176 | 			isa = PBXCopyFilesBuildPhase;
 177 | 			buildActionMask = 2147483647;
 178 | 			dstPath = /usr/share/man/man1/;
 179 | 			dstSubfolderSpec = 0;
 180 | 			files = (
 181 | 			);
 182 | 			runOnlyForDeploymentPostprocessing = 1;
 183 | 		};
 184 | 		C3A8B3D72B92A0880002EFB8 /* Embed Frameworks */ = {
 185 | 			isa = PBXCopyFilesBuildPhase;
 186 | 			buildActionMask = 2147483647;
 187 | 			dstPath = "";
 188 | 			dstSubfolderSpec = 10;
 189 | 			files = (
 190 | 			);
 191 | 			name = "Embed Frameworks";
 192 | 			runOnlyForDeploymentPostprocessing = 0;
 193 | 		};
 194 | 		C3A8B3FC2B92A3360002EFB8 /* Embed Frameworks */ = {
 195 | 			isa = PBXCopyFilesBuildPhase;
 196 | 			buildActionMask = 2147483647;
 197 | 			dstPath = "";
 198 | 			dstSubfolderSpec = 10;
 199 | 			files = (
 200 | 			);
 201 | 			name = "Embed Frameworks";
 202 | 			runOnlyForDeploymentPostprocessing = 0;
 203 | 		};
 204 | /* End PBXCopyFilesBuildPhase section */
 205 | 
 206 | /* Begin PBXFileReference section */
 207 | 		0AC74EBB2D136221003C90A7 /* VLMEval.app */ = {isa = PBXFileReference; explicitFileType = wrapper.application; includeInIndex = 0; path = VLMEval.app; sourceTree = BUILT_PRODUCTS_DIR; };
 208 | 		0AC74EC92D13622A003C90A7 /* Preview Assets.xcassets */ = {isa = PBXFileReference; lastKnownFileType = folder.assetcatalog; path = "Preview Assets.xcassets"; sourceTree = "<group>"; };
 209 | 		0AC74ECB2D13622A003C90A7 /* Assets.xcassets */ = {isa = PBXFileReference; lastKnownFileType = folder.assetcatalog; path = Assets.xcassets; sourceTree = "<group>"; };
 210 | 		0AC74ECC2D13622A003C90A7 /* ContentView.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = ContentView.swift; sourceTree = "<group>"; };
 211 | 		0AC74ECD2D13622A003C90A7 /* VLMEvalApp.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = VLMEvalApp.swift; sourceTree = "<group>"; };
 212 | 		0AC74ED42D13630D003C90A7 /* VLMEval.entitlements */ = {isa = PBXFileReference; lastKnownFileType = text.plist.entitlements; path = VLMEval.entitlements; sourceTree = "<group>"; };
 213 | 		0AC74F282D1376F1003C90A7 /* README.md */ = {isa = PBXFileReference; lastKnownFileType = net.daringfireball.markdown; path = README.md; sourceTree = "<group>"; };
 214 | 		0F5AD7412DB70C0300745C06 /* MLXChatExample.app */ = {isa = PBXFileReference; explicitFileType = wrapper.application; includeInIndex = 0; path = MLXChatExample.app; sourceTree = BUILT_PRODUCTS_DIR; };
 215 | 		12305EAE2B9D864400C92FEE /* PredictionView.swift */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = sourcecode.swift; path = PredictionView.swift; sourceTree = "<group>"; };
 216 | 		819BEFF62BAF8B4E0002CCEE /* DeviceStat.swift */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = sourcecode.swift; path = DeviceStat.swift; sourceTree = "<group>"; };
 217 | 		C3056BA12BCD973400A31D04 /* test.jsonl */ = {isa = PBXFileReference; lastKnownFileType = text; path = test.jsonl; sourceTree = "<group>"; };
 218 | 		C3056BA22BCD973400A31D04 /* train.jsonl */ = {isa = PBXFileReference; lastKnownFileType = text; path = train.jsonl; sourceTree = "<group>"; };
 219 | 		C3056BA32BCD973400A31D04 /* valid.jsonl */ = {isa = PBXFileReference; lastKnownFileType = text; path = valid.jsonl; sourceTree = "<group>"; };
 220 | 		C3056BA42BCD973400A31D04 /* wikisql.py */ = {isa = PBXFileReference; lastKnownFileType = text.script.python; path = wikisql.py; sourceTree = "<group>"; };
 221 | 		C3056BAB2BCD97B700A31D04 /* LoRATrainingExample.app */ = {isa = PBXFileReference; explicitFileType = wrapper.application; includeInIndex = 0; path = LoRATrainingExample.app; sourceTree = BUILT_PRODUCTS_DIR; };
 222 | 		C3056BAD2BCD97B700A31D04 /* LoRATrainingExampleApp.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = LoRATrainingExampleApp.swift; sourceTree = "<group>"; };
 223 | 		C3056BAF2BCD97B700A31D04 /* ContentView.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = ContentView.swift; sourceTree = "<group>"; };
 224 | 		C3056BB12BCD97B800A31D04 /* Assets.xcassets */ = {isa = PBXFileReference; lastKnownFileType = folder.assetcatalog; path = Assets.xcassets; sourceTree = "<group>"; };
 225 | 		C3056BB32BCD97B800A31D04 /* LoRATrainingExample.entitlements */ = {isa = PBXFileReference; lastKnownFileType = text.plist.entitlements; path = LoRATrainingExample.entitlements; sourceTree = "<group>"; };
 226 | 		C3056BB52BCD97B800A31D04 /* Preview Assets.xcassets */ = {isa = PBXFileReference; lastKnownFileType = folder.assetcatalog; path = "Preview Assets.xcassets"; sourceTree = "<group>"; };
 227 | 		C3056BC42BCDAB8600A31D04 /* README.md */ = {isa = PBXFileReference; lastKnownFileType = net.daringfireball.markdown; path = README.md; sourceTree = "<group>"; };
 228 | 		C3132C232DFB2FFB00270E8E /* ExampleLLM */ = {isa = PBXFileReference; explicitFileType = "compiled.mach-o.executable"; includeInIndex = 0; path = ExampleLLM; sourceTree = BUILT_PRODUCTS_DIR; };
 229 | 		C3132C2A2DFB301A00270E8E /* main.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = main.swift; sourceTree = "<group>"; };
 230 | 		C3208E6E2DB19451006AE6CA /* MLXLMTests.xctest */ = {isa = PBXFileReference; explicitFileType = wrapper.cfbundle; includeInIndex = 0; path = MLXLMTests.xctest; sourceTree = BUILT_PRODUCTS_DIR; };
 231 | 		C325DE3F2B648CDB00628871 /* README.md */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = net.daringfireball.markdown; path = README.md; sourceTree = "<group>"; };
 232 | 		C3288D732B6D9313009FF608 /* LinearModelTraining */ = {isa = PBXFileReference; explicitFileType = "compiled.mach-o.executable"; includeInIndex = 0; path = LinearModelTraining; sourceTree = BUILT_PRODUCTS_DIR; };
 233 | 		C3288D752B6D9313009FF608 /* LinearModelTraining.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = LinearModelTraining.swift; sourceTree = "<group>"; };
 234 | 		C3288D842B6D94BD009FF608 /* README.md */ = {isa = PBXFileReference; lastKnownFileType = net.daringfireball.markdown; path = README.md; sourceTree = "<group>"; };
 235 | 		C34E48F42B696F0B00FCB841 /* LLMTool.swift */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = sourcecode.swift; path = LLMTool.swift; sourceTree = "<group>"; };
 236 | 		C34E48F92B69930300FCB841 /* README.md */ = {isa = PBXFileReference; lastKnownFileType = net.daringfireball.markdown; path = README.md; sourceTree = "<group>"; };
 237 | 		C34E49212B6A026F00FCB841 /* mnist-tool */ = {isa = PBXFileReference; explicitFileType = "compiled.mach-o.executable"; includeInIndex = 0; path = "mnist-tool"; sourceTree = BUILT_PRODUCTS_DIR; };
 238 | 		C34E49232B6A026F00FCB841 /* MNISTTool.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = MNISTTool.swift; sourceTree = "<group>"; };
 239 | 		C36BEFB32BBDEA69002D4AFE /* LoraCommands.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = LoraCommands.swift; sourceTree = "<group>"; };
 240 | 		C36BEFB62BBDECBC002D4AFE /* Arguments.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = Arguments.swift; sourceTree = "<group>"; };
 241 | 		C36BEFE02BC32988002D4AFE /* image-tool */ = {isa = PBXFileReference; explicitFileType = "compiled.mach-o.executable"; includeInIndex = 0; path = "image-tool"; sourceTree = BUILT_PRODUCTS_DIR; };
 242 | 		C36BEFE22BC32988002D4AFE /* ImageTool.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = ImageTool.swift; sourceTree = "<group>"; };
 243 | 		C36BF0012BC5CE55002D4AFE /* StableDiffusionExample.app */ = {isa = PBXFileReference; explicitFileType = wrapper.application; includeInIndex = 0; path = StableDiffusionExample.app; sourceTree = BUILT_PRODUCTS_DIR; };
 244 | 		C36BF0032BC5CE55002D4AFE /* StableDiffusionExampleApp.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = StableDiffusionExampleApp.swift; sourceTree = "<group>"; };
 245 | 		C36BF0052BC5CE55002D4AFE /* ContentView.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = ContentView.swift; sourceTree = "<group>"; };
 246 | 		C36BF0072BC5CE56002D4AFE /* Assets.xcassets */ = {isa = PBXFileReference; lastKnownFileType = folder.assetcatalog; path = Assets.xcassets; sourceTree = "<group>"; };
 247 | 		C36BF0092BC5CE56002D4AFE /* StableDiffusionExample.entitlements */ = {isa = PBXFileReference; lastKnownFileType = text.plist.entitlements; path = StableDiffusionExample.entitlements; sourceTree = "<group>"; };
 248 | 		C36BF00B2BC5CE56002D4AFE /* Preview Assets.xcassets */ = {isa = PBXFileReference; lastKnownFileType = folder.assetcatalog; path = "Preview Assets.xcassets"; sourceTree = "<group>"; };
 249 | 		C36BF0342BC70F11002D4AFE /* Arguments.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = Arguments.swift; sourceTree = "<group>"; };
 250 | 		C37133A12DD6524B00D19830 /* ListCommands.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = ListCommands.swift; sourceTree = "<group>"; };
 251 | 		C38BA3A92DB8321600BAFA88 /* Chat.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = Chat.swift; sourceTree = "<group>"; };
 252 | 		C39273742B606A0A00368D5D /* Tutorial */ = {isa = PBXFileReference; explicitFileType = "compiled.mach-o.executable"; includeInIndex = 0; path = Tutorial; sourceTree = BUILT_PRODUCTS_DIR; };
 253 | 		C392737C2B606A1D00368D5D /* Tutorial.swift */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = sourcecode.swift; path = Tutorial.swift; sourceTree = "<group>"; };
 254 | 		C397C58B2B62C6A9004B084D /* llm-tool */ = {isa = PBXFileReference; explicitFileType = "compiled.mach-o.executable"; includeInIndex = 0; path = "llm-tool"; sourceTree = BUILT_PRODUCTS_DIR; };
 255 | 		C3A5CCF52DFB8FFC004708FF /* README.md */ = {isa = PBXFileReference; lastKnownFileType = net.daringfireball.markdown; path = README.md; sourceTree = "<group>"; };
 256 | 		C3A8B3B22B9295090002EFB8 /* MNISTTrainer.app */ = {isa = PBXFileReference; explicitFileType = wrapper.application; includeInIndex = 0; path = MNISTTrainer.app; sourceTree = BUILT_PRODUCTS_DIR; };
 257 | 		C3A8B3C22B92951E0002EFB8 /* MNISTTrainer-Info.plist */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = text.plist.xml; path = "MNISTTrainer-Info.plist"; sourceTree = "<group>"; };
 258 | 		C3A8B3C32B92951E0002EFB8 /* Assets.xcassets */ = {isa = PBXFileReference; lastKnownFileType = folder.assetcatalog; path = Assets.xcassets; sourceTree = "<group>"; };
 259 | 		C3A8B3C42B92951E0002EFB8 /* MNISTTrainerApp.swift */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = sourcecode.swift; path = MNISTTrainerApp.swift; sourceTree = "<group>"; };
 260 | 		C3A8B3C62B92951E0002EFB8 /* Preview Assets.xcassets */ = {isa = PBXFileReference; lastKnownFileType = folder.assetcatalog; path = "Preview Assets.xcassets"; sourceTree = "<group>"; };
 261 | 		C3A8B3C72B92951E0002EFB8 /* MNISTTrainer.entitlements */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = text.plist.entitlements; path = MNISTTrainer.entitlements; sourceTree = "<group>"; };
 262 | 		C3A8B3C82B92951E0002EFB8 /* README.md */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = net.daringfireball.markdown; path = README.md; sourceTree = "<group>"; };
 263 | 		C3A8B3C92B92951E0002EFB8 /* ContentView.swift */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = sourcecode.swift; path = ContentView.swift; sourceTree = "<group>"; };
 264 | 		C3A8B3DC2B92A29E0002EFB8 /* LLMEval.app */ = {isa = PBXFileReference; explicitFileType = wrapper.application; includeInIndex = 0; path = LLMEval.app; sourceTree = BUILT_PRODUCTS_DIR; };
 265 | 		C3A8B3EC2B92A2A90002EFB8 /* Assets.xcassets */ = {isa = PBXFileReference; lastKnownFileType = folder.assetcatalog; path = Assets.xcassets; sourceTree = "<group>"; };
 266 | 		C3A8B3ED2B92A2A90002EFB8 /* LLMEvalApp.swift */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = sourcecode.swift; path = LLMEvalApp.swift; sourceTree = "<group>"; };
 267 | 		C3A8B3EF2B92A2A90002EFB8 /* Preview Assets.xcassets */ = {isa = PBXFileReference; lastKnownFileType = folder.assetcatalog; path = "Preview Assets.xcassets"; sourceTree = "<group>"; };
 268 | 		C3A8B3F02B92A2A90002EFB8 /* README.md */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = net.daringfireball.markdown; path = README.md; sourceTree = "<group>"; };
 269 | 		C3A8B3F12B92A2A90002EFB8 /* LLMEval.entitlements */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = text.plist.entitlements; path = LLMEval.entitlements; sourceTree = "<group>"; };
 270 | 		C3A8B3F22B92A2A90002EFB8 /* ContentView.swift */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = sourcecode.swift; path = ContentView.swift; sourceTree = "<group>"; };
 271 | 		C3C3240B2B6CA689007D2D9A /* README.md */ = {isa = PBXFileReference; lastKnownFileType = net.daringfireball.markdown; path = README.md; sourceTree = "<group>"; };
 272 | 		C3C36A6B2CA714600099FFA4 /* Build.xcconfig */ = {isa = PBXFileReference; lastKnownFileType = text.xcconfig; path = Build.xcconfig; sourceTree = "<group>"; };
 273 | 		C3C7C4D92DB16C83000373CF /* AVFoundation.framework */ = {isa = PBXFileReference; lastKnownFileType = wrapper.framework; name = AVFoundation.framework; path = Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS18.4.sdk/System/Library/Frameworks/AVFoundation.framework; sourceTree = DEVELOPER_DIR; };
 274 | 		C3C7C4DB2DB16C89000373CF /* CoreImage.framework */ = {isa = PBXFileReference; lastKnownFileType = wrapper.framework; name = CoreImage.framework; path = Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS18.4.sdk/System/Library/Frameworks/CoreImage.framework; sourceTree = DEVELOPER_DIR; };
 275 | 		C3C7C4DD2DB16CA2000373CF /* CoreAudioTypes.framework */ = {isa = PBXFileReference; lastKnownFileType = wrapper.framework; name = CoreAudioTypes.framework; path = Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS18.4.sdk/System/Library/Frameworks/CoreAudioTypes.framework; sourceTree = DEVELOPER_DIR; };
 276 | 		C3C7C4FA2DB19026000373CF /* AVKit.framework */ = {isa = PBXFileReference; lastKnownFileType = wrapper.framework; name = AVKit.framework; path = Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS18.4.sdk/System/Library/Frameworks/AVKit.framework; sourceTree = DEVELOPER_DIR; };
 277 | 		C3D573052C40701E00857A35 /* README.md */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = net.daringfireball.markdown; path = README.md; sourceTree = "<group>"; };
 278 | 		F8D7023A2BB4E223003D7CF5 /* Package.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = Package.swift; sourceTree = "<group>"; };
 279 | /* End PBXFileReference section */
 280 | 
 281 | /* Begin PBXFileSystemSynchronizedBuildFileExceptionSet section */
 282 | 		C3208F272DB19614006AE6CA /* PBXFileSystemSynchronizedBuildFileExceptionSet */ = {
 283 | 			isa = PBXFileSystemSynchronizedBuildFileExceptionSet;
 284 | 			membershipExceptions = (
 285 | 				MLXLMTests/BaseConfigurationTests.swift,
 286 |         MLXLMTests/ToolTests.swift,
 287 | 				MLXLMTests/EvalTests.swift,
 288 | 				MLXLMTests/StreamlinedTests.swift,
 289 | 				MLXLMTests/UserInputTests.swift,
 290 | 			);
 291 | 			target = C3208E6D2DB19451006AE6CA /* MLXLMTests */;
 292 | 		};
 293 | /* End PBXFileSystemSynchronizedBuildFileExceptionSet section */
 294 | 
 295 | /* Begin PBXFileSystemSynchronizedRootGroup section */
 296 | 		0F5AD7422DB70C0300745C06 /* MLXChatExample */ = {isa = PBXFileSystemSynchronizedRootGroup; explicitFileTypes = {}; explicitFolders = (); path = MLXChatExample; sourceTree = "<group>"; };
 297 | 		C397D92E2CD440EF00B87EE2 /* Libraries */ = {isa = PBXFileSystemSynchronizedRootGroup; explicitFileTypes = {}; explicitFolders = (); path = Libraries; sourceTree = "<group>"; };
 298 | 		C3C7C4222DB16A02000373CF /* Tests */ = {isa = PBXFileSystemSynchronizedRootGroup; exceptions = (C3208F272DB19614006AE6CA /* PBXFileSystemSynchronizedBuildFileExceptionSet */, ); explicitFileTypes = {}; explicitFolders = (); path = Tests; sourceTree = "<group>"; };
 299 | /* End PBXFileSystemSynchronizedRootGroup section */
 300 | 
 301 | /* Begin PBXFrameworksBuildPhase section */
 302 | 		0AC74EB82D136221003C90A7 /* Frameworks */ = {
 303 | 			isa = PBXFrameworksBuildPhase;
 304 | 			buildActionMask = 2147483647;
 305 | 			files = (
 306 | 				C3C7C4FB2DB19026000373CF /* AVKit.framework in Frameworks */,
 307 | 				0A8284222D13863900BEF338 /* MLXVLM in Frameworks */,
 308 | 				C32B4C6D2DA7136000EF663D /* AsyncAlgorithms in Frameworks */,
 309 | 			);
 310 | 			runOnlyForDeploymentPostprocessing = 0;
 311 | 		};
 312 | 		0F5AD73E2DB70C0300745C06 /* Frameworks */ = {
 313 | 			isa = PBXFrameworksBuildPhase;
 314 | 			buildActionMask = 2147483647;
 315 | 			files = (
 316 | 				0F5AD8012DB70E6300745C06 /* MLXVLM in Frameworks */,
 317 | 				0F5AD8002DB70E6300745C06 /* MLXLLM in Frameworks */,
 318 | 			);
 319 | 			runOnlyForDeploymentPostprocessing = 0;
 320 | 		};
 321 | 		C3056BA82BCD97B700A31D04 /* Frameworks */ = {
 322 | 			isa = PBXFrameworksBuildPhase;
 323 | 			buildActionMask = 2147483647;
 324 | 			files = (
 325 | 				C32A18072CFFD1AA0092A5B6 /* MLXLLM in Frameworks */,
 326 | 			);
 327 | 			runOnlyForDeploymentPostprocessing = 0;
 328 | 		};
 329 | 		C3132C202DFB2FFB00270E8E /* Frameworks */ = {
 330 | 			isa = PBXFrameworksBuildPhase;
 331 | 			buildActionMask = 2147483647;
 332 | 			files = (
 333 | 				C3132C2E2DFB317C00270E8E /* MLXVLM in Frameworks */,
 334 | 				C3132C302DFB317C00270E8E /* MLXLLM in Frameworks */,
 335 | 			);
 336 | 			runOnlyForDeploymentPostprocessing = 0;
 337 | 		};
 338 | 		C3208E6B2DB19451006AE6CA /* Frameworks */ = {
 339 | 			isa = PBXFrameworksBuildPhase;
 340 | 			buildActionMask = 2147483647;
 341 | 			files = (
 342 | 				C3208E7B2DB194ED006AE6CA /* MLXLLM in Frameworks */,
 343 | 				C3208ED02DB195A7006AE6CA /* MLXVLM in Frameworks */,
 344 | 				C3208E7A2DB1945D006AE6CA /* MLXNN in Frameworks */,
 345 | 				C3208E762DB1945D006AE6CA /* MLX in Frameworks */,
 346 | 			);
 347 | 			runOnlyForDeploymentPostprocessing = 0;
 348 | 		};
 349 | 		C3288D702B6D9313009FF608 /* Frameworks */ = {
 350 | 			isa = PBXFrameworksBuildPhase;
 351 | 			buildActionMask = 2147483647;
 352 | 			files = (
 353 | 				C32A184C2D00E1540092A5B6 /* MLXOptimizers in Frameworks */,
 354 | 				C32A184A2D00E1540092A5B6 /* MLXNN in Frameworks */,
 355 | 				C32A18482D00E1540092A5B6 /* MLX in Frameworks */,
 356 | 				C3288D7B2B6D9339009FF608 /* ArgumentParser in Frameworks */,
 357 | 			);
 358 | 			runOnlyForDeploymentPostprocessing = 0;
 359 | 		};
 360 | 		C34E491E2B6A026F00FCB841 /* Frameworks */ = {
 361 | 			isa = PBXFrameworksBuildPhase;
 362 | 			buildActionMask = 2147483647;
 363 | 			files = (
 364 | 				C32A18012CFFD1810092A5B6 /* MLXMNIST in Frameworks */,
 365 | 				C34E49292B6A028100FCB841 /* ArgumentParser in Frameworks */,
 366 | 			);
 367 | 			runOnlyForDeploymentPostprocessing = 0;
 368 | 		};
 369 | 		C36BEFDD2BC32988002D4AFE /* Frameworks */ = {
 370 | 			isa = PBXFrameworksBuildPhase;
 371 | 			buildActionMask = 2147483647;
 372 | 			files = (
 373 | 				C36BEFEF2BC329C5002D4AFE /* ArgumentParser in Frameworks */,
 374 | 				C3E7D94D2CF6C9B20056C095 /* StableDiffusion in Frameworks */,
 375 | 				C36BEFF22BC32A9A002D4AFE /* Progress in Frameworks */,
 376 | 			);
 377 | 			runOnlyForDeploymentPostprocessing = 0;
 378 | 		};
 379 | 		C36BEFFE2BC5CE55002D4AFE /* Frameworks */ = {
 380 | 			isa = PBXFrameworksBuildPhase;
 381 | 			buildActionMask = 2147483647;
 382 | 			files = (
 383 | 				C32A18092CFFD1B70092A5B6 /* StableDiffusion in Frameworks */,
 384 | 			);
 385 | 			runOnlyForDeploymentPostprocessing = 0;
 386 | 		};
 387 | 		C39273712B606A0A00368D5D /* Frameworks */ = {
 388 | 			isa = PBXFrameworksBuildPhase;
 389 | 			buildActionMask = 2147483647;
 390 | 			files = (
 391 | 				C32A18462D00E1490092A5B6 /* MLX in Frameworks */,
 392 | 			);
 393 | 			runOnlyForDeploymentPostprocessing = 0;
 394 | 		};
 395 | 		C397C5882B62C6A9004B084D /* Frameworks */ = {
 396 | 			isa = PBXFrameworksBuildPhase;
 397 | 			buildActionMask = 2147483647;
 398 | 			files = (
 399 | 				C32A17FD2CFFB98A0092A5B6 /* MLXLLM in Frameworks */,
 400 | 				C397C59C2B62C6D0004B084D /* ArgumentParser in Frameworks */,
 401 | 				C32A17FF2CFFB98A0092A5B6 /* MLXVLM in Frameworks */,
 402 | 			);
 403 | 			runOnlyForDeploymentPostprocessing = 0;
 404 | 		};
 405 | 		C3A8B3AF2B9295090002EFB8 /* Frameworks */ = {
 406 | 			isa = PBXFrameworksBuildPhase;
 407 | 			buildActionMask = 2147483647;
 408 | 			files = (
 409 | 				C32A18032CFFD1920092A5B6 /* MLXMNIST in Frameworks */,
 410 | 			);
 411 | 			runOnlyForDeploymentPostprocessing = 0;
 412 | 		};
 413 | 		C3A8B3D92B92A29D0002EFB8 /* Frameworks */ = {
 414 | 			isa = PBXFrameworksBuildPhase;
 415 | 			buildActionMask = 2147483647;
 416 | 			files = (
 417 | 				C32A18052CFFD19F0092A5B6 /* MLXLLM in Frameworks */,
 418 | 				81695B412BA373D300F260D8 /* MarkdownUI in Frameworks */,
 419 | 				C32B4C6F2DA71ADC00EF663D /* AsyncAlgorithms in Frameworks */,
 420 | 			);
 421 | 			runOnlyForDeploymentPostprocessing = 0;
 422 | 		};
 423 | /* End PBXFrameworksBuildPhase section */
 424 | 
 425 | /* Begin PBXGroup section */
 426 | 		0AC74ECA2D13622A003C90A7 /* Preview Content */ = {
 427 | 			isa = PBXGroup;
 428 | 			children = (
 429 | 				0AC74EC92D13622A003C90A7 /* Preview Assets.xcassets */,
 430 | 			);
 431 | 			path = "Preview Content";
 432 | 			sourceTree = "<group>";
 433 | 		};
 434 | 		0AC74ECE2D13622A003C90A7 /* VLMEval */ = {
 435 | 			isa = PBXGroup;
 436 | 			children = (
 437 | 				0AC74F282D1376F1003C90A7 /* README.md */,
 438 | 				0AC74ED42D13630D003C90A7 /* VLMEval.entitlements */,
 439 | 				0AC74ECA2D13622A003C90A7 /* Preview Content */,
 440 | 				0AC74ECB2D13622A003C90A7 /* Assets.xcassets */,
 441 | 				0AC74ECC2D13622A003C90A7 /* ContentView.swift */,
 442 | 				0AC74ECD2D13622A003C90A7 /* VLMEvalApp.swift */,
 443 | 			);
 444 | 			path = VLMEval;
 445 | 			sourceTree = "<group>";
 446 | 		};
 447 | 		819BEFF72BAF8B4E0002CCEE /* ViewModels */ = {
 448 | 			isa = PBXGroup;
 449 | 			children = (
 450 | 				819BEFF62BAF8B4E0002CCEE /* DeviceStat.swift */,
 451 | 			);
 452 | 			path = ViewModels;
 453 | 			sourceTree = "<group>";
 454 | 		};
 455 | 		C3056BA52BCD973400A31D04 /* lora */ = {
 456 | 			isa = PBXGroup;
 457 | 			children = (
 458 | 				C3056BA12BCD973400A31D04 /* test.jsonl */,
 459 | 				C3056BA22BCD973400A31D04 /* train.jsonl */,
 460 | 				C3056BA32BCD973400A31D04 /* valid.jsonl */,
 461 | 				C3056BA42BCD973400A31D04 /* wikisql.py */,
 462 | 			);
 463 | 			path = lora;
 464 | 			sourceTree = "<group>";
 465 | 		};
 466 | 		C3056BA62BCD973400A31D04 /* Data */ = {
 467 | 			isa = PBXGroup;
 468 | 			children = (
 469 | 				C3056BA52BCD973400A31D04 /* lora */,
 470 | 			);
 471 | 			path = Data;
 472 | 			sourceTree = "<group>";
 473 | 		};
 474 | 		C3056BAC2BCD97B700A31D04 /* LoRATrainingExample */ = {
 475 | 			isa = PBXGroup;
 476 | 			children = (
 477 | 				C3056BAD2BCD97B700A31D04 /* LoRATrainingExampleApp.swift */,
 478 | 				C3056BAF2BCD97B700A31D04 /* ContentView.swift */,
 479 | 				C3056BB12BCD97B800A31D04 /* Assets.xcassets */,
 480 | 				C3056BB32BCD97B800A31D04 /* LoRATrainingExample.entitlements */,
 481 | 				C3056BB42BCD97B800A31D04 /* Preview Content */,
 482 | 				C3056BC42BCDAB8600A31D04 /* README.md */,
 483 | 			);
 484 | 			path = LoRATrainingExample;
 485 | 			sourceTree = "<group>";
 486 | 		};
 487 | 		C3056BB42BCD97B800A31D04 /* Preview Content */ = {
 488 | 			isa = PBXGroup;
 489 | 			children = (
 490 | 				C3056BB52BCD97B800A31D04 /* Preview Assets.xcassets */,
 491 | 			);
 492 | 			path = "Preview Content";
 493 | 			sourceTree = "<group>";
 494 | 		};
 495 | 		C3132C2B2DFB301A00270E8E /* ExampleLLM */ = {
 496 | 			isa = PBXGroup;
 497 | 			children = (
 498 | 				C3132C2A2DFB301A00270E8E /* main.swift */,
 499 | 				C3A5CCF52DFB8FFC004708FF /* README.md */,
 500 | 			);
 501 | 			path = ExampleLLM;
 502 | 			sourceTree = "<group>";
 503 | 		};
 504 | 		C3288D742B6D9313009FF608 /* LinearModelTraining */ = {
 505 | 			isa = PBXGroup;
 506 | 			children = (
 507 | 				C3288D752B6D9313009FF608 /* LinearModelTraining.swift */,
 508 | 				C3288D842B6D94BD009FF608 /* README.md */,
 509 | 			);
 510 | 			path = LinearModelTraining;
 511 | 			sourceTree = "<group>";
 512 | 		};
 513 | 		C34E48F32B696F0B00FCB841 /* llm-tool */ = {
 514 | 			isa = PBXGroup;
 515 | 			children = (
 516 | 				C34E48F92B69930300FCB841 /* README.md */,
 517 | 				C34E48F42B696F0B00FCB841 /* LLMTool.swift */,
 518 | 				C36BEFB32BBDEA69002D4AFE /* LoraCommands.swift */,
 519 | 				C36BEFB62BBDECBC002D4AFE /* Arguments.swift */,
 520 | 				C38BA3A92DB8321600BAFA88 /* Chat.swift */,
 521 | 				C37133A12DD6524B00D19830 /* ListCommands.swift */,
 522 | 			);
 523 | 			path = "llm-tool";
 524 | 			sourceTree = "<group>";
 525 | 		};
 526 | 		C34E49222B6A026F00FCB841 /* mnist-tool */ = {
 527 | 			isa = PBXGroup;
 528 | 			children = (
 529 | 				C34E49232B6A026F00FCB841 /* MNISTTool.swift */,
 530 | 				C3C3240B2B6CA689007D2D9A /* README.md */,
 531 | 			);
 532 | 			path = "mnist-tool";
 533 | 			sourceTree = "<group>";
 534 | 		};
 535 | 		C36BEFE12BC32988002D4AFE /* image-tool */ = {
 536 | 			isa = PBXGroup;
 537 | 			children = (
 538 | 				C36BEFE22BC32988002D4AFE /* ImageTool.swift */,
 539 | 				C36BF0342BC70F11002D4AFE /* Arguments.swift */,
 540 | 			);
 541 | 			path = "image-tool";
 542 | 			sourceTree = "<group>";
 543 | 		};
 544 | 		C36BF0022BC5CE55002D4AFE /* StableDiffusionExample */ = {
 545 | 			isa = PBXGroup;
 546 | 			children = (
 547 | 				C3D573052C40701E00857A35 /* README.md */,
 548 | 				C36BF0032BC5CE55002D4AFE /* StableDiffusionExampleApp.swift */,
 549 | 				C36BF0052BC5CE55002D4AFE /* ContentView.swift */,
 550 | 				C36BF0072BC5CE56002D4AFE /* Assets.xcassets */,
 551 | 				C36BF0092BC5CE56002D4AFE /* StableDiffusionExample.entitlements */,
 552 | 				C36BF00A2BC5CE56002D4AFE /* Preview Content */,
 553 | 			);
 554 | 			path = StableDiffusionExample;
 555 | 			sourceTree = "<group>";
 556 | 		};
 557 | 		C36BF00A2BC5CE56002D4AFE /* Preview Content */ = {
 558 | 			isa = PBXGroup;
 559 | 			children = (
 560 | 				C36BF00B2BC5CE56002D4AFE /* Preview Assets.xcassets */,
 561 | 			);
 562 | 			path = "Preview Content";
 563 | 			sourceTree = "<group>";
 564 | 		};
 565 | 		C39273672B60697700368D5D = {
 566 | 			isa = PBXGroup;
 567 | 			children = (
 568 | 				C325DE3F2B648CDB00628871 /* README.md */,
 569 | 				F8D7023A2BB4E223003D7CF5 /* Package.swift */,
 570 | 				C3C36A6C2CA714600099FFA4 /* Configuration */,
 571 | 				C3056BA62BCD973400A31D04 /* Data */,
 572 | 				C397D92E2CD440EF00B87EE2 /* Libraries */,
 573 | 				C3C7C4222DB16A02000373CF /* Tests */,
 574 | 				C3A8B3AD2B9294E30002EFB8 /* Applications */,
 575 | 				C39273812B606A7400368D5D /* Tools */,
 576 | 				C39273752B606A0A00368D5D /* Products */,
 577 | 				C392737E2B606A2C00368D5D /* Frameworks */,
 578 | 			);
 579 | 			sourceTree = "<group>";
 580 | 		};
 581 | 		C39273752B606A0A00368D5D /* Products */ = {
 582 | 			isa = PBXGroup;
 583 | 			children = (
 584 | 				C39273742B606A0A00368D5D /* Tutorial */,
 585 | 				C397C58B2B62C6A9004B084D /* llm-tool */,
 586 | 				C34E49212B6A026F00FCB841 /* mnist-tool */,
 587 | 				C3288D732B6D9313009FF608 /* LinearModelTraining */,
 588 | 				C3A8B3B22B9295090002EFB8 /* MNISTTrainer.app */,
 589 | 				C3A8B3DC2B92A29E0002EFB8 /* LLMEval.app */,
 590 | 				C3056BAB2BCD97B700A31D04 /* LoRATrainingExample.app */,
 591 | 				C36BEFE02BC32988002D4AFE /* image-tool */,
 592 | 				C36BF0012BC5CE55002D4AFE /* StableDiffusionExample.app */,
 593 | 				0AC74EBB2D136221003C90A7 /* VLMEval.app */,
 594 | 				C3208E6E2DB19451006AE6CA /* MLXLMTests.xctest */,
 595 | 				0F5AD7412DB70C0300745C06 /* MLXChatExample.app */,
 596 | 				C3132C232DFB2FFB00270E8E /* ExampleLLM */,
 597 | 			);
 598 | 			name = Products;
 599 | 			sourceTree = "<group>";
 600 | 		};
 601 | 		C39273762B606A0A00368D5D /* Tutorial */ = {
 602 | 			isa = PBXGroup;
 603 | 			children = (
 604 | 				C392737C2B606A1D00368D5D /* Tutorial.swift */,
 605 | 			);
 606 | 			path = Tutorial;
 607 | 			sourceTree = "<group>";
 608 | 		};
 609 | 		C392737E2B606A2C00368D5D /* Frameworks */ = {
 610 | 			isa = PBXGroup;
 611 | 			children = (
 612 | 				C3C7C4FA2DB19026000373CF /* AVKit.framework */,
 613 | 				C3C7C4DD2DB16CA2000373CF /* CoreAudioTypes.framework */,
 614 | 				C3C7C4DB2DB16C89000373CF /* CoreImage.framework */,
 615 | 				C3C7C4D92DB16C83000373CF /* AVFoundation.framework */,
 616 | 			);
 617 | 			name = Frameworks;
 618 | 			sourceTree = "<group>";
 619 | 		};
 620 | 		C39273812B606A7400368D5D /* Tools */ = {
 621 | 			isa = PBXGroup;
 622 | 			children = (
 623 | 				C3132C2B2DFB301A00270E8E /* ExampleLLM */,
 624 | 				C36BEFE12BC32988002D4AFE /* image-tool */,
 625 | 				C3288D742B6D9313009FF608 /* LinearModelTraining */,
 626 | 				C34E49222B6A026F00FCB841 /* mnist-tool */,
 627 | 				C34E48F32B696F0B00FCB841 /* llm-tool */,
 628 | 				C39273762B606A0A00368D5D /* Tutorial */,
 629 | 			);
 630 | 			path = Tools;
 631 | 			sourceTree = "<group>";
 632 | 		};
 633 | 		C3A8B3AD2B9294E30002EFB8 /* Applications */ = {
 634 | 			isa = PBXGroup;
 635 | 			children = (
 636 | 				C3056BAC2BCD97B700A31D04 /* LoRATrainingExample */,
 637 | 				C36BF0022BC5CE55002D4AFE /* StableDiffusionExample */,
 638 | 				C3A8B3EB2B92A2A90002EFB8 /* LLMEval */,
 639 | 				C3A8B3C12B92951E0002EFB8 /* MNISTTrainer */,
 640 | 				0AC74ECE2D13622A003C90A7 /* VLMEval */,
 641 | 				0F5AD7422DB70C0300745C06 /* MLXChatExample */,
 642 | 			);
 643 | 			path = Applications;
 644 | 			sourceTree = "<group>";
 645 | 		};
 646 | 		C3A8B3C12B92951E0002EFB8 /* MNISTTrainer */ = {
 647 | 			isa = PBXGroup;
 648 | 			children = (
 649 | 				C3A8B3C32B92951E0002EFB8 /* Assets.xcassets */,
 650 | 				C3A8B3C92B92951E0002EFB8 /* ContentView.swift */,
 651 | 				12305EAE2B9D864400C92FEE /* PredictionView.swift */,
 652 | 				C3A8B3C22B92951E0002EFB8 /* MNISTTrainer-Info.plist */,
 653 | 				C3A8B3C72B92951E0002EFB8 /* MNISTTrainer.entitlements */,
 654 | 				C3A8B3C42B92951E0002EFB8 /* MNISTTrainerApp.swift */,
 655 | 				C3A8B3C52B92951E0002EFB8 /* Preview Content */,
 656 | 				C3A8B3C82B92951E0002EFB8 /* README.md */,
 657 | 			);
 658 | 			path = MNISTTrainer;
 659 | 			sourceTree = "<group>";
 660 | 		};
 661 | 		C3A8B3C52B92951E0002EFB8 /* Preview Content */ = {
 662 | 			isa = PBXGroup;
 663 | 			children = (
 664 | 				C3A8B3C62B92951E0002EFB8 /* Preview Assets.xcassets */,
 665 | 			);
 666 | 			path = "Preview Content";
 667 | 			sourceTree = "<group>";
 668 | 		};
 669 | 		C3A8B3EB2B92A2A90002EFB8 /* LLMEval */ = {
 670 | 			isa = PBXGroup;
 671 | 			children = (
 672 | 				819BEFF72BAF8B4E0002CCEE /* ViewModels */,
 673 | 				C3A8B3EC2B92A2A90002EFB8 /* Assets.xcassets */,
 674 | 				C3A8B3F22B92A2A90002EFB8 /* ContentView.swift */,
 675 | 				C3A8B3F12B92A2A90002EFB8 /* LLMEval.entitlements */,
 676 | 				C3A8B3ED2B92A2A90002EFB8 /* LLMEvalApp.swift */,
 677 | 				C3A8B3EE2B92A2A90002EFB8 /* Preview Content */,
 678 | 				C3A8B3F02B92A2A90002EFB8 /* README.md */,
 679 | 			);
 680 | 			path = LLMEval;
 681 | 			sourceTree = "<group>";
 682 | 		};
 683 | 		C3A8B3EE2B92A2A90002EFB8 /* Preview Content */ = {
 684 | 			isa = PBXGroup;
 685 | 			children = (
 686 | 				C3A8B3EF2B92A2A90002EFB8 /* Preview Assets.xcassets */,
 687 | 			);
 688 | 			path = "Preview Content";
 689 | 			sourceTree = "<group>";
 690 | 		};
 691 | 		C3C36A6C2CA714600099FFA4 /* Configuration */ = {
 692 | 			isa = PBXGroup;
 693 | 			children = (
 694 | 				C3C36A6B2CA714600099FFA4 /* Build.xcconfig */,
 695 | 			);
 696 | 			path = Configuration;
 697 | 			sourceTree = "<group>";
 698 | 		};
 699 | /* End PBXGroup section */
 700 | 
 701 | /* Begin PBXNativeTarget section */
 702 | 		0AC74EBA2D136221003C90A7 /* VLMEval */ = {
 703 | 			isa = PBXNativeTarget;
 704 | 			buildConfigurationList = 0AC74EC62D136223003C90A7 /* Build configuration list for PBXNativeTarget "VLMEval" */;
 705 | 			buildPhases = (
 706 | 				0AC74EB72D136221003C90A7 /* Sources */,
 707 | 				0AC74EB82D136221003C90A7 /* Frameworks */,
 708 | 				0AC74EB92D136221003C90A7 /* Resources */,
 709 | 			);
 710 | 			buildRules = (
 711 | 			);
 712 | 			dependencies = (
 713 | 			);
 714 | 			name = VLMEval;
 715 | 			packageProductDependencies = (
 716 | 				0AC74ED92D136223003C90A7 /* MLXVLM */,
 717 | 				C32B4C6C2DA7136000EF663D /* AsyncAlgorithms */,
 718 | 			);
 719 | 			productName = VLMEval;
 720 | 			productReference = 0AC74EBB2D136221003C90A7 /* VLMEval.app */;
 721 | 			productType = "com.apple.product-type.application";
 722 | 		};
 723 | 		0F5AD7402DB70C0300745C06 /* MLXChatExample */ = {
 724 | 			isa = PBXNativeTarget;
 725 | 			buildConfigurationList = 0F5AD74C2DB70C0400745C06 /* Build configuration list for PBXNativeTarget "MLXChatExample" */;
 726 | 			buildPhases = (
 727 | 				0F5AD73D2DB70C0300745C06 /* Sources */,
 728 | 				0F5AD73E2DB70C0300745C06 /* Frameworks */,
 729 | 				0F5AD73F2DB70C0300745C06 /* Resources */,
 730 | 			);
 731 | 			buildRules = (
 732 | 			);
 733 | 			dependencies = (
 734 | 			);
 735 | 			fileSystemSynchronizedGroups = (
 736 | 				0F5AD7422DB70C0300745C06 /* MLXChatExample */,
 737 | 			);
 738 | 			name = MLXChatExample;
 739 | 			packageProductDependencies = (
 740 | 				C32A17FC2CFFB98A0092A5B6 /* MLXLLM */,
 741 | 				C32A17FE2CFFB98A0092A5B6 /* MLXVLM */,
 742 | 			);
 743 | 			productName = MLXChatExample;
 744 | 			productReference = 0F5AD7412DB70C0300745C06 /* MLXChatExample.app */;
 745 | 			productType = "com.apple.product-type.application";
 746 | 		};
 747 | 		C3056BAA2BCD97B700A31D04 /* LoRATrainingExample */ = {
 748 | 			isa = PBXNativeTarget;
 749 | 			buildConfigurationList = C3056BB72BCD97B800A31D04 /* Build configuration list for PBXNativeTarget "LoRATrainingExample" */;
 750 | 			buildPhases = (
 751 | 				C3056BA72BCD97B700A31D04 /* Sources */,
 752 | 				C3056BA82BCD97B700A31D04 /* Frameworks */,
 753 | 				C3056BA92BCD97B700A31D04 /* Resources */,
 754 | 				C3056BC12BCD984F00A31D04 /* Embed Frameworks */,
 755 | 			);
 756 | 			buildRules = (
 757 | 			);
 758 | 			dependencies = (
 759 | 			);
 760 | 			name = LoRATrainingExample;
 761 | 			packageProductDependencies = (
 762 | 				C32A18062CFFD1AA0092A5B6 /* MLXLLM */,
 763 | 			);
 764 | 			productName = LoRATrainingExample;
 765 | 			productReference = C3056BAB2BCD97B700A31D04 /* LoRATrainingExample.app */;
 766 | 			productType = "com.apple.product-type.application";
 767 | 		};
 768 | 		C3132C222DFB2FFB00270E8E /* ExampleLLM */ = {
 769 | 			isa = PBXNativeTarget;
 770 | 			buildConfigurationList = C3132C272DFB2FFB00270E8E /* Build configuration list for PBXNativeTarget "ExampleLLM" */;
 771 | 			buildPhases = (
 772 | 				C3132C1F2DFB2FFB00270E8E /* Sources */,
 773 | 				C3132C202DFB2FFB00270E8E /* Frameworks */,
 774 | 				C3132C212DFB2FFB00270E8E /* CopyFiles */,
 775 | 			);
 776 | 			buildRules = (
 777 | 			);
 778 | 			dependencies = (
 779 | 			);
 780 | 			name = ExampleLLM;
 781 | 			packageProductDependencies = (
 782 | 				C3132C2D2DFB317C00270E8E /* MLXVLM */,
 783 | 				C3132C2F2DFB317C00270E8E /* MLXLLM */,
 784 | 			);
 785 | 			productName = ExampleLLM;
 786 | 			productReference = C3132C232DFB2FFB00270E8E /* ExampleLLM */;
 787 | 			productType = "com.apple.product-type.tool";
 788 | 		};
 789 | 		C3208E6D2DB19451006AE6CA /* MLXLMTests */ = {
 790 | 			isa = PBXNativeTarget;
 791 | 			buildConfigurationList = C3208E722DB19451006AE6CA /* Build configuration list for PBXNativeTarget "MLXLMTests" */;
 792 | 			buildPhases = (
 793 | 				C3208E6A2DB19451006AE6CA /* Sources */,
 794 | 				C3208E6B2DB19451006AE6CA /* Frameworks */,
 795 | 				C3208E6C2DB19451006AE6CA /* Resources */,
 796 | 			);
 797 | 			buildRules = (
 798 | 			);
 799 | 			dependencies = (
 800 | 			);
 801 | 			name = MLXLMTests;
 802 | 			packageProductDependencies = (
 803 | 				C3208E752DB1945D006AE6CA /* MLX */,
 804 | 				C3208E792DB1945D006AE6CA /* MLXNN */,
 805 | 			);
 806 | 			productName = MLXLMTests;
 807 | 			productReference = C3208E6E2DB19451006AE6CA /* MLXLMTests.xctest */;
 808 | 			productType = "com.apple.product-type.bundle.unit-test";
 809 | 		};
 810 | 		C3288D722B6D9313009FF608 /* LinearModelTraining */ = {
 811 | 			isa = PBXNativeTarget;
 812 | 			buildConfigurationList = C3288D792B6D9313009FF608 /* Build configuration list for PBXNativeTarget "LinearModelTraining" */;
 813 | 			buildPhases = (
 814 | 				C3288D6F2B6D9313009FF608 /* Sources */,
 815 | 				C3288D702B6D9313009FF608 /* Frameworks */,
 816 | 				C3288D712B6D9313009FF608 /* CopyFiles */,
 817 | 			);
 818 | 			buildRules = (
 819 | 			);
 820 | 			dependencies = (
 821 | 			);
 822 | 			name = LinearModelTraining;
 823 | 			packageProductDependencies = (
 824 | 				C3288D7A2B6D9339009FF608 /* ArgumentParser */,
 825 | 				C32A18472D00E1540092A5B6 /* MLX */,
 826 | 				C32A18492D00E1540092A5B6 /* MLXNN */,
 827 | 				C32A184B2D00E1540092A5B6 /* MLXOptimizers */,
 828 | 			);
 829 | 			productName = LinearFunctionModelTraining;
 830 | 			productReference = C3288D732B6D9313009FF608 /* LinearModelTraining */;
 831 | 			productType = "com.apple.product-type.tool";
 832 | 		};
 833 | 		C34E49202B6A026F00FCB841 /* mnist-tool */ = {
 834 | 			isa = PBXNativeTarget;
 835 | 			buildConfigurationList = C34E49252B6A026F00FCB841 /* Build configuration list for PBXNativeTarget "mnist-tool" */;
 836 | 			buildPhases = (
 837 | 				C34E491D2B6A026F00FCB841 /* Sources */,
 838 | 				C34E491E2B6A026F00FCB841 /* Frameworks */,
 839 | 				C34E491F2B6A026F00FCB841 /* CopyFiles */,
 840 | 				C34E492E2B6A028800FCB841 /* Embed Frameworks */,
 841 | 			);
 842 | 			buildRules = (
 843 | 			);
 844 | 			dependencies = (
 845 | 			);
 846 | 			name = "mnist-tool";
 847 | 			packageProductDependencies = (
 848 | 				C34E49282B6A028100FCB841 /* ArgumentParser */,
 849 | 				C32A18002CFFD1810092A5B6 /* MLXMNIST */,
 850 | 			);
 851 | 			productName = "mnist-tool";
 852 | 			productReference = C34E49212B6A026F00FCB841 /* mnist-tool */;
 853 | 			productType = "com.apple.product-type.tool";
 854 | 		};
 855 | 		C36BEFDF2BC32988002D4AFE /* image-tool */ = {
 856 | 			isa = PBXNativeTarget;
 857 | 			buildConfigurationList = C36BEFE42BC32988002D4AFE /* Build configuration list for PBXNativeTarget "image-tool" */;
 858 | 			buildPhases = (
 859 | 				C36BEFDC2BC32988002D4AFE /* Sources */,
 860 | 				C36BEFDD2BC32988002D4AFE /* Frameworks */,
 861 | 				C36BEFDE2BC32988002D4AFE /* CopyFiles */,
 862 | 				C36BEFEB2BC329AB002D4AFE /* Embed Frameworks */,
 863 | 			);
 864 | 			buildRules = (
 865 | 			);
 866 | 			dependencies = (
 867 | 			);
 868 | 			name = "image-tool";
 869 | 			packageProductDependencies = (
 870 | 				C36BEFEE2BC329C5002D4AFE /* ArgumentParser */,
 871 | 				C36BEFF12BC32A9A002D4AFE /* Progress */,
 872 | 				C3E7D94C2CF6C9B20056C095 /* StableDiffusion */,
 873 | 			);
 874 | 			productName = "image-tool";
 875 | 			productReference = C36BEFE02BC32988002D4AFE /* image-tool */;
 876 | 			productType = "com.apple.product-type.tool";
 877 | 		};
 878 | 		C36BF0002BC5CE55002D4AFE /* StableDiffusionExample */ = {
 879 | 			isa = PBXNativeTarget;
 880 | 			buildConfigurationList = C36BF00D2BC5CE56002D4AFE /* Build configuration list for PBXNativeTarget "StableDiffusionExample" */;
 881 | 			buildPhases = (
 882 | 				C36BEFFD2BC5CE55002D4AFE /* Sources */,
 883 | 				C36BEFFE2BC5CE55002D4AFE /* Frameworks */,
 884 | 				C36BEFFF2BC5CE55002D4AFE /* Resources */,
 885 | 				C36BF0142BC5CF17002D4AFE /* Embed Frameworks */,
 886 | 			);
 887 | 			buildRules = (
 888 | 			);
 889 | 			dependencies = (
 890 | 			);
 891 | 			name = StableDiffusionExample;
 892 | 			productName = StableDiffusionEval;
 893 | 			productReference = C36BF0012BC5CE55002D4AFE /* StableDiffusionExample.app */;
 894 | 			productType = "com.apple.product-type.application";
 895 | 		};
 896 | 		C39273732B606A0A00368D5D /* Tutorial */ = {
 897 | 			isa = PBXNativeTarget;
 898 | 			buildConfigurationList = C39273792B606A0A00368D5D /* Build configuration list for PBXNativeTarget "Tutorial" */;
 899 | 			buildPhases = (
 900 | 				C39273702B606A0A00368D5D /* Sources */,
 901 | 				C39273712B606A0A00368D5D /* Frameworks */,
 902 | 				C39273722B606A0A00368D5D /* CopyFiles */,
 903 | 			);
 904 | 			buildRules = (
 905 | 			);
 906 | 			dependencies = (
 907 | 			);
 908 | 			name = Tutorial;
 909 | 			packageProductDependencies = (
 910 | 				C32A18452D00E1490092A5B6 /* MLX */,
 911 | 			);
 912 | 			productName = Tutorial;
 913 | 			productReference = C39273742B606A0A00368D5D /* Tutorial */;
 914 | 			productType = "com.apple.product-type.tool";
 915 | 		};
 916 | 		C397C58A2B62C6A9004B084D /* llm-tool */ = {
 917 | 			isa = PBXNativeTarget;
 918 | 			buildConfigurationList = C397C58F2B62C6A9004B084D /* Build configuration list for PBXNativeTarget "llm-tool" */;
 919 | 			buildPhases = (
 920 | 				C397C5872B62C6A9004B084D /* Sources */,
 921 | 				C397C5882B62C6A9004B084D /* Frameworks */,
 922 | 				C397C5892B62C6A9004B084D /* CopyFiles */,
 923 | 				C38935DB2B869CCE0037B833 /* Embed Frameworks */,
 924 | 			);
 925 | 			buildRules = (
 926 | 			);
 927 | 			dependencies = (
 928 | 			);
 929 | 			name = "llm-tool";
 930 | 			packageProductDependencies = (
 931 | 				C397C59B2B62C6D0004B084D /* ArgumentParser */,
 932 | 				C32A17FC2CFFB98A0092A5B6 /* MLXLLM */,
 933 | 				C32A17FE2CFFB98A0092A5B6 /* MLXVLM */,
 934 | 			);
 935 | 			productName = "mistral-tool";
 936 | 			productReference = C397C58B2B62C6A9004B084D /* llm-tool */;
 937 | 			productType = "com.apple.product-type.tool";
 938 | 		};
 939 | 		C3A8B3B12B9295090002EFB8 /* MNISTTrainer */ = {
 940 | 			isa = PBXNativeTarget;
 941 | 			buildConfigurationList = C3A8B3BE2B92950A0002EFB8 /* Build configuration list for PBXNativeTarget "MNISTTrainer" */;
 942 | 			buildPhases = (
 943 | 				C3A8B3AE2B9295090002EFB8 /* Sources */,
 944 | 				C3A8B3AF2B9295090002EFB8 /* Frameworks */,
 945 | 				C3A8B3B02B9295090002EFB8 /* Resources */,
 946 | 				C3A8B3D72B92A0880002EFB8 /* Embed Frameworks */,
 947 | 			);
 948 | 			buildRules = (
 949 | 			);
 950 | 			dependencies = (
 951 | 			);
 952 | 			name = MNISTTrainer;
 953 | 			packageProductDependencies = (
 954 | 				C32A18022CFFD1920092A5B6 /* MLXMNIST */,
 955 | 			);
 956 | 			productName = MNISTTrainer;
 957 | 			productReference = C3A8B3B22B9295090002EFB8 /* MNISTTrainer.app */;
 958 | 			productType = "com.apple.product-type.application";
 959 | 		};
 960 | 		C3A8B3DB2B92A29D0002EFB8 /* LLMEval */ = {
 961 | 			isa = PBXNativeTarget;
 962 | 			buildConfigurationList = C3A8B3E82B92A29E0002EFB8 /* Build configuration list for PBXNativeTarget "LLMEval" */;
 963 | 			buildPhases = (
 964 | 				C3A8B3D82B92A29D0002EFB8 /* Sources */,
 965 | 				C3A8B3D92B92A29D0002EFB8 /* Frameworks */,
 966 | 				C3A8B3DA2B92A29D0002EFB8 /* Resources */,
 967 | 				C3A8B3FC2B92A3360002EFB8 /* Embed Frameworks */,
 968 | 			);
 969 | 			buildRules = (
 970 | 			);
 971 | 			dependencies = (
 972 | 			);
 973 | 			name = LLMEval;
 974 | 			packageProductDependencies = (
 975 | 				81695B402BA373D300F260D8 /* MarkdownUI */,
 976 | 				C32A18042CFFD19F0092A5B6 /* MLXLLM */,
 977 | 				C32B4C6E2DA71ADC00EF663D /* AsyncAlgorithms */,
 978 | 			);
 979 | 			productName = LLMEval;
 980 | 			productReference = C3A8B3DC2B92A29E0002EFB8 /* LLMEval.app */;
 981 | 			productType = "com.apple.product-type.application";
 982 | 		};
 983 | /* End PBXNativeTarget section */
 984 | 
 985 | /* Begin PBXProject section */
 986 | 		C39273682B60697700368D5D /* Project object */ = {
 987 | 			isa = PBXProject;
 988 | 			attributes = {
 989 | 				BuildIndependentTargetsInParallel = 1;
 990 | 				LastSwiftUpdateCheck = 1630;
 991 | 				LastUpgradeCheck = 1500;
 992 | 				TargetAttributes = {
 993 | 					0F5AD7402DB70C0300745C06 = {
 994 | 						CreatedOnToolsVersion = 16.2;
 995 | 					};
 996 | 					C3056BAA2BCD97B700A31D04 = {
 997 | 						CreatedOnToolsVersion = 15.3;
 998 | 					};
 999 | 					C3132C222DFB2FFB00270E8E = {
1000 | 						CreatedOnToolsVersion = 16.3;
1001 | 					};
1002 | 					C3288D722B6D9313009FF608 = {
1003 | 						CreatedOnToolsVersion = 15.0.1;
1004 | 					};
1005 | 					C34E49202B6A026F00FCB841 = {
1006 | 						CreatedOnToolsVersion = 15.0.1;
1007 | 					};
1008 | 					C36BEFDF2BC32988002D4AFE = {
1009 | 						CreatedOnToolsVersion = 15.3;
1010 | 					};
1011 | 					C36BF0002BC5CE55002D4AFE = {
1012 | 						CreatedOnToolsVersion = 15.3;
1013 | 					};
1014 | 					C39273732B606A0A00368D5D = {
1015 | 						CreatedOnToolsVersion = 15.0.1;
1016 | 					};
1017 | 					C397C58A2B62C6A9004B084D = {
1018 | 						CreatedOnToolsVersion = 15.0.1;
1019 | 					};
1020 | 					C3A8B3B12B9295090002EFB8 = {
1021 | 						CreatedOnToolsVersion = 15.2;
1022 | 					};
1023 | 					C3A8B3DB2B92A29D0002EFB8 = {
1024 | 						CreatedOnToolsVersion = 15.2;
1025 | 					};
1026 | 				};
1027 | 			};
1028 | 			buildConfigurationList = C392736B2B60697700368D5D /* Build configuration list for PBXProject "mlx-swift-examples" */;
1029 | 			compatibilityVersion = "Xcode 14.0";
1030 | 			developmentRegion = en;
1031 | 			hasScannedForEncodings = 0;
1032 | 			knownRegions = (
1033 | 				en,
1034 | 				Base,
1035 | 			);
1036 | 			mainGroup = C39273672B60697700368D5D;
1037 | 			packageReferences = (
1038 | 				C392736E2B60699100368D5D /* XCRemoteSwiftPackageReference "swift-argument-parser" */,
1039 | 				C34E491A2B69C43600FCB841 /* XCRemoteSwiftPackageReference "GzipSwift" */,
1040 | 				81695B3F2BA373D300F260D8 /* XCRemoteSwiftPackageReference "swift-markdown-ui" */,
1041 | 				C36BEFF02BC32A8C002D4AFE /* XCRemoteSwiftPackageReference "Progress" */,
1042 | 				C397D8F22CD2F60B00B87EE2 /* XCLocalSwiftPackageReference "Libraries/.." */,
1043 | 				C32A18442D00E13E0092A5B6 /* XCRemoteSwiftPackageReference "mlx-swift" */,
1044 | 				C32B4C6B2DA7132C00EF663D /* XCRemoteSwiftPackageReference "swift-async-algorithms" */,
1045 | 			);
1046 | 			productRefGroup = C39273752B606A0A00368D5D /* Products */;
1047 | 			projectDirPath = "";
1048 | 			projectRoot = "";
1049 | 			targets = (
1050 | 				C39273732B606A0A00368D5D /* Tutorial */,
1051 | 				C397C58A2B62C6A9004B084D /* llm-tool */,
1052 | 				C34E49202B6A026F00FCB841 /* mnist-tool */,
1053 | 				C3288D722B6D9313009FF608 /* LinearModelTraining */,
1054 | 				C3A8B3B12B9295090002EFB8 /* MNISTTrainer */,
1055 | 				C3A8B3DB2B92A29D0002EFB8 /* LLMEval */,
1056 | 				C3056BAA2BCD97B700A31D04 /* LoRATrainingExample */,
1057 | 				C36BEFDF2BC32988002D4AFE /* image-tool */,
1058 | 				C36BF0002BC5CE55002D4AFE /* StableDiffusionExample */,
1059 | 				0AC74EBA2D136221003C90A7 /* VLMEval */,
1060 | 				C3208E6D2DB19451006AE6CA /* MLXLMTests */,
1061 | 				0F5AD7402DB70C0300745C06 /* MLXChatExample */,
1062 | 				C3132C222DFB2FFB00270E8E /* ExampleLLM */,
1063 | 			);
1064 | 		};
1065 | /* End PBXProject section */
1066 | 
1067 | /* Begin PBXResourcesBuildPhase section */
1068 | 		0AC74EB92D136221003C90A7 /* Resources */ = {
1069 | 			isa = PBXResourcesBuildPhase;
1070 | 			buildActionMask = 2147483647;
1071 | 			files = (
1072 | 				0AC74ECF2D13622A003C90A7 /* Preview Assets.xcassets in Resources */,
1073 | 				0AC74ED02D13622A003C90A7 /* Assets.xcassets in Resources */,
1074 | 			);
1075 | 			runOnlyForDeploymentPostprocessing = 0;
1076 | 		};
1077 | 		0F5AD73F2DB70C0300745C06 /* Resources */ = {
1078 | 			isa = PBXResourcesBuildPhase;
1079 | 			buildActionMask = 2147483647;
1080 | 			files = (
1081 | 			);
1082 | 			runOnlyForDeploymentPostprocessing = 0;
1083 | 		};
1084 | 		C3056BA92BCD97B700A31D04 /* Resources */ = {
1085 | 			isa = PBXResourcesBuildPhase;
1086 | 			buildActionMask = 2147483647;
1087 | 			files = (
1088 | 				C3056BBA2BCD981900A31D04 /* train.jsonl in Resources */,
1089 | 				C3056BBB2BCD981900A31D04 /* test.jsonl in Resources */,
1090 | 				C3056BBC2BCD981900A31D04 /* valid.jsonl in Resources */,
1091 | 				C3056BB62BCD97B800A31D04 /* Preview Assets.xcassets in Resources */,
1092 | 				C3056BB22BCD97B800A31D04 /* Assets.xcassets in Resources */,
1093 | 			);
1094 | 			runOnlyForDeploymentPostprocessing = 0;
1095 | 		};
1096 | 		C3208E6C2DB19451006AE6CA /* Resources */ = {
1097 | 			isa = PBXResourcesBuildPhase;
1098 | 			buildActionMask = 2147483647;
1099 | 			files = (
1100 | 			);
1101 | 			runOnlyForDeploymentPostprocessing = 0;
1102 | 		};
1103 | 		C36BEFFF2BC5CE55002D4AFE /* Resources */ = {
1104 | 			isa = PBXResourcesBuildPhase;
1105 | 			buildActionMask = 2147483647;
1106 | 			files = (
1107 | 				C36BF00C2BC5CE56002D4AFE /* Preview Assets.xcassets in Resources */,
1108 | 				C36BF0082BC5CE56002D4AFE /* Assets.xcassets in Resources */,
1109 | 			);
1110 | 			runOnlyForDeploymentPostprocessing = 0;
1111 | 		};
1112 | 		C3A8B3B02B9295090002EFB8 /* Resources */ = {
1113 | 			isa = PBXResourcesBuildPhase;
1114 | 			buildActionMask = 2147483647;
1115 | 			files = (
1116 | 				C3A8B3CD2B92951E0002EFB8 /* Preview Assets.xcassets in Resources */,
1117 | 				C3A8B3CB2B92951E0002EFB8 /* Assets.xcassets in Resources */,
1118 | 			);
1119 | 			runOnlyForDeploymentPostprocessing = 0;
1120 | 		};
1121 | 		C3A8B3DA2B92A29D0002EFB8 /* Resources */ = {
1122 | 			isa = PBXResourcesBuildPhase;
1123 | 			buildActionMask = 2147483647;
1124 | 			files = (
1125 | 				C3A8B3F52B92A2A90002EFB8 /* Preview Assets.xcassets in Resources */,
1126 | 				C3A8B3F32B92A2A90002EFB8 /* Assets.xcassets in Resources */,
1127 | 			);
1128 | 			runOnlyForDeploymentPostprocessing = 0;
1129 | 		};
1130 | /* End PBXResourcesBuildPhase section */
1131 | 
1132 | /* Begin PBXSourcesBuildPhase section */
1133 | 		0AC74EB72D136221003C90A7 /* Sources */ = {
1134 | 			isa = PBXSourcesBuildPhase;
1135 | 			buildActionMask = 2147483647;
1136 | 			files = (
1137 | 				0AC74ED12D13622A003C90A7 /* ContentView.swift in Sources */,
1138 | 				0AC74ED32D136265003C90A7 /* DeviceStat.swift in Sources */,
1139 | 				0AC74ED22D13622A003C90A7 /* VLMEvalApp.swift in Sources */,
1140 | 			);
1141 | 			runOnlyForDeploymentPostprocessing = 0;
1142 | 		};
1143 | 		0F5AD73D2DB70C0300745C06 /* Sources */ = {
1144 | 			isa = PBXSourcesBuildPhase;
1145 | 			buildActionMask = 2147483647;
1146 | 			files = (
1147 | 			);
1148 | 			runOnlyForDeploymentPostprocessing = 0;
1149 | 		};
1150 | 		C3056BA72BCD97B700A31D04 /* Sources */ = {
1151 | 			isa = PBXSourcesBuildPhase;
1152 | 			buildActionMask = 2147483647;
1153 | 			files = (
1154 | 				C3056BB02BCD97B700A31D04 /* ContentView.swift in Sources */,
1155 | 				C3056BAE2BCD97B700A31D04 /* LoRATrainingExampleApp.swift in Sources */,
1156 | 			);
1157 | 			runOnlyForDeploymentPostprocessing = 0;
1158 | 		};
1159 | 		C3132C1F2DFB2FFB00270E8E /* Sources */ = {
1160 | 			isa = PBXSourcesBuildPhase;
1161 | 			buildActionMask = 2147483647;
1162 | 			files = (
1163 | 				C3132C2C2DFB301A00270E8E /* main.swift in Sources */,
1164 | 			);
1165 | 			runOnlyForDeploymentPostprocessing = 0;
1166 | 		};
1167 | 		C3208E6A2DB19451006AE6CA /* Sources */ = {
1168 | 			isa = PBXSourcesBuildPhase;
1169 | 			buildActionMask = 2147483647;
1170 | 			files = (
1171 | 			);
1172 | 			runOnlyForDeploymentPostprocessing = 0;
1173 | 		};
1174 | 		C3288D6F2B6D9313009FF608 /* Sources */ = {
1175 | 			isa = PBXSourcesBuildPhase;
1176 | 			buildActionMask = 2147483647;
1177 | 			files = (
1178 | 				C3288D762B6D9313009FF608 /* LinearModelTraining.swift in Sources */,
1179 | 			);
1180 | 			runOnlyForDeploymentPostprocessing = 0;
1181 | 		};
1182 | 		C34E491D2B6A026F00FCB841 /* Sources */ = {
1183 | 			isa = PBXSourcesBuildPhase;
1184 | 			buildActionMask = 2147483647;
1185 | 			files = (
1186 | 				C34E49242B6A026F00FCB841 /* MNISTTool.swift in Sources */,
1187 | 			);
1188 | 			runOnlyForDeploymentPostprocessing = 0;
1189 | 		};
1190 | 		C36BEFDC2BC32988002D4AFE /* Sources */ = {
1191 | 			isa = PBXSourcesBuildPhase;
1192 | 			buildActionMask = 2147483647;
1193 | 			files = (
1194 | 				C36BF0352BC70F11002D4AFE /* Arguments.swift in Sources */,
1195 | 				C36BEFE32BC32988002D4AFE /* ImageTool.swift in Sources */,
1196 | 			);
1197 | 			runOnlyForDeploymentPostprocessing = 0;
1198 | 		};
1199 | 		C36BEFFD2BC5CE55002D4AFE /* Sources */ = {
1200 | 			isa = PBXSourcesBuildPhase;
1201 | 			buildActionMask = 2147483647;
1202 | 			files = (
1203 | 				C36BF0062BC5CE55002D4AFE /* ContentView.swift in Sources */,
1204 | 				C36BF0042BC5CE55002D4AFE /* StableDiffusionExampleApp.swift in Sources */,
1205 | 			);
1206 | 			runOnlyForDeploymentPostprocessing = 0;
1207 | 		};
1208 | 		C39273702B606A0A00368D5D /* Sources */ = {
1209 | 			isa = PBXSourcesBuildPhase;
1210 | 			buildActionMask = 2147483647;
1211 | 			files = (
1212 | 				C392737D2B606A1D00368D5D /* Tutorial.swift in Sources */,
1213 | 			);
1214 | 			runOnlyForDeploymentPostprocessing = 0;
1215 | 		};
1216 | 		C397C5872B62C6A9004B084D /* Sources */ = {
1217 | 			isa = PBXSourcesBuildPhase;
1218 | 			buildActionMask = 2147483647;
1219 | 			files = (
1220 | 				C36BEFB82BBDED51002D4AFE /* Arguments.swift in Sources */,
1221 | 				C38BA3AA2DB8321600BAFA88 /* Chat.swift in Sources */,
1222 | 				C34E48F52B696F0B00FCB841 /* LLMTool.swift in Sources */,
1223 | 				C36BEFB52BBDEAD8002D4AFE /* LoraCommands.swift in Sources */,
1224 | 				C37133A22DD6524B00D19830 /* ListCommands.swift in Sources */,
1225 | 			);
1226 | 			runOnlyForDeploymentPostprocessing = 0;
1227 | 		};
1228 | 		C3A8B3AE2B9295090002EFB8 /* Sources */ = {
1229 | 			isa = PBXSourcesBuildPhase;
1230 | 			buildActionMask = 2147483647;
1231 | 			files = (
1232 | 				12305EAF2B9D864400C92FEE /* PredictionView.swift in Sources */,
1233 | 				C3A8B3CC2B92951E0002EFB8 /* MNISTTrainerApp.swift in Sources */,
1234 | 				C3A8B3CF2B92951E0002EFB8 /* ContentView.swift in Sources */,
1235 | 			);
1236 | 			runOnlyForDeploymentPostprocessing = 0;
1237 | 		};
1238 | 		C3A8B3D82B92A29D0002EFB8 /* Sources */ = {
1239 | 			isa = PBXSourcesBuildPhase;
1240 | 			buildActionMask = 2147483647;
1241 | 			files = (
1242 | 				C3A8B3F42B92A2A90002EFB8 /* LLMEvalApp.swift in Sources */,
1243 | 				C3A8B3F72B92A2A90002EFB8 /* ContentView.swift in Sources */,
1244 | 				819BEFF82BAF8B4E0002CCEE /* DeviceStat.swift in Sources */,
1245 | 			);
1246 | 			runOnlyForDeploymentPostprocessing = 0;
1247 | 		};
1248 | /* End PBXSourcesBuildPhase section */
1249 | 
1250 | /* Begin XCBuildConfiguration section */
1251 | 		0AC74EC72D136223003C90A7 /* Debug */ = {
1252 | 			isa = XCBuildConfiguration;
1253 | 			buildSettings = {
1254 | 				ALWAYS_SEARCH_USER_PATHS = NO;
1255 | 				ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;
1256 | 				ASSETCATALOG_COMPILER_GLOBAL_ACCENT_COLOR_NAME = AccentColor;
1257 | 				CLANG_ANALYZER_NONNULL = YES;
1258 | 				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
1259 | 				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
1260 | 				CLANG_ENABLE_MODULES = YES;
1261 | 				CLANG_ENABLE_OBJC_ARC = YES;
1262 | 				CLANG_ENABLE_OBJC_WEAK = YES;
1263 | 				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
1264 | 				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
1265 | 				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
1266 | 				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
1267 | 				CODE_SIGN_ENTITLEMENTS = Applications/VLMEval/VLMEval.entitlements;
1268 | 				CODE_SIGN_STYLE = Automatic;
1269 | 				COPY_PHASE_STRIP = NO;
1270 | 				CURRENT_PROJECT_VERSION = 1;
1271 | 				DEBUG_INFORMATION_FORMAT = dwarf;
1272 | 				DEVELOPMENT_ASSET_PATHS = "\"Applications/VLMEval/Preview Content\"";
1273 | 				DEVELOPMENT_TEAM = "";
1274 | 				ENABLE_PREVIEWS = YES;
1275 | 				ENABLE_USER_SCRIPT_SANDBOXING = YES;
1276 | 				GCC_C_LANGUAGE_STANDARD = gnu17;
1277 | 				GCC_DYNAMIC_NO_PIC = NO;
1278 | 				GCC_OPTIMIZATION_LEVEL = 0;
1279 | 				GCC_PREPROCESSOR_DEFINITIONS = (
1280 | 					"DEBUG=1",
1281 | 					"$(inherited)",
1282 | 				);
1283 | 				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
1284 | 				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
1285 | 				GENERATE_INFOPLIST_FILE = YES;
1286 | 				INFOPLIST_KEY_UIApplicationSceneManifest_Generation = YES;
1287 | 				INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents = YES;
1288 | 				INFOPLIST_KEY_UILaunchScreen_Generation = YES;
1289 | 				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPad = "UIInterfaceOrientationPortrait UIInterfaceOrientationPortraitUpsideDown UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
1290 | 				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPhone = "UIInterfaceOrientationPortrait UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
1291 | 				IPHONEOS_DEPLOYMENT_TARGET = 17.2;
1292 | 				LD_RUNPATH_SEARCH_PATHS = (
1293 | 					"$(inherited)",
1294 | 					"@executable_path/Frameworks",
1295 | 				);
1296 | 				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
1297 | 				MACOSX_DEPLOYMENT_TARGET = 14.2;
1298 | 				MARKETING_VERSION = 1.0;
1299 | 				MTL_ENABLE_DEBUG_INFO = INCLUDE_SOURCE;
1300 | 				MTL_FAST_MATH = YES;
1301 | 				PRODUCT_BUNDLE_IDENTIFIER = "mlx.VLMEval${DISAMBIGUATOR}";
1302 | 				PRODUCT_NAME = "$(TARGET_NAME)";
1303 | 				REGISTER_APP_GROUPS = NO;
1304 | 				SDKROOT = iphoneos;
1305 | 				SUPPORTED_PLATFORMS = "iphoneos iphonesimulator macosx xros xrsimulator";
1306 | 				SUPPORTS_MACCATALYST = NO;
1307 | 				SUPPORTS_MAC_DESIGNED_FOR_IPHONE_IPAD = NO;
1308 | 				SUPPORTS_XR_DESIGNED_FOR_IPHONE_IPAD = NO;
1309 | 				SWIFT_ACTIVE_COMPILATION_CONDITIONS = "DEBUG $(inherited)";
1310 | 				SWIFT_EMIT_LOC_STRINGS = YES;
1311 | 				SWIFT_OPTIMIZATION_LEVEL = "-Onone";
1312 | 				SWIFT_VERSION = 5.0;
1313 | 				TARGETED_DEVICE_FAMILY = "1,2,7";
1314 | 			};
1315 | 			name = Debug;
1316 | 		};
1317 | 		0AC74EC82D136223003C90A7 /* Release */ = {
1318 | 			isa = XCBuildConfiguration;
1319 | 			buildSettings = {
1320 | 				ALWAYS_SEARCH_USER_PATHS = NO;
1321 | 				ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;
1322 | 				ASSETCATALOG_COMPILER_GLOBAL_ACCENT_COLOR_NAME = AccentColor;
1323 | 				CLANG_ANALYZER_NONNULL = YES;
1324 | 				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
1325 | 				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
1326 | 				CLANG_ENABLE_MODULES = YES;
1327 | 				CLANG_ENABLE_OBJC_ARC = YES;
1328 | 				CLANG_ENABLE_OBJC_WEAK = YES;
1329 | 				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
1330 | 				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
1331 | 				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
1332 | 				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
1333 | 				CODE_SIGN_ENTITLEMENTS = Applications/VLMEval/VLMEval.entitlements;
1334 | 				CODE_SIGN_STYLE = Automatic;
1335 | 				COPY_PHASE_STRIP = NO;
1336 | 				CURRENT_PROJECT_VERSION = 1;
1337 | 				DEBUG_INFORMATION_FORMAT = "dwarf-with-dsym";
1338 | 				DEVELOPMENT_ASSET_PATHS = "\"Applications/VLMEval/Preview Content\"";
1339 | 				DEVELOPMENT_TEAM = "";
1340 | 				ENABLE_NS_ASSERTIONS = NO;
1341 | 				ENABLE_PREVIEWS = YES;
1342 | 				ENABLE_USER_SCRIPT_SANDBOXING = YES;
1343 | 				GCC_C_LANGUAGE_STANDARD = gnu17;
1344 | 				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
1345 | 				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
1346 | 				GENERATE_INFOPLIST_FILE = YES;
1347 | 				INFOPLIST_KEY_UIApplicationSceneManifest_Generation = YES;
1348 | 				INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents = YES;
1349 | 				INFOPLIST_KEY_UILaunchScreen_Generation = YES;
1350 | 				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPad = "UIInterfaceOrientationPortrait UIInterfaceOrientationPortraitUpsideDown UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
1351 | 				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPhone = "UIInterfaceOrientationPortrait UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
1352 | 				IPHONEOS_DEPLOYMENT_TARGET = 17.2;
1353 | 				LD_RUNPATH_SEARCH_PATHS = (
1354 | 					"$(inherited)",
1355 | 					"@executable_path/Frameworks",
1356 | 				);
1357 | 				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
1358 | 				MACOSX_DEPLOYMENT_TARGET = 14.2;
1359 | 				MARKETING_VERSION = 1.0;
1360 | 				MTL_ENABLE_DEBUG_INFO = NO;
1361 | 				MTL_FAST_MATH = YES;
1362 | 				PRODUCT_BUNDLE_IDENTIFIER = "mlx.VLMEval${DISAMBIGUATOR}";
1363 | 				PRODUCT_NAME = "$(TARGET_NAME)";
1364 | 				REGISTER_APP_GROUPS = NO;
1365 | 				SDKROOT = iphoneos;
1366 | 				SUPPORTED_PLATFORMS = "iphoneos iphonesimulator macosx xros xrsimulator";
1367 | 				SUPPORTS_MACCATALYST = NO;
1368 | 				SUPPORTS_MAC_DESIGNED_FOR_IPHONE_IPAD = NO;
1369 | 				SUPPORTS_XR_DESIGNED_FOR_IPHONE_IPAD = NO;
1370 | 				SWIFT_COMPILATION_MODE = wholemodule;
1371 | 				SWIFT_EMIT_LOC_STRINGS = YES;
1372 | 				SWIFT_VERSION = 5.0;
1373 | 				TARGETED_DEVICE_FAMILY = "1,2,7";
1374 | 				VALIDATE_PRODUCT = YES;
1375 | 			};
1376 | 			name = Release;
1377 | 		};
1378 | 		0F5AD74D2DB70C0400745C06 /* Debug */ = {
1379 | 			isa = XCBuildConfiguration;
1380 | 			buildSettings = {
1381 | 				ALWAYS_SEARCH_USER_PATHS = NO;
1382 | 				ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;
1383 | 				ASSETCATALOG_COMPILER_GLOBAL_ACCENT_COLOR_NAME = AccentColor;
1384 | 				CLANG_ANALYZER_NONNULL = YES;
1385 | 				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
1386 | 				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
1387 | 				CLANG_ENABLE_MODULES = YES;
1388 | 				CLANG_ENABLE_OBJC_ARC = YES;
1389 | 				CLANG_ENABLE_OBJC_WEAK = YES;
1390 | 				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
1391 | 				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
1392 | 				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
1393 | 				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
1394 | 				CODE_SIGN_ENTITLEMENTS = Applications/MLXChatExample/MLXChatExample.entitlements;
1395 | 				CODE_SIGN_STYLE = Automatic;
1396 | 				COPY_PHASE_STRIP = NO;
1397 | 				CURRENT_PROJECT_VERSION = 1;
1398 | 				DEBUG_INFORMATION_FORMAT = dwarf;
1399 | 				DEVELOPMENT_ASSET_PATHS = "\"Applications/MLXChatExample/Support/Preview Content\"";
1400 | 				ENABLE_PREVIEWS = YES;
1401 | 				ENABLE_USER_SCRIPT_SANDBOXING = YES;
1402 | 				GCC_C_LANGUAGE_STANDARD = gnu17;
1403 | 				GCC_DYNAMIC_NO_PIC = NO;
1404 | 				GCC_OPTIMIZATION_LEVEL = 0;
1405 | 				GCC_PREPROCESSOR_DEFINITIONS = (
1406 | 					"DEBUG=1",
1407 | 					"$(inherited)",
1408 | 				);
1409 | 				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
1410 | 				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
1411 | 				GENERATE_INFOPLIST_FILE = YES;
1412 | 				INFOPLIST_KEY_UIApplicationSceneManifest_Generation = YES;
1413 | 				INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents = YES;
1414 | 				INFOPLIST_KEY_UILaunchScreen_Generation = YES;
1415 | 				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPad = "UIInterfaceOrientationPortrait UIInterfaceOrientationPortraitUpsideDown UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
1416 | 				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPhone = "UIInterfaceOrientationPortrait UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
1417 | 				IPHONEOS_DEPLOYMENT_TARGET = 18.0;
1418 | 				LD_RUNPATH_SEARCH_PATHS = (
1419 | 					"$(inherited)",
1420 | 					"@executable_path/Frameworks",
1421 | 				);
1422 | 				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
1423 | 				MACOSX_DEPLOYMENT_TARGET = 15.0;
1424 | 				MARKETING_VERSION = 1.0;
1425 | 				MTL_ENABLE_DEBUG_INFO = INCLUDE_SOURCE;
1426 | 				MTL_FAST_MATH = YES;
1427 | 				PRODUCT_BUNDLE_IDENTIFIER = "mlx.MLXChatExample${DISAMBIGUATOR}";
1428 | 				PRODUCT_NAME = "$(TARGET_NAME)";
1429 | 				REGISTER_APP_GROUPS = NO;
1430 | 				SDKROOT = iphoneos;
1431 | 				SUPPORTED_PLATFORMS = "iphoneos iphonesimulator macosx xros xrsimulator";
1432 | 				SUPPORTS_MACCATALYST = NO;
1433 | 				SUPPORTS_MAC_DESIGNED_FOR_IPHONE_IPAD = NO;
1434 | 				SUPPORTS_XR_DESIGNED_FOR_IPHONE_IPAD = NO;
1435 | 				SWIFT_ACTIVE_COMPILATION_CONDITIONS = "DEBUG $(inherited)";
1436 | 				SWIFT_EMIT_LOC_STRINGS = YES;
1437 | 				SWIFT_OPTIMIZATION_LEVEL = "-Onone";
1438 | 				SWIFT_VERSION = 5.0;
1439 | 				TARGETED_DEVICE_FAMILY = "1,2,7";
1440 | 				XROS_DEPLOYMENT_TARGET = 2.0;
1441 | 			};
1442 | 			name = Debug;
1443 | 		};
1444 | 		0F5AD74E2DB70C0400745C06 /* Release */ = {
1445 | 			isa = XCBuildConfiguration;
1446 | 			buildSettings = {
1447 | 				ALWAYS_SEARCH_USER_PATHS = NO;
1448 | 				ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;
1449 | 				ASSETCATALOG_COMPILER_GLOBAL_ACCENT_COLOR_NAME = AccentColor;
1450 | 				CLANG_ANALYZER_NONNULL = YES;
1451 | 				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
1452 | 				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
1453 | 				CLANG_ENABLE_MODULES = YES;
1454 | 				CLANG_ENABLE_OBJC_ARC = YES;
1455 | 				CLANG_ENABLE_OBJC_WEAK = YES;
1456 | 				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
1457 | 				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
1458 | 				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
1459 | 				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
1460 | 				CODE_SIGN_ENTITLEMENTS = Applications/MLXChatExample/MLXChatExample.entitlements;
1461 | 				CODE_SIGN_STYLE = Automatic;
1462 | 				COPY_PHASE_STRIP = NO;
1463 | 				CURRENT_PROJECT_VERSION = 1;
1464 | 				DEBUG_INFORMATION_FORMAT = "dwarf-with-dsym";
1465 | 				DEVELOPMENT_ASSET_PATHS = "\"Applications/MLXChatExample/Support/Preview Content\"";
1466 | 				ENABLE_NS_ASSERTIONS = NO;
1467 | 				ENABLE_PREVIEWS = YES;
1468 | 				ENABLE_USER_SCRIPT_SANDBOXING = YES;
1469 | 				GCC_C_LANGUAGE_STANDARD = gnu17;
1470 | 				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
1471 | 				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
1472 | 				GENERATE_INFOPLIST_FILE = YES;
1473 | 				INFOPLIST_KEY_UIApplicationSceneManifest_Generation = YES;
1474 | 				INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents = YES;
1475 | 				INFOPLIST_KEY_UILaunchScreen_Generation = YES;
1476 | 				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPad = "UIInterfaceOrientationPortrait UIInterfaceOrientationPortraitUpsideDown UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
1477 | 				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPhone = "UIInterfaceOrientationPortrait UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
1478 | 				IPHONEOS_DEPLOYMENT_TARGET = 18.0;
1479 | 				LD_RUNPATH_SEARCH_PATHS = (
1480 | 					"$(inherited)",
1481 | 					"@executable_path/Frameworks",
1482 | 				);
1483 | 				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
1484 | 				MACOSX_DEPLOYMENT_TARGET = 15.0;
1485 | 				MARKETING_VERSION = 1.0;
1486 | 				MTL_ENABLE_DEBUG_INFO = NO;
1487 | 				MTL_FAST_MATH = YES;
1488 | 				PRODUCT_BUNDLE_IDENTIFIER = "mlx.MLXChatExample${DISAMBIGUATOR}";
1489 | 				PRODUCT_NAME = "$(TARGET_NAME)";
1490 | 				REGISTER_APP_GROUPS = NO;
1491 | 				SDKROOT = iphoneos;
1492 | 				SUPPORTED_PLATFORMS = "iphoneos iphonesimulator macosx xros xrsimulator";
1493 | 				SUPPORTS_MACCATALYST = NO;
1494 | 				SUPPORTS_MAC_DESIGNED_FOR_IPHONE_IPAD = NO;
1495 | 				SUPPORTS_XR_DESIGNED_FOR_IPHONE_IPAD = NO;
1496 | 				SWIFT_COMPILATION_MODE = wholemodule;
1497 | 				SWIFT_EMIT_LOC_STRINGS = YES;
1498 | 				SWIFT_VERSION = 5.0;
1499 | 				TARGETED_DEVICE_FAMILY = "1,2,7";
1500 | 				VALIDATE_PRODUCT = YES;
1501 | 				XROS_DEPLOYMENT_TARGET = 2.0;
1502 | 			};
1503 | 			name = Release;
1504 | 		};
1505 | 		C3056BB82BCD97B800A31D04 /* Debug */ = {
1506 | 			isa = XCBuildConfiguration;
1507 | 			buildSettings = {
1508 | 				ALWAYS_SEARCH_USER_PATHS = NO;
1509 | 				ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;
1510 | 				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
1511 | 				ASSETCATALOG_COMPILER_GLOBAL_ACCENT_COLOR_NAME = AccentColor;
1512 | 				CLANG_ANALYZER_NONNULL = YES;
1513 | 				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
1514 | 				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
1515 | 				CLANG_ENABLE_MODULES = YES;
1516 | 				CLANG_ENABLE_OBJC_ARC = YES;
1517 | 				CLANG_ENABLE_OBJC_WEAK = YES;
1518 | 				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
1519 | 				CLANG_WARN_BOOL_CONVERSION = YES;
1520 | 				CLANG_WARN_COMMA = YES;
1521 | 				CLANG_WARN_CONSTANT_CONVERSION = YES;
1522 | 				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
1523 | 				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
1524 | 				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
1525 | 				CLANG_WARN_EMPTY_BODY = YES;
1526 | 				CLANG_WARN_ENUM_CONVERSION = YES;
1527 | 				CLANG_WARN_INFINITE_RECURSION = YES;
1528 | 				CLANG_WARN_INT_CONVERSION = YES;
1529 | 				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
1530 | 				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
1531 | 				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
1532 | 				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
1533 | 				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
1534 | 				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
1535 | 				CLANG_WARN_STRICT_PROTOTYPES = YES;
1536 | 				CLANG_WARN_SUSPICIOUS_MOVE = YES;
1537 | 				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
1538 | 				CLANG_WARN_UNREACHABLE_CODE = YES;
1539 | 				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
1540 | 				CODE_SIGN_ENTITLEMENTS = Applications/LoRATrainingExample/LoRATrainingExample.entitlements;
1541 | 				CODE_SIGN_STYLE = Automatic;
1542 | 				COPY_PHASE_STRIP = NO;
1543 | 				CURRENT_PROJECT_VERSION = 1;
1544 | 				DEAD_CODE_STRIPPING = YES;
1545 | 				DEBUG_INFORMATION_FORMAT = dwarf;
1546 | 				DEVELOPMENT_ASSET_PATHS = "\"Applications/LoRATrainingExample/Preview Content\"";
1547 | 				DEVELOPMENT_TEAM = "";
1548 | 				ENABLE_PREVIEWS = YES;
1549 | 				ENABLE_STRICT_OBJC_MSGSEND = YES;
1550 | 				ENABLE_TESTABILITY = YES;
1551 | 				ENABLE_USER_SCRIPT_SANDBOXING = YES;
1552 | 				GCC_C_LANGUAGE_STANDARD = gnu17;
1553 | 				GCC_DYNAMIC_NO_PIC = NO;
1554 | 				GCC_NO_COMMON_BLOCKS = YES;
1555 | 				GCC_OPTIMIZATION_LEVEL = 0;
1556 | 				GCC_PREPROCESSOR_DEFINITIONS = (
1557 | 					"DEBUG=1",
1558 | 					"$(inherited)",
1559 | 				);
1560 | 				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
1561 | 				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
1562 | 				GCC_WARN_UNDECLARED_SELECTOR = YES;
1563 | 				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
1564 | 				GCC_WARN_UNUSED_FUNCTION = YES;
1565 | 				GCC_WARN_UNUSED_VARIABLE = YES;
1566 | 				GENERATE_INFOPLIST_FILE = YES;
1567 | 				"INFOPLIST_KEY_UIApplicationSceneManifest_Generation[sdk=iphoneos*]" = YES;
1568 | 				"INFOPLIST_KEY_UIApplicationSceneManifest_Generation[sdk=iphonesimulator*]" = YES;
1569 | 				"INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents[sdk=iphoneos*]" = YES;
1570 | 				"INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents[sdk=iphonesimulator*]" = YES;
1571 | 				"INFOPLIST_KEY_UILaunchScreen_Generation[sdk=iphoneos*]" = YES;
1572 | 				"INFOPLIST_KEY_UILaunchScreen_Generation[sdk=iphonesimulator*]" = YES;
1573 | 				"INFOPLIST_KEY_UIStatusBarStyle[sdk=iphoneos*]" = UIStatusBarStyleDefault;
1574 | 				"INFOPLIST_KEY_UIStatusBarStyle[sdk=iphonesimulator*]" = UIStatusBarStyleDefault;
1575 | 				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPad = "UIInterfaceOrientationPortrait UIInterfaceOrientationPortraitUpsideDown UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
1576 | 				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPhone = "UIInterfaceOrientationPortrait UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
1577 | 				IPHONEOS_DEPLOYMENT_TARGET = 17.2;
1578 | 				LD_RUNPATH_SEARCH_PATHS = "@executable_path/Frameworks";
1579 | 				"LD_RUNPATH_SEARCH_PATHS[sdk=macosx*]" = "@executable_path/../Frameworks";
1580 | 				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
1581 | 				MACOSX_DEPLOYMENT_TARGET = 14.2;
1582 | 				MARKETING_VERSION = 1.0;
1583 | 				MTL_ENABLE_DEBUG_INFO = INCLUDE_SOURCE;
1584 | 				MTL_FAST_MATH = YES;
1585 | 				PRODUCT_BUNDLE_IDENTIFIER = "mlx.LoRATrainingExample${DISAMBIGUATOR}";
1586 | 				PRODUCT_NAME = "$(TARGET_NAME)";
1587 | 				SDKROOT = auto;
1588 | 				SUPPORTED_PLATFORMS = "iphoneos iphonesimulator macosx xros xrsimulator";
1589 | 				SUPPORTS_MACCATALYST = NO;
1590 | 				SWIFT_ACTIVE_COMPILATION_CONDITIONS = "DEBUG $(inherited)";
1591 | 				SWIFT_EMIT_LOC_STRINGS = YES;
1592 | 				SWIFT_OPTIMIZATION_LEVEL = "-Onone";
1593 | 				SWIFT_STRICT_CONCURRENCY = complete;
1594 | 				SWIFT_VERSION = 5.0;
1595 | 				TARGETED_DEVICE_FAMILY = "1,2,7";
1596 | 			};
1597 | 			name = Debug;
1598 | 		};
1599 | 		C3056BB92BCD97B800A31D04 /* Release */ = {
1600 | 			isa = XCBuildConfiguration;
1601 | 			buildSettings = {
1602 | 				ALWAYS_SEARCH_USER_PATHS = NO;
1603 | 				ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;
1604 | 				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
1605 | 				ASSETCATALOG_COMPILER_GLOBAL_ACCENT_COLOR_NAME = AccentColor;
1606 | 				CLANG_ANALYZER_NONNULL = YES;
1607 | 				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
1608 | 				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
1609 | 				CLANG_ENABLE_MODULES = YES;
1610 | 				CLANG_ENABLE_OBJC_ARC = YES;
1611 | 				CLANG_ENABLE_OBJC_WEAK = YES;
1612 | 				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
1613 | 				CLANG_WARN_BOOL_CONVERSION = YES;
1614 | 				CLANG_WARN_COMMA = YES;
1615 | 				CLANG_WARN_CONSTANT_CONVERSION = YES;
1616 | 				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
1617 | 				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
1618 | 				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
1619 | 				CLANG_WARN_EMPTY_BODY = YES;
1620 | 				CLANG_WARN_ENUM_CONVERSION = YES;
1621 | 				CLANG_WARN_INFINITE_RECURSION = YES;
1622 | 				CLANG_WARN_INT_CONVERSION = YES;
1623 | 				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
1624 | 				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
1625 | 				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
1626 | 				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
1627 | 				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
1628 | 				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
1629 | 				CLANG_WARN_STRICT_PROTOTYPES = YES;
1630 | 				CLANG_WARN_SUSPICIOUS_MOVE = YES;
1631 | 				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
1632 | 				CLANG_WARN_UNREACHABLE_CODE = YES;
1633 | 				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
1634 | 				CODE_SIGN_ENTITLEMENTS = Applications/LoRATrainingExample/LoRATrainingExample.entitlements;
1635 | 				CODE_SIGN_STYLE = Automatic;
1636 | 				COPY_PHASE_STRIP = NO;
1637 | 				CURRENT_PROJECT_VERSION = 1;
1638 | 				DEAD_CODE_STRIPPING = YES;
1639 | 				DEBUG_INFORMATION_FORMAT = "dwarf-with-dsym";
1640 | 				DEVELOPMENT_ASSET_PATHS = "\"Applications/LoRATrainingExample/Preview Content\"";
1641 | 				DEVELOPMENT_TEAM = "";
1642 | 				ENABLE_NS_ASSERTIONS = NO;
1643 | 				ENABLE_PREVIEWS = YES;
1644 | 				ENABLE_STRICT_OBJC_MSGSEND = YES;
1645 | 				ENABLE_USER_SCRIPT_SANDBOXING = YES;
1646 | 				GCC_C_LANGUAGE_STANDARD = gnu17;
1647 | 				GCC_NO_COMMON_BLOCKS = YES;
1648 | 				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
1649 | 				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
1650 | 				GCC_WARN_UNDECLARED_SELECTOR = YES;
1651 | 				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
1652 | 				GCC_WARN_UNUSED_FUNCTION = YES;
1653 | 				GCC_WARN_UNUSED_VARIABLE = YES;
1654 | 				GENERATE_INFOPLIST_FILE = YES;
1655 | 				"INFOPLIST_KEY_UIApplicationSceneManifest_Generation[sdk=iphoneos*]" = YES;
1656 | 				"INFOPLIST_KEY_UIApplicationSceneManifest_Generation[sdk=iphonesimulator*]" = YES;
1657 | 				"INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents[sdk=iphoneos*]" = YES;
1658 | 				"INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents[sdk=iphonesimulator*]" = YES;
1659 | 				"INFOPLIST_KEY_UILaunchScreen_Generation[sdk=iphoneos*]" = YES;
1660 | 				"INFOPLIST_KEY_UILaunchScreen_Generation[sdk=iphonesimulator*]" = YES;
1661 | 				"INFOPLIST_KEY_UIStatusBarStyle[sdk=iphoneos*]" = UIStatusBarStyleDefault;
1662 | 				"INFOPLIST_KEY_UIStatusBarStyle[sdk=iphonesimulator*]" = UIStatusBarStyleDefault;
1663 | 				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPad = "UIInterfaceOrientationPortrait UIInterfaceOrientationPortraitUpsideDown UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
1664 | 				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPhone = "UIInterfaceOrientationPortrait UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
1665 | 				IPHONEOS_DEPLOYMENT_TARGET = 17.2;
1666 | 				LD_RUNPATH_SEARCH_PATHS = "@executable_path/Frameworks";
1667 | 				"LD_RUNPATH_SEARCH_PATHS[sdk=macosx*]" = "@executable_path/../Frameworks";
1668 | 				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
1669 | 				MACOSX_DEPLOYMENT_TARGET = 14.2;
1670 | 				MARKETING_VERSION = 1.0;
1671 | 				MTL_ENABLE_DEBUG_INFO = NO;
1672 | 				MTL_FAST_MATH = YES;
1673 | 				PRODUCT_BUNDLE_IDENTIFIER = "mlx.LoRATrainingExample${DISAMBIGUATOR}";
1674 | 				PRODUCT_NAME = "$(TARGET_NAME)";
1675 | 				SDKROOT = auto;
1676 | 				SUPPORTED_PLATFORMS = "iphoneos iphonesimulator macosx xros xrsimulator";
1677 | 				SUPPORTS_MACCATALYST = NO;
1678 | 				SWIFT_COMPILATION_MODE = wholemodule;
1679 | 				SWIFT_EMIT_LOC_STRINGS = YES;
1680 | 				SWIFT_STRICT_CONCURRENCY = complete;
1681 | 				SWIFT_VERSION = 5.0;
1682 | 				TARGETED_DEVICE_FAMILY = "1,2,7";
1683 | 			};
1684 | 			name = Release;
1685 | 		};
1686 | 		C3132C282DFB2FFB00270E8E /* Debug */ = {
1687 | 			isa = XCBuildConfiguration;
1688 | 			buildSettings = {
1689 | 				ALWAYS_SEARCH_USER_PATHS = NO;
1690 | 				CLANG_ANALYZER_NONNULL = YES;
1691 | 				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
1692 | 				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
1693 | 				CLANG_ENABLE_MODULES = YES;
1694 | 				CLANG_ENABLE_OBJC_ARC = YES;
1695 | 				CLANG_ENABLE_OBJC_WEAK = YES;
1696 | 				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
1697 | 				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
1698 | 				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
1699 | 				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
1700 | 				CODE_SIGN_STYLE = Automatic;
1701 | 				COPY_PHASE_STRIP = NO;
1702 | 				DEBUG_INFORMATION_FORMAT = dwarf;
1703 | 				DEVELOPMENT_TEAM = 565ARCVNXV;
1704 | 				ENABLE_HARDENED_RUNTIME = YES;
1705 | 				ENABLE_USER_SCRIPT_SANDBOXING = YES;
1706 | 				GCC_C_LANGUAGE_STANDARD = gnu17;
1707 | 				GCC_DYNAMIC_NO_PIC = NO;
1708 | 				GCC_OPTIMIZATION_LEVEL = 0;
1709 | 				GCC_PREPROCESSOR_DEFINITIONS = (
1710 | 					"DEBUG=1",
1711 | 					"$(inherited)",
1712 | 				);
1713 | 				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
1714 | 				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
1715 | 				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
1716 | 				MACOSX_DEPLOYMENT_TARGET = 15.4;
1717 | 				MTL_ENABLE_DEBUG_INFO = INCLUDE_SOURCE;
1718 | 				MTL_FAST_MATH = YES;
1719 | 				PRODUCT_NAME = "$(TARGET_NAME)";
1720 | 				SDKROOT = macosx;
1721 | 				SWIFT_ACTIVE_COMPILATION_CONDITIONS = "DEBUG $(inherited)";
1722 | 				SWIFT_OPTIMIZATION_LEVEL = "-Onone";
1723 | 				SWIFT_VERSION = 6.0;
1724 | 			};
1725 | 			name = Debug;
1726 | 		};
1727 | 		C3132C292DFB2FFB00270E8E /* Release */ = {
1728 | 			isa = XCBuildConfiguration;
1729 | 			buildSettings = {
1730 | 				ALWAYS_SEARCH_USER_PATHS = NO;
1731 | 				CLANG_ANALYZER_NONNULL = YES;
1732 | 				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
1733 | 				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
1734 | 				CLANG_ENABLE_MODULES = YES;
1735 | 				CLANG_ENABLE_OBJC_ARC = YES;
1736 | 				CLANG_ENABLE_OBJC_WEAK = YES;
1737 | 				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
1738 | 				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
1739 | 				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
1740 | 				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
1741 | 				CODE_SIGN_STYLE = Automatic;
1742 | 				COPY_PHASE_STRIP = NO;
1743 | 				DEBUG_INFORMATION_FORMAT = "dwarf-with-dsym";
1744 | 				DEVELOPMENT_TEAM = 565ARCVNXV;
1745 | 				ENABLE_HARDENED_RUNTIME = YES;
1746 | 				ENABLE_NS_ASSERTIONS = NO;
1747 | 				ENABLE_USER_SCRIPT_SANDBOXING = YES;
1748 | 				GCC_C_LANGUAGE_STANDARD = gnu17;
1749 | 				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
1750 | 				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
1751 | 				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
1752 | 				MACOSX_DEPLOYMENT_TARGET = 15.4;
1753 | 				MTL_ENABLE_DEBUG_INFO = NO;
1754 | 				MTL_FAST_MATH = YES;
1755 | 				PRODUCT_NAME = "$(TARGET_NAME)";
1756 | 				SDKROOT = macosx;
1757 | 				SWIFT_COMPILATION_MODE = wholemodule;
1758 | 				SWIFT_VERSION = 6.0;
1759 | 			};
1760 | 			name = Release;
1761 | 		};
1762 | 		C3208E732DB19451006AE6CA /* Debug */ = {
1763 | 			isa = XCBuildConfiguration;
1764 | 			buildSettings = {
1765 | 				ALWAYS_SEARCH_USER_PATHS = NO;
1766 | 				CLANG_ANALYZER_NONNULL = YES;
1767 | 				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
1768 | 				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
1769 | 				CLANG_ENABLE_MODULES = YES;
1770 | 				CLANG_ENABLE_OBJC_ARC = YES;
1771 | 				CLANG_ENABLE_OBJC_WEAK = YES;
1772 | 				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
1773 | 				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
1774 | 				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
1775 | 				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
1776 | 				CODE_SIGN_STYLE = Automatic;
1777 | 				COPY_PHASE_STRIP = NO;
1778 | 				CURRENT_PROJECT_VERSION = 1;
1779 | 				DEBUG_INFORMATION_FORMAT = dwarf;
1780 | 				ENABLE_USER_SCRIPT_SANDBOXING = YES;
1781 | 				GCC_C_LANGUAGE_STANDARD = gnu17;
1782 | 				GCC_DYNAMIC_NO_PIC = NO;
1783 | 				GCC_OPTIMIZATION_LEVEL = 0;
1784 | 				GCC_PREPROCESSOR_DEFINITIONS = (
1785 | 					"DEBUG=1",
1786 | 					"$(inherited)",
1787 | 				);
1788 | 				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
1789 | 				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
1790 | 				GENERATE_INFOPLIST_FILE = YES;
1791 | 				IPHONEOS_DEPLOYMENT_TARGET = 17.6;
1792 | 				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
1793 | 				MACOSX_DEPLOYMENT_TARGET = 14.6;
1794 | 				MARKETING_VERSION = 1.0;
1795 | 				MTL_ENABLE_DEBUG_INFO = INCLUDE_SOURCE;
1796 | 				MTL_FAST_MATH = YES;
1797 | 				PRODUCT_BUNDLE_IDENTIFIER = mlx.MLXLMTests;
1798 | 				PRODUCT_NAME = "$(TARGET_NAME)";
1799 | 				SDKROOT = auto;
1800 | 				SUPPORTED_PLATFORMS = "iphoneos iphonesimulator macosx xros xrsimulator";
1801 | 				SWIFT_ACTIVE_COMPILATION_CONDITIONS = "DEBUG $(inherited)";
1802 | 				SWIFT_EMIT_LOC_STRINGS = NO;
1803 | 				SWIFT_OPTIMIZATION_LEVEL = "-Onone";
1804 | 				SWIFT_VERSION = 5.0;
1805 | 				TARGETED_DEVICE_FAMILY = "1,2,7";
1806 | 				XROS_DEPLOYMENT_TARGET = 2.0;
1807 | 			};
1808 | 			name = Debug;
1809 | 		};
1810 | 		C3208E742DB19451006AE6CA /* Release */ = {
1811 | 			isa = XCBuildConfiguration;
1812 | 			buildSettings = {
1813 | 				ALWAYS_SEARCH_USER_PATHS = NO;
1814 | 				CLANG_ANALYZER_NONNULL = YES;
1815 | 				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
1816 | 				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
1817 | 				CLANG_ENABLE_MODULES = YES;
1818 | 				CLANG_ENABLE_OBJC_ARC = YES;
1819 | 				CLANG_ENABLE_OBJC_WEAK = YES;
1820 | 				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
1821 | 				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
1822 | 				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
1823 | 				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
1824 | 				CODE_SIGN_STYLE = Automatic;
1825 | 				COPY_PHASE_STRIP = NO;
1826 | 				CURRENT_PROJECT_VERSION = 1;
1827 | 				DEBUG_INFORMATION_FORMAT = "dwarf-with-dsym";
1828 | 				ENABLE_NS_ASSERTIONS = NO;
1829 | 				ENABLE_USER_SCRIPT_SANDBOXING = YES;
1830 | 				GCC_C_LANGUAGE_STANDARD = gnu17;
1831 | 				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
1832 | 				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
1833 | 				GENERATE_INFOPLIST_FILE = YES;
1834 | 				IPHONEOS_DEPLOYMENT_TARGET = 17.6;
1835 | 				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
1836 | 				MACOSX_DEPLOYMENT_TARGET = 14.6;
1837 | 				MARKETING_VERSION = 1.0;
1838 | 				MTL_ENABLE_DEBUG_INFO = NO;
1839 | 				MTL_FAST_MATH = YES;
1840 | 				PRODUCT_BUNDLE_IDENTIFIER = mlx.MLXLMTests;
1841 | 				PRODUCT_NAME = "$(TARGET_NAME)";
1842 | 				SDKROOT = auto;
1843 | 				SUPPORTED_PLATFORMS = "iphoneos iphonesimulator macosx xros xrsimulator";
1844 | 				SWIFT_COMPILATION_MODE = wholemodule;
1845 | 				SWIFT_EMIT_LOC_STRINGS = NO;
1846 | 				SWIFT_VERSION = 5.0;
1847 | 				TARGETED_DEVICE_FAMILY = "1,2,7";
1848 | 				XROS_DEPLOYMENT_TARGET = 2.0;
1849 | 			};
1850 | 			name = Release;
1851 | 		};
1852 | 		C3288D772B6D9313009FF608 /* Debug */ = {
1853 | 			isa = XCBuildConfiguration;
1854 | 			buildSettings = {
1855 | 				ALWAYS_SEARCH_USER_PATHS = NO;
1856 | 				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
1857 | 				CLANG_ANALYZER_NONNULL = YES;
1858 | 				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
1859 | 				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
1860 | 				CLANG_ENABLE_MODULES = YES;
1861 | 				CLANG_ENABLE_OBJC_ARC = YES;
1862 | 				CLANG_ENABLE_OBJC_WEAK = YES;
1863 | 				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
1864 | 				CLANG_WARN_BOOL_CONVERSION = YES;
1865 | 				CLANG_WARN_COMMA = YES;
1866 | 				CLANG_WARN_CONSTANT_CONVERSION = YES;
1867 | 				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
1868 | 				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
1869 | 				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
1870 | 				CLANG_WARN_EMPTY_BODY = YES;
1871 | 				CLANG_WARN_ENUM_CONVERSION = YES;
1872 | 				CLANG_WARN_INFINITE_RECURSION = YES;
1873 | 				CLANG_WARN_INT_CONVERSION = YES;
1874 | 				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
1875 | 				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
1876 | 				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
1877 | 				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
1878 | 				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
1879 | 				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
1880 | 				CLANG_WARN_STRICT_PROTOTYPES = YES;
1881 | 				CLANG_WARN_SUSPICIOUS_MOVE = YES;
1882 | 				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
1883 | 				CLANG_WARN_UNREACHABLE_CODE = YES;
1884 | 				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
1885 | 				CODE_SIGN_STYLE = Automatic;
1886 | 				COPY_PHASE_STRIP = NO;
1887 | 				DEAD_CODE_STRIPPING = YES;
1888 | 				DEBUG_INFORMATION_FORMAT = dwarf;
1889 | 				ENABLE_STRICT_OBJC_MSGSEND = YES;
1890 | 				ENABLE_TESTABILITY = YES;
1891 | 				ENABLE_USER_SCRIPT_SANDBOXING = YES;
1892 | 				GCC_C_LANGUAGE_STANDARD = gnu17;
1893 | 				GCC_DYNAMIC_NO_PIC = NO;
1894 | 				GCC_NO_COMMON_BLOCKS = YES;
1895 | 				GCC_OPTIMIZATION_LEVEL = 0;
1896 | 				GCC_PREPROCESSOR_DEFINITIONS = (
1897 | 					"DEBUG=1",
1898 | 					"$(inherited)",
1899 | 				);
1900 | 				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
1901 | 				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
1902 | 				GCC_WARN_UNDECLARED_SELECTOR = YES;
1903 | 				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
1904 | 				GCC_WARN_UNUSED_FUNCTION = YES;
1905 | 				GCC_WARN_UNUSED_VARIABLE = YES;
1906 | 				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
1907 | 				MACOSX_DEPLOYMENT_TARGET = 14.0;
1908 | 				MTL_ENABLE_DEBUG_INFO = INCLUDE_SOURCE;
1909 | 				MTL_FAST_MATH = YES;
1910 | 				PRODUCT_NAME = "$(TARGET_NAME)";
1911 | 				SDKROOT = macosx;
1912 | 				SWIFT_ACTIVE_COMPILATION_CONDITIONS = "DEBUG $(inherited)";
1913 | 				SWIFT_OPTIMIZATION_LEVEL = "-Onone";
1914 | 				SWIFT_STRICT_CONCURRENCY = complete;
1915 | 				SWIFT_VERSION = 5.0;
1916 | 			};
1917 | 			name = Debug;
1918 | 		};
1919 | 		C3288D782B6D9313009FF608 /* Release */ = {
1920 | 			isa = XCBuildConfiguration;
1921 | 			buildSettings = {
1922 | 				ALWAYS_SEARCH_USER_PATHS = NO;
1923 | 				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
1924 | 				CLANG_ANALYZER_NONNULL = YES;
1925 | 				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
1926 | 				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
1927 | 				CLANG_ENABLE_MODULES = YES;
1928 | 				CLANG_ENABLE_OBJC_ARC = YES;
1929 | 				CLANG_ENABLE_OBJC_WEAK = YES;
1930 | 				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
1931 | 				CLANG_WARN_BOOL_CONVERSION = YES;
1932 | 				CLANG_WARN_COMMA = YES;
1933 | 				CLANG_WARN_CONSTANT_CONVERSION = YES;
1934 | 				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
1935 | 				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
1936 | 				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
1937 | 				CLANG_WARN_EMPTY_BODY = YES;
1938 | 				CLANG_WARN_ENUM_CONVERSION = YES;
1939 | 				CLANG_WARN_INFINITE_RECURSION = YES;
1940 | 				CLANG_WARN_INT_CONVERSION = YES;
1941 | 				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
1942 | 				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
1943 | 				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
1944 | 				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
1945 | 				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
1946 | 				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
1947 | 				CLANG_WARN_STRICT_PROTOTYPES = YES;
1948 | 				CLANG_WARN_SUSPICIOUS_MOVE = YES;
1949 | 				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
1950 | 				CLANG_WARN_UNREACHABLE_CODE = YES;
1951 | 				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
1952 | 				CODE_SIGN_STYLE = Automatic;
1953 | 				COPY_PHASE_STRIP = NO;
1954 | 				DEAD_CODE_STRIPPING = YES;
1955 | 				DEBUG_INFORMATION_FORMAT = "dwarf-with-dsym";
1956 | 				ENABLE_NS_ASSERTIONS = NO;
1957 | 				ENABLE_STRICT_OBJC_MSGSEND = YES;
1958 | 				ENABLE_USER_SCRIPT_SANDBOXING = YES;
1959 | 				GCC_C_LANGUAGE_STANDARD = gnu17;
1960 | 				GCC_NO_COMMON_BLOCKS = YES;
1961 | 				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
1962 | 				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
1963 | 				GCC_WARN_UNDECLARED_SELECTOR = YES;
1964 | 				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
1965 | 				GCC_WARN_UNUSED_FUNCTION = YES;
1966 | 				GCC_WARN_UNUSED_VARIABLE = YES;
1967 | 				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
1968 | 				MACOSX_DEPLOYMENT_TARGET = 14.0;
1969 | 				MTL_ENABLE_DEBUG_INFO = NO;
1970 | 				MTL_FAST_MATH = YES;
1971 | 				PRODUCT_NAME = "$(TARGET_NAME)";
1972 | 				SDKROOT = macosx;
1973 | 				SWIFT_COMPILATION_MODE = wholemodule;
1974 | 				SWIFT_STRICT_CONCURRENCY = complete;
1975 | 				SWIFT_VERSION = 5.0;
1976 | 			};
1977 | 			name = Release;
1978 | 		};
1979 | 		C34E49262B6A026F00FCB841 /* Debug */ = {
1980 | 			isa = XCBuildConfiguration;
1981 | 			buildSettings = {
1982 | 				ALWAYS_SEARCH_USER_PATHS = NO;
1983 | 				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
1984 | 				CLANG_ANALYZER_NONNULL = YES;
1985 | 				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
1986 | 				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
1987 | 				CLANG_ENABLE_MODULES = YES;
1988 | 				CLANG_ENABLE_OBJC_ARC = YES;
1989 | 				CLANG_ENABLE_OBJC_WEAK = YES;
1990 | 				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
1991 | 				CLANG_WARN_BOOL_CONVERSION = YES;
1992 | 				CLANG_WARN_COMMA = YES;
1993 | 				CLANG_WARN_CONSTANT_CONVERSION = YES;
1994 | 				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
1995 | 				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
1996 | 				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
1997 | 				CLANG_WARN_EMPTY_BODY = YES;
1998 | 				CLANG_WARN_ENUM_CONVERSION = YES;
1999 | 				CLANG_WARN_INFINITE_RECURSION = YES;
2000 | 				CLANG_WARN_INT_CONVERSION = YES;
2001 | 				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
2002 | 				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
2003 | 				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
2004 | 				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
2005 | 				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
2006 | 				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
2007 | 				CLANG_WARN_STRICT_PROTOTYPES = YES;
2008 | 				CLANG_WARN_SUSPICIOUS_MOVE = YES;
2009 | 				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
2010 | 				CLANG_WARN_UNREACHABLE_CODE = YES;
2011 | 				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
2012 | 				CODE_SIGN_STYLE = Automatic;
2013 | 				COPY_PHASE_STRIP = NO;
2014 | 				DEAD_CODE_STRIPPING = YES;
2015 | 				DEBUG_INFORMATION_FORMAT = dwarf;
2016 | 				ENABLE_STRICT_OBJC_MSGSEND = YES;
2017 | 				ENABLE_TESTABILITY = YES;
2018 | 				ENABLE_USER_SCRIPT_SANDBOXING = YES;
2019 | 				GCC_C_LANGUAGE_STANDARD = gnu17;
2020 | 				GCC_DYNAMIC_NO_PIC = NO;
2021 | 				GCC_NO_COMMON_BLOCKS = YES;
2022 | 				GCC_OPTIMIZATION_LEVEL = 0;
2023 | 				GCC_PREPROCESSOR_DEFINITIONS = (
2024 | 					"DEBUG=1",
2025 | 					"$(inherited)",
2026 | 				);
2027 | 				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
2028 | 				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
2029 | 				GCC_WARN_UNDECLARED_SELECTOR = YES;
2030 | 				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
2031 | 				GCC_WARN_UNUSED_FUNCTION = YES;
2032 | 				GCC_WARN_UNUSED_VARIABLE = YES;
2033 | 				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
2034 | 				MACOSX_DEPLOYMENT_TARGET = 14.2;
2035 | 				MTL_ENABLE_DEBUG_INFO = INCLUDE_SOURCE;
2036 | 				MTL_FAST_MATH = YES;
2037 | 				PRODUCT_NAME = "$(TARGET_NAME)";
2038 | 				SDKROOT = macosx;
2039 | 				SWIFT_ACTIVE_COMPILATION_CONDITIONS = "DEBUG $(inherited)";
2040 | 				SWIFT_OPTIMIZATION_LEVEL = "-Onone";
2041 | 				SWIFT_STRICT_CONCURRENCY = complete;
2042 | 				SWIFT_VERSION = 5.0;
2043 | 			};
2044 | 			name = Debug;
2045 | 		};
2046 | 		C34E49272B6A026F00FCB841 /* Release */ = {
2047 | 			isa = XCBuildConfiguration;
2048 | 			buildSettings = {
2049 | 				ALWAYS_SEARCH_USER_PATHS = NO;
2050 | 				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
2051 | 				CLANG_ANALYZER_NONNULL = YES;
2052 | 				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
2053 | 				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
2054 | 				CLANG_ENABLE_MODULES = YES;
2055 | 				CLANG_ENABLE_OBJC_ARC = YES;
2056 | 				CLANG_ENABLE_OBJC_WEAK = YES;
2057 | 				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
2058 | 				CLANG_WARN_BOOL_CONVERSION = YES;
2059 | 				CLANG_WARN_COMMA = YES;
2060 | 				CLANG_WARN_CONSTANT_CONVERSION = YES;
2061 | 				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
2062 | 				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
2063 | 				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
2064 | 				CLANG_WARN_EMPTY_BODY = YES;
2065 | 				CLANG_WARN_ENUM_CONVERSION = YES;
2066 | 				CLANG_WARN_INFINITE_RECURSION = YES;
2067 | 				CLANG_WARN_INT_CONVERSION = YES;
2068 | 				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
2069 | 				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
2070 | 				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
2071 | 				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
2072 | 				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
2073 | 				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
2074 | 				CLANG_WARN_STRICT_PROTOTYPES = YES;
2075 | 				CLANG_WARN_SUSPICIOUS_MOVE = YES;
2076 | 				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
2077 | 				CLANG_WARN_UNREACHABLE_CODE = YES;
2078 | 				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
2079 | 				CODE_SIGN_STYLE = Automatic;
2080 | 				COPY_PHASE_STRIP = NO;
2081 | 				DEAD_CODE_STRIPPING = YES;
2082 | 				DEBUG_INFORMATION_FORMAT = "dwarf-with-dsym";
2083 | 				ENABLE_NS_ASSERTIONS = NO;
2084 | 				ENABLE_STRICT_OBJC_MSGSEND = YES;
2085 | 				ENABLE_USER_SCRIPT_SANDBOXING = YES;
2086 | 				GCC_C_LANGUAGE_STANDARD = gnu17;
2087 | 				GCC_NO_COMMON_BLOCKS = YES;
2088 | 				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
2089 | 				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
2090 | 				GCC_WARN_UNDECLARED_SELECTOR = YES;
2091 | 				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
2092 | 				GCC_WARN_UNUSED_FUNCTION = YES;
2093 | 				GCC_WARN_UNUSED_VARIABLE = YES;
2094 | 				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
2095 | 				MACOSX_DEPLOYMENT_TARGET = 14.2;
2096 | 				MTL_ENABLE_DEBUG_INFO = NO;
2097 | 				MTL_FAST_MATH = YES;
2098 | 				PRODUCT_NAME = "$(TARGET_NAME)";
2099 | 				SDKROOT = macosx;
2100 | 				SWIFT_COMPILATION_MODE = wholemodule;
2101 | 				SWIFT_STRICT_CONCURRENCY = complete;
2102 | 				SWIFT_VERSION = 5.0;
2103 | 			};
2104 | 			name = Release;
2105 | 		};
2106 | 		C36BEFE52BC32988002D4AFE /* Debug */ = {
2107 | 			isa = XCBuildConfiguration;
2108 | 			buildSettings = {
2109 | 				ALWAYS_SEARCH_USER_PATHS = NO;
2110 | 				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
2111 | 				CLANG_ANALYZER_NONNULL = YES;
2112 | 				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
2113 | 				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
2114 | 				CLANG_ENABLE_MODULES = YES;
2115 | 				CLANG_ENABLE_OBJC_ARC = YES;
2116 | 				CLANG_ENABLE_OBJC_WEAK = YES;
2117 | 				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
2118 | 				CLANG_WARN_BOOL_CONVERSION = YES;
2119 | 				CLANG_WARN_COMMA = YES;
2120 | 				CLANG_WARN_CONSTANT_CONVERSION = YES;
2121 | 				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
2122 | 				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
2123 | 				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
2124 | 				CLANG_WARN_EMPTY_BODY = YES;
2125 | 				CLANG_WARN_ENUM_CONVERSION = YES;
2126 | 				CLANG_WARN_INFINITE_RECURSION = YES;
2127 | 				CLANG_WARN_INT_CONVERSION = YES;
2128 | 				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
2129 | 				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
2130 | 				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
2131 | 				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
2132 | 				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
2133 | 				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
2134 | 				CLANG_WARN_STRICT_PROTOTYPES = YES;
2135 | 				CLANG_WARN_SUSPICIOUS_MOVE = YES;
2136 | 				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
2137 | 				CLANG_WARN_UNREACHABLE_CODE = YES;
2138 | 				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
2139 | 				CODE_SIGN_STYLE = Automatic;
2140 | 				COPY_PHASE_STRIP = NO;
2141 | 				DEBUG_INFORMATION_FORMAT = dwarf;
2142 | 				ENABLE_STRICT_OBJC_MSGSEND = YES;
2143 | 				ENABLE_TESTABILITY = YES;
2144 | 				ENABLE_USER_SCRIPT_SANDBOXING = YES;
2145 | 				GCC_C_LANGUAGE_STANDARD = gnu17;
2146 | 				GCC_DYNAMIC_NO_PIC = NO;
2147 | 				GCC_NO_COMMON_BLOCKS = YES;
2148 | 				GCC_OPTIMIZATION_LEVEL = 0;
2149 | 				GCC_PREPROCESSOR_DEFINITIONS = (
2150 | 					"DEBUG=1",
2151 | 					"$(inherited)",
2152 | 				);
2153 | 				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
2154 | 				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
2155 | 				GCC_WARN_UNDECLARED_SELECTOR = YES;
2156 | 				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
2157 | 				GCC_WARN_UNUSED_FUNCTION = YES;
2158 | 				GCC_WARN_UNUSED_VARIABLE = YES;
2159 | 				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
2160 | 				MACOSX_DEPLOYMENT_TARGET = 14.4;
2161 | 				MTL_ENABLE_DEBUG_INFO = INCLUDE_SOURCE;
2162 | 				MTL_FAST_MATH = YES;
2163 | 				PRODUCT_NAME = "$(TARGET_NAME)";
2164 | 				SDKROOT = macosx;
2165 | 				SWIFT_ACTIVE_COMPILATION_CONDITIONS = "DEBUG $(inherited)";
2166 | 				SWIFT_OPTIMIZATION_LEVEL = "-Onone";
2167 | 				SWIFT_STRICT_CONCURRENCY = complete;
2168 | 				SWIFT_VERSION = 5.0;
2169 | 			};
2170 | 			name = Debug;
2171 | 		};
2172 | 		C36BEFE62BC32988002D4AFE /* Release */ = {
2173 | 			isa = XCBuildConfiguration;
2174 | 			buildSettings = {
2175 | 				ALWAYS_SEARCH_USER_PATHS = NO;
2176 | 				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
2177 | 				CLANG_ANALYZER_NONNULL = YES;
2178 | 				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
2179 | 				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
2180 | 				CLANG_ENABLE_MODULES = YES;
2181 | 				CLANG_ENABLE_OBJC_ARC = YES;
2182 | 				CLANG_ENABLE_OBJC_WEAK = YES;
2183 | 				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
2184 | 				CLANG_WARN_BOOL_CONVERSION = YES;
2185 | 				CLANG_WARN_COMMA = YES;
2186 | 				CLANG_WARN_CONSTANT_CONVERSION = YES;
2187 | 				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
2188 | 				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
2189 | 				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
2190 | 				CLANG_WARN_EMPTY_BODY = YES;
2191 | 				CLANG_WARN_ENUM_CONVERSION = YES;
2192 | 				CLANG_WARN_INFINITE_RECURSION = YES;
2193 | 				CLANG_WARN_INT_CONVERSION = YES;
2194 | 				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
2195 | 				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
2196 | 				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
2197 | 				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
2198 | 				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
2199 | 				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
2200 | 				CLANG_WARN_STRICT_PROTOTYPES = YES;
2201 | 				CLANG_WARN_SUSPICIOUS_MOVE = YES;
2202 | 				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
2203 | 				CLANG_WARN_UNREACHABLE_CODE = YES;
2204 | 				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
2205 | 				CODE_SIGN_STYLE = Automatic;
2206 | 				COPY_PHASE_STRIP = NO;
2207 | 				DEBUG_INFORMATION_FORMAT = "dwarf-with-dsym";
2208 | 				ENABLE_NS_ASSERTIONS = NO;
2209 | 				ENABLE_STRICT_OBJC_MSGSEND = YES;
2210 | 				ENABLE_USER_SCRIPT_SANDBOXING = YES;
2211 | 				GCC_C_LANGUAGE_STANDARD = gnu17;
2212 | 				GCC_NO_COMMON_BLOCKS = YES;
2213 | 				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
2214 | 				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
2215 | 				GCC_WARN_UNDECLARED_SELECTOR = YES;
2216 | 				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
2217 | 				GCC_WARN_UNUSED_FUNCTION = YES;
2218 | 				GCC_WARN_UNUSED_VARIABLE = YES;
2219 | 				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
2220 | 				MACOSX_DEPLOYMENT_TARGET = 14.4;
2221 | 				MTL_ENABLE_DEBUG_INFO = NO;
2222 | 				MTL_FAST_MATH = YES;
2223 | 				PRODUCT_NAME = "$(TARGET_NAME)";
2224 | 				SDKROOT = macosx;
2225 | 				SWIFT_COMPILATION_MODE = wholemodule;
2226 | 				SWIFT_STRICT_CONCURRENCY = complete;
2227 | 				SWIFT_VERSION = 5.0;
2228 | 			};
2229 | 			name = Release;
2230 | 		};
2231 | 		C36BF00E2BC5CE56002D4AFE /* Debug */ = {
2232 | 			isa = XCBuildConfiguration;
2233 | 			buildSettings = {
2234 | 				ALWAYS_SEARCH_USER_PATHS = NO;
2235 | 				ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;
2236 | 				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
2237 | 				ASSETCATALOG_COMPILER_GLOBAL_ACCENT_COLOR_NAME = AccentColor;
2238 | 				CLANG_ANALYZER_NONNULL = YES;
2239 | 				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
2240 | 				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
2241 | 				CLANG_ENABLE_MODULES = YES;
2242 | 				CLANG_ENABLE_OBJC_ARC = YES;
2243 | 				CLANG_ENABLE_OBJC_WEAK = YES;
2244 | 				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
2245 | 				CLANG_WARN_BOOL_CONVERSION = YES;
2246 | 				CLANG_WARN_COMMA = YES;
2247 | 				CLANG_WARN_CONSTANT_CONVERSION = YES;
2248 | 				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
2249 | 				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
2250 | 				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
2251 | 				CLANG_WARN_EMPTY_BODY = YES;
2252 | 				CLANG_WARN_ENUM_CONVERSION = YES;
2253 | 				CLANG_WARN_INFINITE_RECURSION = YES;
2254 | 				CLANG_WARN_INT_CONVERSION = YES;
2255 | 				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
2256 | 				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
2257 | 				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
2258 | 				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
2259 | 				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
2260 | 				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
2261 | 				CLANG_WARN_STRICT_PROTOTYPES = YES;
2262 | 				CLANG_WARN_SUSPICIOUS_MOVE = YES;
2263 | 				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
2264 | 				CLANG_WARN_UNREACHABLE_CODE = YES;
2265 | 				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
2266 | 				CODE_SIGN_ENTITLEMENTS = Applications/StableDiffusionExample/StableDiffusionExample.entitlements;
2267 | 				CODE_SIGN_STYLE = Automatic;
2268 | 				COPY_PHASE_STRIP = NO;
2269 | 				CURRENT_PROJECT_VERSION = 1;
2270 | 				DEBUG_INFORMATION_FORMAT = dwarf;
2271 | 				DEVELOPMENT_ASSET_PATHS = "\"Applications/StableDiffusionExample/Preview Content\"";
2272 | 				DEVELOPMENT_TEAM = "";
2273 | 				ENABLE_PREVIEWS = YES;
2274 | 				ENABLE_STRICT_OBJC_MSGSEND = YES;
2275 | 				ENABLE_TESTABILITY = YES;
2276 | 				ENABLE_USER_SCRIPT_SANDBOXING = YES;
2277 | 				GCC_C_LANGUAGE_STANDARD = gnu17;
2278 | 				GCC_DYNAMIC_NO_PIC = NO;
2279 | 				GCC_NO_COMMON_BLOCKS = YES;
2280 | 				GCC_OPTIMIZATION_LEVEL = 0;
2281 | 				GCC_PREPROCESSOR_DEFINITIONS = (
2282 | 					"DEBUG=1",
2283 | 					"$(inherited)",
2284 | 				);
2285 | 				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
2286 | 				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
2287 | 				GCC_WARN_UNDECLARED_SELECTOR = YES;
2288 | 				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
2289 | 				GCC_WARN_UNUSED_FUNCTION = YES;
2290 | 				GCC_WARN_UNUSED_VARIABLE = YES;
2291 | 				GENERATE_INFOPLIST_FILE = YES;
2292 | 				"INFOPLIST_KEY_UIApplicationSceneManifest_Generation[sdk=iphoneos*]" = YES;
2293 | 				"INFOPLIST_KEY_UIApplicationSceneManifest_Generation[sdk=iphonesimulator*]" = YES;
2294 | 				"INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents[sdk=iphoneos*]" = YES;
2295 | 				"INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents[sdk=iphonesimulator*]" = YES;
2296 | 				"INFOPLIST_KEY_UILaunchScreen_Generation[sdk=iphoneos*]" = YES;
2297 | 				"INFOPLIST_KEY_UILaunchScreen_Generation[sdk=iphonesimulator*]" = YES;
2298 | 				"INFOPLIST_KEY_UIStatusBarStyle[sdk=iphoneos*]" = UIStatusBarStyleDefault;
2299 | 				"INFOPLIST_KEY_UIStatusBarStyle[sdk=iphonesimulator*]" = UIStatusBarStyleDefault;
2300 | 				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPad = "UIInterfaceOrientationPortrait UIInterfaceOrientationPortraitUpsideDown UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
2301 | 				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPhone = "UIInterfaceOrientationPortrait UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
2302 | 				IPHONEOS_DEPLOYMENT_TARGET = 17.2;
2303 | 				LD_RUNPATH_SEARCH_PATHS = "@executable_path/Frameworks";
2304 | 				"LD_RUNPATH_SEARCH_PATHS[sdk=macosx*]" = "@executable_path/../Frameworks";
2305 | 				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
2306 | 				MACOSX_DEPLOYMENT_TARGET = 14.2;
2307 | 				MARKETING_VERSION = 1.0;
2308 | 				MTL_ENABLE_DEBUG_INFO = INCLUDE_SOURCE;
2309 | 				MTL_FAST_MATH = YES;
2310 | 				PRODUCT_BUNDLE_IDENTIFIER = "mlx.StableDiffusionExample${DISAMBIGUATOR}";
2311 | 				PRODUCT_NAME = "$(TARGET_NAME)";
2312 | 				SDKROOT = auto;
2313 | 				SUPPORTED_PLATFORMS = "iphoneos iphonesimulator macosx xros xrsimulator";
2314 | 				SUPPORTS_MACCATALYST = NO;
2315 | 				SWIFT_ACTIVE_COMPILATION_CONDITIONS = "DEBUG $(inherited)";
2316 | 				SWIFT_EMIT_LOC_STRINGS = YES;
2317 | 				SWIFT_OPTIMIZATION_LEVEL = "-Onone";
2318 | 				SWIFT_STRICT_CONCURRENCY = complete;
2319 | 				SWIFT_VERSION = 5.0;
2320 | 				TARGETED_DEVICE_FAMILY = "1,2,7";
2321 | 			};
2322 | 			name = Debug;
2323 | 		};
2324 | 		C36BF00F2BC5CE56002D4AFE /* Release */ = {
2325 | 			isa = XCBuildConfiguration;
2326 | 			buildSettings = {
2327 | 				ALWAYS_SEARCH_USER_PATHS = NO;
2328 | 				ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;
2329 | 				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
2330 | 				ASSETCATALOG_COMPILER_GLOBAL_ACCENT_COLOR_NAME = AccentColor;
2331 | 				CLANG_ANALYZER_NONNULL = YES;
2332 | 				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
2333 | 				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
2334 | 				CLANG_ENABLE_MODULES = YES;
2335 | 				CLANG_ENABLE_OBJC_ARC = YES;
2336 | 				CLANG_ENABLE_OBJC_WEAK = YES;
2337 | 				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
2338 | 				CLANG_WARN_BOOL_CONVERSION = YES;
2339 | 				CLANG_WARN_COMMA = YES;
2340 | 				CLANG_WARN_CONSTANT_CONVERSION = YES;
2341 | 				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
2342 | 				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
2343 | 				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
2344 | 				CLANG_WARN_EMPTY_BODY = YES;
2345 | 				CLANG_WARN_ENUM_CONVERSION = YES;
2346 | 				CLANG_WARN_INFINITE_RECURSION = YES;
2347 | 				CLANG_WARN_INT_CONVERSION = YES;
2348 | 				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
2349 | 				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
2350 | 				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
2351 | 				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
2352 | 				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
2353 | 				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
2354 | 				CLANG_WARN_STRICT_PROTOTYPES = YES;
2355 | 				CLANG_WARN_SUSPICIOUS_MOVE = YES;
2356 | 				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
2357 | 				CLANG_WARN_UNREACHABLE_CODE = YES;
2358 | 				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
2359 | 				CODE_SIGN_ENTITLEMENTS = Applications/StableDiffusionExample/StableDiffusionExample.entitlements;
2360 | 				CODE_SIGN_STYLE = Automatic;
2361 | 				COPY_PHASE_STRIP = NO;
2362 | 				CURRENT_PROJECT_VERSION = 1;
2363 | 				DEBUG_INFORMATION_FORMAT = "dwarf-with-dsym";
2364 | 				DEVELOPMENT_ASSET_PATHS = "\"Applications/StableDiffusionExample/Preview Content\"";
2365 | 				DEVELOPMENT_TEAM = "";
2366 | 				ENABLE_NS_ASSERTIONS = NO;
2367 | 				ENABLE_PREVIEWS = YES;
2368 | 				ENABLE_STRICT_OBJC_MSGSEND = YES;
2369 | 				ENABLE_USER_SCRIPT_SANDBOXING = YES;
2370 | 				GCC_C_LANGUAGE_STANDARD = gnu17;
2371 | 				GCC_NO_COMMON_BLOCKS = YES;
2372 | 				GCC_OPTIMIZATION_LEVEL = 2;
2373 | 				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
2374 | 				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
2375 | 				GCC_WARN_UNDECLARED_SELECTOR = YES;
2376 | 				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
2377 | 				GCC_WARN_UNUSED_FUNCTION = YES;
2378 | 				GCC_WARN_UNUSED_VARIABLE = YES;
2379 | 				GENERATE_INFOPLIST_FILE = YES;
2380 | 				"INFOPLIST_KEY_UIApplicationSceneManifest_Generation[sdk=iphoneos*]" = YES;
2381 | 				"INFOPLIST_KEY_UIApplicationSceneManifest_Generation[sdk=iphonesimulator*]" = YES;
2382 | 				"INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents[sdk=iphoneos*]" = YES;
2383 | 				"INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents[sdk=iphonesimulator*]" = YES;
2384 | 				"INFOPLIST_KEY_UILaunchScreen_Generation[sdk=iphoneos*]" = YES;
2385 | 				"INFOPLIST_KEY_UILaunchScreen_Generation[sdk=iphonesimulator*]" = YES;
2386 | 				"INFOPLIST_KEY_UIStatusBarStyle[sdk=iphoneos*]" = UIStatusBarStyleDefault;
2387 | 				"INFOPLIST_KEY_UIStatusBarStyle[sdk=iphonesimulator*]" = UIStatusBarStyleDefault;
2388 | 				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPad = "UIInterfaceOrientationPortrait UIInterfaceOrientationPortraitUpsideDown UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
2389 | 				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPhone = "UIInterfaceOrientationPortrait UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
2390 | 				IPHONEOS_DEPLOYMENT_TARGET = 17.2;
2391 | 				LD_RUNPATH_SEARCH_PATHS = "@executable_path/Frameworks";
2392 | 				"LD_RUNPATH_SEARCH_PATHS[sdk=macosx*]" = "@executable_path/../Frameworks";
2393 | 				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
2394 | 				MACOSX_DEPLOYMENT_TARGET = 14.2;
2395 | 				MARKETING_VERSION = 1.0;
2396 | 				MTL_ENABLE_DEBUG_INFO = NO;
2397 | 				MTL_FAST_MATH = YES;
2398 | 				PRODUCT_BUNDLE_IDENTIFIER = "mlx.StableDiffusionExample${DISAMBIGUATOR}";
2399 | 				PRODUCT_NAME = "$(TARGET_NAME)";
2400 | 				SDKROOT = auto;
2401 | 				SUPPORTED_PLATFORMS = "iphoneos iphonesimulator macosx xros xrsimulator";
2402 | 				SUPPORTS_MACCATALYST = NO;
2403 | 				SWIFT_COMPILATION_MODE = wholemodule;
2404 | 				SWIFT_EMIT_LOC_STRINGS = YES;
2405 | 				SWIFT_STRICT_CONCURRENCY = complete;
2406 | 				SWIFT_VERSION = 5.0;
2407 | 				TARGETED_DEVICE_FAMILY = "1,2,7";
2408 | 			};
2409 | 			name = Release;
2410 | 		};
2411 | 		C392736C2B60697700368D5D /* Debug */ = {
2412 | 			isa = XCBuildConfiguration;
2413 | 			baseConfigurationReference = C3C36A6B2CA714600099FFA4 /* Build.xcconfig */;
2414 | 			buildSettings = {
2415 | 				ARCHS = "$(ARCHS_STANDARD)";
2416 | 				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
2417 | 				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
2418 | 				CLANG_WARN_BOOL_CONVERSION = YES;
2419 | 				CLANG_WARN_COMMA = YES;
2420 | 				CLANG_WARN_CONSTANT_CONVERSION = YES;
2421 | 				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
2422 | 				CLANG_WARN_EMPTY_BODY = YES;
2423 | 				CLANG_WARN_ENUM_CONVERSION = YES;
2424 | 				CLANG_WARN_INFINITE_RECURSION = YES;
2425 | 				CLANG_WARN_INT_CONVERSION = YES;
2426 | 				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
2427 | 				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
2428 | 				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
2429 | 				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
2430 | 				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
2431 | 				CLANG_WARN_STRICT_PROTOTYPES = YES;
2432 | 				CLANG_WARN_SUSPICIOUS_MOVE = YES;
2433 | 				CLANG_WARN_UNREACHABLE_CODE = YES;
2434 | 				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
2435 | 				DEAD_CODE_STRIPPING = YES;
2436 | 				ENABLE_STRICT_OBJC_MSGSEND = YES;
2437 | 				ENABLE_TESTABILITY = YES;
2438 | 				EXCLUDED_ARCHS = x86_64;
2439 | 				GCC_NO_COMMON_BLOCKS = YES;
2440 | 				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
2441 | 				GCC_WARN_ABOUT_RETURN_TYPE = YES;
2442 | 				GCC_WARN_UNDECLARED_SELECTOR = YES;
2443 | 				GCC_WARN_UNINITIALIZED_AUTOS = YES;
2444 | 				GCC_WARN_UNUSED_FUNCTION = YES;
2445 | 				GCC_WARN_UNUSED_VARIABLE = YES;
2446 | 				ONLY_ACTIVE_ARCH = YES;
2447 | 			};
2448 | 			name = Debug;
2449 | 		};
2450 | 		C392736D2B60697700368D5D /* Release */ = {
2451 | 			isa = XCBuildConfiguration;
2452 | 			baseConfigurationReference = C3C36A6B2CA714600099FFA4 /* Build.xcconfig */;
2453 | 			buildSettings = {
2454 | 				ARCHS = "$(ARCHS_STANDARD)";
2455 | 				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
2456 | 				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
2457 | 				CLANG_WARN_BOOL_CONVERSION = YES;
2458 | 				CLANG_WARN_COMMA = YES;
2459 | 				CLANG_WARN_CONSTANT_CONVERSION = YES;
2460 | 				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
2461 | 				CLANG_WARN_EMPTY_BODY = YES;
2462 | 				CLANG_WARN_ENUM_CONVERSION = YES;
2463 | 				CLANG_WARN_INFINITE_RECURSION = YES;
2464 | 				CLANG_WARN_INT_CONVERSION = YES;
2465 | 				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
2466 | 				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
2467 | 				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
2468 | 				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
2469 | 				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
2470 | 				CLANG_WARN_STRICT_PROTOTYPES = YES;
2471 | 				CLANG_WARN_SUSPICIOUS_MOVE = YES;
2472 | 				CLANG_WARN_UNREACHABLE_CODE = YES;
2473 | 				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
2474 | 				DEAD_CODE_STRIPPING = YES;
2475 | 				ENABLE_STRICT_OBJC_MSGSEND = YES;
2476 | 				EXCLUDED_ARCHS = x86_64;
2477 | 				GCC_NO_COMMON_BLOCKS = YES;
2478 | 				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
2479 | 				GCC_WARN_ABOUT_RETURN_TYPE = YES;
2480 | 				GCC_WARN_UNDECLARED_SELECTOR = YES;
2481 | 				GCC_WARN_UNINITIALIZED_AUTOS = YES;
2482 | 				GCC_WARN_UNUSED_FUNCTION = YES;
2483 | 				GCC_WARN_UNUSED_VARIABLE = YES;
2484 | 				ONLY_ACTIVE_ARCH = YES;
2485 | 			};
2486 | 			name = Release;
2487 | 		};
2488 | 		C392737A2B606A0A00368D5D /* Debug */ = {
2489 | 			isa = XCBuildConfiguration;
2490 | 			buildSettings = {
2491 | 				ALWAYS_SEARCH_USER_PATHS = NO;
2492 | 				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
2493 | 				CLANG_ANALYZER_NONNULL = YES;
2494 | 				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
2495 | 				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
2496 | 				CLANG_ENABLE_MODULES = YES;
2497 | 				CLANG_ENABLE_OBJC_ARC = YES;
2498 | 				CLANG_ENABLE_OBJC_WEAK = YES;
2499 | 				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
2500 | 				CLANG_WARN_BOOL_CONVERSION = YES;
2501 | 				CLANG_WARN_COMMA = YES;
2502 | 				CLANG_WARN_CONSTANT_CONVERSION = YES;
2503 | 				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
2504 | 				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
2505 | 				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
2506 | 				CLANG_WARN_EMPTY_BODY = YES;
2507 | 				CLANG_WARN_ENUM_CONVERSION = YES;
2508 | 				CLANG_WARN_INFINITE_RECURSION = YES;
2509 | 				CLANG_WARN_INT_CONVERSION = YES;
2510 | 				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
2511 | 				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
2512 | 				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
2513 | 				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
2514 | 				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
2515 | 				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
2516 | 				CLANG_WARN_STRICT_PROTOTYPES = YES;
2517 | 				CLANG_WARN_SUSPICIOUS_MOVE = YES;
2518 | 				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
2519 | 				CLANG_WARN_UNREACHABLE_CODE = YES;
2520 | 				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
2521 | 				CODE_SIGN_STYLE = Automatic;
2522 | 				COPY_PHASE_STRIP = NO;
2523 | 				DEAD_CODE_STRIPPING = YES;
2524 | 				DEBUG_INFORMATION_FORMAT = dwarf;
2525 | 				ENABLE_STRICT_OBJC_MSGSEND = YES;
2526 | 				ENABLE_TESTABILITY = YES;
2527 | 				ENABLE_USER_SCRIPT_SANDBOXING = YES;
2528 | 				GCC_C_LANGUAGE_STANDARD = gnu17;
2529 | 				GCC_DYNAMIC_NO_PIC = NO;
2530 | 				GCC_NO_COMMON_BLOCKS = YES;
2531 | 				GCC_OPTIMIZATION_LEVEL = 0;
2532 | 				GCC_PREPROCESSOR_DEFINITIONS = (
2533 | 					"DEBUG=1",
2534 | 					"$(inherited)",
2535 | 				);
2536 | 				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
2537 | 				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
2538 | 				GCC_WARN_UNDECLARED_SELECTOR = YES;
2539 | 				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
2540 | 				GCC_WARN_UNUSED_FUNCTION = YES;
2541 | 				GCC_WARN_UNUSED_VARIABLE = YES;
2542 | 				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
2543 | 				MACOSX_DEPLOYMENT_TARGET = 14.0;
2544 | 				MTL_ENABLE_DEBUG_INFO = INCLUDE_SOURCE;
2545 | 				MTL_FAST_MATH = YES;
2546 | 				ONLY_ACTIVE_ARCH = YES;
2547 | 				PRODUCT_NAME = "$(TARGET_NAME)";
2548 | 				SDKROOT = macosx;
2549 | 				SWIFT_ACTIVE_COMPILATION_CONDITIONS = "DEBUG $(inherited)";
2550 | 				SWIFT_OPTIMIZATION_LEVEL = "-Onone";
2551 | 				SWIFT_STRICT_CONCURRENCY = complete;
2552 | 				SWIFT_VERSION = 5.0;
2553 | 			};
2554 | 			name = Debug;
2555 | 		};
2556 | 		C392737B2B606A0A00368D5D /* Release */ = {
2557 | 			isa = XCBuildConfiguration;
2558 | 			buildSettings = {
2559 | 				ALWAYS_SEARCH_USER_PATHS = NO;
2560 | 				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
2561 | 				CLANG_ANALYZER_NONNULL = YES;
2562 | 				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
2563 | 				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
2564 | 				CLANG_ENABLE_MODULES = YES;
2565 | 				CLANG_ENABLE_OBJC_ARC = YES;
2566 | 				CLANG_ENABLE_OBJC_WEAK = YES;
2567 | 				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
2568 | 				CLANG_WARN_BOOL_CONVERSION = YES;
2569 | 				CLANG_WARN_COMMA = YES;
2570 | 				CLANG_WARN_CONSTANT_CONVERSION = YES;
2571 | 				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
2572 | 				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
2573 | 				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
2574 | 				CLANG_WARN_EMPTY_BODY = YES;
2575 | 				CLANG_WARN_ENUM_CONVERSION = YES;
2576 | 				CLANG_WARN_INFINITE_RECURSION = YES;
2577 | 				CLANG_WARN_INT_CONVERSION = YES;
2578 | 				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
2579 | 				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
2580 | 				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
2581 | 				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
2582 | 				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
2583 | 				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
2584 | 				CLANG_WARN_STRICT_PROTOTYPES = YES;
2585 | 				CLANG_WARN_SUSPICIOUS_MOVE = YES;
2586 | 				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
2587 | 				CLANG_WARN_UNREACHABLE_CODE = YES;
2588 | 				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
2589 | 				CODE_SIGN_STYLE = Automatic;
2590 | 				COPY_PHASE_STRIP = NO;
2591 | 				DEAD_CODE_STRIPPING = YES;
2592 | 				DEBUG_INFORMATION_FORMAT = "dwarf-with-dsym";
2593 | 				ENABLE_NS_ASSERTIONS = NO;
2594 | 				ENABLE_STRICT_OBJC_MSGSEND = YES;
2595 | 				ENABLE_USER_SCRIPT_SANDBOXING = YES;
2596 | 				GCC_C_LANGUAGE_STANDARD = gnu17;
2597 | 				GCC_NO_COMMON_BLOCKS = YES;
2598 | 				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
2599 | 				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
2600 | 				GCC_WARN_UNDECLARED_SELECTOR = YES;
2601 | 				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
2602 | 				GCC_WARN_UNUSED_FUNCTION = YES;
2603 | 				GCC_WARN_UNUSED_VARIABLE = YES;
2604 | 				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
2605 | 				MACOSX_DEPLOYMENT_TARGET = 14.0;
2606 | 				MTL_ENABLE_DEBUG_INFO = NO;
2607 | 				MTL_FAST_MATH = YES;
2608 | 				PRODUCT_NAME = "$(TARGET_NAME)";
2609 | 				SDKROOT = macosx;
2610 | 				SWIFT_COMPILATION_MODE = wholemodule;
2611 | 				SWIFT_STRICT_CONCURRENCY = complete;
2612 | 				SWIFT_VERSION = 5.0;
2613 | 			};
2614 | 			name = Release;
2615 | 		};
2616 | 		C397C5902B62C6A9004B084D /* Debug */ = {
2617 | 			isa = XCBuildConfiguration;
2618 | 			buildSettings = {
2619 | 				ALWAYS_SEARCH_USER_PATHS = NO;
2620 | 				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
2621 | 				CLANG_ANALYZER_NONNULL = YES;
2622 | 				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
2623 | 				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
2624 | 				CLANG_ENABLE_MODULES = YES;
2625 | 				CLANG_ENABLE_OBJC_ARC = YES;
2626 | 				CLANG_ENABLE_OBJC_WEAK = YES;
2627 | 				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
2628 | 				CLANG_WARN_BOOL_CONVERSION = YES;
2629 | 				CLANG_WARN_COMMA = YES;
2630 | 				CLANG_WARN_CONSTANT_CONVERSION = YES;
2631 | 				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
2632 | 				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
2633 | 				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
2634 | 				CLANG_WARN_EMPTY_BODY = YES;
2635 | 				CLANG_WARN_ENUM_CONVERSION = YES;
2636 | 				CLANG_WARN_INFINITE_RECURSION = YES;
2637 | 				CLANG_WARN_INT_CONVERSION = YES;
2638 | 				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
2639 | 				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
2640 | 				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
2641 | 				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
2642 | 				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
2643 | 				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
2644 | 				CLANG_WARN_STRICT_PROTOTYPES = YES;
2645 | 				CLANG_WARN_SUSPICIOUS_MOVE = YES;
2646 | 				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
2647 | 				CLANG_WARN_UNREACHABLE_CODE = YES;
2648 | 				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
2649 | 				CODE_SIGN_STYLE = Automatic;
2650 | 				COPY_PHASE_STRIP = NO;
2651 | 				DEAD_CODE_STRIPPING = YES;
2652 | 				DEBUG_INFORMATION_FORMAT = dwarf;
2653 | 				ENABLE_STRICT_OBJC_MSGSEND = YES;
2654 | 				ENABLE_TESTABILITY = YES;
2655 | 				ENABLE_USER_SCRIPT_SANDBOXING = YES;
2656 | 				GCC_C_LANGUAGE_STANDARD = gnu17;
2657 | 				GCC_DYNAMIC_NO_PIC = NO;
2658 | 				GCC_NO_COMMON_BLOCKS = YES;
2659 | 				GCC_OPTIMIZATION_LEVEL = 0;
2660 | 				GCC_PREPROCESSOR_DEFINITIONS = (
2661 | 					"DEBUG=1",
2662 | 					"$(inherited)",
2663 | 				);
2664 | 				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
2665 | 				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
2666 | 				GCC_WARN_UNDECLARED_SELECTOR = YES;
2667 | 				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
2668 | 				GCC_WARN_UNUSED_FUNCTION = YES;
2669 | 				GCC_WARN_UNUSED_VARIABLE = YES;
2670 | 				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
2671 | 				MACOSX_DEPLOYMENT_TARGET = 14.0;
2672 | 				MTL_ENABLE_DEBUG_INFO = INCLUDE_SOURCE;
2673 | 				MTL_FAST_MATH = YES;
2674 | 				PRODUCT_NAME = "$(TARGET_NAME)";
2675 | 				SDKROOT = macosx;
2676 | 				SWIFT_ACTIVE_COMPILATION_CONDITIONS = "DEBUG $(inherited)";
2677 | 				SWIFT_OPTIMIZATION_LEVEL = "-Onone";
2678 | 				SWIFT_STRICT_CONCURRENCY = complete;
2679 | 				SWIFT_VERSION = 5.0;
2680 | 			};
2681 | 			name = Debug;
2682 | 		};
2683 | 		C397C5912B62C6A9004B084D /* Release */ = {
2684 | 			isa = XCBuildConfiguration;
2685 | 			buildSettings = {
2686 | 				ALWAYS_SEARCH_USER_PATHS = NO;
2687 | 				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
2688 | 				CLANG_ANALYZER_NONNULL = YES;
2689 | 				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
2690 | 				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
2691 | 				CLANG_ENABLE_MODULES = YES;
2692 | 				CLANG_ENABLE_OBJC_ARC = YES;
2693 | 				CLANG_ENABLE_OBJC_WEAK = YES;
2694 | 				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
2695 | 				CLANG_WARN_BOOL_CONVERSION = YES;
2696 | 				CLANG_WARN_COMMA = YES;
2697 | 				CLANG_WARN_CONSTANT_CONVERSION = YES;
2698 | 				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
2699 | 				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
2700 | 				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
2701 | 				CLANG_WARN_EMPTY_BODY = YES;
2702 | 				CLANG_WARN_ENUM_CONVERSION = YES;
2703 | 				CLANG_WARN_INFINITE_RECURSION = YES;
2704 | 				CLANG_WARN_INT_CONVERSION = YES;
2705 | 				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
2706 | 				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
2707 | 				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
2708 | 				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
2709 | 				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
2710 | 				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
2711 | 				CLANG_WARN_STRICT_PROTOTYPES = YES;
2712 | 				CLANG_WARN_SUSPICIOUS_MOVE = YES;
2713 | 				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
2714 | 				CLANG_WARN_UNREACHABLE_CODE = YES;
2715 | 				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
2716 | 				CODE_SIGN_STYLE = Automatic;
2717 | 				COPY_PHASE_STRIP = NO;
2718 | 				DEAD_CODE_STRIPPING = YES;
2719 | 				DEBUG_INFORMATION_FORMAT = "dwarf-with-dsym";
2720 | 				ENABLE_NS_ASSERTIONS = NO;
2721 | 				ENABLE_STRICT_OBJC_MSGSEND = YES;
2722 | 				ENABLE_USER_SCRIPT_SANDBOXING = YES;
2723 | 				GCC_C_LANGUAGE_STANDARD = gnu17;
2724 | 				GCC_NO_COMMON_BLOCKS = YES;
2725 | 				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
2726 | 				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
2727 | 				GCC_WARN_UNDECLARED_SELECTOR = YES;
2728 | 				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
2729 | 				GCC_WARN_UNUSED_FUNCTION = YES;
2730 | 				GCC_WARN_UNUSED_VARIABLE = YES;
2731 | 				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
2732 | 				MACOSX_DEPLOYMENT_TARGET = 14.0;
2733 | 				MTL_ENABLE_DEBUG_INFO = NO;
2734 | 				MTL_FAST_MATH = YES;
2735 | 				PRODUCT_NAME = "$(TARGET_NAME)";
2736 | 				SDKROOT = macosx;
2737 | 				SWIFT_COMPILATION_MODE = wholemodule;
2738 | 				SWIFT_STRICT_CONCURRENCY = complete;
2739 | 				SWIFT_VERSION = 5.0;
2740 | 			};
2741 | 			name = Release;
2742 | 		};
2743 | 		C3A8B3BF2B92950A0002EFB8 /* Debug */ = {
2744 | 			isa = XCBuildConfiguration;
2745 | 			buildSettings = {
2746 | 				ALWAYS_SEARCH_USER_PATHS = NO;
2747 | 				ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;
2748 | 				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
2749 | 				ASSETCATALOG_COMPILER_GLOBAL_ACCENT_COLOR_NAME = AccentColor;
2750 | 				CLANG_ANALYZER_NONNULL = YES;
2751 | 				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
2752 | 				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
2753 | 				CLANG_ENABLE_MODULES = YES;
2754 | 				CLANG_ENABLE_OBJC_ARC = YES;
2755 | 				CLANG_ENABLE_OBJC_WEAK = YES;
2756 | 				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
2757 | 				CLANG_WARN_BOOL_CONVERSION = YES;
2758 | 				CLANG_WARN_COMMA = YES;
2759 | 				CLANG_WARN_CONSTANT_CONVERSION = YES;
2760 | 				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
2761 | 				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
2762 | 				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
2763 | 				CLANG_WARN_EMPTY_BODY = YES;
2764 | 				CLANG_WARN_ENUM_CONVERSION = YES;
2765 | 				CLANG_WARN_INFINITE_RECURSION = YES;
2766 | 				CLANG_WARN_INT_CONVERSION = YES;
2767 | 				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
2768 | 				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
2769 | 				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
2770 | 				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
2771 | 				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
2772 | 				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
2773 | 				CLANG_WARN_STRICT_PROTOTYPES = YES;
2774 | 				CLANG_WARN_SUSPICIOUS_MOVE = YES;
2775 | 				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
2776 | 				CLANG_WARN_UNREACHABLE_CODE = YES;
2777 | 				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
2778 | 				CODE_SIGN_ENTITLEMENTS = Applications/MNISTTrainer/MNISTTrainer.entitlements;
2779 | 				CODE_SIGN_STYLE = Automatic;
2780 | 				COPY_PHASE_STRIP = NO;
2781 | 				CURRENT_PROJECT_VERSION = 1;
2782 | 				DEAD_CODE_STRIPPING = YES;
2783 | 				DEBUG_INFORMATION_FORMAT = dwarf;
2784 | 				DEVELOPMENT_ASSET_PATHS = "\"Applications/MNISTTrainer/Preview Content\"";
2785 | 				ENABLE_PREVIEWS = YES;
2786 | 				ENABLE_STRICT_OBJC_MSGSEND = YES;
2787 | 				ENABLE_TESTABILITY = YES;
2788 | 				ENABLE_USER_SCRIPT_SANDBOXING = YES;
2789 | 				GCC_C_LANGUAGE_STANDARD = gnu17;
2790 | 				GCC_DYNAMIC_NO_PIC = NO;
2791 | 				GCC_NO_COMMON_BLOCKS = YES;
2792 | 				GCC_OPTIMIZATION_LEVEL = 0;
2793 | 				GCC_PREPROCESSOR_DEFINITIONS = (
2794 | 					"DEBUG=1",
2795 | 					"$(inherited)",
2796 | 				);
2797 | 				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
2798 | 				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
2799 | 				GCC_WARN_UNDECLARED_SELECTOR = YES;
2800 | 				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
2801 | 				GCC_WARN_UNUSED_FUNCTION = YES;
2802 | 				GCC_WARN_UNUSED_VARIABLE = YES;
2803 | 				GENERATE_INFOPLIST_FILE = YES;
2804 | 				INFOPLIST_FILE = "Applications/MNISTTrainer/MNISTTrainer-Info.plist";
2805 | 				"INFOPLIST_KEY_UIApplicationSceneManifest_Generation[sdk=iphoneos*]" = YES;
2806 | 				"INFOPLIST_KEY_UIApplicationSceneManifest_Generation[sdk=iphonesimulator*]" = YES;
2807 | 				"INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents[sdk=iphoneos*]" = YES;
2808 | 				"INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents[sdk=iphonesimulator*]" = YES;
2809 | 				"INFOPLIST_KEY_UILaunchScreen_Generation[sdk=iphoneos*]" = YES;
2810 | 				"INFOPLIST_KEY_UILaunchScreen_Generation[sdk=iphonesimulator*]" = YES;
2811 | 				"INFOPLIST_KEY_UIStatusBarStyle[sdk=iphoneos*]" = UIStatusBarStyleDefault;
2812 | 				"INFOPLIST_KEY_UIStatusBarStyle[sdk=iphonesimulator*]" = UIStatusBarStyleDefault;
2813 | 				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPad = "UIInterfaceOrientationPortrait UIInterfaceOrientationPortraitUpsideDown UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
2814 | 				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPhone = "UIInterfaceOrientationPortrait UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
2815 | 				IPHONEOS_DEPLOYMENT_TARGET = 17.2;
2816 | 				LD_RUNPATH_SEARCH_PATHS = "@executable_path/Frameworks";
2817 | 				"LD_RUNPATH_SEARCH_PATHS[sdk=macosx*]" = "@executable_path/../Frameworks";
2818 | 				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
2819 | 				MACOSX_DEPLOYMENT_TARGET = 14.2;
2820 | 				MARKETING_VERSION = 1.0;
2821 | 				MTL_ENABLE_DEBUG_INFO = INCLUDE_SOURCE;
2822 | 				MTL_FAST_MATH = YES;
2823 | 				PRODUCT_BUNDLE_IDENTIFIER = "mlx.MNISTTrainer${DISAMBIGUATOR}";
2824 | 				PRODUCT_NAME = "$(TARGET_NAME)";
2825 | 				SDKROOT = auto;
2826 | 				SUPPORTED_PLATFORMS = "iphoneos iphonesimulator macosx";
2827 | 				SWIFT_ACTIVE_COMPILATION_CONDITIONS = "DEBUG $(inherited)";
2828 | 				SWIFT_EMIT_LOC_STRINGS = YES;
2829 | 				SWIFT_OPTIMIZATION_LEVEL = "-Onone";
2830 | 				SWIFT_STRICT_CONCURRENCY = complete;
2831 | 				SWIFT_VERSION = 5.0;
2832 | 				TARGETED_DEVICE_FAMILY = "1,2";
2833 | 			};
2834 | 			name = Debug;
2835 | 		};
2836 | 		C3A8B3C02B92950A0002EFB8 /* Release */ = {
2837 | 			isa = XCBuildConfiguration;
2838 | 			buildSettings = {
2839 | 				ALWAYS_SEARCH_USER_PATHS = NO;
2840 | 				ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;
2841 | 				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
2842 | 				ASSETCATALOG_COMPILER_GLOBAL_ACCENT_COLOR_NAME = AccentColor;
2843 | 				CLANG_ANALYZER_NONNULL = YES;
2844 | 				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
2845 | 				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
2846 | 				CLANG_ENABLE_MODULES = YES;
2847 | 				CLANG_ENABLE_OBJC_ARC = YES;
2848 | 				CLANG_ENABLE_OBJC_WEAK = YES;
2849 | 				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
2850 | 				CLANG_WARN_BOOL_CONVERSION = YES;
2851 | 				CLANG_WARN_COMMA = YES;
2852 | 				CLANG_WARN_CONSTANT_CONVERSION = YES;
2853 | 				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
2854 | 				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
2855 | 				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
2856 | 				CLANG_WARN_EMPTY_BODY = YES;
2857 | 				CLANG_WARN_ENUM_CONVERSION = YES;
2858 | 				CLANG_WARN_INFINITE_RECURSION = YES;
2859 | 				CLANG_WARN_INT_CONVERSION = YES;
2860 | 				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
2861 | 				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
2862 | 				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
2863 | 				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
2864 | 				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
2865 | 				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
2866 | 				CLANG_WARN_STRICT_PROTOTYPES = YES;
2867 | 				CLANG_WARN_SUSPICIOUS_MOVE = YES;
2868 | 				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
2869 | 				CLANG_WARN_UNREACHABLE_CODE = YES;
2870 | 				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
2871 | 				CODE_SIGN_ENTITLEMENTS = Applications/MNISTTrainer/MNISTTrainer.entitlements;
2872 | 				CODE_SIGN_STYLE = Automatic;
2873 | 				COPY_PHASE_STRIP = NO;
2874 | 				CURRENT_PROJECT_VERSION = 1;
2875 | 				DEAD_CODE_STRIPPING = YES;
2876 | 				DEBUG_INFORMATION_FORMAT = "dwarf-with-dsym";
2877 | 				DEVELOPMENT_ASSET_PATHS = "\"Applications/MNISTTrainer/Preview Content\"";
2878 | 				ENABLE_NS_ASSERTIONS = NO;
2879 | 				ENABLE_PREVIEWS = YES;
2880 | 				ENABLE_STRICT_OBJC_MSGSEND = YES;
2881 | 				ENABLE_USER_SCRIPT_SANDBOXING = YES;
2882 | 				GCC_C_LANGUAGE_STANDARD = gnu17;
2883 | 				GCC_NO_COMMON_BLOCKS = YES;
2884 | 				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
2885 | 				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
2886 | 				GCC_WARN_UNDECLARED_SELECTOR = YES;
2887 | 				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
2888 | 				GCC_WARN_UNUSED_FUNCTION = YES;
2889 | 				GCC_WARN_UNUSED_VARIABLE = YES;
2890 | 				GENERATE_INFOPLIST_FILE = YES;
2891 | 				INFOPLIST_FILE = "Applications/MNISTTrainer/MNISTTrainer-Info.plist";
2892 | 				"INFOPLIST_KEY_UIApplicationSceneManifest_Generation[sdk=iphoneos*]" = YES;
2893 | 				"INFOPLIST_KEY_UIApplicationSceneManifest_Generation[sdk=iphonesimulator*]" = YES;
2894 | 				"INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents[sdk=iphoneos*]" = YES;
2895 | 				"INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents[sdk=iphonesimulator*]" = YES;
2896 | 				"INFOPLIST_KEY_UILaunchScreen_Generation[sdk=iphoneos*]" = YES;
2897 | 				"INFOPLIST_KEY_UILaunchScreen_Generation[sdk=iphonesimulator*]" = YES;
2898 | 				"INFOPLIST_KEY_UIStatusBarStyle[sdk=iphoneos*]" = UIStatusBarStyleDefault;
2899 | 				"INFOPLIST_KEY_UIStatusBarStyle[sdk=iphonesimulator*]" = UIStatusBarStyleDefault;
2900 | 				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPad = "UIInterfaceOrientationPortrait UIInterfaceOrientationPortraitUpsideDown UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
2901 | 				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPhone = "UIInterfaceOrientationPortrait UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
2902 | 				IPHONEOS_DEPLOYMENT_TARGET = 17.2;
2903 | 				LD_RUNPATH_SEARCH_PATHS = "@executable_path/Frameworks";
2904 | 				"LD_RUNPATH_SEARCH_PATHS[sdk=macosx*]" = "@executable_path/../Frameworks";
2905 | 				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
2906 | 				MACOSX_DEPLOYMENT_TARGET = 14.2;
2907 | 				MARKETING_VERSION = 1.0;
2908 | 				MTL_ENABLE_DEBUG_INFO = NO;
2909 | 				MTL_FAST_MATH = YES;
2910 | 				PRODUCT_BUNDLE_IDENTIFIER = "mlx.MNISTTrainer${DISAMBIGUATOR}";
2911 | 				PRODUCT_NAME = "$(TARGET_NAME)";
2912 | 				SDKROOT = auto;
2913 | 				SUPPORTED_PLATFORMS = "iphoneos iphonesimulator macosx";
2914 | 				SWIFT_COMPILATION_MODE = wholemodule;
2915 | 				SWIFT_EMIT_LOC_STRINGS = YES;
2916 | 				SWIFT_STRICT_CONCURRENCY = complete;
2917 | 				SWIFT_VERSION = 5.0;
2918 | 				TARGETED_DEVICE_FAMILY = "1,2";
2919 | 			};
2920 | 			name = Release;
2921 | 		};
2922 | 		C3A8B3E92B92A29E0002EFB8 /* Debug */ = {
2923 | 			isa = XCBuildConfiguration;
2924 | 			buildSettings = {
2925 | 				ALWAYS_SEARCH_USER_PATHS = NO;
2926 | 				ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;
2927 | 				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
2928 | 				ASSETCATALOG_COMPILER_GLOBAL_ACCENT_COLOR_NAME = AccentColor;
2929 | 				CLANG_ANALYZER_NONNULL = YES;
2930 | 				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
2931 | 				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
2932 | 				CLANG_ENABLE_MODULES = YES;
2933 | 				CLANG_ENABLE_OBJC_ARC = YES;
2934 | 				CLANG_ENABLE_OBJC_WEAK = YES;
2935 | 				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
2936 | 				CLANG_WARN_BOOL_CONVERSION = YES;
2937 | 				CLANG_WARN_COMMA = YES;
2938 | 				CLANG_WARN_CONSTANT_CONVERSION = YES;
2939 | 				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
2940 | 				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
2941 | 				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
2942 | 				CLANG_WARN_EMPTY_BODY = YES;
2943 | 				CLANG_WARN_ENUM_CONVERSION = YES;
2944 | 				CLANG_WARN_INFINITE_RECURSION = YES;
2945 | 				CLANG_WARN_INT_CONVERSION = YES;
2946 | 				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
2947 | 				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
2948 | 				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
2949 | 				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
2950 | 				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
2951 | 				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
2952 | 				CLANG_WARN_STRICT_PROTOTYPES = YES;
2953 | 				CLANG_WARN_SUSPICIOUS_MOVE = YES;
2954 | 				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
2955 | 				CLANG_WARN_UNREACHABLE_CODE = YES;
2956 | 				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
2957 | 				CODE_SIGN_ENTITLEMENTS = Applications/LLMEval/LLMEval.entitlements;
2958 | 				CODE_SIGN_STYLE = Automatic;
2959 | 				COPY_PHASE_STRIP = NO;
2960 | 				CURRENT_PROJECT_VERSION = 1;
2961 | 				DEAD_CODE_STRIPPING = YES;
2962 | 				DEBUG_INFORMATION_FORMAT = dwarf;
2963 | 				DEVELOPMENT_ASSET_PATHS = "\"Applications/LLMEval/Preview Content\"";
2964 | 				DEVELOPMENT_TEAM = "";
2965 | 				ENABLE_PREVIEWS = YES;
2966 | 				ENABLE_STRICT_OBJC_MSGSEND = YES;
2967 | 				ENABLE_TESTABILITY = YES;
2968 | 				ENABLE_USER_SCRIPT_SANDBOXING = YES;
2969 | 				GCC_C_LANGUAGE_STANDARD = gnu17;
2970 | 				GCC_DYNAMIC_NO_PIC = NO;
2971 | 				GCC_NO_COMMON_BLOCKS = YES;
2972 | 				GCC_OPTIMIZATION_LEVEL = 0;
2973 | 				GCC_PREPROCESSOR_DEFINITIONS = (
2974 | 					"DEBUG=1",
2975 | 					"$(inherited)",
2976 | 				);
2977 | 				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
2978 | 				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
2979 | 				GCC_WARN_UNDECLARED_SELECTOR = YES;
2980 | 				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
2981 | 				GCC_WARN_UNUSED_FUNCTION = YES;
2982 | 				GCC_WARN_UNUSED_VARIABLE = YES;
2983 | 				GENERATE_INFOPLIST_FILE = YES;
2984 | 				"INFOPLIST_KEY_UIApplicationSceneManifest_Generation[sdk=iphoneos*]" = YES;
2985 | 				"INFOPLIST_KEY_UIApplicationSceneManifest_Generation[sdk=iphonesimulator*]" = YES;
2986 | 				"INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents[sdk=iphoneos*]" = YES;
2987 | 				"INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents[sdk=iphonesimulator*]" = YES;
2988 | 				"INFOPLIST_KEY_UILaunchScreen_Generation[sdk=iphoneos*]" = YES;
2989 | 				"INFOPLIST_KEY_UILaunchScreen_Generation[sdk=iphonesimulator*]" = YES;
2990 | 				"INFOPLIST_KEY_UIStatusBarStyle[sdk=iphoneos*]" = UIStatusBarStyleDefault;
2991 | 				"INFOPLIST_KEY_UIStatusBarStyle[sdk=iphonesimulator*]" = UIStatusBarStyleDefault;
2992 | 				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPad = "UIInterfaceOrientationPortrait UIInterfaceOrientationPortraitUpsideDown UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
2993 | 				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPhone = "UIInterfaceOrientationPortrait UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
2994 | 				IPHONEOS_DEPLOYMENT_TARGET = 17.2;
2995 | 				LD_RUNPATH_SEARCH_PATHS = "@executable_path/Frameworks";
2996 | 				"LD_RUNPATH_SEARCH_PATHS[sdk=macosx*]" = "@executable_path/../Frameworks";
2997 | 				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
2998 | 				MACOSX_DEPLOYMENT_TARGET = 14.2;
2999 | 				MARKETING_VERSION = 1.0;
3000 | 				MTL_ENABLE_DEBUG_INFO = INCLUDE_SOURCE;
3001 | 				MTL_FAST_MATH = YES;
3002 | 				PRODUCT_BUNDLE_IDENTIFIER = "mlx.LLMEval${DISAMBIGUATOR}";
3003 | 				PRODUCT_NAME = "$(TARGET_NAME)";
3004 | 				SDKROOT = auto;
3005 | 				SUPPORTED_PLATFORMS = "iphoneos iphonesimulator macosx xros xrsimulator";
3006 | 				SWIFT_ACTIVE_COMPILATION_CONDITIONS = "DEBUG $(inherited)";
3007 | 				SWIFT_EMIT_LOC_STRINGS = YES;
3008 | 				SWIFT_OPTIMIZATION_LEVEL = "-Onone";
3009 | 				SWIFT_STRICT_CONCURRENCY = complete;
3010 | 				SWIFT_VERSION = 5.0;
3011 | 				TARGETED_DEVICE_FAMILY = "1,2,7";
3012 | 			};
3013 | 			name = Debug;
3014 | 		};
3015 | 		C3A8B3EA2B92A29E0002EFB8 /* Release */ = {
3016 | 			isa = XCBuildConfiguration;
3017 | 			buildSettings = {
3018 | 				ALWAYS_SEARCH_USER_PATHS = NO;
3019 | 				ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;
3020 | 				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
3021 | 				ASSETCATALOG_COMPILER_GLOBAL_ACCENT_COLOR_NAME = AccentColor;
3022 | 				CLANG_ANALYZER_NONNULL = YES;
3023 | 				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
3024 | 				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
3025 | 				CLANG_ENABLE_MODULES = YES;
3026 | 				CLANG_ENABLE_OBJC_ARC = YES;
3027 | 				CLANG_ENABLE_OBJC_WEAK = YES;
3028 | 				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
3029 | 				CLANG_WARN_BOOL_CONVERSION = YES;
3030 | 				CLANG_WARN_COMMA = YES;
3031 | 				CLANG_WARN_CONSTANT_CONVERSION = YES;
3032 | 				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
3033 | 				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
3034 | 				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
3035 | 				CLANG_WARN_EMPTY_BODY = YES;
3036 | 				CLANG_WARN_ENUM_CONVERSION = YES;
3037 | 				CLANG_WARN_INFINITE_RECURSION = YES;
3038 | 				CLANG_WARN_INT_CONVERSION = YES;
3039 | 				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
3040 | 				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
3041 | 				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
3042 | 				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
3043 | 				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
3044 | 				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
3045 | 				CLANG_WARN_STRICT_PROTOTYPES = YES;
3046 | 				CLANG_WARN_SUSPICIOUS_MOVE = YES;
3047 | 				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
3048 | 				CLANG_WARN_UNREACHABLE_CODE = YES;
3049 | 				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
3050 | 				CODE_SIGN_ENTITLEMENTS = Applications/LLMEval/LLMEval.entitlements;
3051 | 				CODE_SIGN_STYLE = Automatic;
3052 | 				COPY_PHASE_STRIP = NO;
3053 | 				CURRENT_PROJECT_VERSION = 1;
3054 | 				DEAD_CODE_STRIPPING = YES;
3055 | 				DEBUG_INFORMATION_FORMAT = "dwarf-with-dsym";
3056 | 				DEVELOPMENT_ASSET_PATHS = "\"Applications/LLMEval/Preview Content\"";
3057 | 				DEVELOPMENT_TEAM = "";
3058 | 				ENABLE_NS_ASSERTIONS = NO;
3059 | 				ENABLE_PREVIEWS = YES;
3060 | 				ENABLE_STRICT_OBJC_MSGSEND = YES;
3061 | 				ENABLE_USER_SCRIPT_SANDBOXING = YES;
3062 | 				GCC_C_LANGUAGE_STANDARD = gnu17;
3063 | 				GCC_NO_COMMON_BLOCKS = YES;
3064 | 				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
3065 | 				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
3066 | 				GCC_WARN_UNDECLARED_SELECTOR = YES;
3067 | 				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
3068 | 				GCC_WARN_UNUSED_FUNCTION = YES;
3069 | 				GCC_WARN_UNUSED_VARIABLE = YES;
3070 | 				GENERATE_INFOPLIST_FILE = YES;
3071 | 				"INFOPLIST_KEY_UIApplicationSceneManifest_Generation[sdk=iphoneos*]" = YES;
3072 | 				"INFOPLIST_KEY_UIApplicationSceneManifest_Generation[sdk=iphonesimulator*]" = YES;
3073 | 				"INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents[sdk=iphoneos*]" = YES;
3074 | 				"INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents[sdk=iphonesimulator*]" = YES;
3075 | 				"INFOPLIST_KEY_UILaunchScreen_Generation[sdk=iphoneos*]" = YES;
3076 | 				"INFOPLIST_KEY_UILaunchScreen_Generation[sdk=iphonesimulator*]" = YES;
3077 | 				"INFOPLIST_KEY_UIStatusBarStyle[sdk=iphoneos*]" = UIStatusBarStyleDefault;
3078 | 				"INFOPLIST_KEY_UIStatusBarStyle[sdk=iphonesimulator*]" = UIStatusBarStyleDefault;
3079 | 				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPad = "UIInterfaceOrientationPortrait UIInterfaceOrientationPortraitUpsideDown UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
3080 | 				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPhone = "UIInterfaceOrientationPortrait UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
3081 | 				IPHONEOS_DEPLOYMENT_TARGET = 17.2;
3082 | 				LD_RUNPATH_SEARCH_PATHS = "@executable_path/Frameworks";
3083 | 				"LD_RUNPATH_SEARCH_PATHS[sdk=macosx*]" = "@executable_path/../Frameworks";
3084 | 				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
3085 | 				MACOSX_DEPLOYMENT_TARGET = 14.2;
3086 | 				MARKETING_VERSION = 1.0;
3087 | 				MTL_ENABLE_DEBUG_INFO = NO;
3088 | 				MTL_FAST_MATH = YES;
3089 | 				PRODUCT_BUNDLE_IDENTIFIER = "mlx.LLMEval${DISAMBIGUATOR}";
3090 | 				PRODUCT_NAME = "$(TARGET_NAME)";
3091 | 				SDKROOT = auto;
3092 | 				SUPPORTED_PLATFORMS = "iphoneos iphonesimulator macosx xros xrsimulator";
3093 | 				SWIFT_COMPILATION_MODE = wholemodule;
3094 | 				SWIFT_EMIT_LOC_STRINGS = YES;
3095 | 				SWIFT_STRICT_CONCURRENCY = complete;
3096 | 				SWIFT_VERSION = 5.0;
3097 | 				TARGETED_DEVICE_FAMILY = "1,2,7";
3098 | 			};
3099 | 			name = Release;
3100 | 		};
3101 | /* End XCBuildConfiguration section */
3102 | 
3103 | /* Begin XCConfigurationList section */
3104 | 		0AC74EC62D136223003C90A7 /* Build configuration list for PBXNativeTarget "VLMEval" */ = {
3105 | 			isa = XCConfigurationList;
3106 | 			buildConfigurations = (
3107 | 				0AC74EC72D136223003C90A7 /* Debug */,
3108 | 				0AC74EC82D136223003C90A7 /* Release */,
3109 | 			);
3110 | 			defaultConfigurationIsVisible = 0;
3111 | 			defaultConfigurationName = Release;
3112 | 		};
3113 | 		0F5AD74C2DB70C0400745C06 /* Build configuration list for PBXNativeTarget "MLXChatExample" */ = {
3114 | 			isa = XCConfigurationList;
3115 | 			buildConfigurations = (
3116 | 				0F5AD74D2DB70C0400745C06 /* Debug */,
3117 | 				0F5AD74E2DB70C0400745C06 /* Release */,
3118 | 			);
3119 | 			defaultConfigurationIsVisible = 0;
3120 | 			defaultConfigurationName = Release;
3121 | 		};
3122 | 		C3056BB72BCD97B800A31D04 /* Build configuration list for PBXNativeTarget "LoRATrainingExample" */ = {
3123 | 			isa = XCConfigurationList;
3124 | 			buildConfigurations = (
3125 | 				C3056BB82BCD97B800A31D04 /* Debug */,
3126 | 				C3056BB92BCD97B800A31D04 /* Release */,
3127 | 			);
3128 | 			defaultConfigurationIsVisible = 0;
3129 | 			defaultConfigurationName = Release;
3130 | 		};
3131 | 		C3132C272DFB2FFB00270E8E /* Build configuration list for PBXNativeTarget "ExampleLLM" */ = {
3132 | 			isa = XCConfigurationList;
3133 | 			buildConfigurations = (
3134 | 				C3132C282DFB2FFB00270E8E /* Debug */,
3135 | 				C3132C292DFB2FFB00270E8E /* Release */,
3136 | 			);
3137 | 			defaultConfigurationIsVisible = 0;
3138 | 			defaultConfigurationName = Release;
3139 | 		};
3140 | 		C3208E722DB19451006AE6CA /* Build configuration list for PBXNativeTarget "MLXLMTests" */ = {
3141 | 			isa = XCConfigurationList;
3142 | 			buildConfigurations = (
3143 | 				C3208E732DB19451006AE6CA /* Debug */,
3144 | 				C3208E742DB19451006AE6CA /* Release */,
3145 | 			);
3146 | 			defaultConfigurationIsVisible = 0;
3147 | 			defaultConfigurationName = Release;
3148 | 		};
3149 | 		C3288D792B6D9313009FF608 /* Build configuration list for PBXNativeTarget "LinearModelTraining" */ = {
3150 | 			isa = XCConfigurationList;
3151 | 			buildConfigurations = (
3152 | 				C3288D772B6D9313009FF608 /* Debug */,
3153 | 				C3288D782B6D9313009FF608 /* Release */,
3154 | 			);
3155 | 			defaultConfigurationIsVisible = 0;
3156 | 			defaultConfigurationName = Release;
3157 | 		};
3158 | 		C34E49252B6A026F00FCB841 /* Build configuration list for PBXNativeTarget "mnist-tool" */ = {
3159 | 			isa = XCConfigurationList;
3160 | 			buildConfigurations = (
3161 | 				C34E49262B6A026F00FCB841 /* Debug */,
3162 | 				C34E49272B6A026F00FCB841 /* Release */,
3163 | 			);
3164 | 			defaultConfigurationIsVisible = 0;
3165 | 			defaultConfigurationName = Release;
3166 | 		};
3167 | 		C36BEFE42BC32988002D4AFE /* Build configuration list for PBXNativeTarget "image-tool" */ = {
3168 | 			isa = XCConfigurationList;
3169 | 			buildConfigurations = (
3170 | 				C36BEFE52BC32988002D4AFE /* Debug */,
3171 | 				C36BEFE62BC32988002D4AFE /* Release */,
3172 | 			);
3173 | 			defaultConfigurationIsVisible = 0;
3174 | 			defaultConfigurationName = Release;
3175 | 		};
3176 | 		C36BF00D2BC5CE56002D4AFE /* Build configuration list for PBXNativeTarget "StableDiffusionExample" */ = {
3177 | 			isa = XCConfigurationList;
3178 | 			buildConfigurations = (
3179 | 				C36BF00E2BC5CE56002D4AFE /* Debug */,
3180 | 				C36BF00F2BC5CE56002D4AFE /* Release */,
3181 | 			);
3182 | 			defaultConfigurationIsVisible = 0;
3183 | 			defaultConfigurationName = Release;
3184 | 		};
3185 | 		C392736B2B60697700368D5D /* Build configuration list for PBXProject "mlx-swift-examples" */ = {
3186 | 			isa = XCConfigurationList;
3187 | 			buildConfigurations = (
3188 | 				C392736C2B60697700368D5D /* Debug */,
3189 | 				C392736D2B60697700368D5D /* Release */,
3190 | 			);
3191 | 			defaultConfigurationIsVisible = 0;
3192 | 			defaultConfigurationName = Release;
3193 | 		};
3194 | 		C39273792B606A0A00368D5D /* Build configuration list for PBXNativeTarget "Tutorial" */ = {
3195 | 			isa = XCConfigurationList;
3196 | 			buildConfigurations = (
3197 | 				C392737A2B606A0A00368D5D /* Debug */,
3198 | 				C392737B2B606A0A00368D5D /* Release */,
3199 | 			);
3200 | 			defaultConfigurationIsVisible = 0;
3201 | 			defaultConfigurationName = Release;
3202 | 		};
3203 | 		C397C58F2B62C6A9004B084D /* Build configuration list for PBXNativeTarget "llm-tool" */ = {
3204 | 			isa = XCConfigurationList;
3205 | 			buildConfigurations = (
3206 | 				C397C5902B62C6A9004B084D /* Debug */,
3207 | 				C397C5912B62C6A9004B084D /* Release */,
3208 | 			);
3209 | 			defaultConfigurationIsVisible = 0;
3210 | 			defaultConfigurationName = Release;
3211 | 		};
3212 | 		C3A8B3BE2B92950A0002EFB8 /* Build configuration list for PBXNativeTarget "MNISTTrainer" */ = {
3213 | 			isa = XCConfigurationList;
3214 | 			buildConfigurations = (
3215 | 				C3A8B3BF2B92950A0002EFB8 /* Debug */,
3216 | 				C3A8B3C02B92950A0002EFB8 /* Release */,
3217 | 			);
3218 | 			defaultConfigurationIsVisible = 0;
3219 | 			defaultConfigurationName = Release;
3220 | 		};
3221 | 		C3A8B3E82B92A29E0002EFB8 /* Build configuration list for PBXNativeTarget "LLMEval" */ = {
3222 | 			isa = XCConfigurationList;
3223 | 			buildConfigurations = (
3224 | 				C3A8B3E92B92A29E0002EFB8 /* Debug */,
3225 | 				C3A8B3EA2B92A29E0002EFB8 /* Release */,
3226 | 			);
3227 | 			defaultConfigurationIsVisible = 0;
3228 | 			defaultConfigurationName = Release;
3229 | 		};
3230 | /* End XCConfigurationList section */
3231 | 
3232 | /* Begin XCLocalSwiftPackageReference section */
3233 | 		C397D8F22CD2F60B00B87EE2 /* XCLocalSwiftPackageReference "Libraries/.." */ = {
3234 | 			isa = XCLocalSwiftPackageReference;
3235 | 			relativePath = Libraries/..;
3236 | 		};
3237 | /* End XCLocalSwiftPackageReference section */
3238 | 
3239 | /* Begin XCRemoteSwiftPackageReference section */
3240 | 		81695B3F2BA373D300F260D8 /* XCRemoteSwiftPackageReference "swift-markdown-ui" */ = {
3241 | 			isa = XCRemoteSwiftPackageReference;
3242 | 			repositoryURL = "https://github.com/gonzalezreal/swift-markdown-ui";
3243 | 			requirement = {
3244 | 				kind = upToNextMajorVersion;
3245 | 				minimumVersion = 2.3.1;
3246 | 			};
3247 | 		};
3248 | 		C32A18442D00E13E0092A5B6 /* XCRemoteSwiftPackageReference "mlx-swift" */ = {
3249 | 			isa = XCRemoteSwiftPackageReference;
3250 | 			repositoryURL = "https://github.com/ml-explore/mlx-swift";
3251 | 			requirement = {
3252 | 				kind = upToNextMajorVersion;
3253 | 				minimumVersion = 0.25.4;
3254 | 			};
3255 | 		};
3256 | 		C32B4C6B2DA7132C00EF663D /* XCRemoteSwiftPackageReference "swift-async-algorithms" */ = {
3257 | 			isa = XCRemoteSwiftPackageReference;
3258 | 			repositoryURL = "https://github.com/apple/swift-async-algorithms.git";
3259 | 			requirement = {
3260 | 				kind = upToNextMajorVersion;
3261 | 				minimumVersion = 1.0.3;
3262 | 			};
3263 | 		};
3264 | 		C34E491A2B69C43600FCB841 /* XCRemoteSwiftPackageReference "GzipSwift" */ = {
3265 | 			isa = XCRemoteSwiftPackageReference;
3266 | 			repositoryURL = "https://github.com/1024jp/GzipSwift";
3267 | 			requirement = {
3268 | 				kind = exactVersion;
3269 | 				version = 6.0.1;
3270 | 			};
3271 | 		};
3272 | 		C36BEFF02BC32A8C002D4AFE /* XCRemoteSwiftPackageReference "Progress" */ = {
3273 | 			isa = XCRemoteSwiftPackageReference;
3274 | 			repositoryURL = "https://github.com/jkandzi/Progress.swift";
3275 | 			requirement = {
3276 | 				kind = upToNextMajorVersion;
3277 | 				minimumVersion = 0.4.0;
3278 | 			};
3279 | 		};
3280 | 		C392736E2B60699100368D5D /* XCRemoteSwiftPackageReference "swift-argument-parser" */ = {
3281 | 			isa = XCRemoteSwiftPackageReference;
3282 | 			repositoryURL = "https://github.com/apple/swift-argument-parser.git";
3283 | 			requirement = {
3284 | 				kind = upToNextMajorVersion;
3285 | 				minimumVersion = 1.4.0;
3286 | 			};
3287 | 		};
3288 | /* End XCRemoteSwiftPackageReference section */
3289 | 
3290 | /* Begin XCSwiftPackageProductDependency section */
3291 | 		0AC74ED92D136223003C90A7 /* MLXVLM */ = {
3292 | 			isa = XCSwiftPackageProductDependency;
3293 | 			package = C397D8F22CD2F60B00B87EE2 /* XCLocalSwiftPackageReference "Libraries/.." */;
3294 | 			productName = MLXVLM;
3295 | 		};
3296 | 		81695B402BA373D300F260D8 /* MarkdownUI */ = {
3297 | 			isa = XCSwiftPackageProductDependency;
3298 | 			package = 81695B3F2BA373D300F260D8 /* XCRemoteSwiftPackageReference "swift-markdown-ui" */;
3299 | 			productName = MarkdownUI;
3300 | 		};
3301 | 		C3132C2D2DFB317C00270E8E /* MLXVLM */ = {
3302 | 			isa = XCSwiftPackageProductDependency;
3303 | 			package = C397D8F22CD2F60B00B87EE2 /* XCLocalSwiftPackageReference "Libraries/.." */;
3304 | 			productName = MLXVLM;
3305 | 		};
3306 | 		C3132C2F2DFB317C00270E8E /* MLXLLM */ = {
3307 | 			isa = XCSwiftPackageProductDependency;
3308 | 			package = C397D8F22CD2F60B00B87EE2 /* XCLocalSwiftPackageReference "Libraries/.." */;
3309 | 			productName = MLXLLM;
3310 | 		};
3311 | 		C3208E752DB1945D006AE6CA /* MLX */ = {
3312 | 			isa = XCSwiftPackageProductDependency;
3313 | 			package = C32A18442D00E13E0092A5B6 /* XCRemoteSwiftPackageReference "mlx-swift" */;
3314 | 			productName = MLX;
3315 | 		};
3316 | 		C3208E792DB1945D006AE6CA /* MLXNN */ = {
3317 | 			isa = XCSwiftPackageProductDependency;
3318 | 			package = C32A18442D00E13E0092A5B6 /* XCRemoteSwiftPackageReference "mlx-swift" */;
3319 | 			productName = MLXNN;
3320 | 		};
3321 | 		C3288D7A2B6D9339009FF608 /* ArgumentParser */ = {
3322 | 			isa = XCSwiftPackageProductDependency;
3323 | 			package = C392736E2B60699100368D5D /* XCRemoteSwiftPackageReference "swift-argument-parser" */;
3324 | 			productName = ArgumentParser;
3325 | 		};
3326 | 		C32A17FC2CFFB98A0092A5B6 /* MLXLLM */ = {
3327 | 			isa = XCSwiftPackageProductDependency;
3328 | 			package = C397D8F22CD2F60B00B87EE2 /* XCLocalSwiftPackageReference "Libraries/.." */;
3329 | 			productName = MLXLLM;
3330 | 		};
3331 | 		C32A17FE2CFFB98A0092A5B6 /* MLXVLM */ = {
3332 | 			isa = XCSwiftPackageProductDependency;
3333 | 			package = C397D8F22CD2F60B00B87EE2 /* XCLocalSwiftPackageReference "Libraries/.." */;
3334 | 			productName = MLXVLM;
3335 | 		};
3336 | 		C32A18002CFFD1810092A5B6 /* MLXMNIST */ = {
3337 | 			isa = XCSwiftPackageProductDependency;
3338 | 			package = C397D8F22CD2F60B00B87EE2 /* XCLocalSwiftPackageReference "Libraries/.." */;
3339 | 			productName = MLXMNIST;
3340 | 		};
3341 | 		C32A18022CFFD1920092A5B6 /* MLXMNIST */ = {
3342 | 			isa = XCSwiftPackageProductDependency;
3343 | 			package = C397D8F22CD2F60B00B87EE2 /* XCLocalSwiftPackageReference "Libraries/.." */;
3344 | 			productName = MLXMNIST;
3345 | 		};
3346 | 		C32A18042CFFD19F0092A5B6 /* MLXLLM */ = {
3347 | 			isa = XCSwiftPackageProductDependency;
3348 | 			package = C397D8F22CD2F60B00B87EE2 /* XCLocalSwiftPackageReference "Libraries/.." */;
3349 | 			productName = MLXLLM;
3350 | 		};
3351 | 		C32A18062CFFD1AA0092A5B6 /* MLXLLM */ = {
3352 | 			isa = XCSwiftPackageProductDependency;
3353 | 			package = C397D8F22CD2F60B00B87EE2 /* XCLocalSwiftPackageReference "Libraries/.." */;
3354 | 			productName = MLXLLM;
3355 | 		};
3356 | 		C32A18082CFFD1B70092A5B6 /* StableDiffusion */ = {
3357 | 			isa = XCSwiftPackageProductDependency;
3358 | 			package = C397D8F22CD2F60B00B87EE2 /* XCLocalSwiftPackageReference "Libraries/.." */;
3359 | 			productName = StableDiffusion;
3360 | 		};
3361 | 		C32A18452D00E1490092A5B6 /* MLX */ = {
3362 | 			isa = XCSwiftPackageProductDependency;
3363 | 			package = C32A18442D00E13E0092A5B6 /* XCRemoteSwiftPackageReference "mlx-swift" */;
3364 | 			productName = MLX;
3365 | 		};
3366 | 		C32A18472D00E1540092A5B6 /* MLX */ = {
3367 | 			isa = XCSwiftPackageProductDependency;
3368 | 			package = C32A18442D00E13E0092A5B6 /* XCRemoteSwiftPackageReference "mlx-swift" */;
3369 | 			productName = MLX;
3370 | 		};
3371 | 		C32A18492D00E1540092A5B6 /* MLXNN */ = {
3372 | 			isa = XCSwiftPackageProductDependency;
3373 | 			package = C32A18442D00E13E0092A5B6 /* XCRemoteSwiftPackageReference "mlx-swift" */;
3374 | 			productName = MLXNN;
3375 | 		};
3376 | 		C32A184B2D00E1540092A5B6 /* MLXOptimizers */ = {
3377 | 			isa = XCSwiftPackageProductDependency;
3378 | 			package = C32A18442D00E13E0092A5B6 /* XCRemoteSwiftPackageReference "mlx-swift" */;
3379 | 			productName = MLXOptimizers;
3380 | 		};
3381 | 		C32B4C6C2DA7136000EF663D /* AsyncAlgorithms */ = {
3382 | 			isa = XCSwiftPackageProductDependency;
3383 | 			package = C32B4C6B2DA7132C00EF663D /* XCRemoteSwiftPackageReference "swift-async-algorithms" */;
3384 | 			productName = AsyncAlgorithms;
3385 | 		};
3386 | 		C32B4C6E2DA71ADC00EF663D /* AsyncAlgorithms */ = {
3387 | 			isa = XCSwiftPackageProductDependency;
3388 | 			package = C32B4C6B2DA7132C00EF663D /* XCRemoteSwiftPackageReference "swift-async-algorithms" */;
3389 | 			productName = AsyncAlgorithms;
3390 | 		};
3391 | 		C34E49282B6A028100FCB841 /* ArgumentParser */ = {
3392 | 			isa = XCSwiftPackageProductDependency;
3393 | 			package = C392736E2B60699100368D5D /* XCRemoteSwiftPackageReference "swift-argument-parser" */;
3394 | 			productName = ArgumentParser;
3395 | 		};
3396 | 		C36BEFEE2BC329C5002D4AFE /* ArgumentParser */ = {
3397 | 			isa = XCSwiftPackageProductDependency;
3398 | 			package = C392736E2B60699100368D5D /* XCRemoteSwiftPackageReference "swift-argument-parser" */;
3399 | 			productName = ArgumentParser;
3400 | 		};
3401 | 		C36BEFF12BC32A9A002D4AFE /* Progress */ = {
3402 | 			isa = XCSwiftPackageProductDependency;
3403 | 			package = C36BEFF02BC32A8C002D4AFE /* XCRemoteSwiftPackageReference "Progress" */;
3404 | 			productName = Progress;
3405 | 		};
3406 | 		C397C59B2B62C6D0004B084D /* ArgumentParser */ = {
3407 | 			isa = XCSwiftPackageProductDependency;
3408 | 			package = C392736E2B60699100368D5D /* XCRemoteSwiftPackageReference "swift-argument-parser" */;
3409 | 			productName = ArgumentParser;
3410 | 		};
3411 | 		C3E7D94C2CF6C9B20056C095 /* StableDiffusion */ = {
3412 | 			isa = XCSwiftPackageProductDependency;
3413 | 			package = C397D8F22CD2F60B00B87EE2 /* XCLocalSwiftPackageReference "Libraries/.." */;
3414 | 			productName = StableDiffusion;
3415 | 		};
3416 | /* End XCSwiftPackageProductDependency section */
3417 | 	};
3418 | 	rootObject = C39273682B60697700368D5D /* Project object */;
3419 | }
3420 | 


--------------------------------------------------------------------------------
/mlx-swift-examples.xcodeproj/project.xcworkspace/contents.xcworkspacedata:
--------------------------------------------------------------------------------
1 | <?xml version="1.0" encoding="UTF-8"?>
2 | <Workspace
3 |    version = "1.0">
4 |    <FileRef
5 |       location = "self:">
6 |    </FileRef>
7 | </Workspace>
8 | 


--------------------------------------------------------------------------------
/mlx-swift-examples.xcodeproj/project.xcworkspace/xcshareddata/IDEWorkspaceChecks.plist:
--------------------------------------------------------------------------------
1 | <?xml version="1.0" encoding="UTF-8"?>
2 | <!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
3 | <plist version="1.0">
4 | <dict>
5 | 	<key>IDEDidComputeMac32BitWarning</key>
6 | 	<true/>
7 | </dict>
8 | </plist>
9 | 


--------------------------------------------------------------------------------
/mlx-swift-examples.xcodeproj/project.xcworkspace/xcshareddata/swiftpm/Package.resolved:
--------------------------------------------------------------------------------
  1 | {
  2 |   "originHash" : "6c3d58787193c406294dfd4fa330ba611ece7c4a64b00302aa63c9ccafd8f43f",
  3 |   "pins" : [
  4 |     {
  5 |       "identity" : "gzipswift",
  6 |       "kind" : "remoteSourceControl",
  7 |       "location" : "https://github.com/1024jp/GzipSwift",
  8 |       "state" : {
  9 |         "revision" : "731037f6cc2be2ec01562f6597c1d0aa3fe6fd05",
 10 |         "version" : "6.0.1"
 11 |       }
 12 |     },
 13 |     {
 14 |       "identity" : "jinja",
 15 |       "kind" : "remoteSourceControl",
 16 |       "location" : "https://github.com/johnmai-dev/Jinja",
 17 |       "state" : {
 18 |         "revision" : "31c4dd39bcdc07eaa42a384bdc88ea599022b800",
 19 |         "version" : "1.1.2"
 20 |       }
 21 |     },
 22 |     {
 23 |       "identity" : "mlx-swift",
 24 |       "kind" : "remoteSourceControl",
 25 |       "location" : "https://github.com/ml-explore/mlx-swift",
 26 |       "state" : {
 27 |         "revision" : "b94473af8c50010edba87a48bbd60c3d7f949852",
 28 |         "version" : "0.25.4"
 29 |       }
 30 |     },
 31 |     {
 32 |       "identity" : "networkimage",
 33 |       "kind" : "remoteSourceControl",
 34 |       "location" : "https://github.com/gonzalezreal/NetworkImage",
 35 |       "state" : {
 36 |         "revision" : "2849f5323265386e200484b0d0f896e73c3411b9",
 37 |         "version" : "6.0.1"
 38 |       }
 39 |     },
 40 |     {
 41 |       "identity" : "progress.swift",
 42 |       "kind" : "remoteSourceControl",
 43 |       "location" : "https://github.com/jkandzi/Progress.swift",
 44 |       "state" : {
 45 |         "revision" : "fed6598735d7982058690acf8f52a0a5fdaeb3e0",
 46 |         "version" : "0.4.0"
 47 |       }
 48 |     },
 49 |     {
 50 |       "identity" : "swift-argument-parser",
 51 |       "kind" : "remoteSourceControl",
 52 |       "location" : "https://github.com/apple/swift-argument-parser.git",
 53 |       "state" : {
 54 |         "revision" : "0fbc8848e389af3bb55c182bc19ca9d5dc2f255b",
 55 |         "version" : "1.4.0"
 56 |       }
 57 |     },
 58 |     {
 59 |       "identity" : "swift-async-algorithms",
 60 |       "kind" : "remoteSourceControl",
 61 |       "location" : "https://github.com/apple/swift-async-algorithms.git",
 62 |       "state" : {
 63 |         "revision" : "042e1c4d9d19748c9c228f8d4ebc97bb1e339b0b",
 64 |         "version" : "1.0.4"
 65 |       }
 66 |     },
 67 |     {
 68 |       "identity" : "swift-cmark",
 69 |       "kind" : "remoteSourceControl",
 70 |       "location" : "https://github.com/swiftlang/swift-cmark",
 71 |       "state" : {
 72 |         "revision" : "b022b08312decdc46585e0b3440d97f6f22ef703",
 73 |         "version" : "0.6.0"
 74 |       }
 75 |     },
 76 |     {
 77 |       "identity" : "swift-collections",
 78 |       "kind" : "remoteSourceControl",
 79 |       "location" : "https://github.com/apple/swift-collections.git",
 80 |       "state" : {
 81 |         "revision" : "c1805596154bb3a265fd91b8ac0c4433b4348fb0",
 82 |         "version" : "1.2.0"
 83 |       }
 84 |     },
 85 |     {
 86 |       "identity" : "swift-markdown-ui",
 87 |       "kind" : "remoteSourceControl",
 88 |       "location" : "https://github.com/gonzalezreal/swift-markdown-ui",
 89 |       "state" : {
 90 |         "revision" : "5f613358148239d0292c0cef674a3c2314737f9e",
 91 |         "version" : "2.4.1"
 92 |       }
 93 |     },
 94 |     {
 95 |       "identity" : "swift-numerics",
 96 |       "kind" : "remoteSourceControl",
 97 |       "location" : "https://github.com/apple/swift-numerics",
 98 |       "state" : {
 99 |         "revision" : "e0ec0f5f3af6f3e4d5e7a19d2af26b481acb6ba8",
100 |         "version" : "1.0.3"
101 |       }
102 |     },
103 |     {
104 |       "identity" : "swift-transformers",
105 |       "kind" : "remoteSourceControl",
106 |       "location" : "https://github.com/huggingface/swift-transformers",
107 |       "state" : {
108 |         "revision" : "c2f302a74cca59cbde683b1425ab43c05685515a",
109 |         "version" : "0.1.21"
110 |       }
111 |     }
112 |   ],
113 |   "version" : 3
114 | }
115 | 


--------------------------------------------------------------------------------
/mlx-swift-examples.xcodeproj/xcshareddata/xcschemes/ExampleLLM.xcscheme:
--------------------------------------------------------------------------------
 1 | <?xml version="1.0" encoding="UTF-8"?>
 2 | <Scheme
 3 |    LastUpgradeVersion = "1630"
 4 |    version = "1.7">
 5 |    <BuildAction
 6 |       parallelizeBuildables = "YES"
 7 |       buildImplicitDependencies = "YES"
 8 |       buildArchitectures = "Automatic">
 9 |       <BuildActionEntries>
10 |          <BuildActionEntry
11 |             buildForTesting = "YES"
12 |             buildForRunning = "YES"
13 |             buildForProfiling = "YES"
14 |             buildForArchiving = "YES"
15 |             buildForAnalyzing = "YES">
16 |             <BuildableReference
17 |                BuildableIdentifier = "primary"
18 |                BlueprintIdentifier = "C3132C222DFB2FFB00270E8E"
19 |                BuildableName = "ExampleLLM"
20 |                BlueprintName = "ExampleLLM"
21 |                ReferencedContainer = "container:mlx-swift-examples.xcodeproj">
22 |             </BuildableReference>
23 |          </BuildActionEntry>
24 |       </BuildActionEntries>
25 |    </BuildAction>
26 |    <TestAction
27 |       buildConfiguration = "Debug"
28 |       selectedDebuggerIdentifier = "Xcode.DebuggerFoundation.Debugger.LLDB"
29 |       selectedLauncherIdentifier = "Xcode.DebuggerFoundation.Launcher.LLDB"
30 |       shouldUseLaunchSchemeArgsEnv = "YES"
31 |       shouldAutocreateTestPlan = "YES">
32 |    </TestAction>
33 |    <LaunchAction
34 |       buildConfiguration = "Release"
35 |       selectedDebuggerIdentifier = "Xcode.DebuggerFoundation.Debugger.LLDB"
36 |       selectedLauncherIdentifier = "Xcode.DebuggerFoundation.Launcher.LLDB"
37 |       launchStyle = "0"
38 |       useCustomWorkingDirectory = "NO"
39 |       ignoresPersistentStateOnLaunch = "NO"
40 |       debugDocumentVersioning = "YES"
41 |       debugServiceExtension = "internal"
42 |       allowLocationSimulation = "YES"
43 |       viewDebuggingEnabled = "No">
44 |       <BuildableProductRunnable
45 |          runnableDebuggingMode = "0">
46 |          <BuildableReference
47 |             BuildableIdentifier = "primary"
48 |             BlueprintIdentifier = "C3132C222DFB2FFB00270E8E"
49 |             BuildableName = "ExampleLLM"
50 |             BlueprintName = "ExampleLLM"
51 |             ReferencedContainer = "container:mlx-swift-examples.xcodeproj">
52 |          </BuildableReference>
53 |       </BuildableProductRunnable>
54 |    </LaunchAction>
55 |    <ProfileAction
56 |       buildConfiguration = "Release"
57 |       shouldUseLaunchSchemeArgsEnv = "YES"
58 |       savedToolIdentifier = ""
59 |       useCustomWorkingDirectory = "NO"
60 |       debugDocumentVersioning = "YES">
61 |       <BuildableProductRunnable
62 |          runnableDebuggingMode = "0">
63 |          <BuildableReference
64 |             BuildableIdentifier = "primary"
65 |             BlueprintIdentifier = "C3132C222DFB2FFB00270E8E"
66 |             BuildableName = "ExampleLLM"
67 |             BlueprintName = "ExampleLLM"
68 |             ReferencedContainer = "container:mlx-swift-examples.xcodeproj">
69 |          </BuildableReference>
70 |       </BuildableProductRunnable>
71 |    </ProfileAction>
72 |    <AnalyzeAction
73 |       buildConfiguration = "Debug">
74 |    </AnalyzeAction>
75 |    <ArchiveAction
76 |       buildConfiguration = "Release"
77 |       revealArchiveInOrganizer = "YES">
78 |    </ArchiveAction>
79 | </Scheme>
80 | 


--------------------------------------------------------------------------------
/mlx-swift-examples.xcodeproj/xcshareddata/xcschemes/LLMEval.xcscheme:
--------------------------------------------------------------------------------
 1 | <?xml version="1.0" encoding="UTF-8"?>
 2 | <Scheme
 3 |    LastUpgradeVersion = "1520"
 4 |    version = "1.7">
 5 |    <BuildAction
 6 |       parallelizeBuildables = "YES"
 7 |       buildImplicitDependencies = "YES">
 8 |       <BuildActionEntries>
 9 |          <BuildActionEntry
10 |             buildForTesting = "YES"
11 |             buildForRunning = "YES"
12 |             buildForProfiling = "YES"
13 |             buildForArchiving = "YES"
14 |             buildForAnalyzing = "YES">
15 |             <BuildableReference
16 |                BuildableIdentifier = "primary"
17 |                BlueprintIdentifier = "C3A8B3DB2B92A29D0002EFB8"
18 |                BuildableName = "LLMEval.app"
19 |                BlueprintName = "LLMEval"
20 |                ReferencedContainer = "container:mlx-swift-examples.xcodeproj">
21 |             </BuildableReference>
22 |          </BuildActionEntry>
23 |       </BuildActionEntries>
24 |    </BuildAction>
25 |    <TestAction
26 |       buildConfiguration = "Debug"
27 |       selectedDebuggerIdentifier = "Xcode.DebuggerFoundation.Debugger.LLDB"
28 |       selectedLauncherIdentifier = "Xcode.DebuggerFoundation.Launcher.LLDB"
29 |       shouldUseLaunchSchemeArgsEnv = "YES"
30 |       shouldAutocreateTestPlan = "YES">
31 |    </TestAction>
32 |    <LaunchAction
33 |       buildConfiguration = "Release"
34 |       selectedDebuggerIdentifier = "Xcode.DebuggerFoundation.Debugger.LLDB"
35 |       selectedLauncherIdentifier = "Xcode.DebuggerFoundation.Launcher.LLDB"
36 |       launchStyle = "0"
37 |       useCustomWorkingDirectory = "NO"
38 |       ignoresPersistentStateOnLaunch = "NO"
39 |       debugDocumentVersioning = "YES"
40 |       debugServiceExtension = "internal"
41 |       allowLocationSimulation = "YES">
42 |       <BuildableProductRunnable
43 |          runnableDebuggingMode = "0">
44 |          <BuildableReference
45 |             BuildableIdentifier = "primary"
46 |             BlueprintIdentifier = "C3A8B3DB2B92A29D0002EFB8"
47 |             BuildableName = "LLMEval.app"
48 |             BlueprintName = "LLMEval"
49 |             ReferencedContainer = "container:mlx-swift-examples.xcodeproj">
50 |          </BuildableReference>
51 |       </BuildableProductRunnable>
52 |    </LaunchAction>
53 |    <ProfileAction
54 |       buildConfiguration = "Release"
55 |       shouldUseLaunchSchemeArgsEnv = "YES"
56 |       savedToolIdentifier = ""
57 |       useCustomWorkingDirectory = "NO"
58 |       debugDocumentVersioning = "YES">
59 |       <BuildableProductRunnable
60 |          runnableDebuggingMode = "0">
61 |          <BuildableReference
62 |             BuildableIdentifier = "primary"
63 |             BlueprintIdentifier = "C3A8B3DB2B92A29D0002EFB8"
64 |             BuildableName = "LLMEval.app"
65 |             BlueprintName = "LLMEval"
66 |             ReferencedContainer = "container:mlx-swift-examples.xcodeproj">
67 |          </BuildableReference>
68 |       </BuildableProductRunnable>
69 |    </ProfileAction>
70 |    <AnalyzeAction
71 |       buildConfiguration = "Debug">
72 |    </AnalyzeAction>
73 |    <ArchiveAction
74 |       buildConfiguration = "Release"
75 |       revealArchiveInOrganizer = "YES">
76 |    </ArchiveAction>
77 | </Scheme>
78 | 


--------------------------------------------------------------------------------
/mlx-swift-examples.xcodeproj/xcshareddata/xcschemes/StableDiffusionExample.xcscheme:
--------------------------------------------------------------------------------
 1 | <?xml version="1.0" encoding="UTF-8"?>
 2 | <Scheme
 3 |    LastUpgradeVersion = "1600"
 4 |    version = "1.7">
 5 |    <BuildAction
 6 |       parallelizeBuildables = "YES"
 7 |       buildImplicitDependencies = "YES"
 8 |       buildArchitectures = "Automatic">
 9 |       <BuildActionEntries>
10 |          <BuildActionEntry
11 |             buildForTesting = "YES"
12 |             buildForRunning = "YES"
13 |             buildForProfiling = "YES"
14 |             buildForArchiving = "YES"
15 |             buildForAnalyzing = "YES">
16 |             <BuildableReference
17 |                BuildableIdentifier = "primary"
18 |                BlueprintIdentifier = "C36BF0002BC5CE55002D4AFE"
19 |                BuildableName = "StableDiffusionExample.app"
20 |                BlueprintName = "StableDiffusionExample"
21 |                ReferencedContainer = "container:mlx-swift-examples.xcodeproj">
22 |             </BuildableReference>
23 |          </BuildActionEntry>
24 |       </BuildActionEntries>
25 |    </BuildAction>
26 |    <TestAction
27 |       buildConfiguration = "Debug"
28 |       selectedDebuggerIdentifier = "Xcode.DebuggerFoundation.Debugger.LLDB"
29 |       selectedLauncherIdentifier = "Xcode.DebuggerFoundation.Launcher.LLDB"
30 |       shouldUseLaunchSchemeArgsEnv = "YES"
31 |       shouldAutocreateTestPlan = "YES">
32 |    </TestAction>
33 |    <LaunchAction
34 |       buildConfiguration = "Debug"
35 |       selectedDebuggerIdentifier = "Xcode.DebuggerFoundation.Debugger.LLDB"
36 |       selectedLauncherIdentifier = "Xcode.DebuggerFoundation.Launcher.LLDB"
37 |       launchStyle = "0"
38 |       useCustomWorkingDirectory = "NO"
39 |       ignoresPersistentStateOnLaunch = "NO"
40 |       debugDocumentVersioning = "YES"
41 |       debugServiceExtension = "internal"
42 |       allowLocationSimulation = "YES">
43 |       <BuildableProductRunnable
44 |          runnableDebuggingMode = "0">
45 |          <BuildableReference
46 |             BuildableIdentifier = "primary"
47 |             BlueprintIdentifier = "C36BF0002BC5CE55002D4AFE"
48 |             BuildableName = "StableDiffusionExample.app"
49 |             BlueprintName = "StableDiffusionExample"
50 |             ReferencedContainer = "container:mlx-swift-examples.xcodeproj">
51 |          </BuildableReference>
52 |       </BuildableProductRunnable>
53 |    </LaunchAction>
54 |    <ProfileAction
55 |       buildConfiguration = "Release"
56 |       shouldUseLaunchSchemeArgsEnv = "YES"
57 |       savedToolIdentifier = ""
58 |       useCustomWorkingDirectory = "NO"
59 |       debugDocumentVersioning = "YES">
60 |       <BuildableProductRunnable
61 |          runnableDebuggingMode = "0">
62 |          <BuildableReference
63 |             BuildableIdentifier = "primary"
64 |             BlueprintIdentifier = "C36BF0002BC5CE55002D4AFE"
65 |             BuildableName = "StableDiffusionExample.app"
66 |             BlueprintName = "StableDiffusionExample"
67 |             ReferencedContainer = "container:mlx-swift-examples.xcodeproj">
68 |          </BuildableReference>
69 |       </BuildableProductRunnable>
70 |    </ProfileAction>
71 |    <AnalyzeAction
72 |       buildConfiguration = "Debug">
73 |    </AnalyzeAction>
74 |    <ArchiveAction
75 |       buildConfiguration = "Release"
76 |       revealArchiveInOrganizer = "YES">
77 |    </ArchiveAction>
78 | </Scheme>
79 | 


--------------------------------------------------------------------------------
/mlx-swift-examples.xcodeproj/xcshareddata/xcschemes/VLMEval.xcscheme:
--------------------------------------------------------------------------------
 1 | <?xml version="1.0" encoding="UTF-8"?>
 2 | <Scheme
 3 |    LastUpgradeVersion = "1620"
 4 |    version = "1.7">
 5 |    <BuildAction
 6 |       parallelizeBuildables = "YES"
 7 |       buildImplicitDependencies = "YES"
 8 |       buildArchitectures = "Automatic">
 9 |       <BuildActionEntries>
10 |          <BuildActionEntry
11 |             buildForTesting = "YES"
12 |             buildForRunning = "YES"
13 |             buildForProfiling = "YES"
14 |             buildForArchiving = "YES"
15 |             buildForAnalyzing = "YES">
16 |             <BuildableReference
17 |                BuildableIdentifier = "primary"
18 |                BlueprintIdentifier = "0AC74EBA2D136221003C90A7"
19 |                BuildableName = "VLMEval.app"
20 |                BlueprintName = "VLMEval"
21 |                ReferencedContainer = "container:mlx-swift-examples.xcodeproj">
22 |             </BuildableReference>
23 |          </BuildActionEntry>
24 |       </BuildActionEntries>
25 |    </BuildAction>
26 |    <TestAction
27 |       buildConfiguration = "Debug"
28 |       selectedDebuggerIdentifier = "Xcode.DebuggerFoundation.Debugger.LLDB"
29 |       selectedLauncherIdentifier = "Xcode.DebuggerFoundation.Launcher.LLDB"
30 |       shouldUseLaunchSchemeArgsEnv = "YES"
31 |       shouldAutocreateTestPlan = "YES">
32 |    </TestAction>
33 |    <LaunchAction
34 |       buildConfiguration = "Debug"
35 |       selectedDebuggerIdentifier = "Xcode.DebuggerFoundation.Debugger.LLDB"
36 |       selectedLauncherIdentifier = "Xcode.DebuggerFoundation.Launcher.LLDB"
37 |       launchStyle = "0"
38 |       useCustomWorkingDirectory = "NO"
39 |       ignoresPersistentStateOnLaunch = "NO"
40 |       debugDocumentVersioning = "YES"
41 |       debugServiceExtension = "internal"
42 |       allowLocationSimulation = "YES">
43 |       <BuildableProductRunnable
44 |          runnableDebuggingMode = "0">
45 |          <BuildableReference
46 |             BuildableIdentifier = "primary"
47 |             BlueprintIdentifier = "0AC74EBA2D136221003C90A7"
48 |             BuildableName = "VLMEval.app"
49 |             BlueprintName = "VLMEval"
50 |             ReferencedContainer = "container:mlx-swift-examples.xcodeproj">
51 |          </BuildableReference>
52 |       </BuildableProductRunnable>
53 |    </LaunchAction>
54 |    <ProfileAction
55 |       buildConfiguration = "Release"
56 |       shouldUseLaunchSchemeArgsEnv = "YES"
57 |       savedToolIdentifier = ""
58 |       useCustomWorkingDirectory = "NO"
59 |       debugDocumentVersioning = "YES">
60 |       <BuildableProductRunnable
61 |          runnableDebuggingMode = "0">
62 |          <BuildableReference
63 |             BuildableIdentifier = "primary"
64 |             BlueprintIdentifier = "0AC74EBA2D136221003C90A7"
65 |             BuildableName = "VLMEval.app"
66 |             BlueprintName = "VLMEval"
67 |             ReferencedContainer = "container:mlx-swift-examples.xcodeproj">
68 |          </BuildableReference>
69 |       </BuildableProductRunnable>
70 |    </ProfileAction>
71 |    <AnalyzeAction
72 |       buildConfiguration = "Debug">
73 |    </AnalyzeAction>
74 |    <ArchiveAction
75 |       buildConfiguration = "Release"
76 |       revealArchiveInOrganizer = "YES">
77 |    </ArchiveAction>
78 | </Scheme>
79 | 


--------------------------------------------------------------------------------
/mlx-swift-examples.xcodeproj/xcshareddata/xcschemes/llm-tool.xcscheme:
--------------------------------------------------------------------------------
  1 | <?xml version="1.0" encoding="UTF-8"?>
  2 | <Scheme
  3 |    LastUpgradeVersion = "1520"
  4 |    version = "1.7">
  5 |    <BuildAction
  6 |       parallelizeBuildables = "YES"
  7 |       buildImplicitDependencies = "YES">
  8 |       <BuildActionEntries>
  9 |          <BuildActionEntry
 10 |             buildForTesting = "YES"
 11 |             buildForRunning = "YES"
 12 |             buildForProfiling = "YES"
 13 |             buildForArchiving = "YES"
 14 |             buildForAnalyzing = "YES">
 15 |             <BuildableReference
 16 |                BuildableIdentifier = "primary"
 17 |                BlueprintIdentifier = "C397C58A2B62C6A9004B084D"
 18 |                BuildableName = "llm-tool"
 19 |                BlueprintName = "llm-tool"
 20 |                ReferencedContainer = "container:mlx-swift-examples.xcodeproj">
 21 |             </BuildableReference>
 22 |          </BuildActionEntry>
 23 |       </BuildActionEntries>
 24 |    </BuildAction>
 25 |    <TestAction
 26 |       buildConfiguration = "Debug"
 27 |       selectedDebuggerIdentifier = "Xcode.DebuggerFoundation.Debugger.LLDB"
 28 |       selectedLauncherIdentifier = "Xcode.DebuggerFoundation.Launcher.LLDB"
 29 |       shouldUseLaunchSchemeArgsEnv = "YES"
 30 |       shouldAutocreateTestPlan = "YES">
 31 |    </TestAction>
 32 |    <LaunchAction
 33 |       buildConfiguration = "Release"
 34 |       selectedDebuggerIdentifier = "Xcode.DebuggerFoundation.Debugger.LLDB"
 35 |       selectedLauncherIdentifier = "Xcode.DebuggerFoundation.Launcher.LLDB"
 36 |       launchStyle = "0"
 37 |       useCustomWorkingDirectory = "NO"
 38 |       ignoresPersistentStateOnLaunch = "NO"
 39 |       debugDocumentVersioning = "YES"
 40 |       debugServiceExtension = "internal"
 41 |       allowLocationSimulation = "YES"
 42 |       viewDebuggingEnabled = "No">
 43 |       <BuildableProductRunnable
 44 |          runnableDebuggingMode = "0">
 45 |          <BuildableReference
 46 |             BuildableIdentifier = "primary"
 47 |             BlueprintIdentifier = "C397C58A2B62C6A9004B084D"
 48 |             BuildableName = "llm-tool"
 49 |             BlueprintName = "llm-tool"
 50 |             ReferencedContainer = "container:mlx-swift-examples.xcodeproj">
 51 |          </BuildableReference>
 52 |       </BuildableProductRunnable>
 53 |       <CommandLineArguments>
 54 |          <CommandLineArgument
 55 |             argument = "--model mlx-community/CodeLlama-13b-Instruct-hf-4bit-MLX"
 56 |             isEnabled = "NO">
 57 |          </CommandLineArgument>
 58 |          <CommandLineArgument
 59 |             argument = "--model mlx-community/gemma-3-27b-it-qat-4bit --prompt &apos;Describe the image in English.&apos; --image https://www.gstatic.com/webp/gallery/1.webp"
 60 |             isEnabled = "NO">
 61 |          </CommandLineArgument>
 62 |          <CommandLineArgument
 63 |             argument = "--model microsoft/Phi-4-mini-instruct --prompt &quot;Why is the sky blue?&quot; --extra-eos-token &quot;&lt;|end|&gt;&quot;"
 64 |             isEnabled = "NO">
 65 |          </CommandLineArgument>
 66 |          <CommandLineArgument
 67 |             argument = "--model mlx-community/Qwen2-VL-2B-Instruct-4bit --prompt &apos;Describe the image in English.&apos; --image https://www.gstatic.com/webp/gallery/1.webp"
 68 |             isEnabled = "NO">
 69 |          </CommandLineArgument>
 70 |          <CommandLineArgument
 71 |             argument = "--model mlx-community/Qwen3-1.7B-4bit --prompt &quot;Explain quantum computing in simple terms&quot; --max-tokens 100 --kv-bits 4"
 72 |             isEnabled = "YES">
 73 |          </CommandLineArgument>
 74 |          <CommandLineArgument
 75 |             argument = "--model mlx-community/Qwen3-1.7B-4bit --prompt &quot;Explain quantum computing in simple terms&quot; --max-tokens 100"
 76 |             isEnabled = "NO">
 77 |          </CommandLineArgument>
 78 |          <CommandLineArgument
 79 |             argument = "--repetition-penalty 1.2"
 80 |             isEnabled = "NO">
 81 |          </CommandLineArgument>
 82 |          <CommandLineArgument
 83 |             argument = "--top-p 0.95"
 84 |             isEnabled = "NO">
 85 |          </CommandLineArgument>
 86 |          <CommandLineArgument
 87 |             argument = "--model mlx-community/c4ai-command-r-v01-4bit"
 88 |             isEnabled = "NO">
 89 |          </CommandLineArgument>
 90 |          <CommandLineArgument
 91 |             argument = "--model mlx-community/starcoder2-3b-4bit"
 92 |             isEnabled = "NO">
 93 |          </CommandLineArgument>
 94 |          <CommandLineArgument
 95 |             argument = "--model mlx-community/Qwen1.5-0.5B-Chat-4bit"
 96 |             isEnabled = "NO">
 97 |          </CommandLineArgument>
 98 |          <CommandLineArgument
 99 |             argument = "--prompt &apos;def quick_sort(arr, left=None, right=None):&apos;"
100 |             isEnabled = "NO">
101 |          </CommandLineArgument>
102 |          <CommandLineArgument
103 |             argument = "--prompt &apos;Why is the sky blue?&apos;"
104 |             isEnabled = "NO">
105 |          </CommandLineArgument>
106 |          <CommandLineArgument
107 |             argument = "--model mlx-community/Mistral-7B-v0.1-hf-4bit-mlx"
108 |             isEnabled = "NO">
109 |          </CommandLineArgument>
110 |          <CommandLineArgument
111 |             argument = "--model mlx-community/Llama-3.2-1B-Instruct-4bit"
112 |             isEnabled = "NO">
113 |          </CommandLineArgument>
114 |          <CommandLineArgument
115 |             argument = "--model mlx-community/phi-2-hf-4bit-mlx"
116 |             isEnabled = "NO">
117 |          </CommandLineArgument>
118 |       </CommandLineArguments>
119 |    </LaunchAction>
120 |    <ProfileAction
121 |       buildConfiguration = "Release"
122 |       shouldUseLaunchSchemeArgsEnv = "YES"
123 |       savedToolIdentifier = ""
124 |       useCustomWorkingDirectory = "NO"
125 |       debugDocumentVersioning = "YES">
126 |       <BuildableProductRunnable
127 |          runnableDebuggingMode = "0">
128 |          <BuildableReference
129 |             BuildableIdentifier = "primary"
130 |             BlueprintIdentifier = "C397C58A2B62C6A9004B084D"
131 |             BuildableName = "llm-tool"
132 |             BlueprintName = "llm-tool"
133 |             ReferencedContainer = "container:mlx-swift-examples.xcodeproj">
134 |          </BuildableReference>
135 |       </BuildableProductRunnable>
136 |    </ProfileAction>
137 |    <AnalyzeAction
138 |       buildConfiguration = "Debug">
139 |    </AnalyzeAction>
140 |    <ArchiveAction
141 |       buildConfiguration = "Release"
142 |       revealArchiveInOrganizer = "YES">
143 |    </ArchiveAction>
144 | </Scheme>
145 | 


--------------------------------------------------------------------------------
/mlx-swift-examples.xcodeproj/xcshareddata/xcschemes/mlx-libraries-Package.xcscheme:
--------------------------------------------------------------------------------
  1 | <?xml version="1.0" encoding="UTF-8"?>
  2 | <Scheme
  3 |    LastUpgradeVersion = "1600"
  4 |    version = "1.7">
  5 |    <BuildAction
  6 |       parallelizeBuildables = "YES"
  7 |       buildImplicitDependencies = "YES"
  8 |       buildArchitectures = "Automatic">
  9 |       <BuildActionEntries>
 10 |          <BuildActionEntry
 11 |             buildForTesting = "YES"
 12 |             buildForRunning = "YES"
 13 |             buildForProfiling = "YES"
 14 |             buildForArchiving = "YES"
 15 |             buildForAnalyzing = "YES">
 16 |             <BuildableReference
 17 |                BuildableIdentifier = "primary"
 18 |                BlueprintIdentifier = "MLXEmbedders"
 19 |                BuildableName = "MLXEmbedders"
 20 |                BlueprintName = "MLXEmbedders"
 21 |                ReferencedContainer = "container:">
 22 |             </BuildableReference>
 23 |          </BuildActionEntry>
 24 |          <BuildActionEntry
 25 |             buildForTesting = "YES"
 26 |             buildForRunning = "YES"
 27 |             buildForProfiling = "YES"
 28 |             buildForArchiving = "YES"
 29 |             buildForAnalyzing = "YES">
 30 |             <BuildableReference
 31 |                BuildableIdentifier = "primary"
 32 |                BlueprintIdentifier = "MLXLLM"
 33 |                BuildableName = "MLXLLM"
 34 |                BlueprintName = "MLXLLM"
 35 |                ReferencedContainer = "container:">
 36 |             </BuildableReference>
 37 |          </BuildActionEntry>
 38 |          <BuildActionEntry
 39 |             buildForTesting = "YES"
 40 |             buildForRunning = "YES"
 41 |             buildForProfiling = "YES"
 42 |             buildForArchiving = "YES"
 43 |             buildForAnalyzing = "YES">
 44 |             <BuildableReference
 45 |                BuildableIdentifier = "primary"
 46 |                BlueprintIdentifier = "MLXLMCommon"
 47 |                BuildableName = "MLXLMCommon"
 48 |                BlueprintName = "MLXLMCommon"
 49 |                ReferencedContainer = "container:">
 50 |             </BuildableReference>
 51 |          </BuildActionEntry>
 52 |          <BuildActionEntry
 53 |             buildForTesting = "YES"
 54 |             buildForRunning = "YES"
 55 |             buildForProfiling = "YES"
 56 |             buildForArchiving = "YES"
 57 |             buildForAnalyzing = "YES">
 58 |             <BuildableReference
 59 |                BuildableIdentifier = "primary"
 60 |                BlueprintIdentifier = "MLXMNIST"
 61 |                BuildableName = "MLXMNIST"
 62 |                BlueprintName = "MLXMNIST"
 63 |                ReferencedContainer = "container:">
 64 |             </BuildableReference>
 65 |          </BuildActionEntry>
 66 |          <BuildActionEntry
 67 |             buildForTesting = "YES"
 68 |             buildForRunning = "YES"
 69 |             buildForProfiling = "YES"
 70 |             buildForArchiving = "YES"
 71 |             buildForAnalyzing = "YES">
 72 |             <BuildableReference
 73 |                BuildableIdentifier = "primary"
 74 |                BlueprintIdentifier = "MLXVLM"
 75 |                BuildableName = "MLXVLM"
 76 |                BlueprintName = "MLXVLM"
 77 |                ReferencedContainer = "container:">
 78 |             </BuildableReference>
 79 |          </BuildActionEntry>
 80 |          <BuildActionEntry
 81 |             buildForTesting = "YES"
 82 |             buildForRunning = "YES"
 83 |             buildForProfiling = "YES"
 84 |             buildForArchiving = "YES"
 85 |             buildForAnalyzing = "YES">
 86 |             <BuildableReference
 87 |                BuildableIdentifier = "primary"
 88 |                BlueprintIdentifier = "StableDiffusion"
 89 |                BuildableName = "StableDiffusion"
 90 |                BlueprintName = "StableDiffusion"
 91 |                ReferencedContainer = "container:">
 92 |             </BuildableReference>
 93 |          </BuildActionEntry>
 94 |       </BuildActionEntries>
 95 |    </BuildAction>
 96 |    <TestAction
 97 |       buildConfiguration = "Debug"
 98 |       selectedDebuggerIdentifier = "Xcode.DebuggerFoundation.Debugger.LLDB"
 99 |       selectedLauncherIdentifier = "Xcode.DebuggerFoundation.Launcher.LLDB"
100 |       shouldUseLaunchSchemeArgsEnv = "YES">
101 |       <TestPlans>
102 |          <TestPlanReference
103 |             reference = "container:Tests/mlx-libraries-Package.xctestplan"
104 |             default = "YES">
105 |          </TestPlanReference>
106 |       </TestPlans>
107 |       <Testables>
108 |          <TestableReference
109 |             skipped = "NO">
110 |             <BuildableReference
111 |                BuildableIdentifier = "primary"
112 |                BlueprintIdentifier = "MLXVLMTests"
113 |                BuildableName = "MLXVLMTests"
114 |                BlueprintName = "MLXVLMTests"
115 |                ReferencedContainer = "container:">
116 |             </BuildableReference>
117 |          </TestableReference>
118 |       </Testables>
119 |    </TestAction>
120 |    <LaunchAction
121 |       buildConfiguration = "Debug"
122 |       selectedDebuggerIdentifier = "Xcode.DebuggerFoundation.Debugger.LLDB"
123 |       selectedLauncherIdentifier = "Xcode.DebuggerFoundation.Launcher.LLDB"
124 |       launchStyle = "0"
125 |       useCustomWorkingDirectory = "NO"
126 |       ignoresPersistentStateOnLaunch = "NO"
127 |       debugDocumentVersioning = "YES"
128 |       debugServiceExtension = "internal"
129 |       allowLocationSimulation = "YES">
130 |    </LaunchAction>
131 |    <ProfileAction
132 |       buildConfiguration = "Release"
133 |       shouldUseLaunchSchemeArgsEnv = "YES"
134 |       savedToolIdentifier = ""
135 |       useCustomWorkingDirectory = "NO"
136 |       debugDocumentVersioning = "YES">
137 |       <MacroExpansion>
138 |          <BuildableReference
139 |             BuildableIdentifier = "primary"
140 |             BlueprintIdentifier = "MLXEmbedders"
141 |             BuildableName = "MLXEmbedders"
142 |             BlueprintName = "MLXEmbedders"
143 |             ReferencedContainer = "container:">
144 |          </BuildableReference>
145 |       </MacroExpansion>
146 |    </ProfileAction>
147 |    <AnalyzeAction
148 |       buildConfiguration = "Debug">
149 |    </AnalyzeAction>
150 |    <ArchiveAction
151 |       buildConfiguration = "Release"
152 |       revealArchiveInOrganizer = "YES">
153 |    </ArchiveAction>
154 | </Scheme>
155 | 


--------------------------------------------------------------------------------
/support/generate-run-all-llms.sh:
--------------------------------------------------------------------------------
 1 | #!/bin/sh
 2 | 
 3 | echo "#!/bin/sh"
 4 | echo "# NOTE: GENERATED BY generate-run-all-llms.sh -- DO NOT MODIFY BY HAND"
 5 | 
 6 | ./mlx-run llm-tool list llms | \
 7 | 	awk '{printf "./mlx-run llm-tool eval --download ~/Downloads/huggingface --model %s\n", $0}' | \
 8 | 	awk '{printf "echo\necho ======\necho '\''%s'\''\n%s\n", $0, $0}'
 9 | 
10 | ./mlx-run llm-tool list vlms | \
11 | 	awk '{printf "./mlx-run llm-tool eval --download ~/Downloads/huggingface --model %s --resize 512 --image support/test.jpg\n", $0}' | \
12 | 	awk '{printf "echo\necho ======\necho '\''%s'\''\n%s\n", $0, $0}'
13 | 


--------------------------------------------------------------------------------
/support/run-all-llms.sh:
--------------------------------------------------------------------------------
  1 | #!/bin/sh
  2 | # NOTE: GENERATED BY generate-run-all-llms.sh -- DO NOT MODIFY BY HAND
  3 | echo
  4 | echo ======
  5 | echo './mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/quantized-gemma-2b-it'
  6 | ./mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/quantized-gemma-2b-it
  7 | echo
  8 | echo ======
  9 | echo './mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/granite-3.3-2b-instruct-4bit'
 10 | ./mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/granite-3.3-2b-instruct-4bit
 11 | echo
 12 | echo ======
 13 | echo './mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Mistral-7B-Instruct-v0.3-4bit'
 14 | ./mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Mistral-7B-Instruct-v0.3-4bit
 15 | echo
 16 | echo ======
 17 | echo './mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Meta-Llama-3-8B-Instruct-4bit'
 18 | ./mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Meta-Llama-3-8B-Instruct-4bit
 19 | echo
 20 | echo ======
 21 | echo './mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Qwen3-4B-4bit'
 22 | ./mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Qwen3-4B-4bit
 23 | echo
 24 | echo ======
 25 | echo './mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Qwen1.5-0.5B-Chat-4bit'
 26 | ./mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Qwen1.5-0.5B-Chat-4bit
 27 | echo
 28 | echo ======
 29 | echo './mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Qwen3-1.7B-4bit'
 30 | ./mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Qwen3-1.7B-4bit
 31 | echo
 32 | echo ======
 33 | echo './mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Mistral-Nemo-Instruct-2407-4bit'
 34 | ./mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Mistral-Nemo-Instruct-2407-4bit
 35 | echo
 36 | echo ======
 37 | echo './mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/gemma-2-9b-it-4bit'
 38 | ./mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/gemma-2-9b-it-4bit
 39 | echo
 40 | echo ======
 41 | echo './mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Qwen2.5-7B-Instruct-4bit'
 42 | ./mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Qwen2.5-7B-Instruct-4bit
 43 | echo
 44 | echo ======
 45 | echo './mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Qwen3-0.6B-4bit'
 46 | ./mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Qwen3-0.6B-4bit
 47 | echo
 48 | echo ======
 49 | echo './mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/CodeLlama-13b-Instruct-hf-4bit-MLX'
 50 | ./mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/CodeLlama-13b-Instruct-hf-4bit-MLX
 51 | echo
 52 | echo ======
 53 | echo './mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Qwen3-8B-4bit'
 54 | ./mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Qwen3-8B-4bit
 55 | echo
 56 | echo ======
 57 | echo './mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/DeepSeek-R1-Distill-Qwen-7B-4bit'
 58 | ./mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/DeepSeek-R1-Distill-Qwen-7B-4bit
 59 | echo
 60 | echo ======
 61 | echo './mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/phi-2-hf-4bit-mlx'
 62 | ./mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/phi-2-hf-4bit-mlx
 63 | echo
 64 | echo ======
 65 | echo './mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Qwen2.5-1.5B-Instruct-4bit'
 66 | ./mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Qwen2.5-1.5B-Instruct-4bit
 67 | echo
 68 | echo ======
 69 | echo './mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/GLM-4-9B-0414-4bit'
 70 | ./mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/GLM-4-9B-0414-4bit
 71 | echo
 72 | echo ======
 73 | echo './mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Meta-Llama-3.1-8B-Instruct-4bit'
 74 | ./mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Meta-Llama-3.1-8B-Instruct-4bit
 75 | echo
 76 | echo ======
 77 | echo './mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Phi-3.5-mini-instruct-4bit'
 78 | ./mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Phi-3.5-mini-instruct-4bit
 79 | echo
 80 | echo ======
 81 | echo './mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Llama-3.2-1B-Instruct-4bit'
 82 | ./mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Llama-3.2-1B-Instruct-4bit
 83 | echo
 84 | echo ======
 85 | echo './mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Llama-3.2-3B-Instruct-4bit'
 86 | ./mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Llama-3.2-3B-Instruct-4bit
 87 | echo
 88 | echo ======
 89 | echo './mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/OpenELM-270M-Instruct'
 90 | ./mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/OpenELM-270M-Instruct
 91 | echo
 92 | echo ======
 93 | echo './mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/MiMo-7B-SFT-4bit'
 94 | ./mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/MiMo-7B-SFT-4bit
 95 | echo
 96 | echo ======
 97 | echo './mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/SmolLM-135M-Instruct-4bit'
 98 | ./mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/SmolLM-135M-Instruct-4bit
 99 | echo
100 | echo ======
101 | echo './mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Qwen3-30B-A3B-4bit'
102 | ./mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Qwen3-30B-A3B-4bit
103 | echo
104 | echo ======
105 | echo './mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/gemma-2-2b-it-4bit'
106 | ./mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/gemma-2-2b-it-4bit
107 | echo
108 | echo ======
109 | echo './mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Phi-3.5-MoE-instruct-4bit'
110 | ./mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Phi-3.5-MoE-instruct-4bit
111 | echo
112 | echo ======
113 | echo './mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/SmolVLM-Instruct-4bit --resize 512 --image support/test.jpg'
114 | ./mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/SmolVLM-Instruct-4bit --resize 512 --image support/test.jpg
115 | echo
116 | echo ======
117 | echo './mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/paligemma-3b-mix-448-8bit --resize 512 --image support/test.jpg'
118 | ./mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/paligemma-3b-mix-448-8bit --resize 512 --image support/test.jpg
119 | echo
120 | echo ======
121 | echo './mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Qwen2.5-VL-3B-Instruct-4bit --resize 512 --image support/test.jpg'
122 | ./mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Qwen2.5-VL-3B-Instruct-4bit --resize 512 --image support/test.jpg
123 | echo
124 | echo ======
125 | echo './mlx-run llm-tool eval --download ~/Downloads/huggingface --model HuggingFaceTB/SmolVLM2-500M-Video-Instruct-mlx --resize 512 --image support/test.jpg'
126 | ./mlx-run llm-tool eval --download ~/Downloads/huggingface --model HuggingFaceTB/SmolVLM2-500M-Video-Instruct-mlx --resize 512 --image support/test.jpg
127 | echo
128 | echo ======
129 | echo './mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Qwen2-VL-2B-Instruct-4bit --resize 512 --image support/test.jpg'
130 | ./mlx-run llm-tool eval --download ~/Downloads/huggingface --model mlx-community/Qwen2-VL-2B-Instruct-4bit --resize 512 --image support/test.jpg
131 | 


--------------------------------------------------------------------------------
/support/test.jpg:
--------------------------------------------------------------------------------
https://raw.githubusercontent.com/ml-explore/mlx-swift-examples/2cceb87794bcce960dfc17e79a91946f41c02317/support/test.jpg


--------------------------------------------------------------------------------

